{"intro": ["There is no doubt that experimentation, or A/B testing, has become a driving force of innovation in the online world. It is not just the established players who have bought into the value of experimentation, as shared in several past KDD papers from Microsoft, Google and LinkedIn [1, 2, 3] . Startups and smaller websites have also invested in building out their experimentation program as a necessity of growth [4] . One primary reason that companies have relied on A/B testing is that it accelerates product innovation. It is like having a \"crystal ball\" that can tell us how our users will react and how the business metrics will change if a new feature is rolled out. We can learn quickly, and ultimately, build better products faster. However, how fast we innovate can also be limited by how we experiment. This becomes more apparent the more experiments we run. At companies like LinkedIn, where experimentation is truly embraced as a default step in every product change, this issue can be even more magnified. At LinkedIn, we have over 5,000 experiments a year. Every experiment goes through a \"ramp up\" process. A new feature usually starts out by ramping to a small percentage of users, waits to see if metrics are good, and then repeats by ramping up to a higher percentage, until finally it reaches 100%. Such \"ramp up\" process is a standard practice across the industry to control unknown risks associated with any new feature launches. However, many times the way we ramp slows us down. This can go both ways: we ramp too slowly and much time and resource is wasted; or we ramp too fast and suboptimal decisions are made. On average, an experiment at LinkedIn takes four ramps to reach 100%, where each ramp takes about six days -that's almost a month from start to finish! In addition, experimenters tend to treat each of the incremental ramps the same. For example, on average, we spend 6 days waiting on a 5% ramp, while 6.5 days on a 50% ramp. Therefore, having 4 ramps for one experiment is almost equivalent to running 4 separate experiments sequentially! The message behind these numbers is loud and clear: while we can democratize experimentation with a fully self-served platform [3] , we need principles to guide us on how we should ramp. Running slower doesn't mean we are safer. Taking longer to finish an experiment doesn't mean we are more cautious regarding negative user impact. We dedicate Section 3.1 to demonstrate the common mistakes and misconceptions people have when running experiments. It is important to point out that while there is a need for \"speed\", the principles should not be driven by \"ramping as quickly as possible\", but by \"ramping with the balance of Speed, Quality and Risk\". The core of the paper is to answer the following question: how can we iterate fast while controlling risk and improving decision quality?", "At first, the answer may seem to lie with \"power analysis.\" Power analysis is widely used to calculate the minimum sample size required to detect a given effect size [5] . In the world of online A/B testing, where samples trickle into experiment continuously, this usually translates to deciding on both percentage of traffic ramped to the treatment (ramp percentage) and also the duration for the experiment. However, power analysis fails to serve our needs. (1) It can only be performed for one metric at a time. With hundreds of metrics we monitor closely for each experiment [3] , summary from power analysis of individual metrics becomes uninterpretable. (2) What is considered as \"enough power\" can be different depending on the stage of the ramp process. In general, we can tolerate a lower power (i.e. higher Type II error) during earlier ramps. We will discuss more formally about this in Section 4. (3) Power analysis alone is not actionable. For most experimenters, power is a hard concept to digest, and is even harder to take action upon. If my experiment does not have enough power, what should I do?"], "relatedWork": [], "rq": [" the core of the paper is to answer the following question: how can we iterate fast while controlling risk and improving decision quality?", " (2) what is considered as \"enough power\" can be different depending on the stage of the ramp process. in general, we can tolerate a lower power (i.e. higher type ii error) during earlier ramps. we will discuss more formally about this in section 4. (3) power analysis alone is not actionable. for most experimenters, power is a hard concept to digest, and is even harder to take action upon. if my experiment does not have enough power, what should i do?"]}
{"intro": ["T HE importance of research on face recognition (FR) is fueled by both its scientific challenges and its potential applications. However, most of the systems designed to date can only successfully recognize faces when images are obtained under constrained conditions. Therefore, the general problem of FR remains to be solved. Some important contributions have been made recently that allow systems to compensate for some of the difficulties of an unconstrained environment, for example, to compensate for the illumination changes [3] , [4] , [12] , [50] or the (3D) depth rotations [5] , [43] , [66] . This paper proposes several approaches to deal with some of the difficulties that one encounters when trying to recognize frontal faces in unconstrained domains and when only one sample per class is available to the learning system. This we will do within the appearance-base paradigm (i.e., where only the texture of the face image is considered) using the principal components analysis (PCA or eigenspaces or eigenfaces) approach [58] , [28] , [63] , [43] , [3] . The problems we tackle in this contribution are:", ". Imprecise localization of faces. It is clear that every FR system requires a first localization stage. When an appearance-based method is applied we also require a warping stage to guarantee that all images can be interpreted as a vector [5] , [35] (this is discussed in Section 2.3). However, all localization algorithms have an associated error, namely, they cannot localize every single face feature with pixel precision. Here, localization error refers to those small localization errors that make the feature vector (e.g., eigen-representation) of a test image close to an incorrect class. This effect is depicted in Fig. 1a . In this figure, we display two classes, one drawn using crosses and the other with pentagons. For each class there are two learned feature vectors, each corresponding to the same image but accounting for different localization errors. The test image (which belongs to the \u00aacross\u00ba class) is shown as a square. Note that, while one of the \u00aapentagon\u00ba samples is far from the test feature vector, the other corresponds to the closest sample, that is, while one localization leads to a correct classification, the other does not. This point becomes critical when the learning and testing images differ on facial expression, illumination conditions, etc., as well as for duplicates. A \u00aaduplicate\u00ba is an image of a face that is taken at a different time, weeks, months, or even years later. Note that the localization problem does not mean a failure in localizing faces; rather, we are assuming that the face localization step succeeds, but that small errors of precision make the identification process fail. This problem has not attracted much attention to date. . Partially occluded faces: One of the main drawbacks of the appearance-based paradigm (e.g., PCA), is its failure to robustly recognize partially occluded objects. Fig. 2 shows an example, where Fig. 2a is used for learning and Fig. 2b for testing. . Expression variant faces: Yet another little-studied problem is the expression-invariant one. This problem can be formulated as follows: \u00aahow can we robustly identify a person's face for whom the learning and testing face images differ in facial expression?\u00ba We say that a system is expression-dependent if the image of Fig. 3b is more difficult (or less difficult) to recognize than the image shown in Fig. 3c , given that Fig. 3a was used for learning. An expression-invariant method would correspond to one that equally identifies the identity of the subject independent of the facial expression displayed on the test image. In an attempt to overcome this problem, the PCA approach uses the second order statistics of the image set in the hope that these features will be invariant to different facial expressions. Unfortunately, it can be theoretically shown that, for any given invariant paradigm (the PCA approach in this case) there is always a set of (testing) images for which the learned measure (or function) will not be optimal [13] , [64] . Every system that attempts to recognize faces in unconstrained (or close to unconstrained) domains has to deal with the above described difficulties."], "relatedWork": [], "rq": [" 2a is used for learning and fig. 2b for testing. . expression variant faces: yet another little-studied problem is the expression-invariant one. this problem can be formulated as follows: \u00aahow can we robustly identify a person's face for whom the learning and testing face images differ in facial expression?"]}
{"intro": ["However, these approaches fail to address an underlying problem: novices fail to give high quality feedback because they cannot differentiate good work from bad as well as instructors can, or even identify which aspects matter [41] . The peer submissions learners evaluate may be the only examples of an assignment they see apart from their own. By contrast, instructors evaluate student work with significantly more preparation: they have amassed substantial domain knowledge over years of study, and grade many more submissions in a course to give them a holistic sense of the range in quality. More generally, experts organize information more effectively than novices do [18, 29] , highlight deeper features [9] , and infer and articulate more nuanced abstractions [17] . Where novices are misled by salient but superficial features, experts notice deeper structural features [36] . Given these shortcomings, it is hardly surprising that peer feedback lacks the quality of expert critique. We suggest that to overcome these shortcomings, feedback systems should focus not on mechanical aids such as hints or feedback templates, but on helping peers notice features that experts do, infer and articulate expert-like abstractions, and consequently, give better feedback."], "relatedWork": ["Prior work has also investigated relative grading schemes for peer review. One approach is ordinal grading, which asks learners to rank a set of submissions from best to worst. However, prior work on ordinal grading centers around the theoretical viability of accurately ranking submissions (e.g. [23, 38, 45, 51] ); Juxtapeer instead emphasizes comparison as a scaffold to help reviewers provide better feedback. Perhaps most similarly, the ComPAIR system shows two peer submissions side-by-side. Motivated by many of the same theories as our own work, ComPAIR asks reviewers to provide feedback on each submission individually, then asks students to choose the \"better\" of the pair [37] . This prior work offers a system evaluation and thorough qualitative analysis and self-report data from users, but no prior work offers an experiment evaluating the efficacy of comparative review. In addition to offering the first such evaluation of comparative peer review, Juxtapeer differs from ComPAIR in anchoring the comparison on one submission at a time, and asking reviewers to evaluate the submission quantitatively as well as qualitatively."], "rq": ["study 2: can comparative review reliably rank submission quality?"]}
{"intro": [], "relatedWork": [], "rq": [" 32 what can be said about this class?"]}
{"intro": ["Most studies have aimed to investigate the effects of DDL on learners\" vocabulary and grammar (Coxhead & Byrd, 2007; Huang, 2014; Liu & Jiang, 2009; Ucar & Y\u00fckselir, 2015; Varley, 2009; Vyatkina, 2016; Yunus & Awab, 2014) . Liu and Jiang (2009) examined the effects of integrating corpus and contextualized lexico-grammar in foreign and second language teaching. The analysis of their data revealed that learners improved their command of lexico-grammar, increased critical understanding of grammar, and enhanced discovery learning skills. However, the study brought to light that corpus-based lexico-grammar analysis caused some difficulties for many students. examined the relationship between one type of datadriven learning (DDL) and inductive-deductive learning styles and found that the participants improved their grammar significantly after teacher-led guided DDL induction. Their findings pointed out that guided DDL type induction may be beneficial for both deductive and inductive learners irrespective of their learning styles. Ucar and Y\u00fckselir (2015) investigated the impacts of corpus-based activities on verb-noun collocation learning in EFL classes. Their study had an experimental design and consisted of 30 participants. The experimental group was taught verbnoun collocations through corpus-based materials, and the control group learnt collocations via conventional methods. They found a statistically significant difference between the experimental and the control group which showed that corpus-based activities had a significant impact on the teaching of verb-noun collocations in EFL classes."], "relatedWork": [], "rq": ["rq1. can ddl help efl learners improve their lexico-grammatical use of abstract nouns in their writing?", "rq2: what are the perceptions of efl learners of the effect of ddl on their vocabulary learning and vocabulary use in writing?"]}
{"intro": ["Electric vehicles (EVs) are getting more popular as a longterm vehicular technology to reduce the dependence on fossil fuel and the emission of greenhouse gases. However, with an increase in EV penetration, uncoordinated charging can lead to additional power losses and unacceptable voltage variation that overload the power grid. One way to tackle this problem is to adopt a \"smart grid\" solution, which allows EVs to communicate with the utility that coordinates their charging activities. Besides preventing grid overload, it has been shown that a coordinated EV charging can improve frequency regulation [1] , smooth out the generation intermittency from renewable sources, and increase the efficiency in electricity usage [2] , [3] . In this setting, we consider two types of load connected to the power grid network:", "Considering these two types of loads, the two key problems that we study are: What is the optimal charging schedule for EVs to minimize the total power generation cost and EV charging cost? How to find a near optimal online algorithm if the future price-inelastic load is uncertain (due to the realistic causality constraint)? To formulate these problems, we leverage the well-known optimal power flow (OPF) problem and consider its time-dependent extension. The solution of the OPF problem optimizes the operation of a power grid, and in general is NP-hard and nonconvex. However, the authors of [4] and [5] recently show that most practical power grid configurations surprisingly exhibit a useful property that guarantees zero duality gap between the OPF problem and its convex dual relaxation, thus making efficient polynomial time algorithm for the OPF problem possible."], "relatedWork": [], "rq": [" the two key problems that we study are: what is the optimal charging schedule for evs to minimize the total power generation cost and ev charging cost?"]}
{"intro": ["Efficiently executing resource-intensive parallel scientific applications in dynamic parallel and distributed production environments is a challenging problem. Performance depends on resource scheduling at several levels: job schedulers which multiplex different applications across limited system resources and application schedulers which request resources for their applications. Applications can experience large slowdowns in time-shared systems, or high wait times in space-shared systems, due to contention for these limited valuable resources. Part of the problem is that a limited set of shared resources can rapidly become oversubscribed as the number of applications to execute increases. This is confirmed by several supercomputer workload studies [10] [11] [20] . However, we believe that inefficient resource allocation by job schedulers and inefficient resource utilization within the application further exacerbates this problem."], "relatedWork": ["The concept of integrated scheduling is complementary to the large body of dynamic scheduling research for parallel applications. Application-level scheduling approaches such as [3] [32] [33] each propose policies and mechanisms to make resource allocation decisions that are deemed best for the application. They are focused solely on a single application instance. For example, AppLeS has defined the necessary components to perform application-centric scheduling in distributed environments [3] . In contrast, iSchedulers are designed to support the scheduling of multiple applications. However, iSchedulers could inter-operate with multiple application schedulers (e.g. an AppLeS scheduler or a Prophet scheduler)", "provided each application scheduler supports its interface. Job-level scheduling approaches make fixed resource allocation decisions for a group of jobs that are treated as \"black-boxes\" usually with the goal of high throughput, fairness, and high utilization [5] . Job schedulers require that upfront resource requirements be expressed to the system to enable resource allocation when the application is scheduled. However, they do not allow for adaptive scheduling to further optimize scheduling in response to dynamic changes in application resource demands, changes in system state, or subsequent job arrival. iSchedulers can leverage the large-body of job scheduling algorithms, e.g. backfilling, but further improve performance by allowing for more fluid resource management."], "rq": [" an important question is: why would an application-writer spend the effort to implement adaptivity, particularly the removal of resources?"]}
{"intro": ["A data warehouse (DW) is an information base that stores a large volume of extracted and summarized data for OLAP and decision support systems [4] . These systems are characterized by complex ad-hoc queries (with many joins) over large data sets. Despite the complexity of queries, decision makers want those queries to be evaluated faster. Fast execution of queries and retrieval of data can be achieved if the the physical design of a DW is done properly. There are two major problems associated with the physical design of a DW, namely, the selection of warehouse data (i.e., materialized views) so that all the queries can be answered at the warehouse without accessing the data from underlying sources, and indexing data so that data can be retrieved faster. However, the solutions of these two problems pose additional overheads of maintaining the warehouse data (materialized views) whenever the source data changes and storage cost for maintaining index tables. Given the large size of DWs, these costs are nontrivial. This prompts us to ask the following question: is it possible to reduce the storage and maintenance cost requirements, without sacrificing the query execution efficiency obtained from indexing?"], "relatedWork": [], "rq": [" this prompts us to ask the following question: is it possible to reduce the storage and maintenance cost requirements, without sacrificing the query execution efficiency obtained from indexing?"]}
{"intro": ["In this paper, we explore the employment of passive RFID to deploy an object localization support service for smart home and for other smart environments. While extensive research has been conducted within the field of robotic on this precise topic, we demonstrate that the literature is inadequate [11] to the context of object localization. Indeed, the majority of the works are exploiting active tags, which are battery powered but much more precise [12] than their passive counterparts. However, they are also bigger and clumsier and require timely maintenance due to their internal battery. On the other hand, the current passive RFID localization systems are mostly based on vast deployment of reference tags [13] .", "2 International Journal of Distributed Sensor Networks A tag of reference is a tag that is positioned at a specific known location and that provides a benchmark to localize another one. In some cases, these systems require up to 300 tags [14] . This makes their methods difficult to implement in the context of smart home like ours. Additionally, in the last few years, many researchers tried to address the core issue of localization by proposing systems based on a wide range of other technologies, such as GPS, ultrasonic wave sensors [15] , and video cameras [16] . However, the RFID technology, because of its robustness, its low price, and its flexibility, seems to have imposed itself as one of the best solutions currently available for smart homes. Moreover, each of these other technologies suffers from great weaknesses such as the line-of-sight constraint [13] and high intrusiveness."], "relatedWork": ["As we have seen in Section 2, there is a plethora of localization algorithms in the literature. Many possess their own advantages in their context of application over our method. Nevertheless, compared to other approaches, our model requires less equipment to be deployed and to be functional since we did not use reference tags [11, 13, 14, 31] . Indeed, without these tags, one does not need to perform calibration and precise installation in a new environment. Speed is also another very important criterion to evaluate performance of a localization system. A fast system will be more likely to perform well with the tracking of moving objects. On a standard personal computer, we can track an object in only few milliseconds. The system's speed is, however, limited by our RFID hardware (200 ms). In addition, some of the filters slow down the changes a little bit to around a second. Still, most of the others have not reported the speed of their system, and from those that did it, we are faster [13, 31] . Finally, our system is not only faster and more flexible than other approaches, but it is also more accurate. In fact, we obtained an average accuracy of \u00b114.12 cm over an area of 6 m 2 . Of course, there are existing active RFID [12] or ultrasonic systems [15] that more precisely localizes objects, but as argued in Section 2, these technologies present many disadvantages (need batteries, robustness, cost, etc.) that make them difficult to use at a larger scale in smart homes. Table 3 presents a summary of the comparison with the main systems."], "rq": [" 0) is not in the zone beside. moreover, what happens if we are wrong?"]}
{"intro": [], "relatedWork": ["Since non-technical losses are a major threat for the electrical grid, there are also approaches on the level of smart meters to detect suspicious data. Some intrusion detection systems (IDS) for smart meters apply similar methods to learn the normal behaviour (e.g., sent data) and to check any deviation from it (e.g., Berthier et al., [23] and Tabrizi et al. [24] ). The objective is to be able to detect any attempts to hack into the smart meter, tamper data stored in its memory or manipulate consumption data sent to the central system. However, they are technology dependent and only applicable to a very specific protocol or smart meter OS. Our approach is technology agnostic and is applicable to any protocol or metering system."], "rq": [" effectiveness: can we better detect suspicious values?"]}
{"intro": ["The main focus is on these two research questions: 1. What is the status of using semantic web within BIM? 2. Which research challenges need to be solved and how can they be alternatively be solved with SW? The languages developed for SW are based on metadata models in RDF and RDF Schema format (Brickley and Guha, 2014) and Logic-based Knowledge Representation in Web Ontology Language (W3C, 2014) . In RDFS and OWL, you can build additional ontology languages upon RDF. SW is not concerned of the data structure, but the meaning and understanding of the data. Some semantic technologies include Natural Language Processing (NLP) and Semantic Search. Anyway, both SW and NLP have a common goal to represent information that is understandable to a machine and not just a human being. This fundamental difference gives a completely different perspective on how storage, query, and viewing information can approach. Applications that refer to a large amount of data from many different sources benefit greatly from this feature. However, this does not take advantage of storing large amounts of highly structured transaction data. Therefore, it is important to know when it is wise or not advisable to use SW technologies. SW is based on two basic ideas:"], "relatedWork": [], "rq": [" 1. what is the status of using semantic web within bim?"]}
{"intro": ["In this paper we propose a new algorithm for the computation of the minimal characteristics of a DSS defined as a subsegment of a DSL with known characteristics. Our approach is entirely based on the remainders of the DSL points. For a DSL defined by 0 \u2264 ax \u2212 by \u2212 c < b (with 0 \u2264 a \u2264 b), the remainder is simply the value R a,b,c (x) = ax \u2212 by \u2212 c = ax\u2212c b where n m stands for n mod m (y is a function of x; there is one and only one DSL point per abscissa). We show that there is, under some conditions, an order relationship between the remainders of a point relatively to the DSL and to the DSS minimal characteristics. We show especially that the points with minimum and maximum DSL remainders are leaning points of the DSS. The third leaning point is obtained in a similar way on a sub-interval. The second important result of the paper is that the minimum and maximum of a remainder sequence can be computed in logarithmic time with a very simple algorithm that is akin to the Euclidean algorithm. Determining the three leaning points of a DSS that allow us to determine its characteristics is resumed by searching three times for a minima or maxima in remainder sequences. The resulting algorithm is very simple and efficient, being significantly faster as previous methods [11, 16] . An interesting aspect of this approach is that it offers a new way, with remainders, to explore higher dimensions. It is not however, as can be seen in the conclusion, straightforward."], "relatedWork": [], "rq": [" what about the erosion process in dimension 3? is there a more appropriate alternative to removing a complete row or column?"]}
{"intro": ["The latter approach is close to the one developed in this paper. The general hypothesis is that BCP indicates that something has gone essentially amiss with TWSI. TWSI is based on a semantic principle that is too weak, namely that truthvalues supervene on semantic information (see the quotation above). A semantically stronger approach, according to which information encapsulates truth, can avoid the paradox and is more in line with the ordinary conception of what generally counts as information. The latter thesis has been recently endorsed by several philosophers (Dretske, 1981; Grice, 1989; Barwise and Seligman, 1997; Graham 1999; see Floridi (forthcoming, a) for a defence), but its actual viability is, of course, another matter. In this direction, however, MTC provides some initial reassurance. MTC identifies the quantity of information associated with, or generated by, the occurrence of a signal (an event or the realisation of a state of affairs) with the elimination of possibilities (reduction in uncertainty) represented by that signal (event or state of affairs). 7 In MTC, no counterintuitive inequality comparable to BCP occurs, and the line of argument in this paper will be that, as in the case of MTC, a theory of strongly semantic information (TSSI), based on alethic and discrepancy values rather than probabilities, can also successfully avoid BCP."], "relatedWork": [], "rq": [" w 49 is the complement of w 16 ), and the third column the complements of the second. 23 why?"]}
{"intro": ["Hardware efficiency can have very different meanings depending on the utilization scenario targeted by the designer. For example, a classical metric is to estimate the minimum silicon area required by the primitive to perform the cryptographic operations. This, of course, depends on the parameters of the function itself (the area is highly dependent on the amount of memory required) and most lightweight block ciphers have a rather small block size of 64 bits. It is to be noted that the area is usually not directly linked to the security of a primitive, as adding extra rounds will have an impact on the throughput of the implementation, but only a very limited one concerning the area (we assumed that the function has no weakness that is independent of the number of rounds). Area and other metrics such as throughput, latency or power dissipation can be traded-off for one another, making the comparison between different primitives difficult. In the direction of fairer comparisons of hardware implementations of cryptographic primitives, Bogdanov et al. [11] introduced the efficiency metric throughput/area in order to take in account these tradeoffs. However, the possibility of trading off throughput for power was not taken in account and Badel et al. [4] proposed instead a figure of merit, defined as FOM = throughput/area 2 .", "The construction of good diffusion matrices has always been an important research topic in cryptography, equally important as the search for good confusion functions. The AES [15] for example uses a 4 \u00d7 4 matrix with elements in GF (2 8 ). This matrix is Maximum Distance Separable (MDS), which means that it has a branching number of 5, optimal for a 4 \u00d7 4 matrix. However, this security feature comes at a cost that computations in GF (2 8 ) might not be the best choice for some hardware purposes, even though special care has been taken by the designers to choose a circulant matrix instantiated with lightweight coefficient (i.e. low Hamming weight coefficients, such as 0x01, 0x02 and 0x03). Recently, Guo et al. [16, 17] described a new type of diffusion matrix, so-called serial, that trades more clock cycles in the execution for a smaller area. This idea was later extended to the use of linear Feistel-like structures or Linear Feedback Shift Registers (LFSR) to build the diffusion matrix [21, 23] . On the opposite side, PRESENT [10] uses a simple bit permutation layer, the real diffusion coming in fact directly from the S-box application. The advantage being of course that a bit permutation layer is basically free in a hardware implementation. Now, one may ask the following question: what is better when the goal is to maximize some hardware metric, a very weak diffusion matrix with a low area footprint, or a strong diffusion matrix but requiring more silicon to be performed?"], "relatedWork": [], "rq": [" one may ask the following question: what is better when the goal is to maximize some hardware metric, a very weak diffusion matrix with a low area footprint, or a strong diffusion matrix but requiring more silicon to be performed?"]}
{"intro": [], "relatedWork": [], "rq": [" focuses on generalizations related to the mapping problem: how are syntactic arguments mapped to semantic arguments?"]}
{"intro": ["Recent years have witnessed the explosive growth of online social networks that connect people in the digital world. One of the most important functionalities of online social networks is to effectively spread information such as latest news headlines, movie recommendations, etc, across the networks. This process is called information diffusion. Given the significant role online social networks have played in elections and crisis [1] , it has become increasingly urgent to gain a deep understanding of information diffusion process through online social networks. However, understanding the process of information diffusion over online social networks is a daunting task due to the intricacy of human dynamics and social interactions, the vast scale of users and information, and complexity of these networks.", "Most prior work in information diffusion studied the characteristics of information diffusion over various online social networks using empirical approaches [2] , [3] , [4] , [5] . A few recent effort use mathematical models to predict information diffusion over a time period in online social networks [6] , [7] . However, little attempt has been given on understanding and modeling information diffusion in both temporal and spatial dimensions. This paper explores one key question: How does a piece of information travel over time and space in an online social network? Specifically we are interested in answering the spatio-temporal diffusion problem: for a given information m initiated from a particular user called source s, after a time period t, what is the density of influenced users at distance x from the source? An influenced user is an user who has actively voted or liked the information. We use friendship hops as distance and abstractly translate the information diffusion process in online social networks into two separate processes: growth process and social process. Growth process represents information spreading among users with the same distance from the source and social process is the process through which information randomly spreads among users at different distances from the source."], "relatedWork": [], "rq": [" this paper explores one key question: how does a piece of information travel over time and space in an online social network?"]}
{"intro": ["The syntax and semantics of comparatives have been a source of much debate in linguistic theory. Central to this debate is the question of whether comparatives should be analyzed as quantifiers over degrees (see, e.g., von Stechow (1984) vs. Kennedy (1997) ). In support of a quantificational analysis, Heim (2000) presents certain ambiguities which can arise when sentences containing a comparative phrase are embedded under intensional operators. Heim analyzes these as scope ambiguities resulting from the ability of the comparative phrase to be interpreted either above or below the intensional operator. However, Oda (2008) and Beck (2009) , henceforth O&B, present an account of some of Heim's data which is consistent with a nonquantificational analysis of the comparative phrase. In this paper we spell out a prediction that could distinguish Heim's interpretation of the facts from O&B's. We then present empirical evidence that favors Heim's interpretation, adding further support to a quantificational analysis for the comparative."], "relatedWork": [], "rq": [" a: what do i need to do in order to pay no taxes at all?"]}
{"intro": ["Personal informatics technologies aim to support the collection of personally relevant data for the purpose of self-reflection and gaining self-knowledge [41] , and include systems such as wearable fitness tracking devices, food journaling tools, and smart journals [20] . These selftracking tools offer many potential benefits, such as providing automated data capture and visualisations of behavioural and physiological data to become \"fitter, happier, and more productive\" [19] . However, the design of automated self-tracking technologies lacks flexibility [36] and often fails to support people's practical goals and emotional needs. For example, prior works suggest that people abandon consumer health technologies over time because of a lack of personally meaningful insights [22, 38] , and switch to paper notebooks to avoid unintended effects and to overcome technological boundaries [3, 33] . Addressing these issues, recent research has proposed leveraging flexible self-tracking to account for people's individual and changing self-tracking practices [10, 30, 36] ."], "relatedWork": [], "rq": [" (1) how do bullet journalists design their paper notebooks; (2) how do bullet journalists use their notebooks to engage with others online; and (3) what design implications can be derived for future self-tracking technologies?"]}
{"intro": [], "relatedWork": [], "rq": ["rq1. what is the impact of the validation verdict?"]}
{"intro": ["The class of Horn formulae is a very important and extensively studied subclass of general Boolean formulae. The principal reason for their importance is the fact, that the satisfiability problem (SAT), which is well-known to be NP-complete for general Boolean formulae, can be solved efficiently (in linear time with respect to the length of the formula) for Horn formulae [14, 20, 23] . This has significant practical implications. Many real-life problems require for their solution to solve SAT as a subproblem, and hence are in general intractable; however, they become tractable if the underlying Boolean formula in the problem is Horn. Such problems arise in several application areas, among others in artificial intelligence [12, 18, 19] and database design [13, 22] . The limiting factor in using Horn formulae is their expressing power. Not every real-life problem can be formulated in such a way, that the underlying formula is Horn."], "relatedWork": [], "rq": [" question: is satisfiable?"]}
{"intro": ["More recently, however, new classes of network applications have emerged for which the grouping of nodes is more \"ad hoc\" in the sense that it is not dictated by organizational boundaries or strategic goals. Examples include overlay protocols [3] , [6] and peer-to-peer (P2P) applications. Two distinctive features of such applications are that 1) individual nodes are autonomous, and as such, their membership in a group is motivated solely by the selfish goal of benefiting from that group, and 2) group membership is warranted only as long as a node is interested in being part of the application or protocol, and as such, group membership is expected to be fluid. In light of these characteristics, an important question is this: Are protocols and applications that rely on sharing of distributed resources appropriate for this new breed of ad hoc node associations?"], "relatedWork": [], "rq": [" an important question is this: are protocols and applications that rely on sharing of distributed resources appropriate for this new breed of ad hoc node associations?"]}
{"intro": [], "relatedWork": ["One of the reasons that CNNs are very powerful in computer vision task is that they can automatically extract the deep feature of the image instead of the man-craft features. Although the CNN have superior classification performance, it could only provide the \"image-label\" which means one image can only be classified into one class. In order to delineate the boundary for mapping purpose from remote sensing images, semantic segmentation should be performed. Many types of research (Farabet et al. 2013 , Pinheiro et al. 2014 ) employed patch based CNNs to derive semantic segmentation by classifying the image patch centered in that pixel. However, this approach is computationally intensive. In order to keep the advantages of CNN but saving more computation budget, FCNs (Long, 2015) is proposed with \"pixel-label\" classification by replacing the last fully connected layer with convolutional layers. By keeping convolutional layers from CNN, deep feature extraction still exists. Another advantage of the FCN architecture (Figure 2 .) is that it has the ability to accept any size of the image and output a classification map with the same size. Fully convolutional networks (FCN) has been successfully applied in remote sensing images. It is proved with good accuracies and efficiency computations (Kampffmeyer et al., 2016 , Bittner et al., 2017 , Fu et al., 2017 . (2016) create a large-scale benchmark dataset, TorontoCity, by using a high precision map to create ground truth for labelling airborne and mobile images in order to test different neural networks for various vision tasks. However, the problems of their research is that maps and aerial photos assume perfectly matched and it is not true in most of cases. For example in Section 1.1, it illustrates many mislabeling. Instead of creating benchmarks for different applications by using maps, we directly apply semantic segmentation on VHR images by using maps to provide free training samples. As a result, the classification results will be used for change detection on the map to update the map."], "rq": ["(1) how to define an architecture to derive features from remote sensing images?", " (2) how to fine-tune hyperparameters from pre-trained weight to fit for our dataset?"]}
{"intro": [], "relatedWork": [], "rq": ["1. how to measure the performance according to the various metrics?"]}
{"intro": [], "relatedWork": ["Research into reproductive rights, and abortion more specifically, is well-established across multiple domains; from feminism and sociology, to health, law, and public policy. However, within HCI, there exists a considerable dearth of research that addresses this issue. Indeed, historically, a focus on any aspect of women's health, bodies, and rights remained largely underexplored within the discipline [3] . However, the emergence of Feminist HCI, that calls into question the grand, masculine-bias of the technological field and recognized the erasure of female perspectives in design [11, 12] , has heralded an increasing body of HCI design that addresses broader feminist domains. For example, work in women's health [38, 51] , maternal health [8, 49, 73] , and motherhood [9, 28] has gained increasing traction amongst designers. Equally, whilst design for reproductive rights more specifically is yet to have gained much attention within the field, considerably more HCI work has explored the intersection of activism and technology."], "rq": [" doing so presents a further paradox for designers: how can we foster polyvocality whilst ensuring anonymity and safety?"]}
{"intro": [], "relatedWork": [], "rq": [" a key question that the authors continue to investigate is: how does attribute partitioning-based decomposition support relevance determination (kohavi & john, 1997) in a modular learning architecture?"]}
{"intro": ["Power consumption is becoming the limiting factor for continued semiconductor technology scaling. Future chips will use heterogeneous solution to cope with \"dark\" silicon by using custom IP cores, thereby keeping the chip within the power budget. Application-specific on-chip hardware accelerators can provide orders of magnitude improvements in both power and performance [1] . However, full custom design is expensive in many aspects. The question that we try to answer in this project is: can we design specialized compute fabrics that maintain the efficiency of full custom hardware while providing enough flexibility to execute a whole class of coarse-grain operations?"], "relatedWork": [], "rq": [" the question that we try to answer in this project is: can we design specialized compute fabrics that maintain the efficiency of full custom hardware while providing enough flexibility to execute a whole class of coarse-grain operations?"]}
{"intro": [], "relatedWork": ["However, in apprentice-expert dialogs, the overall task that the apprentice is attempting to perform is known at the outset of the dialog, and the ordering of actions and subtasks in the plan being executed strongly influences the dialog between expert and apprentice. This differs from the kind of information-seeking dialogs that we are investigating, in which the information-seeker is attempting to construct a plan for a task that will be executed at a later time. In such dialogs, the information-seeker's utterances are not tightly constrained by the order in which actions in the task will eventually be executed. For example, in an information-seeking dialog between a client and a travel agent, the client may first plan hotel accommodations and theater attendance in New York before inquiring about ways to reach New York, even though travel to New York will occur before attend a New York theater in a temporal ordering of actions in the resultant plan. Allen ) inferred the goal underlying a speaker's utterance in the context of an information agent in a train setting. This inferred goal was used to account for extra helpful information included in the agent's response, and the inference path connecting the speaker's utterance and the inferred goal was used to interpret indirect speech acts. However, Allen's domain was very restricted; the only domain goals were meeting a train and boarding a train, each of which could be accomplished by a few primitive steps, and his system was primarily concerned with utterances that might occur at the outset of a dialog. In more complex domains, the information-seeker's complete plan will consist of a hierarchy of subplans and subgoals that accomplish his overall goal. Such a complete plan is not immediately evident from a single utterance, and individual utterances must be related to one another to build the user's plan as the dialog progresses. Sidner (1983 Sidner ( , 1985 and Litman (1986) developed enhanced models of plan inference. However, both were concerned with dialogs that were initiated in order to begin or continue execution of an underlying task (display of structures on a graphics terminal and meeting/boarding a train), and the dialogs were therefore constrained by the order in which individual actions in the task had to be executed. In addition, Sidner investigated how discourse markers aid in recognizing the speaker's intent, and Litman studied a meta-plan framework for task dialogs. Pollack (1986) has recently proposed that plans be viewed as mental phenomena. She contends that, in order to comprehend an utterance and relate it to the user's plan, the system must reason about the configuration of beliefs and intentions that it should ascribe to the speaker. This will be discussed further in Section 5.", "Several research efforts have addressed problems related to plan disparity. Kaplan (1982) and McCoy (1986) investigated misconceptions about domain knowledge and proposed responses intended to remove the misconceptions. However, such misconceptions may not be exhibited when they first influence the informationseeker's plan construction; in such cases, disparate plans may result, and correction will entail both a response correcting the misconception and further processing to bring the system's context model and the plan under construction by the information-seeker back into alignment.", "Allen's plan inference system ) could accommodate some user misconceptions. It did not expressly eliminate invalid plans but instead weighted them less favorably than valid ones. However, his model did not consider how potential user misconceptions might affect the partial plan inferred by the system. Pollack (1986) studied removal of the appropriate query assumption of previous planning systems. She proposed a richer model of planning that regarded plans as mental phenomena and explicitly reasoned about the information-seeker's possible beliefs and intentions. She addressed the problem of queries that indicated the information-seeker's plan was inappropriate to his overall goal, and attempted to isolate the erroneous beliefs that led to the inappropriate query. However, queries deemed inappropriate by the system may signal phenomena other than that the query is inappropriate to what the user really wants to do. For example, the information-seeker may have shifted focus to another aspect of the overall task without successfully conveying this to the system, the system's context model may have been in error prior to the query, or, as noted by Pollack (1987) , the information-seeker may be addressing aspects of the task outside the system's limited knowledge."], "rq": [" there is another implication of relaxing the appropriate query assumption that is not considered by pollack: is may ask an irrelevant question that seems perfectly reasonable to the system, thereby leading the system to develop incorrect beliefs about is\\'s objectives. consider, for example, a student advisement system. if only b.a. degrees have a foreign language requirement, the query \"what courses must i take to satisfy the foreign language requirement in french?"]}
{"intro": [], "relatedWork": [], "rq": ["an important question emerges: to what extent do college and university faculty members engage in the work of each of the four domains of scholarship?"]}
{"intro": ["Wireless sensor networks (WSNs) have been widely used for collecting data from environments [1] [2] [3] [4] . However, sensor nodes are resource constrained and distributed all over the monitored area. Programming WSNs to acquire such data is notoriously difficult and tedious. Traditional WSN programming requires system programming in low-level details (e.g., wiring nesC [5] components, coordinating the program flow among nodes in a distributed manner, routing, discovering resources, accessing, and managing remote data) while maintaining low energy consumption and memory usage [6] .", "Widely considered such integration, logic programming is the use of logic as both a declarative and imperative representation language [17] . A logic program consists of declarative sentences in the form of implications. Based on a backwards reasoning theorem prover, logic programming treats the implications as goal-reduction procedures. Logic programmers can exploit the problem-solving behavior of the theorem prover to achieve efficiency. This is similar to how imperative programmers use programs to control the behavior of a program executor. However, unlike pure imperative programs, the correctness of logic programs can be ensured with their declarative and logical interpretation."], "relatedWork": ["Of a particular interest are Kairos [7] , Regiment [10] , and DRN [8] . Kairos presents the programming model that computes a set of sensor devices in parallel and provides a facility to sequentially access remote variables. Unlike Kairos, Regiment is the spatiotemporal macroprogramming system that is based on the concept of functional reactive programming. However, Regiment is designed for longrunning queries (not well-suited for short-lived queries).", "Semantic Stream [18] is a macroprogramming framework with logic programming features that allows users to pose declarative queries over semantic interpretations of sensor data. However, Semantic Stream focuses on finding available services and providing the quality of services instead of problem solving. Furthermore, it is not designed specifically for wireless sensor nodes with limited resources.", "Chu et al. [6] have further developed the concept of logic programming into Snlog for programming WSNs and enabling recursive queries. Snlog, however, is designed for low-level programmers, not for application-level programmers. Unlike Sense2P programmers, Snlog programmers must write rules by focusing on local behaviors of each sensor node (instead of the global behavior as a whole). Therefore, Snlog does not support a join between different nodes. Additionally, the Snlog programmers must deal with networking details and protocols, such as routing, query disseminating, and data collecting. In summary, Sense2P is a macroprogramming approach but Snlog is not.", "TinyDB [9] and Cougar [15] are probably the most cited node-independent abstractions for macroprogramming WSNs. Those approaches abstract a WSN as a relational database. Consequently, WSN programming is reduced to database querying. However, there are several limitations in the mentioned approaches."], "rq": ["0) is not a superset of ?"]}
{"intro": ["(2) a. Questions like those in (4) below, with why and how extracted from the complement of a factive predicate, were of central interest in earlier studies on the factive island effect (e.g., Cinque 1990 , Rooryck 1992 . In fact, it is this type of question that the term factive island was initially applied to. Oshima (2007) suggested that such adjunct questions too instantiate the feature of uniqueness, though less obviously so. If this is correct, their unacceptability can be taken to have the very same source as the unacceptability of factive islands of the sort exemplified in (1). In this paper, however, we will focus on the latter type of factive island, and we will remain agnostic as to whether a uniform account is possible that also captures cases with why and how (see Abrus\u00e1n 2011; Schwarz & Simonenko 2018a ,b for relevant discussion)."], "relatedWork": [], "rq": ["4) is a subset of the active core of (36a). in type r contexts, therefore, (35a) has no less potential for information gain than (33), and it should therefore be an option for a rational speaker to use (35a) instead of (33). symmetrically, the active core of (34) in a type a context is the singleton {a}, a subset of (36b), which is the extension of (35b) and its own active core in a type a context. so in type a contexts, (35b) has no less potential for information gain than (33), and hence it should be an option for the speaker to use (35b) instead of (33). 2 but why would it be obligatory, as opposed to merely permitted, for the speaker to use one of the questions in (35) instead of (33)?"]}
{"intro": [], "relatedWork": [], "rq": ["4) is similar to the corresponding long distance extraction out of an attitude or speech report (dayal 1994 (2) 'what did she say?"]}
{"intro": ["To date, much research effort has been devoted to RWR, including its efficient computation ( [6] , [16] , [5] , [7] , [18] , [8] , [14] ), top-k search ( [7] , [10] , [3] , [17] ), and various mining tasks underpinned by RWR ( [13] , [2] , [12] ). However, insufficient attention has been paid to a fundamental task that arises in many graph-related applications, which is to determine the source nodes that have a large amount of information flowing to a given query node. To illustrate, consider a traffic flow network shown in Figure 1 . Assume severe traffic congestion occurs at node q every day, then the following question is key to improving traffic scheduling and road network design: how do we find the nodes from which the traffic tends to flow into q and cause the congestion problem? Using the RWR measure, the node c is likely to be identified as a major source that causes congestion at q. Even though c is not the direct in-neighbor of q, there are many short paths from c to q. Given that c is a busy transportation hub, a large number of vehicles leaving from c tend to gather at q. Each node is a road intersection, and the node size denotes the daily traffic volume at the intersection. Each edge is a road segment, and the attached number denotes the proportion of the traffic moving along that edge from a specific node.", "To the best of our knowledge, no existing methods can accurately and efficiently answer Ink queries. First, methods ([6] , [16] , [14] , [18] ) have been proposed to compute the approximate RWR between any two nodes with an error bound . However, it is hard to pre-specify a proper for an ad-hoc query node q, because a pre-specified may be either too coarse to generate the correct top-k results, or too fine to avoid unnecessary computation. Second, the k-dash method [7] can compute the exact RWR between any two nodes. However, it uses matrix LU decomposition as a pre-computation step, which has a time complexity of O(n 3 ) and thus is prohibitively expensive for large graphs. Even assuming the LU decomposition is done, later we will see, it is costly and unnecessary to compute the RWR scores from all nodes to the query node q in order to answer Ink queries. Third, techniques ( [16] , [10] , [5] , [9] , [8] ) have recently been reported to process what we call outbound top-k queries, i.e., which k nodes have the largest RWR if we start the random walk from node q? These techniques mostly use the branch-and-bound strategy to prune the search space, but the lower and upper bounds derived for the outbound top-k query cannot be easily adapted for the Ink query."], "relatedWork": ["The efficient computation of RWR has received a substantial amount of attention over the past decade. Though obtaining the closed-form solution of RWR requires the inversion of a matrix (Equation 1) and time-consuming, two popular strategies are widely adopted to address this problem: Monte Carlo sampling [3] , [4] and power iteration [15] . Other techniques for efficiently approximating RWR have also been proposed. Tong et al. [16] introduced an efficient and novel algorithm for computing approximate RWR scores. Their method relies on a pre-processing step, which obtains the low-rank approximation of a large and sparse matrix. Zhu et al. [18] proposed to compute the approximate PPR vector using the inverse P-distance [18] . The key idea is to partition all random walk tours into different layers according to their contributions, and given priority to those important layers when computing the PPR vector. Methods [1] have also been proposed to compute the approximate RWR scores from all the nodes to a given query node. Unfortunately, these approximate algorithms cannot be easily applied to answer the Ink queries as it is hard to pre-specify the desired error bound for an ad-hoc query. Moreover, as suggested by RIPPLE, computing the RWR scores from all the nodes is actually unnecessary. Along another line, much attention has been paid to the outbound top-k search problem. The goal is to retrieve the k nodes with the highest RWR/PPR scores from a query node. Most of the existing techniques for answering outbound top-k search resort to the branch-and-bound strategy to prune the search space. Specifically, Gupta et al. [10] proposed the Basic Push Algorithm, which computes PPR bounds based on bookmark coloring. Bahmani et al. [5] proposed a Monte Carlo based method for finding approximate top-k neighbors. Their results demonstrate that, by precomputing and storing a number of short random walk tours for all the nodes in the graph, the top-k neighbors can be fast approximated with satisfactory accuracy. Fujiwara et al. [7] proposed the k-dash algorithm to identify the top-k nearest neighbors of a query node based on matrix LU decomposition. They later proposed an method [8] that does not rely on offline pre-computation, but estimates the lower and upper bounds in an on-line manner. However, the lower and upper bounds derived for the outbound top-k query cannot be easily adapted for our Ink query."], "rq": [" then the following question is key to improving traffic scheduling and road network design: how do we find the nodes from which the traffic tends to flow into q and cause the congestion problem?"]}
{"intro": [], "relatedWork": [], "rq": ["first you have to think: what are they doing down there?"]}
{"intro": ["Because shoutcasters explain in parallel to gathering their information, we guided part of our investigation using Information Foraging Theory (IFT) [29] , which explains how people go about their information seeking activities. It is based on naturalistic predator-prey models, in which the predator (shoutcaster) searches patches (parts of the information environment) to find prey (evidence of players' decision process) by following the cues (signposts in the environment that seem to point toward prey) based on their scent (predator's guess at how related to the prey a cue is). IFT constructs have been used to explain and predict people's information-seeking behavior in several domains, such as understanding navigations through web sites or programming and software engineering environments [5, 8, 9, 18, 23, 26, 27, 28, 33] . However, to our knowledge, it has not been used before to investigate explaining RTS environments like StarCraft."], "relatedWork": ["Constructing effective explanations of AI is not straightforward, especially when the underlying AI system is complex. Both Kulesza et al. [16] and Guestrin et al. [30] point to a potential trade-off between faithfulness and interpretability in explanation. The latter group developed an algorithm that can explain (in a \"black box\" or \"model-agnostic\" fashion) predictions of any classifier in a faithful way, and also approximate it locally with an interpretable model. They described a fidelity-interpretability trade-off, in which making an explanation more faithful was likely to reduce its interpretability, and vice versa. However, humans manage this trade-off by accounting for many factors, such as the audience's current situation, their background, amount of time available, etc. One goal of the current study is to understand how expert human explainers, like our shoutcasters, manage this trade-off."], "rq": [" results: what information do shoutcasters seek to generate explanations, and where do they find it?", " the how: how do shoutcasters seek the information they seek?", " the how: how do shoutcasters seek the information they seek?", " the how: how do shoutcasters seek the information they seek?"]}
{"intro": ["For yet more difficult problems, the current trend is therefore to design hybrid metaheuristics, by combining different metaheuristics in order to benefit from the individual advantages of each method. An effective approach consists in combining a population-based method with a single-solution method (often a local search procedure). These hybrid methods are called memetic algorithms [12] . However, hybrid metaheuristics are complex procedures, tricky to design, implement and tune.", "An orthogonal approach to address very difficult problems consists in using parallel computation. For instance, several instances of a given metaheuristic can be executed in parallel in order to develop concurrent explorations of the search space, either independently or cooperatively by means of communication between concurrent processes. The independent approach is easiest to implement on parallel computers, since no communication is needed between the processes running a metaheuristics. The whole execution simply stops as soon as any process finds a solution. For some problems this approach provides very good results but in many cases the speedup tends to taper off when increasing the number of processors. A cooperative approach entails adding a communication mechanism in order to share or exchange information among solver instances during the search process. However, designing an efficient cooperative method is a dauntingly complex task [13] , and many issues must be solved: What information is exchanged? Between which processes is it exchanged? When is the information exchanged? How is it exchanged? How is the imported data used? [14] . Moreover, most cooperative choices are problem-dependent (and sometimes even instance-dependent). Bad choices result in poor performance, possibly much worse than what could be obtained with independent parallelism. However, a welltuned cooperation scheme may significantly outperform the independent approach. To this end, a framework -called CPLS -which eases the cooperative parallelization of local search solvers has been recently proposed in [15] , [16] . This framework, available as an open source library written in the IBM X10 concurrent programming language, allows the programmer to tune the search process through an extensive set of parameters. This framework has been successfully used to tackle stable matching problems [17] and very difficult instances of the Quadratic Assignment Problem (QAP) [18] .", "In this work we are interested in hybridizing single-solution metaheuristics with population-based metaheuristics and in particular in memetic algorithms. Theoretically it is possible to plug a population-based metaheuristics into CPLS, we thus tried to hybridize a genetic algorithm with the local search procedures used in ParEOTS (extremal optimization and tabu search) [19] . It turned out that the resulting solver performed worse than ParEOTS! An explanation for this observation is that the CPLS basic cooperation mechanisms are designed for single-solution metaheuristics, and also that CPLS is better at intensification than diversification. ParEOTS performed well because extremal optimization can be conveniently tuned to provide an appropriate level of diversification. Even though some single-solution metaheuristics are rather good at diversification, when well tuned, population-based metaheuristics offers a wider variety of diversification thanks to the large possibilities of evolution offered by a population. However, to be effective in an hybrid procedure, this population should encompass many solutions (e.g. all solution candidates computed by all solver instances)."], "relatedWork": [], "rq": [" and many issues must be solved: what information is exchanged?"]}
{"intro": ["CBA is a decision-making system developed by Jim Suhr that is based on four principles: (1) Decision-makers must learn and skillfully use sound methods, (2) Decisions must be based on the importance of advantages, (3) Decisions must be anchored to relevant facts, and (4) Different decisions call for different methods. This paper will focus on the first principle (Suhr 1999) . CBA has gained more attention in the construction industry in recent years. This increase has been driven by demands for more collaborative project organizations and transparent decision-making processes; by the synergy of CBA with other agendas such as improving sustainability and safety; and by an increasing need to incorporate multiple factors into the decision-making process. However, as the construction industry simultaneously prioritizes delivery logics confined by tight schedules and budgets there is a competing desire to maximize efficiency and timeliness in training processes. Decision-makers seek training protocols with minimal disruption of delivery, even at the expense of quality in education. It is therefore crucial for decision-makers and coaches to think critically about how to provide the most effective training based on the financial resources of the project. This paper compares the experiences of four CBA coaches working separately in different countries and analyzes the skill development of the teams based on the methods of training they received. The primary research question is: What are the benefits and shortcomings of each of the CBA training methods employed by the coaches studied in this paper? This will be evaluated by comparing different styles of training with the learner's subsequent ability to implement CBA. First, this paper will present relevant literature on human learning to contextualize the need for a range of training options. The research methods used to observe more than 30 CBA trainings will be explained and the accompanying data presented. Finally, the outcomes will be discussed, and conclusions drawn regarding how this research can facilitate other coaches and decision-makers in the industry in personalizing CBA trainings to every unique audience."], "relatedWork": [], "rq": [" the primary research question is: what are the benefits and shortcomings of each of the cba training methods employed by the coaches studied in this paper?"]}
{"intro": [], "relatedWork": [], "rq": [" we build on previous studies of the hierarchy of functional projections that host adjectives in the noun phrase to explore another fundamental question asked by cartography: what is the source of the functional hierarchies?"]}
{"intro": [], "relatedWork": [], "rq": [" the question arises: what is going on inside the networks?"]}
{"intro": ["The impact of video captioning on vocabulary learning has been investigated by many studies (e.g., Aldera & Mohsen, 2013; Hsu, Hwan, Chang, & Chang, 2013; Mohsen, 2016; Stewart & Pertusa, 2004; Sydorenko, 2010) . The focus of the previous studies lies on full captions. In addition, almost all of the previous studies were conducted with the help of a computer. However, as far as the author knows, there is only one study that investigated the effect of keyword captions on vocabulary learning (i.e., Yang & Chang, 2013) . They proposed three modes of captions: full, keyword-only, and annotated keyword captions and investigates their contribution to the learning of reduced forms and overall listening comprehension. The results revealed that all three groups exhibited improvement while the annotated keyword caption group exhibited the best performance with the highest mean score. However, as far as the author knows, pronunciation has not yet been investigated in mobile-assisted language learning environments. Therefore, this study is an attempt to fill-in this gap and investigates the effect of keyword video captioning on pronunciation in comparison to full video captioning using mobile devices. The study will seek to answer the question: what is the effect of watching video captioning on the development of the students' pronunciations of L2 words?"], "relatedWork": [], "rq": [" the study will seek to answer the question: what is the effect of watching video captioning on the development of the students' pronunciations of l2 words?"]}
{"intro": ["The design of electronic patient records is a huge challenge for the HCI community, raising a wide range of still unanswered questions related to issues such as screen layout, interaction design, and integration into work processes. Where should the systems be located and who should enter the data? How do we make sure that input is complete and accurate? How are the different work processes in healthcare structured and coordinated? What is the most useful way of displaying and accessing the vast quantity of patient data? [4] . In the light of these questions, a lot of research has been published in the HCI literature about EPR systems and how to meet challenges related to design and use of computer system in healthcare. Specifically, much attention has been given to issues such as information sharing [5] , support for cooperation [6] and privacy [12] . While much of this research is based on studies on the use of traditional paper-based patient records, suggesting viable electronic counterparts, however, little research has been published based on studies that inquire into the use of the mass of EPR systems already in use.", "In Denmark, there is currently an extensive focus on electronic patient records. The government has decided that by 2005 all Danish hospitals must have replaced the traditional paper-based patient records with electronic ones. However, it is up the regional authorities to decide on the details of deployment. Thus a number of pilot projects are currently in progress with the aim of developing and evaluating electronic patient record systems (see e.g. [2] ). In relation to a regional Danish research program entitled \"The Digital Hospital\" we have studied the use of a commercial EPR system currently in use at a large hospital (IBM IPJ 2.3). In addition to this, an experimental mobile EPR prototype extending the current system's functionality was designed and evaluated. Driving this study, we were concerned with the following research questions: 1) what challenges characterize the use of contemporary electronic patient record systems? 2) How can these challenges be met by improved interaction design?"], "relatedWork": [], "rq": [" 1) what challenges characterize the use of contemporary electronic patient record systems?", " 2) how can these challenges be met by improved interaction design?"]}
{"intro": [], "relatedWork": [], "rq": [" the following questions still remain unanswered: how are biclusters integrated into the sensemaking process and used within the sensemaking loop?"]}
{"intro": [], "relatedWork": [], "rq": [" figure 17 is also an excerpt from the switchboard corpus. in this case, the topic of the discussion was home decorating. in utterance 7, speaker a marks a focus pop with the cue word anyway. but what intention is segment 3 related to?", " 18 why does this occur?"]}
{"intro": [], "relatedWork": [], "rq": [" the first is tokenization: how do we segment the text into tokens each of which is a word, a punctuation token, or an instance of a semiotic class?"]}
{"intro": ["One of the important theoretical issues in reinforcement learning are rigorous statements on convergence properties of so called value estimators (e.g. Sutton 1988; Watkins and Dayan 1992; Jaakkola et al. 1994; Bradtke and Barto 1996) which provide an empirical estimate of the expected future reward for every given state. So far most of these convergence results were restricted to the asymptotic case and did not provide statements for the case of a finite number of observations. In practice, however, one wants to choose the estimator which yields the best result for a given number of examples or in the shortest time.", "In this paper we follow a new approach to the finite example case using tools from statistical estimation theory (e.g. Stuart and Ord 1991) . Rather than relying on bounds, on Fig. 1 The figure shows two value estimator classes and four value estimators. On the left the class of unbiased value estimators is shown and on the right the class of Bellman estimators. The graph visualises to which classes the estimators belong and how the two classes are related. The cursive texts state conditions under which different estimators are equivalent, respectively, under which the two classes overlap: (1) The modified TD estimator (Appendix B) is unbiased if the MRP is acyclic, (2) MC is equivalent to the MVU if the Full Information Criterion (FI) is fulfilled and \u03b3 = 1, (3) ML is equivalent to the MVU if the MRP is acyclic or if FI is fulfilled and \u03b3 = 1, (4) in general, the class of unbiased value estimators does not overlap with the class of Bellman estimators. However, both overlap if the MRP is acyclic or if FI is fulfilled and \u03b3 = 1 (indicated by the dashed semi-ellipse). The dashed semi-ellipse suggests that the classes are not equivalent. In Sect. D.3 (p. 329) we present two simple examples that show that this is indeed the case approximations, or on results to be recalculated for every specific MRP this approach allows us to derive general statements. Our main results are sketched in Fig. 1 . The major contribution is the derivation of the optimal unbiased value estimator (Minimum Variance Unbiased estimator (MVU), Sect. 3.3). We show that the Least-Squares Temporal Difference estimator (LSTD) from Bradtke and Barto (1996) is equivalent to the Maximum Likelihood value estimator (ML) (Sect. 3.4.6) and that both are equivalent to the MVU if the discount \u03b3 = 1 (undiscounted) and the Full Information Criterion is fulfilled or if an acyclic MRP is given (Sect. 3.4.3) . In general the ML estimator differs from the MVU because ML fulfills the Bellman equation and because estimators that fulfill the Bellman equation can in general not be unbiased (we refer to estimators that fulfill the Bellman equation in the future as Bellman estimators). The main reason for this effect being the probability measures with which the expectations are taken (Sect. 3.1). The bias of the Bellman estimators vanishes exponentially in the number of observed paths. As both estimators differ in general it is natural to ask which of them is better? We show that in general neither the ML nor the MVU estimator are superior to each other, i.e. examples exist where the MVU is superior and examples exist where ML is superior (Sect. D.2).", "The first-visit MC estimator is unbiased (Singh and Sutton 1996) and therefore inferior to the MVU as the MVU is the optimal unbiased estimator. However, we show that for \u03b3 = 1 the estimator becomes equivalent to the MVU if the Full Information Criterion applies (Sect. 3.5). Furthermore, we show that this equivalence is restricted to the undiscounted case. Finally, we compare the estimators to TD(\u03bb). We show that TD(\u03bb) is essentially unbiased for acyclic MRPs (Appendix B) and is thus inferior to the MVU and to the ML estimator for this case. In the cyclic case TD is biased (Sect. 3.6 )."], "relatedWork": [], "rq": ["6) is equivalent to the maximum likelihood value estimator (ml) (sect. 3.4.6) and that both are equivalent to the mvu if the discount \u03b3 = 1 (undiscounted) and the full information criterion is fulfilled or if an acyclic mrp is given (sect. 3.4.3) . in general the ml estimator differs from the mvu because ml fulfills the bellman equation and because estimators that fulfill the bellman equation can in general not be unbiased (we refer to estimators that fulfill the bellman equation in the future as bellman estimators). the main reason for this effect being the probability measures with which the expectations are taken (sect. 3.1). the bias of the bellman estimators vanishes exponentially in the number of observed paths. as both estimators differ in general it is natural to ask which of them is better?"]}
{"intro": [], "relatedWork": [], "rq": [" -frequency of use: is there sufficient evidence of a structure at this level to warrant investigation?", " -rate of correct uses: is there an adequate rate of correct uses?", " -avoiding the effect of a task: is the usage affected by a task?"]}
{"intro": ["A recent paper [5] introduces the replica location problem in a Data Grid [6, 7] : given a unique logical identifier for desired data content, we need a mechanism to determine the physical locations of one or more copies of this content. A slightly different semantic makes this problem different from cooperative Internet caching: a replica location mechanism for Data Grids might have to serve requests for many or all replicas corresponding to a given logical identifier. Requests for many (or N) replicas might be generated by a brokering service searching for a replica with suitable performance characteristics. Requests for all replicas might be generated, for example, by a service implementing file system functionalities. However, in a distributed, asynchronous environment, where nodes leave the system without warning, it is impossible to provide a completely consistent system view [8, 9] -and thus impossible to serve \"all replicas\" requests reliably in a decentralized manner.", "In this article, we present a probabilistic approach to the replica location problem and show that relaxed consistency constraints allow for a decentralized, low-latency, low-overhead solution. In contrast to traditional hierarchical, (e.g., DNS) and recent distributed search and indexing schemes (e.g., CAN, Chord, Gnutella), nodes in our location mechanism do not route queries but organize into an overlay network and distribute location information. Each node that participates in the distribution network builds, in time, a view of the whole system and can answer queries locally without forwarding requests. This straightforward design brings benefits (e.g., reduced query latency, load sharing, robustness, etc.) in environments such as GriPhyn's [10] large-scale data analysis projects. These environments are characterized by high query rates and significant, albeit lower, rates of replica creation and deletion, and node and network failures. However, as an environment becomes more dynamic and replica create/delete operations start to prevail over queries, query-routing schemes might sa better tradeoff."], "relatedWork": [], "rq": ["0] are compact data structures used for probabilistic representation of a set in order to support membership queries (\"is element x in set y?"]}
{"intro": ["Huge and complex models are popularly used in today's machine learning applications, since they can take advantage of big data to get better performance. Indeed, they have significantly boosted performance of many important tasks, such as image classification (Russakovsky et al. 2015) , speech recognition (Hinton et al. 2012) , dialogue systems (Sordoni et al. 2015) and autonomous driving (Bojarski et al. 2016 ). More recently, they even beat a human champion by a large margin in the game Go (Silver et al. 2016) . However, a primary question arises: how can we provide sufficient fuel (a plethora of annotated data) to propel our rocket (complex models)? The most appealing way may be the crowdsourcing technology (Rus-Bo Han and Quanming Yao have contributed equally to this work.", "The previous efforts have extensively focused on statistical inferences, which aggregate crowdsourced labels when they are already collected (Karger et al. 2011; Liu et al. 2012; Chen et al. 2013; Zhou et al. 2014; Tian and Zhu 2015; Zhang et al. 2016b ). However, as crowdsourced labels are intrinsically noisy, statistical inferences are hard to guarantee that aggregated labels are reliable. In order to improve the label quality, recently, many researchers resort to a complementary direction, namely proposing approaches controlling the process of label collection (Singla et al. 2015; Litman et al. 2015; Chen et al. 2016; Pennock et al. 2016; Zheng et al. 2015b; Fan et al. 2015; Han et al. 2017) .", "These approaches aim to encourage workers to provide more reliable labels at the stage of collection. For example, the skip-based approach encourages workers to skip uncertain tasks. However, if many tasks are difficult, the label requester may collect only a few labels, which are not enough for subsequent learning models (Shah and Zhou 2015; Ding and Zhou 2017) . The self-corrected approach encourages workers to check whether they need to correct their answers after looking at references. However, references consisting of responses from other workers are noisy, which may mislead workers. Moreover, this approach is not realized on crowdsourcing platforms (Shah and Zhou 2016) . Therefore, existing approaches fail to acquire a sufficient number of high-quality labels on real tasks (Table 1) . Besides, these approaches cannot detect and give potentially larger payment to high-quality workers. However, this is very important as these workers are always preferred by crowdsourcing platforms, thus they should be identified and more paid (Ipeirotis et al. 2010) ."], "relatedWork": [], "rq": [" a primary question arises: how can we provide sufficient fuel (a plethora of annotated data) to propel our rocket (complex models)?"]}
{"intro": ["The same holds for the closely related Muchnik lattice M w , which was introduced by Muchnik in [15] . However, this does not mean it is impossible to capture IPC using the Medvedev lattice. For any Brouwer algebra B and any x \u2208 B, the factor B/{y \u2208 B | y \u2265 x} (which we will denote by B/x) is also a Brouwer algebra. Thus one might ask if the next-best thing holds for the Medvedev lattice: is there an A \u2208 M such that the theory of M /A is exactly IPC? Quite impressively, Skvortsova [19] showed that there is such a principal factor of the Medvedev lattice which captures IPC. Unfortunately, the class A generating this factor is unnatural in the sense that it is constructed in an ad hoc manner. This leads to the natural question, posed in Terwijn [24] : are there any natural principal factors of the Medvedev lattice which have IPC as their theory?", "We also study a question posed by Sorbi and Terwijn in [22] . As mentioned above, the theory of the Medvedev lattice is equal to Jankov's logic Jan, the deductive closure of IPC plus the weak law of the excluded middle \u00acp \u2228 \u00ac\u00acp. Let 0 \u2032 be the mass problem consisting of all non-computable functions. Recall that we say that a mass problem is Muchnik if it is upwards closed under Turing reducibility. In [22] it is shown that for all Muchnik B > M 0 \u2032 the theory of the factor M /B is contained in Jan. Therefore, Sorbi and Terwijn asked: is Th(M /B) contained in Jan for all mass problems B > M 0 \u2032 ? Sorbi and Terwijn also proposed a connected question: does every B > M 0 \u2032 bound a join-irreducible Medvedev degree > M 0 \u2032 ? By their results, this would imply that Th(M /B) is always contained in Jan. However, they conjectured the answer to this connected question to be negative, a fact which was later proven by Shafer [18] . Nonetheless, in the same paper, Shafer widened the class of mass problems B for which Th(M /B) \u2286 Jan holds to those B which bound a 'pseudomeet' of a countable sequence of join-irreducible degrees. Unfortunately, Shafer also showed that this still does not cover all B > M 0 \u2032 . We give a positive answer to Sorbi and Terwijn's question. This is accomplished by showing that a relativisation of Theorem 1.1 holds, i.e. that for every B > 0 \u2032 there is in fact a factor C \u2264 M B such that Th(M /C) = IPC."], "relatedWork": [], "rq": [" thus one might ask if the next-best thing holds for the medvedev lattice: is there an a \u2208 m such that the theory of m /a is exactly ipc?", " : are there any natural principal factors of the medvedev lattice which have ipc as their theory?", " sorbi and terwijn asked: is th(m /b) contained in jan for all mass problems b > m 0 \u2032 ?"]}
{"intro": [], "relatedWork": [], "rq": ["2; is that the distribution of the focus markers is determined by grammatical role. that is, n combines with subjects in focus, and ka combines with all other focussed elements. this is supported by the fact that a similar division of focus markers apparently dividing subjects and other arguments is seen in related mabia languages. 6 for instance, the following from gurene (dakubu 2003) shows that there is an optional focus marker n which marks focussed subjects, and another marker ti used to mark non-subjects. this is not an exhaustive list of languages with this property and we refer the reader to kalinowski (2015) for a more comprehensive survey of focus marking in african languages, where the mabia languages are included in the survey. returning to dagbani, it is important to stress that ka and n are focus markers, since they only appear in contexts of new information focus (questions, answers, contrastive focus etc). a reviewer questions whether they could be markers that indicate any a'-dependency. we claim that they are not, which can be shown by the following sentences, all canonical instances of a'-movement, but not focus. as can be seen, n and ka do not show up in relative clauses, tough-movement constructions, or in comparative clauses. note that with the last two instances, tough-movement and comparative clauses, respectively, it is not immediately apparent that a'-movement is involved at all. relative clauses, however, do seem to involve similar properties to english, and they do not allow for n and ka. we leave a fuller investigation of a'-movement in dagbani for future work, but it suffices to note here that n and ka are focus markers, and not general indicators of an a'-dependency. as we have seen, n and ka appear to mark whether a subject or a non-subject has been focussed. further evidence of a split between subjects and non-subjects can be seen in the behaviour of the two classes with regards to in-situ focus. focussed subjects are not allowed to appear in-situ, whilst all other elements are. it is easy to see that in-situ focus is possible with non-subjects. we see that (16-b) since ka appears only in the left-periphery, and never in in-situ positions, we must rely on the context determining that we are truly dealing with focus here. answers to whquestions are generally taken to be focussed since they contain new information. thus, movement to the left periphery is not obligatory for dagbani focus. 9 it is difficult to show that subjects do not allow for in-situ focus. with subjects, movement to the left-periphery is string vacuous; the canonical subject position is spec,tp and already the leftmost element of the focus. however, one of the characteristics of in-situ focus in dagbani is that the focus marker cannot appear with the in-situ focus. thus, , the sentence would be ungrammatical if ka were to accompany the object. we can utilise this property for subjects. if subjects were allowed to remain in-situ under focus, we would expect that they can appear without the focus marker, even when interpreted as in focus. the following pair of examples demonstrates that, leaving the subject in its base position without a focus marker yields a grammatical sentence, however the sentence is infelicitous in the context. (17) a. \u014b\u00f9n\u00ed n\u0301ch\u00e1\u014b p\u00fa\u00fa m\u00e1\u00e1 n\u00ed?", "matrix subjects: why two focus heads?"]}
{"intro": [], "relatedWork": [], "rq": [" shape analysis estimates the shape of the data structure accessible from a given heap-directed pointer: is it tree-like, dag-like or a general graph containing cycles?"]}
{"intro": ["Right adjunction, however, has been severely challenged in more recent proposals. In the antisymmetric program of Kayne 1994 , right adjunction and multiple adjunction are theoretically not allowed. Although Kayne himself doesn't work on right adjuncts in the clausal domain, he suggests that a Larson type analysis (cf. Larson 1988 Larson , 1990 Stroik 1990 ) may constitute a solution to the apparent violation by right adjuncts of the Linear Correspondence Axiom (LCA). Other authors have proposed alternative analyses to the classical treatment of right adjuncts as structural adjuncts, adopting either a base left specifier or left adjunction analysis of all adjuncts followed by successive movements of IP or VP further left (cf. Barbiers 1995; Bianchi 2000) . Others have pursued more radical analyses. Nilsen 2000 treats right adjuncts as reduced relative clauses on the event, in a way similar to Kayne's treatment of reduced relatives in the DP domain. Uriagereka 2001 proposes that right adjuncts have a special derivational status and are inserted later in the derivation."], "relatedWork": [], "rq": ["t receive a straightforward explanation: why should the weight of an adjunct or its category trigger more easily a movement which has a semantic motivation?"]}
{"intro": ["Systems on chip are increasingly becoming complex to design, test and fabricate. SoC design methodologies make intensive use of intellectual properties (IPs) [16] to reduce the design cycle time and meet stringent time to market constraints. However, associated tools still lag behind when addressing the huge associated design space exposed by the combination of soft IP. In addition, failure to meet an efficient distribution in terms of performance, area and energy consumption makes the whole design inappropriate. Although this problem is already hard to solve in the ASIC domain, it is exacerbated in the system on programmable chip (SoPC) domain. SoPC are large scale devices offering abundant resources but in fixed amount and in fixed location on chip. Implementing embedded multiprocessors on these devices present several advantages the most important being to be able to quickly evaluate various configurations and tune them accordingly. Indeed, embedded multiprocessor design is highly application driven and it is therefore highly advantageous to execute applications on real prototypes. However, due to the fact that specific resources are located at fixed positions on these large chips it is hard not take into account the important impact of place and route results on the critical paths and therefore on the overall performance. In this paper we address this multiobjective optimization problem [6] restricted to performance and area through the combination of an efficient design space exploration (DSE) technique coupled with direct execution on an FPGA board [2] . The direct execution removes the prohibitive simulation time associated with the evaluation of embedded multiprocessor systems. A side effect of this approach is that direct execution requires actual on chip implementation of the various multiprocessor configurations to be explored which provides actual post synthesis and place and route area information. The resulting flow is fully integrated from multiprocessor platform specification to execution. The paper is organized as follows. In Section 2, we review previous work. Section 3 describes an example of soft IP based multiprocessor and the breadth of the problem associated with the design of such multiprocessor on a particular instance embedded memories optimization. Section 4 presents our approach MOCDEX based on Multi-objective Evolutionary Algorithms (MOEA) and direct execution. In Section 5 we describe a case study and validation while Section 6 provides exploration results. Finally, we conclude in section 7 with remarks and directions for future works.", "Our work adresses three different points: (1) how to efficiently and automatically explore the design space of multiprocessors on chip ? (2) how to reduce evaluation time of multiprocessor configurations during design space exploration ? how to integrate implementation issues (i.e. chip area) in this exploration ? Traditional parallel computer architectures performance evaluation techniques do not provide automatic exploration besides exhaustive evaluation on a few parameters [8] . Paul and al [18] propose a technique called MESH a high level modeling and simulation technique of single chip programmable heterogeneous multiprocessors based on a layered approach. This technique does not integrate implementation issues and design space exploration is not fully automatic and restricted. Chidester and al [4] propose the use of parallel simulation for the evaluation of chip multiprocessor architecture through the use of a 9 nodes of dual cluster-CPU workstations. Although the evaluation of a single configuration is improved they neither address the issue of automatic exploration nor they tackle implementation issues. Simulation can be obtained in different modes. The most important ways for simulation are : Event-based simulators, cycle-based simulators, transaction based verification and cosimulation. The event-based simulation provides a functional and timing simulation results. It takes as input an HDL behavioural description, RTL code, gate level or transistor level netlist. Cycle-based simulation is well suited for Systems-On-Chips with an important number of processors and peripherals. The runtime of cycle-based simulation is improved by taking into account only the necessary calculations at each cycle clock edge. Another advantage of the cycle-based simulation is that it uses less memory in the host machine allowing simulation of larger designs (compared to event-based simulation) [20] . Even with the runtime improvements provided by the cycle-based simulation, verification of actual systems is still slow and unfeasible. The Transaction Level Modeling is an alternative approach for simulation where the aims were to provide a real improvement in runtime simulation and cycle accuracy for large scale SoC. This is due to the separation of communication and computation aspects of a design and by making abstraction of the communication by considering transaction requests instead [13] . TLM level shows performance improvements of 353 faster than in RTL simulation and a simulation speed able to reach 456 KCycles/sec. This system level modeling permits a rapid performance evaluation. Although, no real information about physical constraints of implementation are provided and till now there is no tested and approved tool assuming conversion of a TLM specification to a synthesizable RTL description. The idea of using FPGA platform for emulating parallel multiprocessors on chip have been used in the RPM [12] and RPM-2 [15] project. Although the approach is based on FPGA neither an automatic exploration flow nor a multi-objective design space exploration approach is proposed. Finally recently Copolla and al [14] proposed an open platform for developing multiprocessor SoCs based on emulation. However again no automatic and multi-objective approach is proposed."], "relatedWork": [], "rq": [" (1) how to efficiently and automatically explore the design space of multiprocessors on chip ?", " (2) how to reduce evaluation time of multiprocessor configurations during design space exploration ?"]}
{"intro": [], "relatedWork": [], "rq": [" whether or not the steady state of (2) is globally stable. 2. does the final distribution always correspond to perfect balancing?"]}
{"intro": ["Deep Neural Networks (DNNs) have been widely used to deliver unprecedented levels of accuracy in various applications. However, they rely on the availability of copious amount of labeled training data, which can be costly to obtain as it requires human effort to label. To address this challenge, a new class of deep networks, called Generative Adversarial Networks (GANs), have been developed with the intention of automatically generating larger and richer datasets from a small initial labeled training dataset. GANs combine a generative model, which attempts to create synthetic data similar to the original training dataset, with a discriminative model, a conventional DNN that attempts to discern if the data produced by the generative model is synthetic, or belongs to the original training dataset [1] . The generative and discriminative models compete with each other in a minimax situation, resulting in a stronger generator and discriminator. As such, GANs can create new impressive datasets that are hardly discernible from the original training datasets. With this power, GANs have gained popularity in numerous domains, such as medicine, where overtly costly humancentric studies need to be conducted to collect relatively small labeled datasets [2] , [3] . Furthermore, the ability to expand the training datasets has gained considerable popularity in robotics [4] , autonomous driving [5] , and media synthesis [6] - [12] as well.", "Currently, advances in acceleration for conventional DNNs are breaking the barriers to adoption [13] - [18] . However, while GANs are set to push the frontiers in deep learning, there is a lack of hardware accelerators that address their computational needs. This paper sets out to explore this state-of-the-art dimension in deep learning from the hardware acceleration perspective. Given the abundance of the accelerators for conventional DNNs [15] - [43] , designing an accelerator for GANs will only be attractive if they pose new challenges in architecture design. By studying the structure of emerging GAN models [6] - [12] , we observe that they use a fundamentally different type of mathematical operator in their generative model, called transpose convolution, that operates on multidimensional input feature maps."], "relatedWork": ["GANAX has fundamentally a different accelerator architecture than the prior proposals for deep network acceleration. In contrast to prior work that mostly focus on convolution operation, GANAX accelerates transposed convolution operation, a fundamentally different operation than conventional convolution. Below, we overview the most relevant work to ours along two dimensions: neural network acceleration and MIMD-SIMD acceleration. Neural network acceleration. Accelerator design for neural networks has become a major line of computer architecture research in recent years. A handful of prior work explored the design space of neural network acceleration, which can be categorized into ASICs [15] , [16] , [18] - [22] , [26] , [27] , [30] , [34] , [37] , [38] , [41] , [42] , FPGA implementations [17] , [28] , [35] , [36] , [43] , using unconventional devices for acceleration [29] , [33] , [40] , and dataflow optimizations [16] , [23] - [25] , [31] , [32] , [39] . Most of these studies have focused on accelerator design and optimization of merely one specific type of convolutional as the most computeintensive operation in deep convolutional neural networks. EYERISS [16] proposes a row stationary dataflow that yields high energy efficiency for convolutional operation. EYERISS exploits data gating to skip zero inputs and further improves the energy efficiency of the accelerator. However, EYERISS still wastes cycles for detecting the zero-valued inputs. Cnvlutin [30] can save compute cycle and energy for zero-values inputs but still wastes resources for zero-valued weights. In contrast, Cambricon-X [26] can skip zero-valued weights but still wastes compute cycles and energy for zero-input values. SCNN [21] proposes an accelerator that can skip both zero-valued inputs and weights and efficiently performs convolution on highly sparse data. However, not only SCNN cannot handle dynamic zero-insertion in input feature maps, but also it is not efficient for non-sparse vector-vector multiplications, which are the dominant operation in discriminative models of GANs. None of these works can perform zero-insertion into the input feature maps, which is fundamentally a requisite for transposed convolution operation in the generative models. Compared to these successful prior work in neural network acceleration, GANAX proposes a unified architecture for efficient acceleration of both conventional convolution and transposed convolution operations. As such, GANAX encompasses the acceleration of a wider range of neural network models. MIMD-SIMD accelerators. While the idea of access-execute is not brand-new, GANAX extends the concept of access-execute architecture [44] - [47] to the finest granularity of computation for each individual operand for deep network acceleration. A wealth of research has studied the benefits of MIMD-SIMD architecture in accelerating specific applications [53] - [61] . Most of these works have focuses on accelerating computer vision applications. For example, PRECISION [54] proposes a reconfigurable hybrid MIMD-SIMD architecture for embedded computer vision. In the same line of research, a recent work [61] proposes a multicore architecture for real-time processing of augmented reality applications. The proposed architecture leverages SIMD and MIMD for data-and task-level parallelism, respectively. While these works have studied the benefits of MIMD-SIMD acceleration mostly for computer vision applications, they did not study the potential gains of using MIMD and SIMD accelerators for modern machine learning applications. Prior to this work, the benefits, limits, and challenges of MIMD-SIMD architectures for modern deep model acceleration was unexplored. Conclusively, the GANAX architecture is the first to explore this uncharted territory of MIMD-SIMD acceleration for the next generation of deep networks."], "rq": [" lenge: how to fully utilize the parallelism between the computations of the output rows that require different number of cycles for horizontal accumulation (two cycles for even-indexed and three cycles for odd-indexed output rows)?"]}
{"intro": ["This paper reports on a study with the initial aim of exploring the potential design of digital and wider design tools to increase reporting of hate crime amongst Lesbian, Gay, Bisexual and Transgender (LGBT) young people. However, as the study unfolded we observed how the young people participating challenged and resisted the criminal justice framing of reporting and expressed ambivalence towards being identified as vulnerable. This resistance to being identified as being 'vulnerable' echoes concerns within HCI and other fields, that simplistic ideas of vulnerability ignore vulnerable people's resilience and capacities, can become a stigmatizing label, and open the door for paternalistic controls [14, 45, 46] .", "In response to this, we consider how HCI research can communicate the risks people face without reproducing stigmatizing narratives of passive vulnerability. We reflect upon the ways that visual communication design was used to communicate our findings by depicting ambivalence towards vulnerability to hate crime. We argue that by including ambivalent elements in our communication design we can 'trouble' ideas of vulnerability and resist static narratives about our participants' identities. We conducted two design-led workshops intended to engage participants, drawn from LGBT youth groups and criminal justice workers (CJW). We used design to structure discussion on the topic of hate crime and discrimination, and used making activities to explore the design space. The first workshop (see Figure 11 ) was centered on a discussion on reporting and the impact of hate crime. Here participants were often quick to distance themselves from the impact of such crime. However, when asked to produce a magical device [4] to report hate crime in the second workshop, many of the participants produced devices which enabled an immediate response, suggesting a desire for action and justice, albeit one not oriented towards the existing criminal justice system. From our analysis of the workshops we gained insights into the importance of recognizing and confirming individual's identities to their engagement with reporting processes and a preference for community-oriented framings of hate crime reporting. This paper also contributes to the understanding of the potential of design to inform critical and reflexive practices in HCI through its capacity to use ambivalent elements to trouble narratives of vulnerability."], "relatedWork": [], "rq": ["s comment that the designs had this troubles the purpose of participatory design workshops: how do we turn anger into 'implication for design'?"]}
{"intro": ["The development of location-based services for mobile devices faces many challenges ranging from issues of determining people's location and orientation in physical space, how to combine satellite imaging, 3D models and cartography, through to issues of what information to provide in response to a particular location, and how to facilitate suitable user interaction with this content. Within the Mobile HCI community it has been widely argued that researchers, designers and software developers need to look more broadly at the context of use of mobile devices and systems in order to understand mobile use better and to be able to produce good and relevant solutions [19] . In response to this, huge efforts have been put into both ethnographic-style studies of mobile work activities, and field studies of technology in use. However, while a lot of research has been done into sensing, modeling and adapting to context, as well as philosophizing over the complex concept of \"context\" (e.g. [9] ), very little work has provided theoretically informed foundations informing the design of context-aware systems, or explained, from a user experience perspective, why some solutions work well and why others don't. Hence, generally applicable rules and guidelines for interaction design, as we know them for desktop and web applications, are lacking, and more research is needed into the user experience of this emerging class of applications. Contributing to this research, this paper presents a user experience study of a prototype location-based service and looks at how people perceive the ensemble of mobile devices and use context from the perspective of Gestalt theory and its principles of proximity, closure, symmetry, continuity, and similarity as a potential theoretical framework for understanding the user experience of this class of mobile systems. In suggesting this perspective on human perception and thinking in relation to technology use, we align ourselves closely with the work of Fr\u00f8kjaer and Hornbaek [15] , [16] on metaphors of human thinking for describing aspects of human-computer interaction. We have been studying the user experience of location-based services through a 2-year project investigating the deployment of mobile and pervasive computing technologies in urban environments. The e-Spective project took its origin in the newly opened civic structure of Federation Square in Melbourne, Australia and involved a series of field studies of urban socialising behaviour within the built environment of inner cities as well as the development and evaluation of a prototype locationbased service providing an informational overlay to Federation Square [22] , [28] . In addition to learning about interaction design for location-based services on mobile devices, one of our most interesting (and somewhat surprising) findings from studying people's use of the prototype location-based service was that people were extremely good at making sense from small and fragmented pieces of information. When faced with incomplete or ambiguous information, people wanted to put the pieces together. They wanted to connect the dots, and they were very good at it. This finding prompted two questions: 1) How can we explain this phenomenon?; and 2) How can knowledge about this phenomenon inform the design of similar location-based services? Motivated by these questions we have analysed our video and interview data from field evaluations of the prototype system from multiple theoretical angles. In doing this, we have found that the perspective of Gestalt theory provides a very useful, and yet simple lens for describing and explaining how people make sense of the content of mobile information systems situated in context. In this paper, we focus on the first question of explaining the observed phenomenon."], "relatedWork": [], "rq": [" 1) how can we explain this phenomenon?", " and 2) how can knowledge about this phenomenon inform the design of similar location-based services?"]}
{"intro": [], "relatedWork": ["Many technologies for cognitive enhancement have been designed to train people's cognitive skills [77] [18]. However, although training cognitive skills is important, some researchers assert that the most likely approaches to succeed in improving cognitive performance will be those that not only directly train cognitive skills, but also indirectly support people's cognitive skills by working to reduce things that impair them and enhance things that support them [26] ."], "rq": [" one question that needs to be asked is: why do people use technologies to regulate how they feel?"]}
{"intro": ["Suppose that our input graph, G, is a rooted tree T . We say that T is ordered if an ordering of the edges incident upon each vertex in T is specified. Otherwise, T is unordered. If all the edges of a drawing of T are straight-line segments, then the drawing of T is a straight-line drawing. We define a Lombardi drawing of a graph G as a drawing of G with perfect angular resolution such that each edge is drawn as a circular arc. When measuring the angle formed by two circular arcs incident to a vertex v, we use the angle formed by the tangents of the two arcs at v. Circular arcs are strictly more general than straight-line segments, since straight-line segments can be viewed as circular arcs with infinite radius. Figure 2 shows an example of a straight-line drawing and a Lombardi drawing for the same tree. Thus, we can define our problems as follows: 1. Is it always possible to produce a straight-line drawing of an unordered tree with perfect angular resolution and polynomial area? 2. Is it always possible to produce a straight-line drawing of an ordered tree with perfect angular resolution and polynomial area? 3. Is it always possible to produce a Lombardi drawing of an ordered tree with perfect angular resolution and polynomial area? Related Work. Tree drawings have interested researchers for many decades: e.g., hierarchical drawings of binary trees date to the 1970's [23] . Many improvements have been proposed since this early work, using space efficiently and generalizing to non-binary trees [2, 5, [12] [13] [14] [20] [21] [22] . These drawings do not achieve all the constraints mentioned above, however, especially the constraint on angular resolution. Alternatively, several methods strive to optimize angular resolution of trees. Radial drawings of trees place nodes at the same distance from the root on a circle around the root node [10] . Circular tree drawings are made of recursive radial-type layouts [19] . Bubble drawings [15] draw trees recursively with each subtree contained within a circle disjoint from its siblings but within the circle of its parent. Balloon drawings [18] take a similar approach and heuristically attempt to optimize space utilization and the ratio between the longest and shortest edges in the tree. Convex drawings [4] partition the plane into unbounded convex polygons with their boundaries formed by tree edges. Although these methods provide several benefits, none of these methods guarantees that they satisfy all of the aforementioned constraints."], "relatedWork": [], "rq": [" 1. is it always possible to produce a straight-line drawing of an unordered tree with perfect angular resolution and polynomial area?", " 2. is it always possible to produce a straight-line drawing of an ordered tree with perfect angular resolution and polynomial area?", " 3. is it always possible to produce a lombardi drawing of an ordered tree with perfect angular resolution and polynomial area?"]}
{"intro": [], "relatedWork": [], "rq": [" da: is it equal to formative assessment?"]}
{"intro": [], "relatedWork": [], "rq": [" frequency of the ad baculum fallacy in everyday argumentative discourse: why ever would 'rational' discussants use hardly efficient means like the ad baculum fallacy, a discussion move they can know and expect to be denounced by the other discussion party?"]}
{"intro": ["The emergence of byte-addressable non-volatile memory technologies [1] , [2] , [3] , [4] , also known as persistent memory, is fast blurring the divide between memory and storage. Being directly attached to the memory bus, persistent memory provides a high-bandwidth and low-latency alternative for durability. However, merely providing a fast non-volatile medium will not suffice. Programmers need guarantees about what will remain in persistent memory upon a crash or a failure.", "In a recent study, Marathe et al. [5] highlight the numerous challenges in designing crash consistent programs and advocate for systematic programming models such as transactions that provide ACID guarantees (Atomicity, Consistency, Isolation and Durability). ACID essentially implies that updates within a transaction are made visible (to other transactions) as well as durable (to a non-volatile medium), in an atomic manner. While the database community has developed a plethora of techniques to guarantee ACID efficiently, these techniques have predominantly been developed with slow block-based media in mind. When applied to in-memory settings, such techniques tend to spend a significant amount of time on concurrency control [6] , [7] , [8] and logging [6] , [9] , [10] . This leads us to ask the question: How fast can we enforce ACID in the presence of fast persistent memory? Related Work. Recently, there have been multiple proposals for providing ACID updates to persistent memory. These proposals are classified in Table I based on how they enforce atomic visibility and atomic durability. The first class of designs [11] , [12] , [13] , [14] support atomic durability via software logging by employing flushing and ordering instructions. Ensuring atomic durability in software, however, comes at a significant performance cost [15] , [16] , [17] , [18] , [19] which motivated the development of the second class of designs that either employ hardware support for atomic durability [15] , [17] , [20] , [21] , [22] , [23] or leverage hardware support for ordering to guarantee atomic durability [16] , [18] , [24] , [25] . However, both of these classes enforce atomic visibility in software using software transactional memory (STM) or locks.", "Another approach to ACID is to leverage commercially available Hardware Transactional Memory (HTM) to support atomic visibility, which is the focus of the remaining classes of designs. However, current commercially available HTM systems have two limitations. First, they efficiently support only small transactions [26] , [27] , [28] , [29] , [30] ; if a cache line written within a transaction is evicted from the L1 cache, the transaction must abort. The severity of the problem has been highlighted by a recent study which finds that transactions whose write-set size is larger than 128 cache lines (quarter of the L1 size) are highly likely to abort [31] . This L1 limitation can significantly limit usability and efficiency for ACID transactions, which tend to have relatively large write working-set sizes (Section V). Second, HTM systems only provide ACI guarantees, i.e., atomic visibility but not atomic durability. To guarantee ACID, the third class of designs [13] , [32] , [33] leverages the HTM for atomic visibility and integrates it with software support for atomic durability. The latter requires the writing of a log entry for every modified object within the transaction, thereby increasing the transaction's write set (and the abort rate). The fourth class supports ACID by integrating HTM with hardware support for durability. However, PTM [34] (the only proposal in this class) not only introduces significant changes to the cache hierarchy, but also continues to suffer from the L1 limitation. Our Approach. Our primary goal is to design an HTM that can support ACID transactions efficiently. A secondary goal is to extend the supported transaction size by supporting overflows from the L1 cache to the last level cache (LLC) without adding significant complexity to the coherence protocol or the LLC.", "One way of achieving these goals is to leverage existing unbounded HTM designs [37] , [38] , [39] that rely on logging to support overflows and make those logs durable [20] . However, such an approach, where durability is treated as a secondary consideration, will have poor performance as persisting the log and/or the data will be in the critical path."], "relatedWork": [], "rq": [" this leads us to ask the question: how fast can we enforce acid in the presence of fast persistent memory?"]}
{"intro": [], "relatedWork": [], "rq": [" this consideration leads also to a purely mathematical question: how fast, in terms of j, can we get close to the infinite limit -the upper bound for fractional chromatic number?"]}
{"intro": ["With the move toward recognition of the probabilistic nature of syntax and a growing interest in the predictors of alternative structures, there is now more emphasis on usage data in the form of computer-readable corpora containing speech and writing from various sources Bresnan and Ford 2010; Jaeger 2010; see Diessel 2007 for an overview). The predictability of higherlevel structures, given possible predictors, can be measured by statistical models of corpus data (Gregory et al. 1999; Gries 2003a; Gahl and Garnsey 2004; Bresnan 2007; Bresnan et al. 2007; Roland et al. 2007; Bresnan and Hay 2008; Tily et al. 2009 ) so that the nature of probability differences and the influence of probability on language can be investigated. However, compared to small structures of linguistic interest, such as single words, morphemes, and phones, large syntactic structures, such as phrases or specific groups of phrases, are relatively infrequent in corpora. Thus, for higher-level structures of any particular type, large corpora are often needed. Unfortunately, not all languages or varieties of a language are well-studied, and so appropriate corpora might be lacking. The question arises: what can researchers do when suitable corpora are lacking for the language or variety of a language they wish to study?", "Common methods in the past to help determine the different probability of structures involved having participants create sentences by completing simple phrases or by using particular verbs. Connine et al. (1984) gave participants verbs and a topic or a setting for the sentence: for example, the verb might be remembered and the topic might be sports or the setting might be home. Kennison (1999) simply gave participants past tense verbs to use in a sentence. Holmes et al. (1989) gave their participants a pronoun followed by a complement verb: for example, They admitted \u2026 . Trueswell et al. (1993) and Garnsey et al. (1997) used a person's name followed by a verb: for example, John insisted \u2026 Pappert et al. (2005) gave participants German fragments consisting of Noun Phrase -Auxiliary -Noun Phrase: for example, Der Doktor wird den Krankenpfleger \u2026 . It has become apparent, however, that there are problems with these methods. Merlo (1994) compared verb biases toward different structures in subsets of the Penn Treebank corpora (Marcus et al. 1993 ) with verb bias counts obtained by Connine et al. (1984) , Holmes et al. (1989) , Trueswell et al. (1993) and Garnsey (1994) . She found that the verb bias counts from the corpora were not strongly correlated with the counts from the simple sentence completion counts. Jurafsky (1998, 2002) compared verb bias counts obtained from other subsets of the Penn Treebank Corpora with counts from the simple sentence completion studies of Connine et al. (1984) and Garnsey et al. (1997) and found that the two types of sources yielded very different results. They note that the 'testtube' sentences obtained from simple sentence completion studies are not the same as 'wild' sentences and that \"seemingly innocuous methodological devices, such as beginning sentences-to-be-completed with proper nouns (Debbie remembered \u2026) can have a strong effect on resulting probabilities\" (Roland and Jurafsky 2002: 327) . More generally, they find, because psycholinguistic experiments usually involve constructed sentences isolated from connected discourse, experimental data lack discourse cohesion and experiment participants resort to default referents. The prompts used in experiments also have significant effects. Roland and Jurafsky (2002) also found differences between the different Penn Treebank corpora, specifically the Brown, Switchboard, and Wall Street Journal corpora. They postulated two important influences on probabilities. First, the discourse context of a sentence influences the sentence and, of course, discourse context varies from corpus to corpus. Second, different verb senses have different subcategorization biases and, of course, different verb senses occur with different frequencies in different corpora. Similarly, in their study of single versus double object sentences, Pappert et al. (2005) argued that counts for alternative structures differ depending on the semantic information being conveyed. In a more comprehensive study of several syntactic structures in a variety of corpora, Roland et al. (2007: 370) conclude that \"the likelihood of a particular structure is influenced by a wide variety of contextual factors including discourse type, the topics under discussion, the information demands of the situation, the degree of fluency of speech, and the senses of the words being used\"."], "relatedWork": [], "rq": [" the question arises: what can researchers do when suitable corpora are lacking for the language or variety of a language they wish to study?"]}
{"intro": [], "relatedWork": ["Pspace-complete b Same as for 1-LTL\\X c The hardness result holds for controlled clique parameterized topologies process quantifiers of the same type (all existential or all universal). Furthermore, this is also the case if one allows enhanced versions of these quantifiers that specify that the processes quantified are different, and/or are neighbours (or not) in the topology (see [2] for a definition of these enhanced quantifiers). This allows one, for example, to express mutualexclusion: \u2200i = j. G(\u00ac(critical, i) \u2228 \u00ac(critical, j)). For the case of full k-indexed LTL\\X where alternation of universal and existential quantifiers is allowed, many of the corresponding upper bounds are still unknown, and represent another direction for future work. We now briefly describe what needs to be done to get this extension. All the upper bounds concerning 1-indexed LTL\\X were stated with respect to homogeneous parameterized topologies. 13 Lemma 3 shows that, for 1- indexed LTL\\X, such systems can be simulated by cliques. However, looking at the proof of the lemma, it is not hard to see that this simulation actually works irrespective of the specification logic. Indeed, in the controllerless case we actually get that the set of runs of the cliques is exactly equal to the set of runs of the homogeneous topologies; and in the controlled case this is also true except for a slight technical mismatch between the structure of global configurations in these two types of systems-due to the fact that the single controller of a controlled clique simulates (using a product process template) all the controllers specified by the homogeneous parameterised topology skeleton. However, this technical mismatch is syntactic in nature, and is easily overcome by mapping each coordinate in the state of the unique clique controller to the corresponding controller vertex in the homogeneous topology. Also note that runs of a given topology G in the homogeneous parameterized topology are simulated by runs of a clique topology G of the same size or smaller; conversely, all runs of a simulating clique topology G correspond to runs of topologies in the parameterized homogeneous topology that are larger by at most a constant factor (namely, the number of controllers in the skeleton of the homogeneous topology minus 1). Armed with the above observations, extending our upperbounds from 1-indexed LTL\\X to the universal and existential fragments of k-indexed LTL\\X now requires the following. First, we can easily extend the construction in the proof of Lemma 3 to have the controller of the clique simulate not only the controllers of the homogeneous topology, but also any other k nodes of k different types. Combining this with the observation made in Lemma 2 that, due to symmetry, in a homogeneous system the executions of all processes of a given type are exactly the same, we reach the following conclusion: we can replace reasoning about properties of the set of all runs of a homogeneous parameterised system projected onto processes of k different types with reasoning about the 1-executions of the unique controller of a parameterized clique topology, projected onto the relevant k simulated nodes of interest. Moreover, this reduction incurs only a constant blowup (assuming k and the communication alphabet are fixed). Observe that in case we started with a homogeneous parameterized topology with no controllers (and k types of interest), we do not have to simulate it with a controlled clique-topology. Instead, we can simulate it with a clique topology with two types: one type that is the disjoint union of all the process templates (as in the basic construction in Lemma 3), and one which is the product of the k process templates of interest (similar to the controller casebut not designated as a controller, i.e., allowing one to have many nodes of this product type). Thus, in all cases we can reduce questions about universal and existential k-indexed LTL\\X formulas with respect to a homogeneous parameterized topology to a question about a 1-indexed LTL\\X formula with respect to a clique topology of the same type (controlled or uncontrolled), with a constant blowup."], "rq": [" (2) can be simulated by a synchronous transition using the message inc (2), where the process at vertex v that is running the controller sends inc(2) and updates it's state from i j to i j+1 (simulating the 2cm move to the next instruction), and some vertex w running a memory process with a 0-valued counter 2 bit receiving inc(2) and updating this bit to 1. simulating a jz instruction is slightly more involved since there is no direct way for the controller to query all memory processes for their bit values. however, if all the bits of counter i in all memory processes are 0, then none of these processes is in a state with an outgoing local transition labeled by dec(i)?"]}
{"intro": ["In the practical development of dialogue systems it is often the case that an initial corpus of task-oriented dialogues is collected, either using \"Wizard of Oz\" methods or a prototype system deployment. This data is usually used to motivate and inspire a new hand-built dialogue system or to modify an existing one. However, given the existence of such data, it should be possible to exploit machine learning methods to automatically build and optimize a new dialogue system. This objective poses two questions: what machine learning methods are effective for this problem? and how can we encode the task in a way which is appropriate for these methods? For the latter challenge, we exploit the Information State Update (ISU) approach to dialogue systems (Bohlin et al. 1999; Larsson and Traum 2000) , which provides the kind of rich and flexible feature-based representations of context that are used with many recent machine learning methods, including the linear function approximation method we use here. For the former challenge, we propose a novel hybrid method that combines reinforcement learning (RL) with supervised learning (SL)."], "relatedWork": [], "rq": [" this objective poses two questions: what machine learning methods are effective for this problem?"]}
{"intro": ["Annotated corpora are sets of structured text used in Natural Language Processing (NLP) that contain supplemental knowledge, such as tagged parts-of-speech, semantic concepts assigned to phrases, or semantic relationships between these concepts. Machine Learning (ML) is a subfield of Artificial Intelligence that studies how computers can obtain knowledge and create predictive models. These models require annotated corpora to learn rules and patterns. However, these annotated corpora must be manually curated for each domain or task, which is labor intensive and tedious (Scannell, 2007) , thereby creating a bottleneck for advancing ML and NLP prediction tools. Furthermore, knowledge captured by a specific annotated corpus is often not transferable to another task, even to the same NLP task in another language. Domain and language specific corpora are useful for many language technology applications, including named entity recognition (NER), machine translation, spelling correction, and machine-readable dictionaries. The An Cr\u00fabad\u00e1n Project, for example, has succeeded in creating corpora for more than 400 of the world's 6000+ languages by Web crawling. With a few exceptions, most of the 400+ corpora, however, lack any linguistic annotations due to the limitations of annotation tools (Rayson et al., 2006) .", "There has been a recent trend to leverage human's fascination in game playing to solve difficult problems through Human Computation. Two such games include ESP and Google's Image Labeler (Ahn and Dabbish, 2004) , in which players annotate images in a cooperative environment to correctly match image tags with their partner. Semantic annotation has also been addressed in the game Phrase Detectives (Chamberlain et al., 2009) , which has the goal of creating large scale training data for anaphora resolution. These types of games are part of a larger, serious games, initiative (Annetta, 2008 how collaborative online gaming can affect annotation throughput and annotation accuracy. There are two main questions for such systems: first, will overall throughput increase compared to traditional methods of annotating, such as the manual construction of the Genia Corpus? Second, how accurate are the collective annotations? A successful human computation environment, such as PackPlay, would represent a paradigm shift in the way annotated corpora are created. However, adoption of such a framework cannot be expected until these questions are answered. We address both of these questions in multiple games in our PackPlay system through evaluation of the collective players' annotations with precision and recall to judge accuracy of players' annotations and the number of games played to judge throughput. We show improvements in both areas over traditional annotation methods and show accuracy comparable to expert prediction systems that could be used for semi-supervised annotation."], "relatedWork": [], "rq": ["08 how collaborative online gaming can affect annotation throughput and annotation accuracy. there are two main questions for such systems: first, will overall throughput increase compared to traditional methods of annotating, such as the manual construction of the genia corpus?"]}
{"intro": ["Most of existing mobile sensing applications consider mobile users who can report and access sensory data through the Internet by cellular networks (e.g., 3G/4G mobile networks) or Wi-Fi connections. However, this approach cannot be applied in some scenarios where network coverage is poor or network access is expensive. For example, dead spots of network coverage are commonly found in remote areas and even in some part of major cities [11] . For another, the infrastructure (such as power and cell towers) is down in disaster recovery scenarios [12] . On the contrary, opportunistic data forwarding among mobile users becomes possible through intermittent connections with short-range radio communications, which offers another way to collect and share data in areas with poor or no network coverage. It works well without requiring any centralized server or infrastructure for communication and management, and also reduces the workload of cellular networks in dense areas. Moreover, the opportunistic data forwarding approach is more energy-efficient and less expensive, which is very important because most users hope to save the battery energy and data usage of their mobile phones. To fully explore these potentials, it is necessary to design new efficient sensing and data forwarding schemes.", "Many opportunistic forwarding protocols, such as Epidemic Routing (ER) [13] and its variations, have been proposed to route data among mobile nodes in Mobile Opportunistic Networks (MONs) [14] and Delay Tolerant Networks (DTNs) [15] . Recently, some work proposed to use mobile phone users as data mules for collecting sensory data opportunistically [11] , [12] , [16] , [17] . However, the spatial-temporal correlation among sensory data and its impact on the network performance have not been thoroughly investigated. In this paper, we integrate the opportunistic forwarding protocols with data fusion (or aggregation) for two reasons. First, the users may be interested only in the aggregated results of the sensory data; for example, only the average value of the relevant parameter (e.g., temperature, noise) may be of interest. Second, sensory data collected in close proximity or time period may be highly correlated, and data fusion can effectively eliminate redundancy and hence reduce network overhead. Although many routing schemes supporting data fusion have been proposed in WSNs [18] , to the best of our knowledge, we are the first to investigate the opportunistic forwarding schemes supporting data fusion in MONs.", "Although the idea of integrating opportunistic forwarding protocols with data fusion seems simple and straightforward, we still face some new challenges for both performance modeling and protocol design in practice. Pervious work on performance modeling of opportunistic forwarding protocols assumed that all packets are propagated independently. However, the packets are spatial-temporal correlated in the forwarding process with data fusion, which causes a more complex propagation process. We derive an ordinary differential equation (ODE) model for analyzing the dissemination law of correlated packets, and theoretically prove the bounds of transmission overhead and delivery delay. Our analysis framework can serve as fundamental guidelines on integrating various opportunistic forwarding protocols with data fusion, and achieving desirable tradeoff among various performance metrics. On the other hand, Binary Spray-and-Wait (BSW) [19] is considered to be one of the most efficient methods to reduce the large overhead of the ER scheme without incurring significant delay penalties. However, when we consider integrating BSW with data fusion, an important question must be answered: how many forwarding tokens should be assigned to the nodes for the new fused packet? In order to solve this problem, we design a series of rules to assign proper number of forwarding tokens to the nodes, and theoretically prove the performance improvement in terms of transmission overhead and delivery delay."], "relatedWork": [], "rq": ["9] is considered to be one of the most efficient methods to reduce the large overhead of the er scheme without incurring significant delay penalties. however, when we consider integrating bsw with data fusion, an important question must be answered: how many forwarding tokens should be assigned to the nodes for the new fused packet?"]}
{"intro": [], "relatedWork": [], "rq": [" we need to ask: is the narrative really the main locus of ideophone use?"]}
{"intro": [], "relatedWork": ["The work in this paper similarly makes use of a miniapplication; however, our application additionally contains a geometric multigrid solver and supports mesh structures with variable node degree. Further, we present an additional use case of the mini-application, to examine the impact of the KNL architecture on this class of application. The HPGMG-FV and LULESH mini-applications are most similar to our mini-application however, the former operates on a structured mesh [15] and the latter does not have a multigrid solver. Another body of work which is similar to our own and that we build upon, deals with the validation of a mini-application's performance characteristics against those of the parent code. The technique employed by Tramm et al. involves comparing the correlation of parallel efficiency loss to performance counters for both the mini-application and the target code [8] . Previously this technique has been applied to mini-applications of a neutron transport code [8] ; we apply this technique to a different class of application. Messer et al. develop three mini-applications and use a comparison between the scalability of the mini-application and the original code as evidence of their similarity [16] . However, the authors focus on distributed memory scalability whereas in this work we focus on intranode shared memory scalability."], "rq": [" 2) what performance characteristics will the miniapplication capture?", " 3) can any part of the development process be automated?", " 4) how can the build system be made as simple as possible?"]}
{"intro": [], "relatedWork": [], "rq": [" namely how to choose the meta-architecture: how many cells shall be used and how should they be connected to build the actual model?"]}
{"intro": ["A probabilistic network (Charniak, 1991; Hajek et al., 1992; Neapolitan, 1990; Pearl, 1988 ) combines a qualitative graphic structure which encodes domain dependencies, with a quantitative probability distribution which encodes the strength of the dependencies. The network structure can be a directed or undirected graph. A Bayesian network (BN) structure is a directed acyclic graph and a decomposable Markov network (DMN) structure is an undirected chordal graph. As many effective probabilistic inference techniques have been developed (Henrion, 1988; Jensen et al., 1990; Lauritzen & Spiegelhalter, 1988; Pearl, 1986; Xiang et al., 1993) and the applicability of probabilistic networks have been amply demonstrated in many artificial intelligence domains (Charniak, 1991) , many researchers turn their attention to automatic learning of such networks from data. Chow and Liu (1968) pioneered learning of probabilistic networks. They developed an algorithm to approximate a joint probability distribution ( jpd) by a tree-structured BN. Rebane and Pearl (1987) extended their method to learn a polytree-structured BN. However, many real world domain models cannot be represented adequately with a tree-structured network. The following algorithms are all applicable to learning a multiply connected network. Herskovits and Cooper (1990) developed the Kutato algorithm to learn a BN from a database of cases by minimizing the entropy of the distribution defined by the BN. Their method starts with an empty graph (no links) and adds one link at each pass during search. Later, they proposed the K2 algorithm (Cooper & Herskovits, 1992 ) that learns a BN based on a Bayesian method which selects a BN with the highest posterior probability given a database. A similar algorithm was independently developed by . Recently, Heckerman et al. (1995) applied the Bayesian method to learning a BN by combining prior knowledge and statistical data. Spirtes and Glymour (1991) developed the PC algorithm that learns a BN by deleting links from a complete graph. Lam and Bacchus (1994) applied a minimal description length (MDL) method to learning a BN. A BN is evaluated as the best if it has the minimal sum of its own encoding length and the encoding length of the data given the BN. Instead of learning a BN, Fung and Crawford (1990) developed the Constructor algorithm that learns a DMN. Dawid and Lauritzen (1993) studies 'hyper Markov laws' in learning numerical parameters of a DMN with a given decomposable graph. Madigan & Raftery (1994) proposed algorithms for learning a set of acceptable models expressed as BNs or DMNs. A more extensive review of literature for learning probabilistic networks can be found in (Buntine, 1994; Cooper & Herskovits, 1992; Heckerman, 1995; Herskovits & Cooper, 1990) .", "In this paper we consider learning a DMN from a database. Pearl (1988) showed that directionality makes BNs a richer language in expressing dependencies. For instance, an induced dependence can be expressed by a BN but not by a DMN. In general, fewer numerical parameters are required to specify a BN than those required to specify a corresponding DMN. However, learning of DMNs is useful for several reasons.", "In all the methods mentioned above, a heuristic method with a single-link lookahead search is adopted in order to avoid the exponential complexity of exhaustive comparison of all possible networks. However, as far as we know, the interplay of the scoring metric and the search process has not been analyzed. Many questions have not been answered. For example, how does the current score determine the next link (dependence) that will be selected? How does the inclusion of a new link change the score and why? Is it possible that once a superfluous link is added, the search may continue until a complete graph structure is generated? We have already had a good 'macroscopic' perspective about which network(s) should be chosen if an exhaustive comparison is possible according to a particular scoring metric. However, in viewing the search process as a chain that connects the initial network to some learned network, we do not seem to have a satisfactory 'microscopic' understanding about what is occurring during the transition from one link to the next on the chain. We do not seem to know how good or how bad the learned network is relative to the global optimal. As pointed out by Spirtes and Glymour (1991) and acknowledged by Cooper and Herskovits (1992) , the \"asymptotic reliability of the procedure is unknown\"."], "relatedWork": [], "rq": [" an important question is unanswered: what might be compromised by using a single-link lookahead search?"]}
{"intro": ["However, in debates about the best practices of teaching computing, little has been said about how to include learner groups that are often overlooked (for emerging work, see e.g., [16, 33, 35] ). In particular, there has been little research on the best ways for teaching computing for mixed special education needs (SEN) settings. Researchers face a number of challenges and opportunities in these settings. For example, in special needs schools, the goal is foremost to teach holistic skills that will lead students to succeed in independent life and work. It is important that academic learning also supports the development of such related skills. In addition, in special needs schools, classrooms are often mixed; students are rarely grouped in classrooms according to their primary diagnosis, such as Autism Spectrum Disorder (ASD), general learning difficulties or sensory impairments. Rather, in mixed classroom settings, students with different profiles have both distinct needs and distinct strengths, often with a larger spread in abilities than in mainstream classrooms. This poses a challenge for researchers and teachers: how can the needs and strengths of students in a mixed SEN classroom be best supported to learn computing?", "However, so far the focus has been on mainstream settings. In these settings, tangible programming has been shown to give rise to more collaboration between learners than purely digital programming [15] , and the realism and physical interaction afforded by physical interfaces has been suggested to lead to more engaging and embodied experiences [41] . What if these same properties could be tapped into and even other benefits discovered for students learning with SENs?"], "relatedWork": [], "rq": [" this poses a challenge for researchers and teachers: how can the needs and strengths of students in a mixed sen classroom be best supported to learn computing?"]}
{"intro": ["In psychology, changes in human preferences due to boredom or a desire for novelty are well known and characterized [9, 20, 28] , but little has been done to incorporate them into recommender systems. Boredom and a desire for novelty have been demonstrated in music listening [15, 14] , however, current systems do not have the means to understand or incorporate these latent preferences, largely due to the challenges of estimating and tracking these cognitive constructs and in using them to improve user's experiences. This creates a fundamental question: how can recommender systems understand when a user is bored or seeking novelty? Another question is how do we determine which user to target for recommendation of new items that do not have any prior history. We answer these two fundamental questions."], "relatedWork": [], "rq": [" this creates a fundamental question: how can recommender systems understand when a user is bored or seeking novelty?"]}
{"intro": ["A blockchain, of which Nakamoto's Bitcoin Protocol [49] was the first and most popularized example, can be described as a combination of three powerful technologies: a distributed ledger, a database shared between multiple actors who are all allocated read and write permissions; immutable storage, where changes to the ledger, or transactions, are stored in 'blocks' and where each copy of the database retains every block in the 'chain' as an immutable history; and consensus algorithms, which are protocols for trustless actors in the network to verify the transactions made on the blockchain, and which achieve a secure, shared consensus about the state of the database. Most famously, these three core features have supported cryptocurrencies, primarily Bitcoin. However, of late, there has been a proliferation of blockchain-based applications. Proponents of blockchain view the technology as utterly transformative, comparable to the Internet in its potential scope and impact [1, 32, 75, 78] . Proposed applications include crowdfunding, payment services, voting, copyright management, supply-chain tracking, authentication services and distributed, autonomous organizations. All of these applications concern elementary issues of establishing online identity, managing online data and privacy, and peerto-peer online collaboration, underpinned by decentralized, algorithmic governance. As such, blockchain technologies and their emergent application areas all speak to a broad set of longstanding topics of interest for the HCI community."], "relatedWork": [], "rq": ["6] what power do the core developers and foundations developing these protocols hold?"]}
{"intro": ["Simulation studies have shown that performance in an RCA is better, over a wide range of cache sizes, than a distributed clientiserver architecture [23] . The performance gains are due to the large amount of remote memory made available by the (symmetric) remote caching architecture. However, the following tradeoff in object replication must be resolved in order to use memory resources efficiently. On the one hand, each site should replicate (i.e., cache) important objects in main-memory, and store less important objects on disk. On the other hand, such naive cache management in an RCA is ineficienr in the sense that memory resources are not utilized as well as they would be in a centrally coordinated system [17] . Some sites should instead cache less important objects, and rely on remote memory to access important objects. This counter-intuitive approach can improve both local and overall system performance, because fewer objects must be accessed on disk. The problem, of course, is to make this idea precise: how many replicas of each object should be maintained?"], "relatedWork": [], "rq": [" is to make this idea precise: how many replicas of each object should be maintained?"]}
{"intro": [], "relatedWork": [], "rq": [" one also needs to specify a measure of similarity: what is meant by \\\\indistinguishable\". the most common measures used ai amongst researchers is probably one of competence (can the arti\u00afcial intelligence accomplish a task?"]}
{"intro": [], "relatedWork": ["We already mentioned the works in [7, 9] . Teague [19] , and subsequently Atallah et al. [1] gave a protocol for the general problem of correlated element selection achieving better efficiency than [7] , but preserving the original solution concept of computational NE. Using results from computational complexity to implement correlation devices was considered by Urbano and Vila [21] , aiming for a similar result to Dodis et al. [7] . However, Teague [20] showed that their approach is flawed. An alternative solution concept for analyzing game theoretical properties of cryptographic protocols was suggested by Pass and shelat [18] ."], "rq": [" with the problems this give: is a strategy expected polynomial time, if terminates in expected polynomial time given that the strategy of the other parties are fixed, or should it guarantee to terminate in expected polynomial time no matter the strategy of the other parties?"]}
{"intro": ["However, forecasting energy prices presents a number of challenges because of the volatility characteristic of prices, especially in such a competitive environment [5] . To overcome this problem, multiresolution decomposition techniques such as the wavelet transform (WT) have been used as a pre-processing procedure. The WT can produce a good local representation of the signal in both the time and frequency domain. Daubechies WT was combined with a forecasting model to forecast spot electricity prices and electricity load in [4] , [11] , [12] , [13] . Because this type of wavelet is symmetric, the wavelet coef cients takes into account future information. However, when forecasting, we can only use data obtained earlier in the time domain. So, symmetric WTs, such as Daubechies or Morlet, are not suitable for this type of application. Several papers have mentioned this problem and use an \u00e0 trous wavelet transform or a redundant Haar wavelet transform in nancial time series forecasting [14] , and electricity load forecasting [15] , [16] , [17] ."], "relatedWork": [], "rq": ["1. why rhwt?"]}
{"intro": ["Imbalance in class distribution is pervasive in a variety of real-world applications, including but not limited to telecommunications, WWW, finance, biology, and medicine. The minority or positive class is often of interest and also accompanied with a higher cost of making errors. A typical example of this problem is fraud detection. The instances of fraud in the population are generally a very small proportion (often in the neighborhood of 2%). However, it is quite important to be able to detect a fraudulent transaction. At the same time, it is also important to minimize false positives because these result in investigation costs and can also result in losing a customer. Thus, there is a distribution of costs associated with both false positives and false negatives. Another example is large-scale simulation. While, there are not \"dollar\" costs attached with errors like there may be with fraud detection, there is a cost attached to time spent in poring over \"uninteresting regions\" in a simulation. It is known that the regions of interest in such simulations are typically rare (Bowyer et al. 2000; Banfield et al. 2005) . Hence, an intelligent tool should be accurate in identifying interesting regions, without too many false alarms. Weiss and Provost (2003) observed that the naturally occurring distribution is not always optimal. Thus, one needs to modify the data distribution, conditioned on an evaluation function. Re-sampling, by adding to the minority (positive) class or removing the majority (negative) class of a given data set, has become a de facto standard to counter the curse of imbalance in various domains. There have been numerous papers and case studies exemplifying their advantages (Chawla et al. 2003 (Chawla et al. , 2004 Kubat and Matwin 1997; Ling and Li 1998; Weiss and Provost 2003; Ferri et al. 2004; Dietterich et al. 2000; Kubat et al. 1998; Zadrozny and Elkan 2001; Batista et al. 2004; Maloof 2003; Drummond and Holte 2003) . However, the one common critique of various works is: how does one effectively identify the potentially optimal sampling technique and parameters for a given data set? An accompanying question is: can the techniques for imbalance generalize across cost-sensitive scenarios?"], "relatedWork": [], "rq": [" the one common critique of various works is: how does one effectively identify the potentially optimal sampling technique and parameters for a given data set?", " an accompanying question is: can the techniques for imbalance generalize across cost-sensitive scenarios?"]}
{"intro": ["In the self-organized critical state of the Abelian sandpile model on a graph with a sink, the transient states have zero probability and the recurrent states occur with equal probability [Dha90] . Therefore, the natural algorithmic question to ask about self-organized criticality for the Abelian sandpile model is: How long does it take for the process to reach a recurrent state? Starting with an empty configuration, if the vertex that receives the grain of sand is chosen uniformly at random in each step, Babai and Gorodezky [BG07] give a simple solution that is polynomial in the number of edges of the graph using a coupon collector argument. In the worst case, however, an adversary can choose where to place the grain of sand in each iteration. Babai and Gorodezky analyze the transience class of the model to understand its worst-case behavior, which is defined as the maximum number of grains that can be added to the empty configuration before the configuration necessarily becomes recurrent. An upper bound for the transience class of a model is an upper bound for the time needed to enter self-organized criticality."], "relatedWork": [], "rq": [" the natural algorithmic question to ask about self-organized criticality for the abelian sandpile model is: how long does it take for the process to reach a recurrent state?"]}
{"intro": ["There has been a growing attention by HCI researchers in recent years on the ways in which digital technologies support parenting. Studies have explored how social media data might support reflection on individuals changing identities as they transition to becoming a parent [42] and the ways in which communication technologies build confidence and support the portrayal of multiple identities for new mothers [18] and fathers [2] . Researchers within and beyond HCI note the various challenges new parents face in terms of transitioning into this new role and maintaining social connectedness [23] . Several studies have highlighted how new parents have a heightened risk of reporting feelings of social isolation as a result of withdrawing from social circles and uncertain patterns of sleep [35, 43] . Indeed, a recent survey noted that 28% of new mothers experience loneliness following giving birth to their first child [6] , which has also been linked with both new mothers and fathers reporting heightened levels of depression [16] . However, despite these studies there is still relatively little understanding of how new parents experience sociality in the days, weeks, and months following the birth of a new child. While work has examined how digital technology supports the sharing of tips, advice, and information related to the care of a newborn, less studied is how sociality and social connectedness is supported by such systems, or indeed how they may alleviate (or amplify) issues of isolation. These issues are becoming increasingly pertinent as entrepreneurs begin to develop applications in this space without a deep understanding of the personal vulnerabilities they may be exacerbating (see, for example, smartphone applications for mothers to find each other, such as Peanut [33] and Mush [30] ). Furthermore, in a time when many nations are experiencing a decline in statefunded services and community infrastructures that incidentally supported socialization among new parents, there is a growing demand to identify alternative ways for facilitating connectivity among parents."], "relatedWork": [], "rq": [" this leads us to a perhaps naively-generalized question: how can we, as researchers and designers of the sociotechnical systems which these parents use to connect to each other, be more sensitive toward the vulnerabilities and insecurities they face?"]}
{"intro": ["Policy-sharing. Multiagent learning tasks often require several agents to learn to cooperate. In general there may be quite different types of agents specialized in solving particular subtasks. Some cooperation tasks, however, can also be solved by teams of essentially identical agents whose behaviors differ only due to different, situation-specific inputs. Our case study will be limited to such teams of agents of identical type. Each agent's modifiable policy is given by a variable data structure: for each action in a given set of possible actions the current policy determines the conditional probability that the agent will execute this action, given its current input. Each team's members share both action set and adaptive policy. If some multiagent cooperation task indeed can be solved by homogeneous agents then policy-sharing is quite natural as it allows for greatly reducing the number of adaptive free parameters. This tends to reduce the number of required training examples (learning time) and increase generalization performance, e.g., Nowlan & Hinton (1992) .", "Published results on learning entire soccer strategies, however, have been limited to extremely reduced scenarios such as Littman's (1994) tiny 5 \u00d7 4 grid world with two single opponent players. 1 Our comparatively complex case study will involve simulations with varying sets of continuous-valued inputs and actions, simple physical laws to model ball bounces and friction, and up to 11 players (agents) on each team. We will include certain results reported in (Salustowicz et al., 1997a (Salustowicz et al., , 1997b ."], "relatedWork": [], "rq": [" : how much did some agent contribute to team performance?"]}
{"intro": ["However, despite the prosperity of the crowdsourcing systems, they are subject to the weaknesses of traditional trustbased model, which brings about some inevitable challenges. First, traditional crowdsourcing systems are vulnerable to DDoS attacks, remote hijacking and mischief attacks, which makes the services unavailable. Elance and oDesk, operated by Upwork presently, downed services for many workers due to be hit by DDoS attacks in May 2014 [2] . Second, the majority of crowdsourcing systems run business on a centralized server, which suffers from single point of failure inherently. In April 2015, a service outage emerged due to hardware failure in Uber China, which caused passengers can't stop the order at the end of services [3] . Third, user's sensitive information (e.g., name, email address and phone number) and task solutions are saved in the database of crowdsourcing systems, which has the risk of privacy disclosure and data loss. For example, one of the most prevalent crowdsourcing systems Freelancer [4] was reported to breach the Privacy Act for uncovering a user's true identity which contains IP addresses, active account and dummy accounts by Office of the Australian Information Commissioner (OAIC) in December 2015. Fourth, when requesters and workers are in dispute, they need help from the crowdsourcing system to give a subjective arbitration, which may lead to a behavior known as \"falsereporting\" [5] . Lastly, crowdsourcing companies are interested in maximizing their own benefits and require requesters paying for services, which in turn increases user's costs. Currently, most of the crowdsourcing systems could demand a sliding services fee for 5 to 20 percent [6] .", "There have been many works to deal with part of the above mentioned issues in crowdsourcing systems. Encryption and differential privacy (DP) are used to protect data privacy [7] , [8] , [9] , [10] , [11] , [12] . Reputation-based mechanisms are proposed to address \"false-reporting\" and \"freeriding\" behavior [13] , [14] , [15] . Distributed architectures are designed to prevent single point of failure and bottleneck problem [16] , [17] , [18] . However, the majority of these researches are built on the traditional triangular structure crowdsourcing models which suffer from breakdown of trust. Up to now, none of existing works has solved all of the above issues simultaneously. Thus, this research is motivated by this: Can we design a decentralized crowdsourcing system with reliability, fairness, security and low services fee? To answer this question, we propose a blockchain-based decentralized framework for crowdsourcing. The framework has many advantages such as increasing user security and service availability, enhancing the flexibility of crowdsourcing with Turing-complete programming language and lowering cost. Therefore, our framework has the potential to disrupt the traditional model in crowdsourcing. In a nutshell, our specific contributions are in the following:"], "relatedWork": ["Centralized Crowdsourcing System. Several crowdsourcing systems are developed in a centralized way [6] , [19] , [20] , [21] , [22] . The crowdsourcing services, like worker selection, incentive mechanism and truth discovery, are provided by these centralized crowdsourcing systems. Upwork and WAZE [20] , as two famous crowdsourcing platforms, allow requesters to efficiently hire workers to obtain task solutions (e.g., traffic jams, accidents in WAZE). However, they required users' detailed information (e.g., WAZE and Freelancer [21] ) and stored user information, task data in the centralized platform, which may suffer from privacy leakage [4] , DDoS/Sybil attacks [2] and the issue of single point of failure [3] . [5] proposed auction-based mechanisms EFT and DFT, and [13] proposed reputation-based incentive mechanism to tackle freeriding and false-reporting attacks in crowdsourcing. But their methods are based on the traditional three parities model, which is different from our scheme. Besides, there exist a number of incentive mechanisms relying on crowdsourcing system to save task reward, which has the risk of bankrupt inherently [23] ."], "rq": [" this research is motivated by this: can we design a decentralized crowdsourcing system with reliability, fairness, security and low services fee?"]}
{"intro": [], "relatedWork": ["Parallel random walks As observed by Aleliunas et al. [3] , the random walk is a simple randomized strategy for exploring a connected graph, visiting all its vertices within O(n 3 ) steps in expectation, regardless of the starting location of the walker. By deploying multiple random walks in parallel, the time required to cover all vertices and edges can be further reduced. By making use of multiple random walks starting from a well-chosen initial distribution over nodes, it is possible to design fast algorithms for solving the seminal undirected s-t-connectivity problem in little memory [10, 16] . More recently, multiple walks have been studied in a worstcase initialization scenario, corresponding to the setting of this paper, where the k agents are placed on some set of starting nodes and deployed in parallel, in synchronous steps. The speedup of coverage of the vertices of a graph by multiple walks with respect to a single walk has been studied by Alon et al. [4] , Efremenko and Reingold [14] , and Els\u00e4sser and Sauerwald [15] , providing a characterization of the speedup for many graph classes, in particular, random graphs in different models, and graphs with special properties, such as small mixing time compared to cover time. However, a central question posed in [4] still remains open: what are the minimum and maximum values of speed-up of the random walk in arbitrary graphs? The smallest known value of speedup is \u0398(log k), attained e.g. for the cycle, while the largest known value is \u0398(k), attained for many graph classes, such as expanders, cliques, and stars.", "Deterministic graph exploration Deterministic approaches which provide guarantees on worst-case cover time even on unknown anonymous graphs are a tempting alternative to random walks. However, their implementation proves complicated when considering anonymous networks, in which the agent is not helped by the environment, and when located at a node, it has to decide on its next move based only on its local state memory, the local port ordering at the node, and the port by which it entered the current node. It is a wellestablished result that no memory-less agent can explore all graphs deterministically; this impossibility result has also been extended to a finite team of memory-less agents with extended capabilities in the so-called JAG (jumping automata on graphs) model. Moreover, it has been shown [17] that an agent must be equipped with at least |V | states (i.e., \u03a9(log |V |) bits of memory) to be able to explore all graphs with |V | nodes. On the positive side, unknown anonymous graphs can be deterministically explored by following so called universal traversal/exploration sequences. These exist for any number of nodes, and have polynomial length [3] . The cover time obtained using such an approach is, however, usually by a factor of about |V | 2 greater than the (expected) cover time of a corresponding random walk. It has only been shown in the last decade [24] that such universal exploration sequences can be constructed and followed using very small memory, and consequently, deterministic graph exploration can be performed by an agent with only O(log n) bits of state memory. However, the exploration time achieved by such a procedure may potentially be extremely long, expressed by a polynomial with a high exponent."], "rq": [" still remains open: what are the minimum and maximum values of speed-up of the random walk in arbitrary graphs?"]}
{"intro": [], "relatedWork": [], "rq": [" therefore the numerator of the rayleigh ratio (9) is a weighted sum of the gradients of \u03c6 at all nodes of the graph, and quantifies the average local distortion created by the map \u03c6. a function that minimizes (9), will still introduce some global distortions. only an isometry will preserve all distances between the time series, and the isometry which is optimal for dimension reduction is given by pca. however, as shown in the experiments, the first pca components are usually not able to capture the nonlinear structure formed by the set of time series. as a result, pca fails to reveal the organization of the dataset in terms of low dimensional activated time series and background time series. thirion and faugeras (2003) have used kernel pca to analyze the distributions of the coefficients of a model fitted to fmri time series. a block design fmri dataset from a macaque monkey is studied. by visual inspection, the authors show that their method can organize the coefficients according to the relative strength of the activation. the eigenvectors of the laplacian have also been used to construct maps of spectral coherence of fmri data in (thirion et al., 2006) . how many new coordinates do we need ?"]}
{"intro": ["Wikis have been claimed to provide a number of potential advantages. Most notably they are claimed to support design collaboration [1] . The backing for this claim, however, has not been rigorously assessed and to date few empirical studies have appeared in the literature. Many empirical studies have focused on the use of wikis in other contexts such as corporate and Wikipedia [5, 6] . I have been asking the following general questions in my dissertation: How do design teams, and individual team members, use wikis? To what extent do wikis support information sharing in interdisciplinary design? Wikis are often held to be simple but in what exact ways are Wikis simple (if at all)? These and similar questions require empirical investigation so that a firmer conceptual understanding for information sharing and management in design can be obtained."], "relatedWork": [], "rq": [" i have been asking the following general questions in my dissertation: how do design teams, and individual team members, use wikis?"]}
{"intro": ["Achieving such networks of enterprises, however, is not an easy task. Among the current challenges in enterprise computing is providing interoperability solutions to support seamless collaboration. Most of today's industries have investments in large legacy systems that were not originally designed to be interoperable, aggravated further by the oversupply or lack of standards, proprietary developments or extensions, and heterogeneous hardware and software platforms. Another challenge is for enterprises to be flexible so that they can continuously adapt to dynamically evolving demands and opportunities from within and outside their business environment. This is especially true when business goals need to be changed, new policies are introduced, temporary partnerships need to be created, technological advancements need to be considered, etc. It is important therefore that such networked enterprises are designed for change so that they can quickly respond to emerging business opportunities [31] ."], "relatedWork": ["Van Eindhoven, et al. [11] also discuss flexibility in terms of separating the business rules from the business process. They use variation points to analyze process variability. The variable and non-variable parts of the process are first identified. Variation points of the process are then isolated. An appropriate combination of workflow patterns is then used to model each variant in a variation point. Finally, these workflow patterns are then implemented as business rules. Our approach does not use variability analysis to determine which parts of the process can be expressed as business rules; however, we do use the GORE approach to decide which requirements must be treated dynamically, and hence be specified as business rules."], "rq": [" declarative approach: how to decide?"]}
{"intro": ["Dense stereo matching is a classic topic in photogrammetry and computer vision, through which 3D scenes can be further reconstructed. Conventional stereo methods could be grouped into four stages: matching cost calculation, matching cost aggregation, disparity calculation and disparity refinement (Scharstein and Szeliski, 2002) . The differences between pixel values or gradients, correlation coefficients and mutual information are typical matching costs. However, these costs are inevitably impacted by texture-less areas, reflective surfaces, thin structures and repetitive patterns (Kendall et al., 2017) . Matching cost aggregation is the strategy to integrate votes (usually measured by the disparity difference between current points and neighbourhood points) from a given neighbourhood and possibly correct the current matching point. SGM (Hirschm\u00fcller, 2007) and Graph Cut (Boykov and Jolly, 2001) are two classic stereo methods that employ different aggregation strategy. The latter uses graph model to minimum energy in a 2D neighbourhood region. The former utilizes several 1D cost aggregations to simulate a 2D optimization problem, and greatly improves efficiency. However, both of the solutions assume that every pixel (and disparity) is independent within the neighbourhood. However, it may be not the case as the context and geometric information could be more complicated. From 2015, the deep learning based methods have been gradually introduced to stereo matching and have shown to be promising. Deep neural convolutional networks (CNN) automatically learn multi-level representations that map the original input to the designated binary or multiple labels (a classification problem), or consecutive vectors (a regression problem). The powerful representation learning ability of CNN has made it gradually replacing the conventional feature handcrafting strategies in detection, classification and stereo applications.", "The MC-CNN (\u017dbontar and Lecun, 2014) is an early attempt to replace the empirical matching cost by multi-layer representations automatically learned by a CNN structure. With proper pre-training for the challenging cases as reflective surface and sharp disparity change, more robust matching cost could be expected. It experimentally proved that MC-CNN obtained better results compared to other matching costs as absolute difference of brightness, census and normalized correlation (\u017dbontar and Lecun, 2014) . Other CNN networks produce disparity map directly from original stereo pair in an end-to-end manner (Kendall et al., 2017; Pang et al., 2017; T.Brox, 2016) . GC-Net (Kendall et al., 2017) learns to incorporate contextual information using 3-D convolutions over a cost volume of cross-disparity feature representations and pack the volume to 2D map to regress disparity values. (Pang et al., 2017) propose a cascade CNN architecture composing of two stages. The first stage utilizes DispNet (T.Brox, 2016) and the second stage rectifies the disparity initialized by the first stage and generates residual signals across multiple scales. (Shaked and Wolf, 2016) Improved Stereo Matching with Constant Highway Networks and Reflective Confidence Learning] presents an three-step pipeline based on a highway network architecture for the stereo matching problem including computing matching cost, cost aggregation and parallax refinement. These end-to-end methods, especially GC-Net which integrates geometric and contextual information in higher dimension, greatly alleviate the assumptions that pixels of a neighbourhood are independent in a traditional matching cost aggregation. Basically, in the open-source KITTI 2012 Datasets (Geiger, 2015 , the deep learning based methods achieve top scores and conventional methods appear uncompetitive. However, deep learning based methods have some challenges. First, the deep learning methods require samples to train their models. Whether a model pretrained on an open dataset could be directly applied to a target dataset requires further inspection. Second, the KITTI and other Datasets as Driving (T.Brox, 2016) are close-range images, whether the deep learning based methods could well function on aerial dataset should be further checked. In this paper, we attempt to answer two questions: 1) Does the deep learning based stereo methods have enough generalization ability which guarantees transfer learning from trained models on some open-source dataset to target dataset, and 2) if they could be used in aerial images and outperform traditional methods? (Hirschm\u00fcller, 2007 ) is a classic stereo method that have been widely studied and applied on photogrammetry and computer vision communities. Many variants are developed from SGM, and the SURE software utilizes a multi-view SGM strategy to generate DSM with high accuracy. The greatest contribution of SGM is the aggregation is achieved by several 1D summing other than 2D summing in neighbourhood like Graph Cut stereo method (Boykov and Jolly, 2001 ) that results in a very slow processing. SGM utilizes crossentropy information for matching cost, and shows better than the difference of pixel values. SURE (Rothermel et al., 2012) firstly generate stereo pairs that are especially convenient for multi-view matching and for multi-view geometry recovering. Then, for each stereo pair in a multi-view group, SGM is applied to obtain the parallax map, separately. At last, the redundant depth estimations across single stereo models are merged through a fusion step. Image pyramid strategy is also utilized to limit the searching area and improve the efficiency."], "relatedWork": [], "rq": ["4) is an early attempt to replace the empirical matching cost by multi-layer representations automatically learned by a cnn structure. with proper pre-training for the challenging cases as reflective surface and sharp disparity change, more robust matching cost could be expected. it experimentally proved that mc-cnn obtained better results compared to other matching costs as absolute difference of brightness, census and normalized correlation (\u017ebontar and lecun, 2014) . other cnn networks produce disparity map directly from original stereo pair in an end-to-end manner (kendall et al., 2017; pang et al., 2017; t.brox, 2016) . gc-net (kendall et al., 2017) learns to incorporate contextual information using 3-d convolutions over a cost volume of cross-disparity feature representations and pack the volume to 2d map to regress disparity values. (pang et al., 2017) propose a cascade cnn architecture composing of two stages. the first stage utilizes dispnet (t.brox, 2016) and the second stage rectifies the disparity initialized by the first stage and generates residual signals across multiple scales. (shaked and wolf, 2016) improved stereo matching with constant highway networks and reflective confidence learning] presents an three-step pipeline based on a highway network architecture for the stereo matching problem including computing matching cost, cost aggregation and parallax refinement. these end-to-end methods, especially gc-net which integrates geometric and contextual information in higher dimension, greatly alleviate the assumptions that pixels of a neighbourhood are independent in a traditional matching cost aggregation. basically, in the open-source kitti 2012 datasets (geiger, 2015 , the deep learning based methods achieve top scores and conventional methods appear uncompetitive. however, deep learning based methods have some challenges. first, the deep learning methods require samples to train their models. whether a model pretrained on an open dataset could be directly applied to a target dataset requires further inspection. second, the kitti and other datasets as driving (t.brox, 2016) are close-range images, whether the deep learning based methods could well function on aerial dataset should be further checked. in this paper, we attempt to answer two questions: 1) does the deep learning based stereo methods have enough generalization ability which guarantees transfer learning from trained models on some open-source dataset to target dataset, and 2) if they could be used in aerial images and outperform traditional methods?"]}
{"intro": ["Software programmers never asked for weak memory models. However, they have to deal with the behaviors, which arise as a consequence of weak memory models in important commercial machines like ARM and POWER. Many of the complications and features of high-level languages (e.g., C++11) arise because of the need to generate efficient codes for ARM and POWER, which have weak memory models [1] . Since the architecture community has unleashed the specter of weak memory models to the world, it should answer the following two questions: 1) Do weak memory models improve PPA (performance/power/area) over strong models? 2) Is there a common semantic base for weak memory models? This question is of practical importance because even experts cannot agree on the precise definitions of different weak models, or the differences between them. The first question is way harder to answer than the second one. While ARM and POWER have weak models, Intel, which has dominated the CPU market for decades, adheres to TSO. There are large number of papers in ISCA/MICRO/HPCA [2] - [20] arguing that implementations of strong memory models may be as fast as those of weak models. It is unlikely that we will reach consensus on this question in the short term, especially because of the entrenched interests of different companies. Also there are no studies that we are aware of showing that one weak memory model is superior to another in terms of PPA.", "This paper tries to answer the second question, i.e., find a common base for weak memory models. Previous studies have taken an empirical approach -starting with an existing machine, the developers of the memory model attempt to come up with a set of axioms or rules that match the observable behavior of the machine. However, we observe that this approach has drowned researchers in the subtly different observed behaviors on commercial machines without providing any insights into the inherent nature shared by all weak models. For example, Sarkar et al. [21] published an operational model for POWER in 2011, and Mador-Haim et al. [22] published an axiomatic model that was proven to match the operational model in 2012. However, in 2014, Alglave et al. [23] showed that the original operational model, as well as the corresponding axiomatic model, ruled out a newly observed behavior on POWER machines. For another instance, in 2016, Flur et al. [24] gave an operational model for ARM, with no corresponding axiomatic model. One year later, ARM released a revision in their ISA manual explicitly forbidding behaviors allowed by Flur's model [25] , and this resulted in another proposed ARM memory model [26] . Clearly, formalizing weak memory models empirically is error-prone and challenging."], "relatedWork": [], "rq": [" 2) is there a common semantic base for weak memory models?"]}
{"intro": ["As far as explaining the robust performance of the NBC, Domingos and Pazzani (1996) have argued that it is largely due to its being applied to classification problems, where errors are counted with respect to whether the classification was correct (yes/no) for a given prediction, not whether the corresponding probability estimate was accurate. Further support for this type of hypothesis comes from the work of Frank et al. (2000) , who showed that the performance of the NBA is substantially worse when applied to regression type problems. Pure classification accuracy, however, is only a single, global measure of classifier performance and there are others that can be more appropriate. For instance, it has been demonstrated in Ling et al. (2003) that the area under the ROC curve is a more discriminating measure than pure classification accuracy. Often of more interest are relative risk scores. This is especially the case in problems such as healthcare costs where a very small (\u223c1%), but high risk group, can generate a large fraction of healthcare costs (\u223c30%). In this circumstance it is very unlikely that a classifier is sufficiently accurate that it would place anyone in such a small group. Rather, what is of interest is the relative degree of risk from one patient to another (Stephens et al. 2005 ). This reasoning, that beyond pure classification the NBA can be more rigorously judged, is circumstantially supported by the fact that the NBA can produce poor probability estimates (Bennett 2000; Monti and Cooper 1999) , though in Lowd and Domingos (2005) it was shown that NB models can be as effective as more general Bayesian networks for general probability estimation tasks.", "However, even in the case of classification, as noted by Zhang Zhang 2004 ), Domingos and Pazzani's argument does not explain why it is not possible to have situations where the inaccurate probability estimates flip the classification. Zhang has proposed that it is not just the presence of dependencies between attributes that affects performance, it is how they distribute between different classes that plays a crucial role in the performance of the NBC, arguing that the effect of dependencies can partially cancel between classes and, further, that dependencies can potentially cancel between different subsets of feature values. The question then is, if this is possible, under what conditions will it occur and can we quantify it and therefore predict a priori when the NBA might be inadequate?", "The second major question revolves around how to improve the NBA and NBC. There have been many generalizations Keogh and Pazzani 1999; Kohavi 1996; Kononenko 1991; Langley 1993; Langley and Sage 1994; Pazzani 1996; Sahami 1996; Singh and Provan 1996; Webb and Pazzani 1998; Webb 2001; Webb et al. 2005 Webb et al. , 2012 Xie et al. 2002; Zheng et al. 1999; Zheng and Webb 2000; Liangxiao et al. 2009 ) of the NBA. Some, such as Lazy Bayesian Rules (Zheng and Webb 2000) , Super Parent TAN (Keogh and Pazzani 1999) and Hidden Naive Bayes (Liangxiao et al. 2009 ), have been shown to have very good performance, with significant improvements over the NBA but at substantial computational cost. A good overview of many of these algorithms can be found in Zheng and Webb (2000) , Liangxiao et al. (2009) . As it is not the primary purpose of this paper to introduce a new, competing algorithm, we will restrict ourselves to some general comments: All these generalizations seek to discover sets of attribute values that have dependencies such that they should either be combined together, or with the class variable. In general, they are such that the improvement associated with combining a set of features is judged a posteriori through the relative performance of the algorithm with or without that combination. As there are a combinatorially large number of possible attribute value combinations that might be considered however, the process of attribute selection can be intensive, so that, generally, studies have been restricted to considering only pairs of attributes with an exhaustive search of those combinations being performed.", "The effectiveness of these generalizations of the NBA is generally judged by comparing the performance of the proposed algorithm against the NBA, and, potentially, a chosen set of other algorithms, on a set of canonical test problems, more often than not taken from the UCI repository. The effectiveness of the new algorithm is then inferred globally across the set of considered problem instances. We know from the No-free lunch theorems (Wolpert 1996; Wolpert and Macready 1997) that no algorithm is better than any other across all problem instances. The question is: can we infer a priori which algorithm will perform better on a given problem instance? This is especially important if performance enhancements are dominated by only a small number of instances. It also requires detailed insight as to how and why a given generalization outperforms the NBA on some data sets and not on others. Also, as comparatively complicated, \"black box\" type algorithms there is no transparent theoretical underpinning with which to understand their relative performance. A by product of the development of generalizations of the NBA has been the construction of diagnostics to determine the degree of attribute dependence and therefore detect which features to potentially combine. The most used diagnostic has been that of conditional mutual information (Rish 2001; Friedman et al. 1997; Zhang and Ling 2003) . However, as Rish has pointed out, this measure does not correlate well with NBA performance, arguing that a better predictor of accuracy is the loss of information that features contain about the class when assuming the NB model. What is required is a measure of attribute dependence that relates directly to the appearance of a corresponding error in the NBA and, further, how these errors combine to yield an overall error for a given feature set or classifier."], "relatedWork": [], "rq": [" the question is: can we infer a priori which algorithm will perform better on a given problem instance?"]}
{"intro": [], "relatedWork": [], "rq": [" but it does lead to an important question: what can be salvaged?"]}
{"intro": ["Following this surge of interest in Internet-based studies, researchers have started an ongoing discussion on how to comply with ethical research principles when conducting research online (Bassettt and O'riordan, 2002; Buchanan 2011; Harriman and Patel 2014) . Though some see such studies as text-based due to the open accessibility of online data, others treat such studies as human subjects research by using the word 'participants' particularly when considering social media materials (Page et al. 2014) . These researchers felt obliged to take measures to protect the privacy of 'participants' (e.g. Stromer-Galley and Martinson 2009: 214) . The blurred boundary between participants of online activities and (passive) participants in Internet-based applied linguistics research seems to be the place where the problem lies. In reality, no matter how researchers perceive their studies, individuals who produce online texts will be involved and could be affected. For instance, after the recent developments in search engines the identity of individuals is no longer safeguarded (Wilkinson and Thelwall 2011) . It is now apparent that individual participants' identities can be traced with the increasingly sophisticated search engines if direct quotations of a considerable size are present in articles (Kraut et al. 2004) , which complicates the ethicsrelated challenges in Internet-based applied linguistics research. In addition, researchers may need to reflect on some of the assumptions they have when studying online discourses. For example, they need to be aware that indiscreet use of publicly accessible online data that contain sensitive personal information can destroy the trust among the online users and harm potential participants if they are identified (Herring 1997) . However, it is also true that absence of physical contact with potential participants in Internet-based research does create obstacles for researchers to fulfill their ethical obligations such as obtaining informed consent from all participants. In light of the growing interest in ethics among applied linguistics researchers (De Costa 2016) , these complexities generate an imperative to identify ethics-related practices in Internet-based applied linguistics research. To this end, we conducted this review of ethics-related practices in Internet-based research that explores and interprets individuals' online self-representation. Studies of this kind deal with netizenproduced texts on private aspects of their lives and often involve direct quotations of varying lengths. For these reasons, such studies are more likely to have ethical issues and thus deserve more attention. Our review answer the following questions: 1) How do researchers address ethics related issues in Internet-based applied linguistics studies? 2) How do research contexts mediate the researchers' observation of ethical research principles?"], "relatedWork": [], "rq": [" 1) how do researchers address ethics related issues in internet-based applied linguistics studies?", " 2) how do research contexts mediate the researchers' observation of ethical research principles?"]}
{"intro": ["In conversation, dialogue partners often become more similar to each other. This phenomenon, known in the literature as entrainment, alignment, accommodation, or adaptation has been found to occur along many acoustic, prosodic, syntactic and lexical dimensions in both human-human interactions (Brennan and Clark, 1996; Coulston et al., 2002; Reitter et al., 2006; Ward and Litman, 2007; Niederhoffer and Pennebaker, 2002; Ward and Mamidipally, 2008; Buder et al., 2010) and humancomputer interactions (Brennan, 1996; Bell et al., 2000; Stoyanchev and Stent, 2009; Bell et al., 2003) and has been associated with dialogue success and naturalness (Pickering and Garrod, 2004; Goleman, 2006; Nenkova et al., 2008) . That is, interlocutors who entrain achieve better communication. However, the question of how best to measure this phenomenon has not been well established. Most research has examined similarity of behavior over a conversation, or has compared similarity in early and later phases of a conversation; more recent work has proposed new metrics of synchrony and convergence (Edlund et al., 2009 ) and measures of similarity at a more local level (Heldner et al., 2010) .", "While a number of dimensions of potential entrainment have been studied in the literature, entrainment in turn-taking behaviors has received little attention. In this paper we examine entrainment in a novel turn-taking dimension: backchannelpreceding cues (BPC)s. 1 Backchannels are short segments of speech uttered to signal continued interest and understanding without taking the floor (Schegloff, 1982) . In a study of the Columbia Games Corpus, Gravano and Hirschberg (2009; 2011) identify five speech phenomena that are significantly correlated with speech followed by backchannels. However, they also note that individual speakers produced different combinations of these cues and varied the way cues were expressed. In our work, we look for evidence that speaker pairs negotiate the choice of such cues and their realizations in a conversation -that is, they entrain to one another in their choice and production of such cues. We test for evidence both at the global and at the local level."], "relatedWork": [], "rq": ["8% are smooth switches, 2% are interruptions, and 11% are backchannels. other turn types include overlaps and pause interruptions; a full description of the columbia games corpus' annotation for turn-taking behavior can be found in (gravano and hirschberg, 2011) . gravano and hirschberg (2009; 2011) identify five cues that tend to be present in speech preceding backchannels. these cues, and the features that model them, are listed in table 1 . the likelihood that a segment of speech will be followed by a backchannel increases quadratically with the number of cues present in the speech. however, they note that individual speakers may display different combinations of cues. furthermore, the realization of a cue may differ from speaker to speaker. we hypothesize that speaker pairs adopt a common set of cues to which each will respond with a backchannel. we look for evidence for this hypothesis using three different measures of entrainment. two of cue feature intonation pitch slope over the ipufinal 200 and 300 ms pitch mean pitch over the final 500 and 1000 ms intensity mean intensity over the final 500 and 1000 ms duration ipu duration in seconds and word count voice quality nhr over the final 500 and 1000 ms these measures capture entrainment globally, over the course of an entire dialogue, while the third looks at entrainment on a local level. the unit of analysis we employ for each experiment is an interpausal unit (ipu), defined as a pause-free segment of speech from a single speaker, where pause is defined as a silence of 50ms or more from the same speaker. we term consecutive pairs of ipus from a single speaker holds, and contrast hold-preceding ipus with backchannel-preceding ipus to isolate cues that are significant in preceding backchannels. that is, when a speaker pauses without giving up the turn, which ipus are followed by backchannels and which are not?"]}
{"intro": ["Several speech interfaces that allow multi-tasking dialogues have been built (e.g., Kun, Miller, and Lenharth 2004; Lemon and Gruenstein 2004; Larsson 2003) . However, it is unclear that the mechanisms of managing multiple verbal tasks in these systems resemble human conventions or do the best to help users with task switching. For complex domains, the user might be confused about which task the interface is talking about, or might be confused about where they left off in a task."], "relatedWork": [], "rq": ["b: what do you have to make a high straight with?"]}
{"intro": [], "relatedWork": [], "rq": [" s1b is there a particular genre you like?"]}
{"intro": ["Recently, Venkatakrishnan et al. proposed a flexible framework, called plug-and-play (PnP), where a state-of-the-art denoiser is seen as a black box and plugged into the iterations of an alternating direction method of multipliers (ADMM) [7] . This approach allows using an arbitrary denoiser as a regularizer in an imaging inverse problem, such as deblurring or super-resolution, tackled via an ADMM algorithm, but departing from its standard use, where the denoiser is the proximity operator of some convex regularizer [8] . However, plugging an arbitrary denoiser (possibly without a closed-form expression) into ADMM begs obvious questions [9, 7] : is the resulting algorithm guaranteed to converge? If so, does it converge to a (global or local) optimum of some objective function? Can we identify this function? Here, we give positive answers to these questions, when the plugged denoiser is a modified Gaussian mixture model (GMM) based denoiser [2, 3, 10] ."], "relatedWork": [], "rq": [" : is the resulting algorithm guaranteed to converge?"]}
{"intro": ["2 The MTC, by contrast, expresses the most local configuration one can think of, since controller and controllee are related by movement and thus represented by copies of the same DP; however, as the discussion in section 4.2 will show, in its current form, the MTC is not really compatible with phase theory either (cf. also Drummond & Hornstein 2014) . Moreover, if we take seriously the notion that islands impose restrictions on syntactic movement, the idea that movement takes place all the way up to the controller's position (as suggested by the MTC) might be too radical given that obligatory control is possible into (certain types of) islands. In fact, proponents of the MTC have proposed some strategies to handle control into adjuncts in particular, but, crucially, these cannot be extended to non-adjoined islands. However, as section 2 reveals, control into islands of this type exists as well. An alternative is clearly needed. This paper will therefore explore a hybrid theory of control (HTC) which will combine aspects of both the MTC and non-movement approaches to control. It will assume that there is movement (to model control in terms of a local relationship and thereby make it compatible with phase theory), but not all the way up to the controller's position (to keep up the idea of strict islandhood). The basic idea is that the controllee is merged as (i) Phase Impenetrability Condition (PIC) (Chomsky 2000: 108) The domain of a head X of a phase XP is not accessible to operations outside XP; only X and its edge are accessible to such operations."], "relatedWork": [], "rq": [" so what this paper sets out to do is answer the following question: is it possible to model control in a strictly phase-based theory in which extra assumptions that ease locality restrictions are not needed?"]}
{"intro": [], "relatedWork": [], "rq": [" and (2) how should the word be dealt with?"]}
{"intro": [], "relatedWork": [], "rq": ["s hypothesis in (8) is related to the lack of formal constraints of dowty\\'s proto-role theory: i.e., if theta-roles are to be regarded as clusters of concepts relevant to the external conception of human life (cf. dowty (1991: 575) ), we would like to know which are the formal constraints that limit the number of the relevant semantic entailments. for example, why five (external) semantic entailments and not ten or twenty-five for each proto-role?"]}
{"intro": ["Initiatives to broaden the accessibility of visualization are hardly novel. However, while traditional efforts to improve information and visualization literacy typically focus on education initiatives [1, 19, 20] , rural education faces infrastructure and funding challenges that make large-scale changes unlikely for the near future. As information visualization continues to serve as a mediator for everyday people to understand how data describes and dictates their lives, it becomes important to question whether findings formed in laboratory settings still apply to audiences in hard-to-access communities that with diverse economic and educational backgrounds."], "relatedWork": [], "rq": [" these observations leave us with more important questions: how can platforms ensure that the first data visualization a person sees is a reliable one?"]}
{"intro": ["The focus of this paper is on developing automatic classifiers to infer working conditions and stress related mental states from a multimodal set of sensor data: computer logging, facial expressions, posture and physiology. We present related work in Section 2. The dataset that we use is presented in Section 3. We identified two methodological and applied machine learning challenges, on which we focus our work: 1) Using several unobtrusive sensors to detect stress in office environments. We found that state of the art research in stress inference often relies on sophisticated sensors (e.g., eye tracker, body sensors), and/or uses data collected in rather artificial settings. We see possibilities to build human state estimation techniques for use in office environments. We aim to combine information from multiple weak indicator variables based on physically unobtrusive measurements. We address the following research questions: Can we distinguish stressful from non-stressful working conditions, and can we estimate mental states of office workers by using several unobtrusive sensors? Which modeling approaches are most successful? Which modalities/ features provide the most useful information? This helps to configure a minimal sensor set-up for office settings. We address these questions in Section 4. 2) Taking into account individual differences. We found that, in affective computing, often one generic model is learned for all users. This may work for something universal, as the expression of emotions. However, in earlier work [5] , [6] , we found that people differ in their (work) behavior: typical behavior of users already differs per person. Moreover, the way in which people express mental effort or stress may differ. This highlights a need to build personalized models for particular users or user groups, instead of one general model. We address the following research questions: How important are individual differences? Can we improve performance by building personalized models for particular user groups? We address these questions in Section 5. Finally, we present our Conclusions and Discussion in Sections 6 and 7."], "relatedWork": ["Setz et al. [10] present work in which they use EDA measurements to distinguish cognitive load and stress. 32 participants solved arithmetic tasks on a computer, without (cognitive load condition) or with time pressure and social evaluation (stress condition). To address individual differences, data was also normalized per participant by using a baseline period. However, the non-relative features turned out to work better. Leave-one-person-out cross validation yielded an accuracy of 82 percent to distinguishing both conditions. The authors 'suggest the use of non-relative features combined with a linear classification method' (p.416)."], "rq": [" we address the following research questions: can we distinguish stressful from non-stressful working conditions, and can we estimate mental states of office workers by using several unobtrusive sensors?", " we address the following research questions: how important are individual differences?"]}
{"intro": [], "relatedWork": [], "rq": ["8% are selected for better recall rate. however the selected rules may subsume each other. shorter rule patterns are usually more general than the longer rules. a further screening process is applied to remove the redundant rules. the final rule sets contain 45839 rules and were used to detect unknown words in the experiment. it achieves the detection rate of 96% and the precision rates of 60%. where detection rate 96% means that for 96% of unknown words in the testing data, at least one of its morpheme was detected as part of unknown word. however the boundaries of unknown words are still not known. for more detail discussion, see (chen & bai 1998) . for convenience, hereafter we use (?"]}
{"intro": ["It has been clear for a long time that P2P applications represent a large proportion of the load on the network infrastructure. This is not only true in terms of traffic volume, but also in terms of the number of flows. According to a recent study P2P traffic has now surpassed web traffic in both senses [1] . Accordingly, the network community has devoted a great effort to dealing with this problem. One approach is to try to filter P2P traffic (e.g., [2] , [3] ). However, P2P technology can support legitimate applications as well, so an alternative approach is to modify P2P clients so that they respect the interests of the ISP and the network infrastructure in general, without sacrificing performance. Many such ISPfriendly algorithms have been proposed, most of which rely on localization techniques, where the common goal is that traffic should cross fewer links, preferably staying inside a single ISP (e.g., [4] , [5] ).", "These ISP-friendly solutions focus on traffic volume and do not consider the problem of the number of flows. However, the number of flows is an increasingly important factor. The network is becoming ever more intelligent, providing context-based services, while the applications and the underlying layers are becoming increasingly infrastructure-aware. The main active device vendors are now opening the black boxes and they are starting to provide environments for running third party applications on their active devices [6] . This will make the intelligent or active network possible [7] .", "The smooth functioning of intelligent services on network devices is, to a large extent, the function of the number of flows. A NAT service or a monitoring service are clear examples. In fact, monitoring all the flows in the backbone has never really been feasible, so sampling techniques need to be applied [8] . However, sampling is not an option for implementing a NAT or a firewall service, or when rare but important traffic needs to be identified such as P2P botnet control traffic [9] ."], "relatedWork": [], "rq": ["00 is indeed a small swarm or a large one?"]}
{"intro": [], "relatedWork": ["Using these techniques based on LP, it is difficult for us to understand how to design more efficient or other types of algorithms to schedule malleable tasks. Indeed, the algorithmic design in [2] has to rely on the LP formulation. However, for the greedy algorithm in [3] , we can seek a different angle than the dual fitting technique to finely understand a basic question: what resource allocation features of tasks can benefit the performance of a greedy algorithm? This question is related to the scheduling objective. Further, we will prove that answering the secondary question \"how could we achieve an optimal schedule so that C machines are optimally utilized by a set of malleable tasks with deadlines in terms of resource utilization?\" plays a core role in (i) understanding the above basic question posed in [3] , (ii) applying the dynamic programming technique to the problem in [2] , [3] , and (iii) designing algorithms for other objectives. Intuitively, for any scheduling objective, an algorithm would be non-optimal if the machines are not optimally utilized, and its performance can be improved by optimally utilizing the machines to allow more tasks to be completed."], "rq": [" we can seek a different angle than the dual fitting technique to finely understand a basic question: what resource allocation features of tasks can benefit the performance of a greedy algorithm?"]}
{"intro": ["The importance of EA principles is clear. However, the challenge is how can we use EA principles to govern the design and evolution of EA in the modern context of ServiceOriented Digitally enabled Architectures and Transformation [11] - [13] ?. In particular, we seek to explore how can EA principles guide the evolution or transformation of the Information Systems (IS) enabled organization? In this \"research in process\" paper, we draw on Organizational Transformation (OT), Organizational Inertia (OI), and EA literature to draft our first step towards answering the subquestion: how do enforcing EA principles contribute to ISenabled OT? Thus, based on a comprehensive literature review, we initially propose five testable hypotheses and a research model, which is a pre-requisite to developing a datadriven theory for this important area of research."], "relatedWork": [], "rq": [" and ea literature to draft our first step towards answering the subquestion: how do enforcing ea principles contribute to isenabled ot?"]}
{"intro": [], "relatedWork": [], "rq": [" this question is linked to another aspect related to the adherence of the model to reality: what is the form of modelling that allows a greater mimesis of the real object?"]}
{"intro": [], "relatedWork": [], "rq": [" so the questions are: how can the model deviate from the real object shape?"]}
{"intro": ["Teaching statistics as a service course presents particular challenges to educators, as students who are required to enrol in a quantitative methods or statistics course do not necessarily have an interest in the subject and may not wish to engage with any study perceived as mathematical (Gordon, 2004) . Indeed, it is well documented in the literature that statistics courses often generate anxiety (Onwuegbuzie & Wilson, 2003) . However, there is much less data on a fundamental question: What do statistics educators understand by good teaching and how do they perceive the relationship between teaching and student learning in statistics?", "For students, on the other hand, the important issue is discovering the interconnectedness of knowledge (Biggs, 1999) . This idea has implications for the context of service statistics courses, and indeed service courses in general, where knowledge may be presented to students in isolation to the important constructs of their home discipline. Sowey (2006) has shown that students studying service statistics courses can pose challenging and searching questions, and that the posing of such questions helps students to understand the 'substantial' nature of statistics. However, the learning environment needs to be appropriate to encourage such questioning and the messages sent by the teacher are fundamental to the quality and tone of this social environment. Indeed, in this and other writing, Sowey articulates a coherent view of the relationship between teaching and learning in statistics.", "In contemporary phenomenographic studies, data are typically collected through a series of in-depth, open-ended interviews that focus on allowing each person to fully describe their experience (Ashworth and Lucas, 2000) . Unlike statistical sampling, participants are not considered to represent a particular population-rather, the aim of the interviews is to capture as much variation as possible (\u00c5kerlind, 2005) . The outcome of a phenomenographic study is a set of conceptions-logically related categories that describe the essential differences between one way of experiencing something and another. These categories and the relations between them constitute the outcome space for the research. The categories are usually reported in order of their inclusivity and sophistication, and they are defined by their qualitative difference from the other categories. However, it is the structure of the variation across the group that emerges through iterative readings of descriptions of the experience. The categories represent the variation found within the entire participant group. Hence the categories are illustrated by quotations from transcripts, but do not represent a phenomenological (that is, a rich life-world) experience of a single individual."], "relatedWork": [], "rq": [" there is much less data on a fundamental question: what do statistics educators understand by good teaching and how do they perceive the relationship between teaching and student learning in statistics?"]}
{"intro": ["Of course, a good model depends on the question studied. A media access study might need a detailed model capturing several low-level aspects. For instance, it has to be taken into account that a message might not be received correctly due to a near-by concurrent transmission. Hence, it is crucial that the model appropriately incorporates interference aspects. However, for a transport layer study, a much simpler model which assumes random transmission errors might be sufficient. This paper discusses various models for sensor networks and strives at putting them into perspective. The selection of models presented is of course far from being complete and also highly subjective. In particular, our focus is on models of higher levels of abstraction; a large body of interesting work about the physical layer (e.g., cf. [6] ) is not considered."], "relatedWork": [], "rq": [" for example: what can be computed in a distributed fashion, and what not?"]}
{"intro": [], "relatedWork": ["There are significant tradeoffs between these solutions, and to-date, their impact on players' experience has received very little attention. Warping may affect players' immersion in the game world, since a player's attention may be drawn toward any sudden discontinuities in the position or motion of objects in the game [15] . A player may also lose context, for example, if an avatar that was in front of him is suddenly behind him, out of his field of view. Smooth corrections may reduce the jarring effect of corrections [20] ; however, they prolong the inconsistency, which may be unacceptable in some game situations. Fiedler [8] suggests the rule of thumb of moving 10% toward the true position during each frame, but if the correction is large, simply warping to the new position. More sophisticated forms of smooth correction are possible; e.g. varying priorities may be given to different objects in the game to ensure the objects of higher importance deviate the least from their true position [6] . While these algorithms are promising, and while ad hoc rules of thumb can be useful, there has been to-date no experimental validation of the impact of these techniques on user experience. There has been little effort to characterize when smooth corrections are effective, and to determine when and how they should be applied."], "rq": ["q3: how do error repair techniques impact player experience?"]}
{"intro": ["In many of these applications, given a database of examples and a query, the following two questions are to be addressed-1) What is the \"closest\" example to the query in the database? 2) What is the \"most probable\" class to which the query belongs? A systematic solution to these problems involves a study of the underlying constraints that the data obeys. The answer to the first question involves a study of the geometric properties of the space, which then leads to appropriate definitions of Riemannian metrics and further to the definition of geodesics, etc. The answer to the second question involves statistical modeling of inter and intraclass variations. It is well known that the space of linear subspaces can be viewed as a Riemannian manifold [10] , [11] . More formally, the space of d-dimensional subspaces in IR n is called the Grassmann manifold. On a related note, the Stiefel manifold is the space of d orthonormal vectors in IR n . The study of these manifolds has important consequences for applications such as dynamic textures [2] , [3] , human activity modeling and recognition [4] , [5] , videobased face recognition [6] , and shape analysis [7] , [8] , where data naturally lie either on the Stiefel or the Grassmann manifold. Estimating linear models of data is standard methodology in many applications and manifests in various forms such as linear regression, linear classification, linear subspace estimation, etc. However, comparatively less attention has been devoted to statistical inference on the space of linear subspaces."], "relatedWork": [], "rq": [" the following two questions are to be addressed-1) what is the \"closest\" example to the query in the database?", " 2) what is the \"most probable\" class to which the query belongs?"]}
{"intro": ["Language is a fast, temporally unfolding signal. Humans must quickly compress large amounts of information into abstract linguistic representations and meanings that contain more manageable amounts of information. However, cues to linguistic categories often do not temporallly co-occur, but are distributed quite broadly across the signal. Rational information integration thus requires maintenance of gradient subcategorical information so as to integrate cues that occur at different points in time. For example, one of the primary cues to the voicing of a syllable-final stop consonant in English is the duration of the preceding vowel (Klatt, 1976) . Thus, in order to obtain a good estimate of the voicing of a syllable-final stop, listeners must retain some subcategorical information about the preceding vowel in memory. This is typical across languages and occurs at multiple timescales: cues to sound categories can come not only from proximate acoustic properties, but also from, e.g., later semantic context that could potentially occur an unlimited distance away from the target. This poses a memory challenge for language comprehenders: how can one possibly maintain subcategorical information for later use when such maintenance should overload working memory?", "This challenge has motivated theories of language processing that contend that listeners compress input into abstract representations as quickly as possible and discard all gradient information after a categorical perceptual decision has been reached (Just and Carpenter, 1980; Christiansen and Chater, 2016) . According to these accounts, listeners cannot maintain gradient sub-categorical information for cue integration at any significant timescale, at certainly not beyond word boundaries. However, a growing body of literature has suggested that listeners are in principle capable of maintaining subcategorical representations (McMurray et al., 2009 ), including at timescales beyond the word boundary (Connine et al., 1991; Brown-Schmidt and Toscano, 2017; Gwilliams et al., 2018) . For example, Connine et al. (1991) exposed participants to sentences that contained two cues about a target word, \"tent\" or \"dent\" in the sentence. The first cue was the voice-onset time (VOT) of the first sound in the word, which was varied to form a continuum from more /t/-like to more /d/-like. The second cue was a subsequent word that contextually biased toward either the \"tent\" interpretation (e.g., \"campground\") or the \"dent\" interpretation (e.g., \"teapot\"). Participants heard sentences like \"When the ?ent Sue had found in the [campground/teapot]...\", and were asked to categorize whether they heard the word \"tent\" or \"dent\" in the sentence. They found that participants' categorizations were influenced both by the VOT of the sound and by subsequent context, suggesting that listeners maintained a gradient representation of the initial sound for later use in cue integration and categorization. Subsequent studies have confirmed that listeners can maintain subcategorical representations well beyond word boundaries (Szostak and Pitt, 2013; Bushong and Jaeger, 2017) ."], "relatedWork": [], "rq": [" this poses a memory challenge for language comprehenders: how can one possibly maintain subcategorical information for later use when such maintenance should overload working memory?", " future work should continue to investigate the limits of subcategorical maintenance: what do listeners do when confronted with the typical demands of natural language use?"]}
{"intro": ["Life in affluent, big city society with huge amounts of easily accessible food results in extensive food waste on different levels, causing greenhouse gas emissions. Stuart (2009) claims that throughout the developed world, food is treated as a disposable commodity disconnected from the social and environmental impact of its production (p. xvi). Food is not just a commodity, however: according to Stuart (2009) , we need to recognize that the world is negatively affected by our consumption and waste of food (p. xvii). It hurts both people and the planet."], "relatedWork": [], "rq": [" but the question remains: is there more to city life than the opportunity to consume (miles & miles, 2004) ?"]}
{"intro": ["In 1946 Baer [l] showed that if 8 was a polarity of a finite projective plane of order IZ with a(6) absolute points, then n + 1 s a(0). In 1970 Seib [7] improved this to show n + 1 s a(0) c n3'* + 1. Furthermore, Seib showed that if u(e) = n3/2 + 1, then the absolute points and non-absolute lines form a unital with parameter u = J&. By conducting a systematic study of polarities of finite projective planes Ganley [3-41 discovered many (mutually non-isomorphic) examples of unitals. However, because they come from polarities of translation planes, all of Ganley's unitals had parameters which were prime powers. Indeed, it was widely conjectured that unitals could only exist for parameters which were prime powers. In a recent paper Mathon [6] has constructed a class of cyclic Steiner 2-designs, including a unital with parameter 6. In another recent paper Bagchi and Bagchi [2] have given a construction for Steiner 2-designs admitting a point-regular group. Their construction includes the cyclic designs of Mathon which are unitals. The unital with parameter 6 is the first example of a unital with non prime-power parameter. It seems natural to ask the following question: Can we embed the unitals with parameter u arising from these constructions in a projective plane of order u* (as the absolute points and non-absolute lines of a polarity)? This is, of course, an exciting concept as it would give the first finite projective planes of non prime-power order."], "relatedWork": [], "rq": [" it seems natural to ask the following question: can we embed the unitals with parameter u arising from these constructions in a projective plane of order u* (as the absolute points and non-absolute lines of a polarity)?"]}
{"intro": [], "relatedWork": ["The universal testing problem that we consider in this paper has extensively been discussed in the context of information theory (see e.g., Ziv, 1988 , Gutman, 1989 , Zeitouni & Gutman, 1991 . In particular, Ziv proposed a discrimination algorithm based on a universal coding scheme (Ziv, 1988) , e.g. Lempel-Ziv universal coding (Ziv & Lempel, 1978) , and proved that his proposed discrimination algorithm is asymptotically optimal with respect to the Neyman-Pearson criterion [see e.g. (Hoeffding 1965) ], i.e., it maximizes the rate of decrease in Type 2 error probability in the limit under the constraint that the rate of decrease in Type 1 error probability is bounded by a fixed number. However, it has not yet been reported how well his proposed discrimination algorithm works for finite sample size. Further note that his algorithm makes no use of any hypothesis class as we do.", "Our own technical approach is unique in this regard in that a discrimination algorithm is designed using a parametric hypothesis class and the MDL (Minimum Description Length) principle (Wallace & Boulton, 1968 , Schwarz, 1978 , Rissanen, 1978 , Rissanen, 1987 , Rissanen, 1989 , Barron & Cover, 1991 rather than universal coding schemes, and that it offers a method of finite test sample analysis. Notice here that while Ziv's analysis concentrated on the issue of asymptotic optimality, we instead consider the issue of how many examples are required to achieve given error probabilities for a given pair of /5 and P* such that d(/5, P*) > c. We are further interested in determining whether any given class is PAD-learnable with sample size polynomial in the size of the domain and other relevant parameters. We stress that asymptotic optimality does not always imply polynomial-sample-size PAD-learnability. The application of the MDL principle to hypothesis testing problems was first suggested by Rissanen in (Rissanen, 1987) (pp. 236-238) , (Rissanen, 1989) , (pp. 109-121) . He proposed an MDL-based approach to hypothesis testing for a number of classes of distributions including binomial distributions, gaussian distributions, and two-way contigency tables. He has not yet reported, however, any general theory for finite-sample-size behavior of the MDL-based approach in the universal hypothesis testing setting."], "rq": ["1) how can we design an efficient algorithm for approximating the mdl discrimination', 'text': 'algorithm?"]}
{"intro": ["Both optimal deterministic and randomized mechanism design problems have been studied intensively during the past decade [Tha04, GHK + 05, Bri08, CHK07, CHMS10, CD11, CMS10, BCKW10, Pav10, WT14, BILW14, DDT14, DDT12, DDT13, DDT15, HN12, MV06, Rub16, LY13, Yao15] . For some instances, randomized mechanisms can achieve strictly higher revenue than deterministic mechanisms. However, deterministic mechanisms (bundle-pricings) are much more widely used in practice (especially \"simple\" pricing schemes); we will focus on deterministic mechanisms in this paper. Recently, much effort has been devoted to understanding the power and limitations of simple pricing schemes, that is, menus that can be described succinctly in a natural way and at the same time induce an easy-tosolve buyer's problem. Some of the examples include (i) selling all items separately (item-pricing), (ii) selling only the grand bundle that consists of all items (grandbundle pricing), and (iii) partition mechanisms, where one partitions the items into disjoint groups, each with its own price, and sells the groups separately. While it is known that none of these solutions is optimal in general among bundle-pricings, there has been substantial work studying basic questions for each of these simple solutions, including the following: How does the revenue achievable by these solutions compare with optimal revenues achievable by bundle or lottery pricings? What are conditions under which these solutions are optimal? Can we compute an optimal solution of each type?", "In case (i) of selling the items separately, we know how to compute efficiently an optimal item-pricing: each item is assigned separately its optimal price following Myerson's theory [Mye81] . In both cases (ii) and (iii) of the grand bundle and partition mechanisms, the problem of finding an optimal solution is intractable (#P-hard [DDT12] and NP-hard [Rub16] , respectively). However, the fact that it is hard to find an optimal solu-tion of a certain type (grand bundle or partition mechanisms) does not mean that one cannot easily find a solution that is not of this type and has higher revenue (for example, by selling also individual items), or possibly even find a solution that is optimal among all bundlepricings. Thus, two central questions remain concerning the bundle-pricing (or optimal deterministic mechanism design) problem:"], "relatedWork": [], "rq": [" including the following: how does the revenue achievable by these solutions compare with optimal revenues achievable by bundle or lottery pricings?"]}
{"intro": [], "relatedWork": [], "rq": [" 52 how are place cells generated if not from grid cells?"]}
{"intro": ["Massive multiple-input-multiple-output (MIMO) have recently attracted a lot of attention [1] , [2] . The idea of massive MIMO is to use a large amount of antennas at the base station (BS) to serve multiple users at the same time and frequency resource block. The ability to increase both spectral efficiency (SE) and energy efficiency makes it one of the key candidates for the 5G cellular networks. The analysis of the performance of massive MIMO is of vast importance and has been done in [3] for uplink single cell systems and in [4] for multi-cell systems. However the analysis is done with the assumption of equal power allocation among the users. Only a few papers has considered power control, however there has not been much optimization of the powers. In order to harvest all the benefits brought by the massive antenna arrays, power control among the users is necessary. This can be done by varying the power of different users to either increase the total system performance or provide services with certain fairness.", "Compared to power control in P2P systems, power control in massive MIMO networks is a relatively new topic. Accurate channel estimates are needed at the BS for carrying out coherent linear processing, e.g. uplink detection and downlink precoding. Due to the large number of antennas in massive MI-MO the instantaneous channel knowledge, which is commonly assumed to be known perfectly in the power control literature, is hard to be obtained perfectly. Therefore one needs to take into account both the pilot power and payload power, and hence optimal power control becomes even harder in massive MIMO. This brings a new challenge to designing algorithms for optimal power control to achieve different objectives. On the other hand, the channel hardening in massive MIMO makes it possible to do power control based on the large-scale fading rather than small-scale fading. Several work tried to tackle this hard problem. In [7] the authors optimize the data power for providing every user the same throughput in multi-cell massive MIMO. In [8] power control is done to minimize the uplink power consumption under target SINR constraints. However we are not aware of any work that jointly optimize the pilot and data power for massive MIMO. The questions we want to answer in this paper are: 1) Is power control on the pilots needed for massive MIMO systems? If the answer is yes, how much can we gain from jointly optimizing the pilot power and payload power, as compared to always using full power? 2) What intuition can be obtained from the optimal power control? This includes the pilot length, and how the pilot and payload power depend on the channel quality."], "relatedWork": [], "rq": [" 1) is power control on the pilots needed for massive mimo systems?", " 2) what intuition can be obtained from the optimal power control?"]}
{"intro": [], "relatedWork": [], "rq": [" : how large would the bit size have to be in such an encoding?", " one way to make this more precise is to ask: how large a grid do you need so that every planar n-point configuration in general position can be realized on it, up to order type?"]}
{"intro": ["In an interesting variation of this method, (Kozareva et al., 2008) describe the 'doubly-anchored pattern' (DAP) that includes a seed term in conjunction with the open slot for the desired terms to be learned, making the pattern itself recursive by allowing learned terms to replace the initial seed terms directly: '<type> such as <seed> and ?'. Context-based information harvesting is well understood and has been the focus of extensive research. The core unsolved problem is the selection of seeds. In current knowledge harvesting algorithms, seeds are chosen either at random (Davidov et al., 2007; Kozareva et al., 2008) , by picking the top N most frequent terms of the desired class (Riloff and Jones, 1999; Igo and Riloff, 2009 ), or by asking experts ). None of these methods is quite satisfactory. (Etzioni et al., 2005) report on the impact of seed set noise on the final performance of semantic class learning, and Pantel et al. observe a tremendous variation in the entity set expansion depending on the initial seed set composition. These studies show that the selection of 'good' seeds is very important. Recently, proposed an automatic system for improving the seeds generated by editors . The results show 34% improvement in final performance using the appropriate seed set. However, using editors to select seeds or to guide their seed selection process is expensive and therefore not always possible. Because of this, we address in this paper two questions: \"What is a good seed?\" and \"How can the goodness of seeds be automatically measured without human intervention?\"."], "relatedWork": [], "rq": ["introduction: what is a good seed?"]}
{"intro": [], "relatedWork": [], "rq": [" such as: can the payload be of zero bytes?"]}
{"intro": ["M ANY vision methods estimate physical properties of a scene from images taken under varying illumination. Some notable examples include recovering surface normals using photometric stereo [1] , [2] , [3] , recovering diffuse reflectance and illumination as intrinsic images [4] , [5] , and computing low-dimensional models of appearance of objects and scenes [6] , [7] . However, these methods typically disregard the effect of the local visibility of illumination in determining shading. Further, many of these methods require calibrated setups (e.g., known lighting directions), special priors (e.g., smoothness of surface reflectance), or limiting assumptions (e.g., no cast shadows).", "In our work, 1 we revisit such estimation problems by posing the following question: what can we tell about a scene point simply by observing its appearance under many different, unknown illumination conditions? The appearance of a point over such an image stack depends on many factors, such as the point's albedo and the distribution of illuminations. However, a key observation is that the local visibility of a point-i.e., its accessibility to light from different directions, often modeled as ambient occlusion (AO) in computer graphics-is also an important property in determining its appearance in images. We show that we can estimate ambient occlusion in Lambertian scenes directly from image observations, by introducing a simple pixel-wise, aggregate statistic (k in Fig. 1 ), and relating this statistic to ambient occlusion. To do so, we consider a physical model of a point with a cone of visibility to the hemisphere, lit by a moving point light and constant ambient light over the image stack. We then combine this model with our statistic to infer ambient occlusion for each scene point. This kind of lighting visibility is often treated as a nuisance in computer vision methods, and in many cases is simply ignored. In contrast, we explicitly model such visibility for each scene point, and use it to aid in estimating other physical parameters, such as surface albedo. The result is a photometric approach to estimating ambient occlusion and albedo."], "relatedWork": [], "rq": [" we revisit such estimation problems by posing the following question: what can we tell about a scene point simply by observing its appearance under many different, unknown illumination conditions?"]}
{"intro": [], "relatedWork": [], "rq": [" is: what about the intermediate level?"]}
{"intro": [], "relatedWork": [], "rq": [" (2) what about developing exact algorithms, possibly measured in n = n + m. notice that the o * (2 n )-and o * (2 m )-algorithms shown in this paper can be easily abused in a win-win scenario to derive an o * ( \u221a 2 n )-algorithm. (2) it is not quite clear if the basic assumption in parameterized algorithmics, namely, that the parameter is only moderately large, is met in this set of problems. are there different parameterizations that are more suitable, at least in some applications?"]}
{"intro": [], "relatedWork": ["Empirical and theoretical evidence indicates that active learning can outperform passive learning on a variety of learning tasks [16] . However, our literature review suggests that, while the field of active learning is well established in terms of its theoretical underpinnings, the application of active learning to real-world problems is in its infancy. In fact, we were only able to find a relatively small number of papers that employed active learning in cyber security settings.", "One problem that has been noted when attempting to apply machine learning to cyber security problems is the lack of publicly available datasets. Many papers use the KDD-CUP'99 dataset [17] , which has several documented issues [19] . One of these issues is that the high percentage of redundant records (78% and 75% in the training and test sets, respectively) biases the models towards the frequent records, which hinders the model's generalization performance on unseen or infrequent records. In addition, the dataset does not present enough of a challenge for the models as even the worst model tested attained 86% accuracy. (A new version of this dataset, NSL-KDD, addresses both of these problems [19] .) However, another issue not mentioned is simply the age of the dataset, meaning that many of the attacks are simply no longer relevant."], "rq": [" the question now becomes: how is the best instance(s) to label selected?"]}
{"intro": ["The rapid growth of decentralized and structured or unstructured peer-to-peer (P2P) networks [16, 13, 11] holds great potential for efficient information exchange in the Internet. A P2P network may exhibit a power-law topology [12] such that it can propagate queries quickly and, if implemented efficiently [16] , it can locate objects in log(n) time, where n is the number of nodes in the network. However, there are remaining problems in the P2P information sharing paradigm which complicate its deployment. Free-riding and the tragedy of the commons are two major problems. As reported in [3] , nearly 70% of Gnutella users do not share any file with others in a P2P community and nearly 50% of all search responses come from the top 1% of content sharing nodes. Therefore, nodes that share information and resources are prone to congestion, leading to the tragedy of the commons [7] . Another problem is that many users intentionally misrepresent their connection speeds so as to discourage others from going to their nodes for file download. Worse yet, Gnutella-like systems give no service differentiation between users who do not share any information with or make any contribution to the P2P community."], "relatedWork": [], "rq": ["1) how to utilize transfer bandwidth resources efficiently?", " 2) how to fairly serve different nodes which have different connection types and contributions in a p2p community?", " 3) how to avoid problems of free-riding and the tragedy of the commons?"]}
{"intro": [], "relatedWork": [], "rq": [" some are concerned about feasibility: is wsd at this level an unattainable goal?", " others with practicality: is this level of detail really needed for most nlp tasks, such as machine translation or question-answering?"]}
{"intro": ["Both dominance and status structure nonverbal behaviour in important ways [20, 12, 15] . From a rich amount of work in social psychology and communication, it is known that several vocalic and kinesic cues [12, 20] are related to dominance and status. For instance, both dominant and high-status people are often more vocally and kinesically expressive than their counterparts, and that both types of people often receive more visual attention. Less clear, however, is whether these cues are correlated in similar amounts with the expression and perception of each construct, and whether automatically extracted cues -likely to be imperfect -would be useful for the prediction of both types of social patterns. This paper addresses two questions. First, can dominance and role-based status in small-group conversations be automatically explained by the same nonverbal cues? While some social psychology literature has found common ground for the nonverbal display and interpretation of both constructs, and recent computational literature has started to investigate models for automatic estimation of dominance [25, 17] or roles [10, 27] in conversations, no attempt has been made to study these two dimensions of social verticality using common data and nonverbal cues together. Second, is it possible to predict these two aspects of verticality from relatively brief observations and using fully automatic nonverbal cues? Although significant evidence in cognitive science support 'thin-slice' explanations for many aspects of social cognition, and such approaches have started to be used with success in computational methods [23] , the question remains open for the two concepts we investigate here."], "relatedWork": [], "rq": ["0] are related to dominance and status. for instance, both dominant and high-status people are often more vocally and kinesically expressive than their counterparts, and that both types of people often receive more visual attention. less clear, however, is whether these cues are correlated in similar amounts with the expression and perception of each construct, and whether automatically extracted cues -likely to be imperfect -would be useful for the prediction of both types of social patterns. this paper addresses two questions. first, can dominance and role-based status in small-group conversations be automatically explained by the same nonverbal cues?"]}
{"intro": ["As computer networks are becoming widespread, the number of distributed services available over networks grows rapidly [1] . However, these services are usually designed for a general purpose, user or situation. In reality, different people have different requirements, therefore they prefer personalized services. The requirements for personalization reflect what the user wants, needs, and likes, all of which may depend on the context at hand (for example, location, physical characteristics of the environment, available resources, people nearby). Personalization also has an evolutional aspect as requirements -demands, needs and preferences-of users change over time. Services have to operate in a constantly evolving environment of people, content, electronic devices, and legacy systems [2] .", "Thus, application functionality provided to users as services should (1) be aligned with the uniqueness of each user's requirements, (2) evolve with changes in these requirements, and (3) take the dynamic context of the user into account. Ideally this would call for tailor-made services; however developing such services from scratch would be economically and technically infeasible. A better approach would be to reuse existing services, configure and compose them to satisfy the unique requirements of each individual user. Service tailoring is a way of creating a new service to satisfy the specific requirements of an individual user. Although service tailoring is an essential feature in any application domain, we focus on homecare application services. Tailorability has been studied extensively in the context of specific technologies and applications [3] [4] [5] [6] [7] , but those approaches are not suitable for homecare domain as homecare services have their own specifications such as high dynamicity of user's requirements and low level of user's technical skills [8] .", "To support well-being, health monitoring and independent living, there is an increasing tendency to provide homecare services for the elderly, especially in developed countries [9] [10] [11] . Several technological challenges concerning homecare applications have been previously studied, but our focus is to address the uniqueness of each elderly person by applying the service tailoring approach. Current homecare systems are generally stand-alone for treating specific diseases, assuming a 'standard' patient and in a static situation [8] . However, in reality each user is unique in the way he or she experiences or is affected by a disease or disability. This is not only because of each individual's mental and physical condition, but also because of the social and physical environment. Current automated homecare systems are often technologydriven. They can be difficult to use by non-technical users and difficult to change or adapt when new requirements arise. The key question therefore is: how can services be tailored to the requirements of the users in this domain, namely, care-receivers and caregivers?"], "relatedWork": [], "rq": [" the key question therefore is: how can services be tailored to the requirements of the users in this domain, namely, care-receivers and caregivers?"]}
{"intro": [], "relatedWork": [], "rq": ["1g is also a cube-ordered chain. figure 8 (c) shows an additional interchange of s-cube halves, resulting in a third cube-ordered chain f0; 1; 3; 5; 7; 14; 15; 12; 11g. the notion of interchanging s-cube halves within a cube-ordered chain is important to an algorithm described later in this section. in the maxport algorithm, for each participating node, say v, the unicasts originating at node v are transmitted on di erent outgoing channels. in this approach, the message is always forwarded to di erent s-cubes; when node v receives the message over channel d, it also receives a list of destination nodes, d, which are in the same d-dimension s-cube as v, say s-cube s. in turn, v issues one unicast into each s-cube within s which (1) does not contain v, (2) is maximal, and (3) contains at least one destination node. as will be shown later, it is possible to input any cubeordered chain, not just a dimension-ordered chain, to maxport and still avoid contention among messages. an ordinary dimension-ordered chain may not be the most appropriate cube-ordered chain to use, however. in fact, performance increase may be gained by exchanging s-cubes of the chain, where possible, so that source nodes (including intermediate source nodes in the multicast tree) always choose the most \\\\crowded\" destination node among available destination nodes. figure 9 shows the proposed weighted sort algorithm, which permutes a cube-ordered chain so that the most \\\\crowded\" node appears as the rst node of each s-cube. this task is accomplished by exchanging s-cube halves (these halves are themselves s-cubes) so that the most populated half occurs rst in the chain. notice that the cube center function is applied to a cube-ordered chain of addresses that are contained within an s-cube of dimension n s . this function returns the starting position of the second (n s ?"]}
{"intro": [], "relatedWork": [], "rq": ["remote difficulty: what is visible?"]}
{"intro": [], "relatedWork": [], "rq": ["1] are a class of attacks wherein memory remanence effects are exploited to extract data from a computer's memory. the idea is that modern computer memories retain data for periods of time after power is removed, so an attacker with physical access to a machine may be able to recover, for example, cryptographic key information. the time during which data is retained can be increased by cooling the memory chips. however, because the memory gradually degrades over time once power is removed, only a noisy version of the data may be recoverable. the question then naturally arises: given a noisy version of a cryptographic key, is it possible to reconstruct the original key?"]}
{"intro": ["Given a connected graph G, the vertex and edge sets and their cardinalities are denoted by V(G), E(G), n and m, respectively. It is assumed that every vertex o is assigned a non-negative real number w(o), called the weight of o, and every edge uv is assigned a positive real number a(uo), the length of uo. The is minimized. The optimal value of r/(X) is often called the p-radius of G. A p-center is any optimal p-set X. If also any point of a network (either on an edge or at a vertex) is allowed to be an element of X, the corresponding problem is referred to as the absolute p-center problem. (Distances are defined as expected.) Clearly, any edge uo with d(u, o) < a(uo) can be deleted without affecting the optimal eccentricity. Therefore we will assume that d(u, o) = a(uo) for every edge uo. Note that we do not assume any other relations between edge lengths. However, some authors define the p-center problem only for complete graphs. In this case they can put d(u, o) := a(uo) whenever the triangle inequality for the lengths is assumed. Both definitions are clearly equivalent as to the p-center problem but not for the absolute p-center one. Therefore we prefer incomplete graphs. Moreover, sometimes a special structure of G can be exploited (see e.g. [17] for trees)."], "relatedWork": [], "rq": [" we raise the following question: is there a polynomial p-approximation algorithm for the unweighted absolute p-center problem for some q < 2?"]}
{"intro": ["Popular approaches to public evaluations include controlled on-the-street studies [6, 25] , observational studies [23, 33] , and steward observation [29] . However, there is limited work reflecting on the bias implications of these approaches. Many investigators decide on approach based on personal experience, preference, and perceptions of reviewer acceptability. Foundational work reflecting on \"in the wild\" methods [for example 3, 11, 20] provides valuable insights, but the field lacks systematic empirical work that investigates the implications of experimenter roles on data collection. Debates about approach focus on how much the experimenter should intervene during evaluation, ranging from overtly creating a controlled experience (for example [6] ) to completely unstewarded experience (for example [35] ). The key question for public evaluation is: how does experimenter intervention distort results? This is a foundational problem throughout science, but public evaluations are particularly fragile with respect to intervention bias."], "relatedWork": [], "rq": [" the key question for public evaluation is: how does experimenter intervention distort results?"]}
{"intro": ["In order to prove this fact about the output of the AFP algorithm (Angluin et al. 1992) , we provide first a new characterization of the GD basis, plus a number of properties related to it; then we provide a new correctness proof of the AFP algorithm, for the particular case of definite Horn targets. From the necessary invariants, we infer that, indeed, the algorithm is constructing the GD basis. This proof (as well as the algorithm) has the additional property that it can be translated into learning sublattices of an arbitrary lattice, so that it may apply to other concept classes that exhibit lattice structure. However, we use Horn CNF here, since this is more standard in the field of query learning theory; the translation to other lattice structures is almost immediate."], "relatedWork": [], "rq": ["2) is able to learn horn cnf with membership and equivalence queries. it was proved in angluin et al. (1992) that the outcome of the algorithm is always equivalent to the target concept. however, the following questions remain open: (1) which of the horn cnf, among the many logically equivalent candidates, is output?"]}
{"intro": ["Some authors indeed argue that books are mereological compounds or complex entities formed by two entities, a volume and a particular kind of content (Arapinis & Vieu 2015; Gotham 2017) . This kind of proposal would answer our question (i) above: when co-predication is possible and when it is not. The response would be that co-predication is possible when the denotation of a polysemous word is a complex entity. However, this is not a completely satisfactory answer, because we still need to know the rationale for including some complex entities in our ontology (while we exclude many others): why is there a complex entity formed by a volume and some informational content, and not a complex entity formed by a balloon and a group of people? As we will see later on, there are some interesting responses in the literature (see especially, Arapinis & Vieu 2015) . Still, there are two pressing problems that affect these kinds of views. We want to present them here briefly to better motivate our own approach.", "One objection raised by Asher (2011) , which has been amply discussed in the literature, concerns whether the alleged dot objects (understood here as the complex objects that dot types are said to denote), can be objects or entities that can be counted. Asher (2011) argues that we can only count informational books or physical books, not pairs formed by physical objects (tomes) and contents (texts). 4 The other pressing objection against the complex entities view relates to the individuation and persistence conditions of the alleged complex entities: it is not clear when one of these complex entities comes in and goes out of existence. In particular, it seems that these alleged complex entities can survive if only one of its constitutive parts survive. However, if this is so (which by itself is problematic), then we would have too many \"survivors\"."], "relatedWork": [], "rq": [": why is there a complex entity formed by a volume and some informational content, and not a complex entity formed by a balloon and a group of people?"]}
{"intro": ["A Historical Note Reviewing the vast literature on neural networks for language is beyond our scope. 4 However, we mention here a few representative studies that focused on analyzing such networks in order to illustrate how recent trends have roots that go back to before the recent deep learning revival. Rumelhart and McClelland (1986) built a feedforward neural network for learning the English past tense and analyzed its performance on a variety of examples and conditions. They were especially concerned with the performance over the course of training, as their goal was to model the past form acquisition in children. They also analyzed a scaled-down version having eight input units and eight output units, which allowed them to describe it exhaustively and examine how certain rules manifest in network weights."], "relatedWork": [], "rq": [" most of the relevant analysis work is concerned with correlation: how correlated are neural network components with linguistic properties?", " what may be lacking is a measure of causation: how does the encoding of linguistic properties affect the system output?"]}
{"intro": [], "relatedWork": [], "rq": ["the question that arises is: why should occasion hold in (7)?"]}
{"intro": [], "relatedWork": [], "rq": [" questions arise about the tools required and communicative processes used to deal with this: how can each collaborator have an equal ability to create, manipulate and view representations?"]}
{"intro": [], "relatedWork": [], "rq": [" discussed above: how can a syntactic operation at some intermediate point of the derivation be driven by the notion of legitimacy at the interfaces if these interfaces access only the final pf and lf representations?"]}
{"intro": [], "relatedWork": [], "rq": [" our first research question was: can cultural authenticity in game design be maintained by presenting children with a narrow perspective of the festival that retains its core message and rituals?"]}
{"intro": ["The concept of argument has long been part of the design rationale body of research in different forms, such as claims in DRL [2] , justifications in RLM [4] , and arguments in IBIS [5] , REMAP [6] , and QOC [7] . The meaning closest to our paper is the one used in the Goal Argumentation Method by Jureta et al. [8] in the context of requirements engineering, who distinguish premises and conclusions in arguments. Nevertheless their approach differs from ours in that they do not employ argumentation schemes, and in that they borrow the concept of defeasible consequence from artificial intelligence [9, p.129 ], which does not consider critical questions. Another related research is the recent work by Yuan and Kelly [10] who do identify argumentation schemes and critical questions in the context of safety requirements. As an example from the field of architectures, the AREL approach [11] , [12] Fig. 1 . Argument from scenario: scheme and some critical questions both qualitative and quantitative design rationale. The former considers, for each design issue, the following parameters: assumptions, constraints, strengths, weaknesses, trade-offs, risks, and nonrisks. The qualitative design rationale considers cost, benefit, implementation risk, and outcome certainty risk. However, the AREL approach does not address the structure of the arguments (or \"assessments\") for explaining why the selected decisions satisfy their associated design concerns."], "relatedWork": [], "rq": [" a possible critical question could be: are the assumptions about real-world phenomena relevant for the obtained conclusion?"]}
{"intro": ["While the existence of some type of identity constraint on VPE is generally agreed upon, the exact nature of this restriction has long been under debate. Much of the 1 VPE is sometimes characterized as post-auxiliary ellipsis, due to examples like Jeff is taller than Mike, and Chris is ____ too, where the elided element may not be a VP (Miller and Pullum 2013) . 2 Throughout, we use identity and syntactic/structural parallelism interchangeably; e.g. (2a) satisfies the identity condition by virtue of the two clauses being structurally parallel, whereas (2b) violates identity because the antecedent and ellipsis clauses are not structurally parallel. We return to the question of exactly how to characterize the parallelism requirement below in Sects. 3 and 5.2. disagreement has centered around two questions. The first is about the generality of the explanation: is the restriction specific to VPE, or does it apply to all sentences/discourses, with VPE simply showing one instance of a much broader phenomenon? The second has to do with level of representation: At what level of linguistic representation (syntax, semantics, information structure, among other possibilities) must the identity constraint hold? While proponents of a structural identity constraint (Hankamer and Sag 1976; Williams 1977; Fiengo and May 1994) have argued for a structural identity condition on the basis of degradation resulting from structural nonidentity, as in (2b), others have argued that elided VPs are proforms, lacking structural content and taking any semantically-matched element as an antecedent (Dalrymple et al. 1991; Hardt 1993 Hardt , 1999 Shieber et al. 1996; Hardt and Romero 2004) . Part of why a unified account of VPE has been elusive is that, as many researchers have noted, the pattern of acceptability judgments is strikingly graded. If the identity constraint is assumed to be categorical (either satisfied or violated, with nothing in between), sentences like (2b) are predicted to be simply ungrammatical. However, not all instances of identity-violating VPE appear to be equal. As noted by Hardt (1993) , Kehler (2000) , Kennedy and Merchant (2000) and Arregui et al. (2006) , among others, the relative weakness or absence of structural mismatch effects in (3) suggests that, at least under certain conditions, strict structural identity may not be required. 3 (3) a. This information could have been released by Gorbachov, but he chose not to release it. (Daniel Shorr, NPR, 10/17/92, from Hardt 1993) b. In March, four fireworks manufacturers asked that the decision be reversed, and on Monday the ICC did reverse it. (from Rosenthal 1988; cited in Dalrymple 1991 , Kehler 2002 ) c. This problem was to have been looked into, but nobody did look into it.", "(from Kehler 2002) In this paper, we present empirical findings that bear on both the generality question and the level of representation question. We show that the acceptability of sentences with VPE is affected by mismatches or clashes at various levels of representation (lexical, syntactic, and discourse), each of which has previously been argued to be the locus of (non-)identity effects for VPE. However, only the syntactic structural mismatch effects are shown to be exclusive to VPE. We construe this as evidence for a VPE-specific structural identity condition, along the lines proposed by e.g. Hankamer and Sag (1976) , though we remain open to alternatives that may superficially resemble structural identity. By delegating different sources of (un)acceptability to their appropriate sources-some general, and some construction-specific-this data sheds light on the generality question, and why there has been so much disagreement among researchers about the basic acceptability data itself."], "relatedWork": [], "rq": [" the first is about the generality of the explanation: is the restriction specific to vpe, or does it apply to all sentences/discourses, with vpe simply showing one instance of a much broader phenomenon?"]}
{"intro": ["In principle, promoting students' learning effectiveness in grades, motivation, and class performance are common goals of TATPs. Therefore, TAs are often required to fulfill multiple functions by both administrative and academic authorities which include leading group discussion, language consulting, assisting in experiments in laboratories, as well as those created for remedial purposes. The fact that TAs are closer in age to their college tutees may make interaction with students different from that of classroom instructors. As what Punyanunt-Carter & Wanger (2005) observed, undergraduate students' perceptions of communicative ways were significantly different between teachers and TAs. However, exactly what types of TA-student interaction in the TATP works more effectively in fostering student satisfaction and learning effectiveness still remain unclear and thus need to be explored. The present study was carried out to examine students' perceptions of English TA tutorial program (ETATP) implementation and TA-student interactive modes. In view of the purpose of this study, two research questions were addressed to guide the study: 1) What are the students' perceptions of the implementation of the ETATP?"], "relatedWork": [], "rq": [" 1) what are the students' perceptions of the implementation of the etatp?"]}
{"intro": ["Decision trees, particularly C4.5 [6] , have been among the more popular algorithms that have been significantly helped by sampling methods for countering the high imbalance in class distributions [3, 4, 7] . In fact, the vast majority of papers in the ICML'03 Workshop on unbalanced Data included C4.5 as the base classifier. While it is understood that sampling generally improves decision tree induction, what is undetermined is the interaction between sampling and how those decision trees are formed. C4.5 [6] and CART [8] are two popular algorithms for decision tree induction; however, their corresponding splitting criteria -information gain and the Gini measure -are considered to be skew sensitive [9] . It is because of this specific sensitivity to class imbalance that use of sampling methods prior to decision tree induction has become a de facto standard in the literature. The sampling methods alter the original class distribution, driving the bias towards the minority or positive class 1 . Dietterich, Kearns, and Mansour [10] suggested an improved splitting criterion for a top down decision tree induction, now known as DKM. Various authors have implemented DKM as a decision tree splitting criterion and shown its improved performance on unbalanced datasets [9, 11, 12] . However, DKM has also been shown to be (weakly) skew-insensitive [9, 11] ."], "relatedWork": [], "rq": [" one question at this stage is: how much do the different decision trees benefit from sampling when compared to their respective baseline performances?"]}
{"intro": ["Scholars in the (Digital) humanities are active and motivated annotators [24] . They annotate all types of media at any level, with many layers of interpretation. Unsworth [21] identified annotating as one of the \"scholarly primitives.\" In the case of audio-visual (AV) or time-based media, manual or semi-automatic annotation of the media content is essential, given the fact that providing fully automated access is more challenging than to textual resources [15] . Scholarly work is also often described as a process [11] , where different stages occur over time (e.g., [4] ). Providing support to scholarly research requires the analysis of the complex research tasks where knowledge construction is involved. These are not limited to searching and retrieving a list of results, but also other series of scholarly primitives, e.g. classifying, linking [20] , comparing, sampling, illustrating, annotating [21] , or writing and collaborating [18] . However, little is known about how those complex tasks are performed in the context of scholarly research where AV media is the focal point, for instance, in media and communication studies. In addition, there is a lack of system support for the different Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. methods that this group of researchers uses in different research phases [9] . To understand how annotation should be supported along the research process of media scholars, different investigations are conducted within the CLARIAH project 1 , with a focus on research behavior, activities, models, and \"tool\" analyses. As part of these investigations, this paper presents two user studies about scholars' research processes, with a particular focus on the types of annotation-related activities. The research problem above results in the following research question: How is annotation of time-based media done in practice by media scholars, and other scholars who make intensive use of AV media, and in which stages of their research process is it used and how?"], "relatedWork": [], "rq": [" the research problem above results in the following research question: how is annotation of time-based media done in practice by media scholars, and other scholars who make intensive use of av media, and in which stages of their research process is it used and how?"]}
{"intro": ["According to [9] , the general version played on a (general) n \u00d7 n board is also known as the Queen Domination Problem, or Board Covering Problem [26] . The task is to find the minimum number of queens that can be placed on a general chessboard so that each square contains a queen or is attacked by one. This amounts to finding the minimum domination number \u03b3 (Q n ) of the general queen chessboard graph Q n . Recent bounds on the corresponding domination number can be found in [7, 8, 10, 25] . It appears that the queen domination number is ''approximately'' n/2, but only few exact values have been found up to today. In fact, the exact characterisation of this number sequence \u03b3 (Q n ) is listed as Problem C18 within Guy's book on ''Unsolved Problems in Number Theory'', see [19] . It is known [16, 25] that \u03b3 (Q n ) \u2265 n/2 for all positive integers n except for n = 3, 11. Regarding upper bounds, the best asymptotic bound appears to be \u03b3 (Q n ) \u2264 69n/133 + O (1) . However, the O(1)-constant involved makes it hard to use this upper bound for determining concrete values of \u03b3 (Q n ). Notice that the mentioned asymptotic upper bounds are essentially derived by ''small'' concrete patterns for boards Q that have to be repeated somehow in order to obtain domination patterns for Q c . Solutions for graphs Q n with n not being a multiple of are derived from the largest c < n, by filling up the ''remaining'' n \u2212 c rows with queens. So, the O(1)-term could be as large as 100, possibly growing further when new (and larger) patterns are discovered in order to further improve on the asymptotic estimate for \u03b3 (Q n ). Problem Variants. By way of contrast, observe that on the n \u00d7 n square beehive, the queen domination number has been established to be (2n + 1)/3 , see [30] . It is clear that similar domination-type problems may be created from other chess pieces as rooks, bishops, or knights (or even artificial chess pieces), see [17] as an example. Also the variant that only certain (predetermined) squares need to be dominated has been considered. For example, the n \u00d7 n Rook Domination Problem is trivially solvable by choosing one main diagonal and putting n rooks on this diagonal.", "By pigeon-hole, any trial to dominate all squares by fewer than n rooks must fail. However, if only a certain number of predetermined squares need to be dominated by placing a minimum number of rooks on one of those predetermined squares, we arrive at a problem also known as matrix domination ; see [15] for further results on this problem. Further variants of the Queen domination problem are discussed in [10] . A related problem on the chessboard is the following one, originally introduced in 1850 by Carl Friedrich Gau\u00df, see pp. 19-21 in [18] : the n-Queens Problem. This problem may be stated as follows: find a placement of n queens on an n \u00d7 n chessboard, such that no queen can be taken by any other, see [27] . Again, this problem can be also considered for other chess pieces and modified boards, see (e.g.) [3] as a recent reference. If (for the moment) we neglect the well-known property that in fact a solution exists to the n-Queens Problem for each n (but n = 2, 3), then the n-Queens Problem corresponds to the task of solving maximum independent set on the queen chessboard graph. In fact, this approach was taken in [24] , where an integer linear programming formulation was presented. However, there are quite simple algorithms that yield solutions for that problem for any n, as explained by the Wikipedia entry: http://en.wikipedia.org/wiki/Eight_queens_problem. A nice treatment of both the independence and the domination problem can be also found in http://mathworld.wolfram.com/QueensProblem.html."], "relatedWork": [], "rq": ["33 can be covered by putting a queen on it. the same bound is true for the -diagonals. however, no improvement shows up for the rows and columns; by elementary combinatorial properties of binomial coefficients, the worst case is assumed when trying to arrange n/2 queens on the board. notice that the bound 69n/133 is purely combinatorial; if the actual bound could be shown to be n/2 (up to some additive constant), then the exponential base would further improve to 37.93 (instead of 39.51), again based on the combinatorial estimate for the diagonals. can we further improve on this algorithm that is based on dynamic programming on subsets?", " : is there an easy computational procedure to solve minimum dominating set of queens?"]}
{"intro": [], "relatedWork": [], "rq": [" we address in this paper the following question: how many causal logs are needed to robustly emulate a write and a read operation of a shared memory over a crash-recovery message passing system?"]}
{"intro": [], "relatedWork": [], "rq": ["5] is cographic, our submatroid (v 2 , f 2 ) is not cographic in general. moreover, \u03c1(v 3 , f ) can be smaller than \u03c1(v 2 , f 2 ); see figure 9 . therefore, we cannot directly use the value \u03bd(v 3 , f ) computed by the faster algorithm by gabow and stallmann. problem 1. can the i2cs problem on graphs of maximum degree 3 be efficiently reduced to the cographic matroid parity problem?"]}
{"intro": [], "relatedWork": [], "rq": [" one must briefl y examine a question which has generated a great deal of debate among researchers: what is a topic?"]}
{"intro": [], "relatedWork": [], "rq": ["3. can we manage passer-by attention through power transitions?", "1. how can sign owners determine the correct content schedule and duty-cycle for a display?", "2. what types of content should be scheduled to coincide with transitions?"]}
{"intro": ["HCI has begun to address the design of digital technologies for justice [17, 20] in a number of different settings such as street or workplace harassment [5, 15] , and the potentials of anti-oppressive design [56] . There has also been a movement in the literature towards topics of sexuality [60] , pornography [55, 69] , and sex work [59, 61] . This paper sits within these converging literatures, as well as alongside sex work research from other disciplines, to build a nuanced understanding of the ways in which digital technologies can be used alongside other forms of service delivery to advance and promote social justice. We premise our understanding of sex work from the communities that engage in it and build on existing literatures (eg. [1, 19, 44] ) that recognize sex work as a type of labour that should not be criminalized, but rather protected by labour and other relevant laws that promote human rights. Carol Leigh, feminist and sex worker rights activist who coined the term 'sex work' in 1987, explains that the term \"acknowledges the work we do rather than defines us by our status [as a sex worker]\". Motivated by her \"desire to reconcile [her] feminist goals with the reality of [her] life and the lives of the women [she] knew\", her activism worked to create an \"atmosphere of tolerance within and outside the women's movement for women working in the sex industry\" [40] . In its current context however, the term sex work is used to refer to an activity practiced by people of all genders. In this paper, we reflect on the use of digital technologies for service delivery within a peer-led sex worker rights organisation called Stella, l'amie de Maimie. After an overview of the organisation, we focus our discussions on the Bad Client and Aggressor List, which is central to their services. This tool was, and continues to be developed, through peer reporting and aims to provide information for sex workers in Montr\u00e9al (and to a certain extent in wider Quebec) about potentially dangerous individuals. The contributions of this paper are twofold: (1) we contribute to the growing debate around using HCI for social justice. While there have been various interpretations of this, there has yet to be an analysis of the ways in which digital technologies could facilitate engagement with alternative narratives of justice, particularly in settings where workers may be criminalized. (2) To address this gap, we provide implications for design framed in Fraser's idea of multidimensional and 'abnormal' justice that will support the development of digital technologies for settings where restorative justice may be prioritized. This is a particularly timely contribution based on current political, social, and criminal justice debates at national and international scales related to wider issues of nationalism, racism, or the prisonindustrial complex. First, we contextualize our work in HCI literatures, Canadian legal structures, and Stella's organizational practice. Second, we describe our methods and outline how service delivery relates to restorative justice. Third, we develop three implications for design aimed at researchers seeking to develop technologies that supports service delivery with and groups that are stigmatized or criminalized."], "relatedWork": [], "rq": [" poses to build a more just world: what is justice?"]}
{"intro": [], "relatedWork": [], "rq": [" fabian: what about the man you fought?"]}
{"intro": [], "relatedWork": [], "rq": [" we consider an important question: why do we use parameterized curves to represent boundaries or regions?"]}
{"intro": [], "relatedWork": [], "rq": [" other questions lead to future research questions: what kinds of interactions allow participants to buy into the illusion and which break this illusion?"]}
{"intro": [], "relatedWork": [], "rq": ["recent automatic evaluation metrics typically frame the evaluation problem as a comparison task: how similar is the machine-produced output to a set of human-produced reference translations for the same source text?"]}
{"intro": [], "relatedWork": [], "rq": ["5] is simple and irreducible, thus s = 3 is possible. is s = 2 possible?"]}
{"intro": [], "relatedWork": [], "rq": ["one common issue with all these strategies is: how to select the right one?"]}
{"intro": ["In this case, video-conferencing technology is employed so that learners, unimpeded by distance or grade level, can interact and participate in synchronous, real-time, virtual educational events. Furthermore, if recorded and then used asynchronously for future reflection, this kind of learning is denoted by \"the [learner's] ability to move backward and forward through different stages of a [learning event]\" (Cuthell, 2006, p. 101) , whereby the learner can further evaluate and reflect upon the event's various components. However, the difficulties of gaining skills in effective pedagogy and associated organizational practices for successful video-conference-based learning may seem significant to those teachers who are neither personally comfortable with nor professionally convinced of the related benefits of ICT-enabled learning. These areas of concern would undoubtedly include the considerable material preparation and organizational time spent outside the classroom environment prior to the learning events themselves. Such perceptions may well induce the technological inertia for which some school systems have long been known. In fact, Cuthell (2006) views schools as supertankers, where \"a change of direction requires a considerable amount of forward planning before it takes effect\" (Cuthell, 2006, abstract) . Such forward planning is demonstrated in this paper, as it includes the pre-event organizational aspects, as well as the carefully planned pedagogy that ultimately culminates in the video-conference-based learning events."], "relatedWork": [], "rq": ["conclusions: what then is effective pedagogic practice for mobile, ict-enabled learning?"]}
{"intro": ["Code-sharing sites like GitHub hold the promise of documenting all the common and uncommon ways of using an API in practice, including many alternative usage scenarios that are not typically shown in curated examples. However, given the large amount of code available online, it is challenging for developers to efficiently browse the enormous volume of search results. It is certainly infeasible for developers to examine more than a few code examples simultaneously. In practice, programmers often investigate a handful of search results and return to their own code due to limited time and attention [3, 23, 5] . Prior work has shown that individual code examples may suffer from API usage violations [29] , insecure coding practices [7] , unchecked obsolete usage [31] , and comprehension difficulties [25] . Therefore, inspecting a few examples may leave out critical safety checks or desirable usage scenarios.", "In the software engineering community, there is a growing interest in leveraging a large collection of open source repositories-so called Big Code-to automatically infer API usage patterns from massive corpora [4, 16, 26, 30] . However, these API usage mining techniques provide limited support to help programmers explore concrete code examples from which API usage patterns are inferred, and understand the commonalities and variances across different uses. To bridge the gap, we aim to visualize hundreds of concrete code examples mined from massive code corpora in a way that reveals their commonalities and variances, and design a navigation model to guide the exploration of these examples. We draw motivation from prior work on visualizing large corpora of related documents, e.g., student coding assignments [8] , text [27, 21] , and image manipulation tutorials [17] , to pose the following research question: How might we extract, align, canonicalize, and display large numbers of usage examples for a given API?"], "relatedWork": [], "rq": [" to pose the following research question: how might we extract, align, canonicalize, and display large numbers of usage examples for a given api?"]}
{"intro": [], "relatedWork": [], "rq": ["2, is in evaluating the potential of temporal adverbials to indicate topic shifts. they use two markers of topic shift they consider reliable -paragraph break and a break in lexical cohesion -to evaluate the segmentation potential of certain lexical expressions, here of temporal adverbials. the observation that sentences containing temporal adverbials have a much higher chance of starting a paragraph and that this chance increases when the temporal adverbial occurs in sentence-initial position leads them to the conclusion that such adverbials are good segmentation markers and therefore can reliably be associated with topic shifts. we will comment later on the assumed link between discontinuity and topic shift. for now, our focus is on the following question: are temporal adverbials intrinsically good discontinuity markers, or do they simply \"ride\" on the segmentation potential of the paragraph break?"]}
{"intro": ["T RADITIONALLY, high-performance computing (HPC) users execute scientific applications on dedicated HPC clusters hosted by national laboratories, companies or universities, typically managed through some kind of block allocation or grant mechanism. However, recently the use of cloud resources to execute HPC applications is becoming a popular alternative, due to factors such as machine availability and lower wait queue time. Success stories of scientific applications at HPC scale on the cloud have appeared in the popular press [20] . Unlike standard HPC clusters, however, cloud resources come with variable usage costs for individual users. Cloud resource providers, such as Amazon EC2, offer several pay-as-you-go offerings for purchasing cloud resources, which presents a complex optimization problem: What is the most cost effective strategy to execute a given high-performance computing application?"], "relatedWork": [], "rq": [" which presents a complex optimization problem: what is the most cost effective strategy to execute a given high-performance computing application?"]}
{"intro": ["One of the most immersive ways of interaction with a VE is being able to physically walk in it [Usoh et al. 1999] . However, the challenge in real walking arises when the VE is larger than the physical space. [Razzaque et al. 2001] were the first to define RDW techniques as one method to overcome this challenge. RDW techniques work based on the fact that when dealing with conflicting visual, vestibular and proprioceptive cues, humans tend to trust their visual cues, and therefore their perception can be \"manipulated\" through the Head Mounted Display (HMD). Using RDW techniques, the one-to-one mapping between the real world and the virtual world can be scaled by 3 types of gain: translational, rotational and curvature gain. However, there is a limit to these gains, identified first by [Razzaque 2005 ] using one-up/one-down staircase method for its time efficiency. [Steinicke et al. 2008 ] later identified these gains using the constant stimulus method. [Bruder et al. 2012 ] performed a similar set of experiments and found that while rotational and translational gain thresholds approximate the results from [Steinicke et al. 2008] , there is difference in the curvature gain threshold, and discussed that it could be due to a different VR setup or subject groups. The difference in thresholds obtained * e-mail: nngoc@ethz.ch \u2020 email: yannick.rothacher@usz.ch Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for thirdparty components of this work must be honored. For all other uses, contact the owner/author(s). c 2016 Copyright held by the owner/author(s). VRST '16, November 02-04, 2016 , Garching bei M\u00fcnchen, Germany ISBN: 978-1-4503-4491-3/16/11 DOI: http://dx.doi.org/10.1145 in these tests raises the question: What are the factors that influence a person's RDTs? Two fundamental types of factors are proposed: intrinsic and extrinsic. Intrinsic factors are factors specific to a person's perception such as visual dependence, how sensitive they are to visual-vestibular conflict or internal body changes, etc. Extrinsic factors include walking velocity [Neth et al. 2012] , environment setup, cognitive load, etc. In the scope of this paper, we focus mainly on the intrinsic factors: visual dependence, susceptibility to visual-vestibular conflict and interoception. Visual dependence is a measure of how much a person depends on visual information during sensory integration. Here, we are particularly interested in how much weighting subjects give to visual cues as compared to vestibular and proprioceptive cues when performing actions critical to the walking process such as estimating their spatial orientation or maintaining their balance. A high visual weighting could imply higher RDTs since the user could be more susceptible to visual \"manipulation\". The so-called rod and frame test by [Witkin and Asch 1948] is one of the established tests measuring the dominance of visual over gravitational vestibular cues. In the test, a rod in the middle of a tilted square frame is presented, and the subjects are asked to rotate the rod until they think it is vertical. People are normally influenced by the visual sensation of the tilted frame, and thus deviating from the true vertical. Another visual dependence test is Romberg's test [Romberg 1853 ], involving the three sensory modalities: visual, proprioceptive and vestibular. For the test, subjects' deviations of the center of pressure (COP) while standing are measured with eyes open and eye closed and the ratio between these are computed. This ratio signifies how much the user relies on visual information for maintaining postural balance. Therefore, we hypothesize that a high Romberg ratio could correlate to higher RDTs. When there is conflict between visual and other sensory cues, an illusion of self-motion could occur, which is called vection. Traditionally, a vection test is performed in an optokinetic drum where a subject sits statically in the middle of a rotating drum with regularly illuminated stripes. Once the subject feels self-motion, the time will be recorded as on-set vection. Since a similar type of sensory conflict (visual-vestibular-proprioceptive) happens in RDW, we hypothesize that a measure of vection -susceptibility to visualvestibular conflict -could correlate positively with RDTs. Interoception refers to a person's sensitivity to internal body changes. Interoception tests are normally cardiac-based procedures: in the commonly used heartbeat detection task, subjects are asked to identify whether a series of sounds is in sync or not with their current heartbeats. Interoception was suggested by [Faivre et al. 2015 ] to have mutual influences on vision, and we hypothesize that the more aware a person is of his/her interoceptive cues, the less he/she relies on visual cues in case of conflict, and therefore will have lower RDTs. Despite the wide use of these aforementioned tests of perception traits in clinical and neurological research, to our knowledge, none has been used in the context of RDW."], "relatedWork": [], "rq": [" in these tests raises the question: what are the factors that influence a person\\'s rdts?"]}
{"intro": ["Compared to the rich set of available information divergences, there is little research on how to select the best one for a given application. This is an important issue because the performance of a given divergence-based estimation or modeling method in a particular task very much depends on the divergence used. Formulating a learning task in a family of divergences greatly increases the flexibility to handle different types of noise in data. For example, euclidean distance is suitable for data with Gaussian noise; Kullback-Leibler (KL) divergence has shown success for finding topics in text documents [7] ; and Itakura-Saito divergence has proven to be suitable for audio signal processing [21] . A conventional workaround is to select among a finite number of candidate divergences using a validation set. This however cannot be applied to divergences that are nonseparable over tensor entries. The validation approach is also problematic for tasks where all data are needed for learning, for example, cluster analysis.", "In Section 3, we propose a new method of statistical learning for selecting the best divergence among the four popular parametric families in any given data modeling task. Our starting-point is the Tweedie distribution [22] , which is known to have a relationship with b-divergence [23] , [24] . The Maximum Tweedie Likelihood (MTL) is in principle a disciplined and straightforward method for choosing the optimal b value. However, in order for this to be feasible in practice, two shortcomings with the MTL method have to be overcome: 1) Tweedie distribution is not defined for all b; 2) calculation of Tweedie likelihood is complicated and prone to numerical problems for large b. To overcome these drawbacks, we propose here a novel distribution using an exponential over the b-divergence with a specific augmentation term. The new distribution has the following nice properties: 1) it is close to the Tweedie distribution, especially at four important special cases; 2) it exists for all b 2 R; 3) its likelihood can be calculated by standard statistical software. We call the new density the Exponential Divergence with Augmentation (EDA). EDA is a non-normalized density, i.e., its likelihood includes a normalizing constant which is not analytically available. But, since the density is univariate the normalizing constant can be efficiently and accurately estimated by numerical integration. The method of Maximizing the Exponential Divergence with Augmentation Likelihood (MEDAL) thus gives a more robust b selection in a wider range than MTL. b estimation on EDA can also be carried out using parameter estimation methods, e.g., Score Matching (SM) [25] , specifically proposed for non-normalized densities. In the experiments section, we show that SM on EDA also performs as accurately as MEDAL."], "relatedWork": [], "rq": [" practitioners must face a choice problem: how to select the best divergence in a family?"]}
{"intro": ["To circumvent those impossibility results, the probabilistic path was taken by Yamauchi and Yamashita [13] . The robots are oblivious, operate in the most general ASYNC model, and can form any general pattern from any general initial configuration (with at least n \u2265 5 robots), without assuming a common coordinate system. However, their approach [13] makes use of three hypotheses that are not proved to be necessary: (i) all robots share a common chirality, (ii) a robot may not make an arbitrary long pause while moving (more precisely, it cannot be observed twice at the same position by the same robot in two different Look-Compute-Move cycles while it is moving), and (iii) infinitely many random bit are required (a robot requests a point chosen uniformly at random in a continuous segment) anytime access to a random source is performed. While the latter two are of more theoretical interest, the first one is intriguing, as a common chirality was also used extensively in the deterministic case. The following natural open question raises: is a common chirality a necessary requirement for mobile robot general pattern formation ? As the answer is yes in the deterministic [6] case, we concentrate on the probabilistic case. Our contribution. In this paper, we propose a new probabilistic pattern formation algorithm for oblivious mobile robots that operate in the ASYNC model. Unlike previous work, our algorithm makes no assumptions about the local coordinate systems of robots (they do not share a common \"North\" nor a common \"Right\"), yet it preserves the ability from any initial configuration that contains at least 5 robots to form any general pattern (and not just patterns such that \u03c1(I) divides \u03c1(F)). Besides relieving the chirality assumption, our proposal also gets rid of the previous assumption [13] that robots do not pause while moving (so, they really are fully asynchronous), and the amount of randomness is kept low -a single random bit per robot is used per use of the random source -(vs. infinitely many previously [13] ). Our protocol consists in the combination of several phases, including a deterministic pattern formation one. As the deterministic phase does not use chirality, it may be of independent interest in the deterministic context."], "relatedWork": [], "rq": [" the following natural open question raises: is a common chirality a necessary requirement for mobile robot general pattern formation ?"]}
{"intro": [], "relatedWork": [], "rq": [" this paper therefore aims to add to the previous literature by focusing on this neglected yet fundamental issue: what is the uncertainty in the values inferred by the statistical methods we routinely employ to identify the targets of structural alignments?"]}
{"intro": ["Evolved from a mere catalog tool, the web now has become a platform for electronic commerce, social networking, entertainment, and much more; however, more aggressive risks appear with increasing web functionalities (Waldow & Gorelik, 2009 (WhiteHat Security, 2011) , the average website has thirteen serious unresolved vulnerabilities. The worst industries are Information Technology, Retail, and Education with an average of 24, 17, and 17 serious vulnerabilities per website, respectively. With these alarming numbers of vulnerabilities, end users may face the danger of disclosing their credit card number, Material published as part of this publication, either on-line or in print, is copyrighted by the Informing Science Institute. Permission to make digital or paper copy of part or all of these works for personal or classroom use is granted without fee provided that the copies are not made or distributed for profit or commercial advantage AND that copies 1) bear this notice in full and 2) give the full citation on the first page. It is permissible to abstract these works so long as credit is given. To copy in all other cases or to republish or to post on a server or to redistribute to lists requires specific permission and payment of a fee. Contact Publisher@InformingScience.org to request redistribution permission. social security number, personal health information, and other private information. To prevent potential losses, end users should actively seek information about web risks to learn how to cope with them. In this paper, risk is defined as things, forces, or circumstances that pose danger to people or to what they value (Stern & Fineberg, 1996) .", "Much research has been conducted on end user security. However, there are several gaps that need to be addressed. First, there is a paucity of research on users' web risk information seeking. Researchers often focus on attitude towards the risks or protective intention, but this stream of research pays little attention to the mechanism that occurs during users' risk information seeking process. Second, most research has targeted traditional security risks such as spyware or viruses in email attachments (Dinev & Hu, 2007; Ng, Kankanhalli, & Xu, 2009) . New emerging web risks have not been investigated. Crimes on the web today are very different from the traditional network attacks. The browsers have become increasingly complex with access to more powerful scripting tools and external plug-ins, attracting more stealthy attacks (Provos, Rajab, & Mavrommatis, 2009 )."], "relatedWork": [], "rq": ["0, how much information would be sufficient for you?"]}
{"intro": [], "relatedWork": [], "rq": ["q2. are iranian efl teachers literate considering online interactions (cmc tools)?"]}
{"intro": ["Within computational linguistics, most work has focussed on sentiment and opinion concerning specific entities or events, and on binary classifications of these. For instance, both Pang and Lee (2002) and Turney (2002) consider the thumbs up/thumbs down decision: is a film review positive or negative? However, Pang and Lee (2005) point out that ranking items or comparing reviews will benefit from finer-grained classifications, over multiple ordered classes: is a film review two-or three-or four-star? And at the same time, some work now considers longer-term affective states. For example, Mishne (2005) aims to classify the primary mood of weblog postings; the study encompasses both fine-grained (but non-ordered) multiple classification (frustrated/loved/etc.) and coarse-grained binary classification (active/passive, positive/negative)."], "relatedWork": [], "rq": ["thumbs down decision: is a film review positive or negative?", " over multiple ordered classes: is a film review two-or three-or four-star?"]}
{"intro": ["Infrastructure-as-a-Service is quickly becoming a ubiquitous model for providing elastic compute capacity to customers who can access resources in a pay-as-you-go manner without long-term commitments, with rapid scaling (up or down) as needed [1] . Cloud service providers (Amazon AWS [2] , Google Compute Engine [3] , and Microsoft Azure [4] ) allow customers to quickly deploy their services without a large initial infrastructure investment. Proliferation of Smaller-scale Clouds. However, there are some non-trivial concerns in obtaining service from large-scale public clouds, including cost and complexity. Massive cloud environments can be costly and inefficient for some customers (e.g., Blippex [5] ), thus resulting in more and more customers building their own smaller-scale clouds (SCs) [6] for better control of resource usage; for example, it is hard to guarantee network performance in large-scale public clouds due to their multi-tenant environments [7] . Moreover, smaller-scale providers exhibit greater flexibility in customizing services for their users, while large-scale public providers minimize their management overhead by simplifying their services; e.g., Linode [8] distinguishes itself by providing clients with easier and more flexible service customization. The use of SCs is one approach to solve cost and complexity issues.", "Despite the potential of SCs, they are likely to suffer from resource under-provisioning during peak demand, which can lead to inability to satisfy service level agreements (SLAs) and consequent loss of customers. SLAs come in many forms, such as the average or maximum waiting time before being served, the probability of requests being rejected, and the amount of resources that each request can obtain. In order not to resort, similarly to large-scale providers, to resource over-provisioning, with all its disadvantages, one approach to realizing the benefits of SCs is to adopt hybrid architectures [9] , [10] that allow private clouds (or small cloud providers) to outsource their requests to larger-scale public providers. However, the use of public clouds can potentially be costly for the small-scale provider.", "However, many of these efforts assume the existence of the cloud federation and largely focus on designing sharing policies in order to maximize the profit of individual SCs [12] , [13] , [20] , [21] . For example, [13] proposes a strategy to terminate less profitable spot instances, in order to accommodate more profitable on-demand VM requests. Moreover, most works do not consider the trade-off between economical benefits (in terms of profit) and performance degradation for individual SCs, which is a significant factor in incentivizing SCs to participate in the cloud federation. Without the analysis of performance degradation due to resource sharing, the feasibility of a federation can be questioned. While [14] studies a federation formation game among cloud providers based on revenue, it only considers a special scenario where all cloud providers share all of their resources with others. In contrast, our work focuses on the fundamental, unanswered question of \"how each SCs should share its resources to be profitable, satisfy customer SLAs, and also motivate other SCs to join the federation.\" Problem Description. We consider an environment with multiple SCs providing on-demand VM instances; an example with 3 SCs is depicted in Fig. 1 . In this work, we also refer to SCs sharing resources with each other as a federation. Each SC has its own SLAs with customers: the maximum waiting time before service of a request is initiated. To satisfy SLAs, SCs use public clouds as a \"backup,\" i.e., they buy additional resources on-demand from large-scale public clouds, when in danger of not being able to meet SLAs. If such SCs form a federation, when an SC exhausts its own resources, it can use resources shared by other SCs at a price lower than that of public clouds. The amount of shared resources directly affects how much workload the federation is able to handle, which in turn affects the profit that each SC is able to achieve. In this sharing scenario, an important question is: Should SCs participate in the federation? If so, how many resources should each SC share? If an SC is too generous (i.e., shares too many of its resources), then it may be in danger of not being able to serve its own workload, resulting in more requests being forwarded to public clouds, thereby reducing profit margins. As a result, an SC should determine the amount of shared resources based on the price of selling and buying resources, i.e., the net profit, compared with the cost of using public clouds. However, if an SC is too selfish, i.e., shares few of its resources for higher profit, then it may get removed from the federation for not being a useful contributor, or the federation may fall apart if most/all SCs tend towards selfish behavior. Thus, another critical question that needs to be addressed is: What prices can make each SC share a reasonable amount of resources so that all SCs will participate in the federation?"], "relatedWork": ["We give an overview of efforts related to ours and highlight the relevant differences. Works on hybrid clouds [9] , [10] are related as they allow private (or smaller-scale) clouds to outsource their requests to large-scale public providers. However, since that can potentially be costly for a smallscale provider, our work differs in that it focuses on a sharing framework, while minimizing cost of using public clouds."], "rq": [" another critical question that needs to be addressed is: what prices can make each sc share a reasonable amount of resources so that all scs will participate in the federation?"]}
{"intro": [". Unfortunately, such a problem without additional constraints is ill-posed, and provided \"answers\" nonsense. Often ridiculously complicated [10] . This is not a surprise, as Cardinality of real/complex transcendental numbers is uncountably infinite (continuum c), while number of formulas and symbols is countable (\u2135 0 ). Therefore probability for randomly chosen real number to be equivalent to some formula is zero. We must therefore restrict to numbers with finite decimal expansion. This is also practical approach, due to widespread of floating-point hardware, with double precision [11] being de facto standard. However, in this case we encounter infinity from the other side. Assuming floating-point constant is a truncated real number 1 , E-mail: andrzej.odrzywolek@uj.edu.pl 1. If it is not, then it is a rational number, e.g 1.82263 = 182263/100000."], "relatedWork": [], "rq": [" important question arises: how many constants, functions and binary operations are required for our virtual calculator to be still fully operational?"]}
{"intro": [], "relatedWork": [], "rq": [" the main question needs to be solved is: how many consumption channels should we provide in a given system?"]}
{"intro": [], "relatedWork": [], "rq": [" this decision is often determined by reflecting on several questions: what to share?"]}
{"intro": [], "relatedWork": [], "rq": ["rq3: is the du map useful for choosing ea parameters or components?"]}
{"intro": [], "relatedWork": [], "rq": [" these distinctions are at the center of the motivation for this paper: how are technological training grounds used by these recreational groups, and how can they be designed to best train people to respond to the life or death situations inherent in their practices?"]}
{"intro": ["The routing problem is defined as follows: Given a specific network and a set of packets of information (a packet being an origin, destination pair), these packets must be routed in parallel to their own destinations such that at most one packet passes through any link of the network at any time and all the packets arrive at their destinations as quickly as possible. To start with, the packets are placed in their origins, one per node. We are interested in a special case of the general routing problem called permutation routing in which the destinations form some permutation of the origins. A routing algorithm is said to be oblivious if the path taken by each packet is only dependent on its source and destination. An oblivious routing strategy is preferable since it will lead to a simple control structure for the individual processing elements. Also oblivious routing algorithms can be used in a distributed environment. In this paper we are concerned with only oblivious routing strategies. Both deterministic and randomized schemes have been studied in solving routing problems ( [27] [17] , [15] .) However, most of the past work has focused on bounded degree networks, such as cube-connected cycles (CCC), butterfly, shuffle-exchange, the mesh, etc. Some research work has also been done on a binary n-cube (hypercube) which is not a bounded degree network. All of these networks (except the mesh) have logarithmic diameter and have randomized routing algorithms that run in logarithmic time. Clearly, these algorithms are optimal. An interesting open question is: 'Can we do optimal routing on a network with sublogarithmic diameter?' In this paper we settle this question in the affirmative. In particular, we present optimal randomized oblivious routing algorithms for the star graph ( [1, 2] ) which has sublogarithmic diameter."], "relatedWork": [], "rq": [" an important open question is: is there a networkindependent routing algorithm that works for a large class of networks, rather than a specific network?"]}
{"intro": ["At the beginning of our design process, we conducted a codesign session with the research team and four teens to introduce the philosophy of Cooperative Inquiry and a few foundational co-design techniques to those who had never participated in intergenerational co-design (\"Preliminaries,\" Table 1 ). Researchers and designers new to co-design often carry misconceptions with them (e.g., adults cannot add ideas, or need to teach children first), so participating in a complete, intergenerational co-design session is key [37] . What we learned about techniques. Our teen co-designers responded readily to brainstorming with sticky notes. For the Bags-of-Stuff technique (Table 1) , however, we found that the teens preferred to share their ideas in a focus group and interview format, rather than devising low-tech prototypes with arts-and-crafts materials. In prior research, low-tech prototyping with Bags-of-Stuff has had mixed results with teens [10, 82, 86] . We build upon these studies by noting that when the teens followed a peer interview-like process, the volume of ideas they generated increased dramatically (two low-tech prototypes with Bags-of-Stuff versus 30-40 ideas with peer interviews). One reason why Bags-of-Stuff may not have worked as well with teens as it does with younger children may be developmental. Prior research has shown that teens may have trouble with openended, hypothetical or \"blue-sky\" situations, as they are still learning to build their own opinions distinct from others [65, 77] . Interestingly, our teens' preference for discussion and interview Q&A contrasts with the difficulties Isomursu et al. [41, 42] faced when they interviewed girls (10-16 years old) for mobile app design ideas. We may have had more success due to our Cooperative Inquiry co-design session format. First, we introduced co-design by inviting all participants to engage in an interactive activity that promoted collaborative problem solving and emphasized equal design partnering between teens and adults. We then held our typical round-robin QoD with everyone sitting on the floor, to physically re-emphasize a balanced power dynamic among co-designers, regardless of age. During our design activities (Table 1) , all teams worked in a large space on the floor. The arrangement of co-design space is an important consideration to mitigate power structure concerns [20, 22] , and our format and approach to co-design sessions was quite different from Isomursu et al. [42] ."], "relatedWork": [], "rq": [" a few of these questions include: can co-design techniques used with younger children be used effectively with teens?"]}
{"intro": ["The Internet of Things (IoT) is considered the next evolution in the era of computing after the Internet. 1, 2 In the IoT paradigm, many objects around us will be connected in networks and will communicate with each other without any assistance or human intervention. IoT devices implement different types of services for applications related to smart health, smart farming, smart city, among others. Normally, these services use a middleware layer that can act as a messaging service, receiving information, storing, and processing it, in addition to making automatic decisions based on the environment status and historical data. 3 Often, brokers or intermediate nodes (also named in some cases as superpeers) are used in this layer to enable communication among applications and things in a publish/subscribe (Pub/Sub) fashion. 4 Pub/Sub is a communication paradigm for asynchronous data dissemination among services and users. 5 In many Pub/Sub systems, clients are able to register subscriptions in a particular broker. In turn, brokers receive post messages from publishers and perform the filtering according to the subscriptions. 6, 7 As an indirect communication pattern, 8 where publishers and subscribers sometimes do not have knowledge of each other, Pub/Sub is useful to implement several IoT scenarios and applications. A smart object is responsible for generating events based on the collected information, which can be used not only by other things or high-level applications, but also further processed by other modules (e.g. an event-processing unit). 4 For instance, we can model a Pub/Sub system when collecting patients vital data via a network of sensors connected to medical devices, so delivering the data to a medical center for storage and processing and guaranteeing ubiquitous access to medical data in the electronic healthcare record (EHR) format. Figure 1 (a) depicts a standard Pub/Sub deployment with a single broker. However, this organization suffers from the traditional problem related to centralized systems, that is, scalability. In other words, this deployment sometimes cannot provide quality-of-service (QoS) when enlarging the network clients, because of the larger amount of data in the system, the larger infrastructure requirements to store, process, and present it efficiently at real-time. Figure 1 (b) illustrates a possibility to explore scalability using a Peer-to-Peer (P2P) network of brokers. This organization could be assembled using a cluster or grid infrastructure. Traditionally, both cluster and grids have a fixed number of resources that must be maintained in terms of infrastructure configuration, scheduling (where tools such as PBS (http://www. pbspro.org), OAR (http://oar.imag.fr), and open grid scheduler (http://gridscheduler.sourceforge.net) are usually employed for resource reservation and job scheduling), and energy consumption. The resource requirements of IoT Pub/Sub applications will inevitably fluctuate over time. 9 Thus, parallel machines such as clusters and grids may lead to either under-provisioning or over-provisioning situations, incurring in performance and cost penalties, in addition to do not addressing the growing demand of IoT systems. 10 IoT platforms should provide facilities to adapt (i.e. dynamically reconfigure) the resources allocated according to the perceived changes to the environment."], "relatedWork": ["This section presents some initiatives that guided us on developing Brokel. Here, we are handling Pub/Sub systems and how they address scalability. Running in cloud environments, E-STREAMHuB covers a mechanism for automatically expanding and reducing resources for Pub/Sub services with contentbased filtering. 21 The E-STREAMHuB system is an extension of STREAMHuB, 17 a scalable but static Pub/Sub engine. Subscribe messages are sent via unicast to a selected operator, while publish messages are transmitted through broadcast to all operators. E-STREAMHuB introduces data parallelism on Pub/Sub systems, enabling the processing of several publish messages in parallel to registered signatures. However, if the number of publish messages grows quickly, all the solution will be overloaded since all messages are sent to all operations for matching purposes. Elastic Queue Service (EQS) 15 is a message queuing architecture classified as topic-based Pub/Sub, designed to allow elastic scalability, performance, and high availability. Its project concerns the deployment of a Cloud Computing infrastructure using Infrastructure as a Service (IaaS). The EQS elasticity is obtained by migrating topics between processing nodes. If the migration is not enough, the model can instantiate new replicas to meet the growing demand. Migration of threads between nodes can lead to cost and potential service disruption.", "Elastic computing framework (ECF) 24 presents an architecture in which programmability of front-end machines is drawn upon to dynamically divide data processing tasks into (1) those that see all of the data but perform simple operations on it; (2) those that see a transformed subset of the data and extract deep insights from it. This results in lifting pressure on network that links the front-end to the back-end and achieves capability adaptive deployment of IoT solutions. In Bellavista and Zanni, 25 the authors propose a distributed architecture combining machine-to-machine industry-mature protocols (i.e. Message Queue Telemetry Transport (MQTT) and Constrained Application Protocol (CoAP)) to enhance scalability of gateways for efficient IoT-cloud integration. In addition, the article presents how they have applied the approach in a practical experience of efficiently and effectively extending the implementation of opensource gateway that is available in the industry-oriented Kura framework for IoT. This architecture is used by Bellavista and Zanni 26 to propose a scalability solution for IoT gateways via MQTT-CoAP integration. The authors use the hierarchical structure of MQTT to enable an easy way to manage messages. Using CoAP, the solution provides a CoapTreeHandler responsible of dealing with changes in the hierarchy. Through this strategy, it is possible to add or remove entities without needing topics reconfiguration. Table 1 presents a comparison among the aforementioned research initiatives. We perceive that some academic researches are focusing on providing cloud elasticity toward Pub/Sub systems. However, this combination is an open challenge when it comes to elasticity support for middlewares. Alternatives have to deal with some limitations related to either difficulty to adapt the application to take advantage from elasticity or middleware implementation restrictions. A common strategy present in the literature to transform a non-elastic application in an elastic one requires changes in the application source code or development of additional scripts. 5, 15, [21] [22] [23] 26 It makes the adoption of resource reorganization of cloud-based Pub/Sub applications more difficult, since developers/administrators must have a deep knowledge about both system environment and application code. Moreover, we perceive that elasticity is mainly addressed by solutions employing threshold-based reactive techniques, combining horizontal and replication techniques. 5, 15, [21] [22] [23] Such strategy requires user experience in configuring specific parameters and deploying cloud resources. However, the use of load threshold to guide resource reorganization can cause the problem of VM thrashing, where sporadic peaks exceeding one of the thresholds can result in an undesired elasticity action, being interpreted as a false-positive action. Finally, the main limitation of the state-of-the-art today refers to the elasticity support, which is offered only in a single level. 5, 16, 24, [27] [28] [29] What happens when users have an additional broker and need to inform or rewrite their own applications to deal with this new broker? Or yet, how can users know the best broker to receive the client (publisher or subscriber) requests in order to reduce communication latency in this interaction?"], "rq": ["9] what happens when users have an additional broker and need to inform or rewrite their own applications to deal with this new broker?"]}
{"intro": ["The use of tabletops for co-located collaborative search is an ongoing topic in HCI research [18] . Tabletops can offer diverse benefits and potentials for collaborative search such as a closer face-to-face collaboration and more equitable working style [22] , an increased awareness and better group work experience [1] , and a horizontal form-factor whose affordances are well-suited to follow-up activities (e.g. sorting, sensemaking, making a purchasing decision) [1, 18] . However, other potentials of tabletops for search are still unexplored, e.g. the use of \"hybrid surfaces\" like [14, 25] that use tangible interaction with physical props in combination with multi-touch [15] . Except a single design study for video search [11] , such hybrid tabletop interaction has not been used in search scenarios yet. Furthermore, in the light of the popularity of tabletops (e.g. Microsoft Surface) in showrooms or flagship stores, it is surprising that no prior research has focused on the obvious task of collaborative search for products in a retail environment. In this paper we therefore present Facet-Streams (Figure 1 ), a novel design for collaborative faceted product search. It uses a hybrid interactive surface that combines information visualization techniques (a filter/flow metaphor [28] ) with tangible and multi-touch interaction to materialize collaborative search on a tabletop. Thereby, unlike in most previous work, our notion of search does not mean to populate an empty workspace with the results from a keyword search. Instead we mean a process of faceted collaborative filtering of a product catalog until the amount of results is sufficiently small to review and decide [10] . Furthermore, in a retail environment like a flagship store a \"good\" customer experience with \"soft\" factors such as fun, innovative design and social experience is often valued over \"hard\" factors such as task completion times and rates. Our work has therefore been guided by three research questions: (Q1) Does our design turn collaborative product search into a fun and social experience with increased group awareness? (Q2) Can we support the great variety of different search strategies and collaboration styles in different teams with a simple but flexible design? (Q3) Can we harness the expressive power of facets and Boolean logic without exposing users to complex formal notations? In the following, we discuss related work and the specifics of our context of use. Then, we introduce Facet-Streams and the underlying design rationale. We describe two user studies and discuss their results in terms of user experience, collaboration styles, and awareness. We conclude by summarizing our results and discussing them with respect to our research questions."], "relatedWork": ["(1) Morris et al. provide a comprehensive overview and analysis of tabletop search systems along dimensions such as search input, collaboration style and application domain [18] : Regarding search input, Facet-Streams is the first approach that uses hybrid surfaces with tangible and touch interaction. All previous approaches entirely rely on touch, mouse, or keyboard input without making use of any physical props as tangible user interface elements. Regarding the collaboration style, Facet-Streams is similar to FourBySix Search [9] , Cambiera [13] , and WeSearch [19] which all support seamless transitions between tightlycoupled collaboration and loosely-coupled parallel work. However, unlike these applications, Facet-Streams does not use keyword search for Web, document, or multimedia retrieval but uses a visual and tangible query language for faceted search. Thus, Facet-Streams shares commonalities with TeamSearch that also creates a faceted search experience based on Boolean-style AND queries on tagged photo collections [17] . Like TeamSearch, we use circular widgets to specify categorical criteria (or facets) but aim at a far greater query expressivity with arbitrary numbers and logical combinations of such widgets including AND and OR. Furthermore, we do not restrict users to only formulate either personal queries or collective queries. Instead we want to enable them to develop multiple personal and collective queries in parallel and to freely shift criteria between them for maximum flexibility in strategies and collaboration styles. A further fundamental difference between previous work and our design is the employed notion of search. Except TeamSearch and PDH [24] all systems in [18] increasingly populate the collaborative workspace with the results of keyword searches. Thereby search has the notion of adding result sets to the shared workspace. In contrast, we want to follow a faceted search approach [20] where search means narrowing down the entirety of products in the workspace to the desired subset. Thus the focus of collaboration in Facet-Streams lies on the formulation and logical combination of the desired facets of a product (e.g. \"price < 100\") before reviewing individual results. This is opposed to related work where collaboration is focused on reviewing and relating results after search. The only systems in [18] following a similar faceted approach have either limited expressivity (TeamSearch) or force users to only navigate a single facet at a time (PDH).", "(2) Ullmer et al. were the first to suggest physically constrained tokens to manipulate database queries and result visualizations [27] . Two kinds of physical tokens (knobs and range sliders) served as tangible input controllers that are put into slots next to a display. Although enabling some basic Boolean logic between the range sliders, the overall expressivity was limited to assigning database fields to the axes of a scatter plot and altering parameters of predefined queries. Nonetheless, this design inspired many further designs of tangible queries (e.g. for facilitating search for children [6] ) or Blackwell et al.'s Query by Argument (QBA) system [3] . QBA enables groups to manifest the course of an argument in spatial configurations of \"statement tokens\", i.e., RFID-tagged cards as place-holders for contributions to the discussion. Each token carries a reference to a virtual information item that contains the contribution (e.g. relevant text passages). By spatially configuring the statement tokens during discussion, the group provides continuous relevance feedback to an information retrieval system that continuously evaluates the spatial structure to adjust its ranking mechanisms. As a result, the system suggests related material on a peripheral screen. QBA's approach to use spatial configurations of tokens to materialize the (chrono-)logical order of an argument during a collaborative process has been inspirational for our use of a network of tokens for faceted search. However, we want to provide users with tokens for precise filtering and immediate feedback. QBA is targeted at working invisibly in the background to gradually adjust its ranking without the same need for precision and immediacy.", "(3) While being of great practical value for search, Boolean AND and OR are concepts difficult to grasp and they contradict the linguistic sense of \"and\" and \"or\" in our natural language use [4] . Therefore many attempts have been made to visualize these concepts in visual query languages and interfaces: Today's faceted search on ecommerce Web sites [20] and faceted visualizations such as [5, 16] allow to formulate the equivalent of sophisticated Boolean queries by taking a series of small, simple navigation steps. However, these designs are for single users only and do not provide a random access to all intermediate steps in navigation history. This hampers their use for co-located collaborative search where all criteria of multiple parallel queries must be accessible for iterative refinement at all times. Young et al.'s filter/flow metaphor [28] achieves this by visualizing a Boolean query as a sequence of logically linked nodes that carry the criteria. However, it only permits a single query per workspace and its simple layout uses too much screen estate for tabletops. FindFlow [8] expands this metaphor into a 2D plane with a more efficient use of screen estate, but lacks parallel queries and the Boolean OR for specifying complex criteria (e.g. \"either the hotel has a good restaurant OR I want to have a small kitchen in my room.\"). It also conflicts with the size limitations and legibility around a tabletop. This is also true for DataMeadow that uses a filter/flow metaphor for connecting data, filters, and visualizations for visual analytics [7] . Similarly LARK uses a filter/flow metaphor for managing multiple-coordinated views for collaborative visual analytics on large multi-touch displays or tabletops [26] . However, in contrast to Facet-Streams, both systems are not used for Boolean search and they also do not employ any tangible user interface elements. "], "rq": [" (q2) can we support the great variety of different search strategies and collaboration styles in different teams with a simple but flexible design?", " (q3) can we harness the expressive power of facets and boolean logic without exposing users to complex formal notations?"]}
{"intro": [], "relatedWork": [], "rq": ["question: is h strict k-colorable?"]}
{"intro": [], "relatedWork": [], "rq": ["1. how can we decide the threshold?", " 2. can the spatial depth function capture the structure of the data cloud?"]}
{"intro": ["Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. For this purpose, several researchers have suggested models of player types or gaming motivations. These models are generally based on conceptual analysis, psychographic methods, or behavioural analysis [17] . However, researchers Hamari and Tuunanen [17] have noted that most of these approaches suffer from similar limitations. First, most of them were based on or were inspired by Bartle's original work [2] , which suffers a number of limitations. Bartle conducted a qualitative and conceptual study of multi-user dungeons (MUD) players to classify them in types. However, his work was never empirically validated nor intended to be used outside of the context of MUDs. Nevertheless, it inspired and became the foundation of most of the posterior attempts to create player typologies for multiple genres. Consequently, many of the proposed models are limited in the number and types of motivational factors considered because most constructs are similar or derivatives of Bartle's four types. Another issue is that most studies were limited to online games or massively multiplayer online games (MMOs). This makes it difficult to transfer insights to other game genres or contexts. Finally, many models attempt to classify players in distinct types [3, 24] . This can be misleading because they try to segment players in dichotomous categories. This approach might not adequately represent reality because humans are complex and players usually display a combination of distinct preferences in variate degrees, instead of a single category of preferences."], "relatedWork": ["We attempted to compare our taxonomy with the existing models from the literature: Yee's gamer motivation profile [36] , the BrainHex [24] , Hamari and Tuunanen's dimensions [17] , and Ferro et al.'s categories [13] . However, we concluded that the groups identified in our taxonomy do not seem to be directly associable to the dimensions in these models. This is because our model does not identify player types or traits; it groups game elements and game playing styles according to how likely it is for players to enjoy them. Nonetheless, one question remains: how do the groups of game elements and game playing styles that we uncovered relate to the dimensions of motivations and player archetypes suggested by the extant literature? Further studies are needed to answer this question."], "rq": [" one question remains: how do the groups of game elements and game playing styles that we uncovered relate to the dimensions of motivations and player archetypes suggested by the extant literature?"]}
{"intro": [], "relatedWork": ["Recently, Awasthi et al. (2015 Awasthi et al. ( , 2016 proposed efficient algorithms to learn under purely instance-dependent noise (PIN), assuming that D is linearly separable with log-concave isotropic marginal over instances. Our use of the Isotron operates with a more structured form of noise (SIN), which is a subset of PIN; however, we do not require an assumption on the marginals, and merely require D to be linearly scorable by belonging in the GLM family. Further, we show ranking as well as classification consistency."], "rq": [" a natural question thus arises: what can we say about the impact of such label noise on the accuracy of our trained models?"]}
{"intro": [], "relatedWork": [], "rq": [" his question is: how can we be guaranteed that a distal goal is realized by a series of proximal actions?"]}
{"intro": [], "relatedWork": [], "rq": ["n remark 1. what does it mean for the bird group to converge?"]}
{"intro": ["(3) f (where) ! what: utilizing spatial-related features to predict interest what. Regardless of the coordinates of a location captured in original records, typical spatial features can be extracted from the field where include, gyration, 9 and the distance between two successive records 5, 9 ; (4) f (where) ! when: utilizing both spatialtemporal related features mentioned in (2) and (3) to predict activity what, such as EW 4 in Yuan et al. 4 However, typical spatial-temporal features that can be used in the strategies above ignore the fact that human behaviors are highly predictable and centralized in spatiality and temporality. On one hand, people spent most of their time at a few locations, [9] [10] [11] such as home and work places. We define these places as hotspots in this article. Most individuals have regular mobility pattern: commuting to workplace in the morning, spending most of their daytime at workplace, taking some leisure activities after work, and returning home in the evening. 12 An example of a typical user in a single day is illustrated in Figure 1 . On the other hand, bursty is a nature of human behavior. 13 The analysis of human temporal behaviors reveals that both memory effect 14, 15 and periodic characteristics 16 can be found in human behaviors, indicating that regular behavior pattern may exist. We then define these hours containing heavy network usage as hot-times. Thus, human behavior shows centrality both on temporality (hot-times) and spatiality (hotspots). Then, an intuitive question is, can these centralities improve the prediction of mobile user interest? Therefore, this article provides a new solution f (TCB(where, when)) ! what, and proposes a feature transformation method based central behavior (TCB) for mobile user interest prediction. Specifically, we utilize centralities both on spatiality and temporality. For each record, the original spatial-temporal information in raw data is projected into a relative vector space, of which coordinates represent the effects that corresponding centralities (hotspots or hot-times) introduce to current online activity. Moreover, TCB collects statistical summaries at hotspots and hot-times to enrich the relative vector space for better user description. Since TCB only requires a small amount of records to obtain users' hotspots and hot-times information, it is quite suitable for user interest prediction in mobile big data."], "relatedWork": [], "rq": [" and periodic characteristics 16 can be found in human behaviors, indicating that regular behavior pattern may exist. we then define these hours containing heavy network usage as hot-times. thus, human behavior shows centrality both on temporality (hot-times) and spatiality (hotspots). then, an intuitive question is, can these centralities improve the prediction of mobile user interest?"]}
{"intro": ["Is the mechanism behind presupposition projection and filtering fundamentally asymmetric or symmetric? That is, when processing presuppositions and determining whether they project, can we take into account only material that precedes a presupposition trigger, or can we also access material that follows the trigger (and, if so, must we do so)? This is a foundational question for the theory of presupposition which has been at the center of attention in recent literature and also bears on broader issues concerning the sources of asymmetries observed in natural language: are these rooted in superficial asymmetries of language use (since language use unfolds in time, which we experience as fundamentally asymmetric) or are they directly referenced in linguistic knowledge and representations? 1 In this paper we aim to make progress on these questions by exploring presupposition projection and filtering across conjunction. The case of conjunction has traditionally been taken as a central piece of evidence that presupposition filtering is asymmetric in general: that is, that presuppositions can be filtered by material to their left, but not by material to their right. Recent work has argued, however that the evidence based on intuitions is much less clear than commonly assumed once we take into account independent issues of redundancy (Schlenker 2009; Rothschild 2008 Rothschild , 2011 Rothschild /2015 Chemla and Schlenker 2012) . In addition, intuitions about the role of order in filtering across conjunction are muddied by the possibility of local accommodation. The question of the role of order for filtering in conjunction thus remains open."], "relatedWork": [], "rq": [" this is a foundational question for the theory of presupposition which has been at the center of attention in recent literature and also bears on broader issues concerning the sources of asymmetries observed in natural language: are these rooted in superficial asymmetries of language use (since language use unfolds in time, which we experience as fundamentally asymmetric) or are they directly referenced in linguistic knowledge and representations?"]}
{"intro": [], "relatedWork": [], "rq": ["the question here is: is gender morphologically expressed in russian nouns?"]}
{"intro": [], "relatedWork": ["Prior work on predicting task success has been done in the context of human-computer spoken dialogue systems. Features such as recognition error rates, natural language understanding confidence and context shifts, confirmations and re-prompts (dialogue management) have been used classify dia-809 logues into successful and problematic ones (Walker et al., 2000) . With these automatically obtainable features, an accuracy of 79% can be achieved given the first two turns of \"How may I help you?\" dialogues, where callers are supposed to be routed given a short statement from them about what they would like to do. From the whole interaction (very rarely more than five turns), 87% accuracy can be achieved (36% of dialogues had been hand-labeled \"problematic\"). However, the most predictive features, which related to automatic speech recognition errors, are neither available in the human-human dialogue we are concerned with, nor are they likely to be the cause of communication problems there."], "rq": ["9% can be achieved given the first two turns of \"how may i help you?"]}
{"intro": [], "relatedWork": [], "rq": [" the statement of the problem should briefly address the question: what is the problem that the research will address?"]}
{"intro": [], "relatedWork": [], "rq": ["the bigger context: what is machine learning good for?"]}
{"intro": ["I N this work we consider the problem of object class recognition in large image databases. Over the last few years this topic has received a growing amount of attention in the vision community [1] , [2] . We argue, however, that nearly all proposed systems have focused on a scenario involving two restrictive assumptions: the first, is that the recognition problem involves a fixed set of categories, known before the creation of the database; the second, is that there are no constraints on the learning and testing time of the object classifiers, besides the requirement that training and testing must be feasible. Such assumptions are reflected in the characteristics of the most popular object categorization benchmarks [3] , [4] , which measure the performance of recognition systems solely in terms of classification accuracy over a predefined set of classes, and without consideration of the computational costs.", "We propose to use as entries of our image descriptor the outputs of a predefined set of nonlinear classifiers evaluated on low-level features computed from the photo. This implies that a simple linear classification model applied to this descriptor effectively implements a nonlinear function of the original low-level features. As demonstrated in recent literature on object categorization [5] , these nonlinearities are critical to achieve good categorization accuracy with low-level features. However, the advantage of our approach is that our classification model, albeit nonlinear in the lowlevel features, remains linear in our descriptor and thus it enables efficient training and testing. In other words, the nonlinear classifiers implementing our features are used as a classification basis to recognize new categories via linear models. Based on this intuition, we refer to our features as basis classes. The final classifier for a novel class is obtained by linearly combining the outputs of the nonlinear classifiers, which we can pre-compute and store for every image in the database, thus enabling efficient novel-class recognition even in large data sets."], "relatedWork": ["The first category comprises techniques to approximate nonlinear kernel distances via explicit feature maps [11] , [12] . For many popular kernels in computer vision, these methods provide analytical mappings to higher-dimensional feature spaces where inner products approximate the kernel distance. This permits to achieve results comparable to those of the best nonlinear classifiers with simple linear models. However, these methods are typically applied to hand-crafted features that are already high-dimensional and they map them to spaces of further increased dimensionality (two or three times as large, depending on the implementation). As a result, they pose high storage costs in large-scale settings.", "In this paper we also consider how to extend classifierbased descriptors to yield good recognition accuracy even for photos containing multiple objects and clutter. Our approach is inspired by Li et al. [17] who have proposed to use the localized outputs of object detectors as image features, to encode spatial information into a global image descriptor. However, this representation is very highdimensional (the \"ObjectBank\" descriptor includes over 40,000 real-valued entries) and as such is not suited for recognition in large databases. Here we present several strategies to aggregate the outputs of basis classifiers evaluated on multiple subwindows in a single low-dimensional descriptor. We show that simple linear classifiers trained on our aggregate vector produce results approaching the stateof-the-art on challenging object-detection and scene recognition benchmarks."], "rq": ["a natural question is: how do our learned meta-classes compare to the nodes in the hand-constructed imagenet hierarchy?"]}
{"intro": [], "relatedWork": [], "rq": [" the question is: how much of transfer and how much of ug are made use of in this process?", "6) is not on the right track. in addition, words presented in (19) are pronounced by all arabs irrespective of the country they belong to, for instance, e.g. egyptians, iraqis, yemenis, moroccans, syrians, etc. all pronounce these words with the emphatic /\u0142/. it seems to us that the reason might be something else, which, we think, is the teacher\"s competence. in that if the learner hears his teachers beginning at school and ending at university pronouncing the emphatic /\u0142/ as /l/, then, how to expect him to pronounce it correctly!?", " the question is: why is it that the participants\" performance is not the expected one?"]}
{"intro": [], "relatedWork": [], "rq": [" the second question that the two observations yield is of a more empirical nature: can we be more precise about the distribution of 'still' in modal contexts?"]}
{"intro": ["It is known [10] that a secure (bit) commitment between two parties is impossible without some kind of encryption, i.e., without a one-way function. However, if the number of parties is at least 3, this becomes possible, as long as parties do not form coalitions to trick other party (or parties). It has to be pointed out though that formal definitions of commitment schemes vary strongly in notation and in flavor, so we have to be specific about our model. We give more formal details in Section 6, while here we just say, informally, that what we achieve is the following: if the committed values are just bits, then after the commitment stage of our scheme is completed, none of the parties can guess any other party's bit with probability greater than 1 2 . We require in our scheme that there are k secure channels for communication between the parties, arranged in a circuit. We also show that less than k secure channels is not enough.", "In Section 9, we consider a related cryptographic primitive known as \"mental poker\", i.e., a fair card dealing (and playing) over distance. Several protocols for doing this, most of them using encryption, have been suggested, the first by Shamir, Rivest, and Adleman [15] , and subsequent proposals include [5] and [8] . As with the bit commitment, a fair card dealing between just two players over distance is impossible without a one-way function since commitment is part of any meaningful card dealing scenario. However, it turns out to be possible if the number of players is k 3. What we require though is that there are k secure channels for communication between players, arranged in a circuit. We also show that our protocol can, in fact, be adapted to deal cards to just two players. Namely, if we have two players, they can use a \"dummy\" player (e.g., a computer), deal cards to three players, and then just ignore the \"dummy's\" cards, i.e., \"put his cards back in the deck\". An assumption on the \"dummy\" player is that he cannot generate any randomness, so randomness has to be supplied to him by the two \"real\" players. Another assumption is that there are secure channels for communication between either \"real\" player and the \"dummy\". We believe that this model is adequate for two players who want to play online but do not trust the server. \"Not trusting\" the server exactly means not trusting with generating randomness. Other, deterministic, operations can be verified at the end of the game; we give more details in Section 9.2."], "relatedWork": [], "rq": [" one of the ways to describe the problem is: how can two players deal cards fairly over the phone?"]}
{"intro": [], "relatedWork": [], "rq": ["0. what type of information is relevant across a distance of ten words?"]}
{"intro": ["Shape matching is a fundamental problem in computational geometry, computer vision, and image processing. A simple version can be stated as follows: given a database D of shapes (or images) and a query shape S, find the shape in D that most resembles S. However, before we can solve this problem, we first need to address an issue: what does it mean for two shapes to be \"similar\"? In the mathematical literature, on can find many different notions of distance between two sets, a prominent example being the Hausdorff distance. Informally, the Hausdorff distance is defined as the maximal distance between two elements when every element of one set is mapped to the closest element in the other. It has the advantage of being simple to describe and easy to compute for discrete sets. In the context of shape matching, however, the Hausdorff distance often turns out to be unsatisfactory: it does not take the continuity of the shapes into account. There are well known examples where the distance fails to capture the similarity of shapes as perceived by human observers [6] ."], "relatedWork": [], "rq": [" we first need to address an issue: what does it mean for two shapes to be \"similar\"?"]}
{"intro": ["The study presented was entangled in the last phase of a research project on supporting burglary prevention advisors in their daily work. In the burglary prevention (BP) scenario, police trained in technical security visit residents at their homes to advise them on how to secure their properties against burglary (Giesbrecht et al. 2015; Comes and Schwabe 2016a, b) . The public mandate of police crime prevention units includes, among others, promoting the implementation of crime prevention measures and enabling communities to prevent burglary cases from happening. BP advisors act upon this task; however, they often lack systematic training for it. Depending on their career, they rely on an introductory hands-on training, general police officer schooling, exposure to burglary or crime cases in their previous appointments (e.g., during patrol or investigator duties), as well as experience from previous advisory encounters, and their technical expertise. Prone to influence by the complex nature of interpersonal communication, they differ significantly in how they motivate or enable their advisees and, as this study unveils, how they shape their task during this communication. Some focus on transferring the message: Bdon't be afraid!^, whereas others exaggerate stories from criminal statistics. After the rollout of the SmartProtector, a tablet-based tool designed according to persuasive technology guidelines to support a range of persuasive practices, the differences between the various advisors emerged. Each advisor favoured a stockpile of routines (stories, arguments, explanations, etc.) which were activated based on their preconceptions and observations about the advisee, the local or situational circumstances and the private perception of the advisor's task. This study makes clear that behaviours that were originally considered a mundane part of a conversation (e.g., a story from the neighbourhood), could be recognized as an essential and routinized persuasive device. The SmartProtector was appropriated as far as it could be meaningfully applied in the routines. This sheds new light on the persuasive aspect of the work: it grows out of a range of conversational routines and is not like a debate with explicit arguments or targeted behaviours. Consequently, supporting persuasion is less about extending the persuasive arsenal with technology but rather about equipping IT with a meaning that fits the stories, explanations, and narratives the advisors used to provide, and about affording new behaviours that may turn into routines. Overall, this study proposes the picture of persuasive practices as routines originating as strong stereotypes, as well as the advisor's opinion towards ongoing organizational discourses and tensions. Transforming those practices with IT requires consideration of multiple cues about the situation and its background rather than an optimistic assumption about the improvisational character of practices. The current study arrived at those insights by pursuing the following research questions:"], "relatedWork": [], "rq": [" this field offers a range of relevant research questions: what is the main point of an advisory service if not the information transfer between an expert and a layperson?", " this manuscript explores persuasive practices and their manifestation in an ongoing collaborative effort: to what extent is it-supported persuasion a routine?", " the transfer or design guidelines from a single-user scenario to a collaborative scenario pose essential challenges: what is the role of the technology?"]}
{"intro": [], "relatedWork": ["Computational approaches are increasingly being viewed as critical for knowledge discovery in the natural sciences (Kell 2012) . On one hand this may be addressed by using \"big data\" to reduce the space of possible solutions (Hey et al. 2009 ). On the other hand, if data is limited, as, for example, is typical for short time-series gene expression experiments, learning can be constrained by relevant domain knowledge (Subramanian et al. 2005) . The latter setting is the one addressed in our work. The view that formalisms used to characterise the semantics of computation can have a central role in understanding systems biology is not new (Regev L, M, H denote \"low\", \"medium\" and \"high\" levels of noise or missing values. \"Depth\" denotes the depthbound provided for the transition-system and meta-interpreter (where this is not shown, the value is 1) . The entries are mean values obtained over 10 repetitions and the numbers in parentheses are standard deviations Table 3 System identification results (part 2) Problem Accuracy and Shapiro 2002). Petri nets have probably been the most widely used such formalism for modelling in systems biology (Koch et al. 2011) . Typically, though, such models are hand-crafted from biological knowledge. Although the problem of identification, or reverse engineering, in systems biology has been the subject of many studies [a recent review is in Villaverde and Banga (2014) ] the work of Durzinsky et al. (2008) appears to be the first published method for the reconstruction of Petri net models of biological systems from time series data, where a combinatorial method is used to generate the set of all pure Petri nets consistent with discrete time-series data; subsequently this was broadened to handle extended Petri nets, with read and inhibitory arcs (Durzinsky et al. 2011b ). In Durzinsky et al. (2011a) their Petri net reconstruction algorithm was reformulated using Answer Set Programming [ASP-see Baral (2003) for an overview]. ASP is an approach to logic programming that has a number of useful features for systems biology modelling, including true negation [as well as default negation or negation as failure (Gelfond 2008) ]; efficient solvers; and a number of declarative built-in language constructs for choice and optimization. They note some advantages of ASP for this work: that it allows a declarative reformulation of their previous implementation; the possibility of addition of declarative biological knowledge as constraints; and, since the ASP system used is based on a constraint solvers, the approach was as efficient as a previous special-purpose implementation. Essentially, the method searches for models that conform to a graph of system states, termed the experiment graph, where models are constrained by clauses specifying the network reconstruction algorithm. There have been several approaches to the identification of Petri nets from data in the area of business process mining. Typical approaches Aalst et al. (2010) use a transition system as an intermediate representation then apply region theory (Ehrenfeucht and Rozenberg 1990) to generate a Petri net. However, this type of approach in practice can suffer from both over-and under-fitting and may not be able to handle noisy data and incompleteness (Aalst et al. 2010 ). More recently work using first-order logic representations has been applied. In Bellodi et al. (2016) a two-stage aproach to workflow mining starts by extracting first-order constraints from process logs, then applies Markov logic to learn parameters of a statistical relational model. This improves predictive accuracy on real-world datasets. An enriched representation of events for process mining in first-order logic is presented in Ferrilli (2016) but no results from learning were given.", "Inoue (2011) formulated Boolean networks as (normal) logic programs, and Inoue et al. (2014) showed how this semantics enabled a method of learning from state transition pairs, where each state is an interpretation. However, this does not identify probabilistic transitions. In Inoue et al. (2013) meta-level abduction is applied to learn biological networks represented as causal graphs. Although these are not dynamic models like Petri nets they do capture inhibitory and activatory effects.", "Clearly, there is a close link between meta-interpretive system identification as we have described it, and recent research on meta-interpretive ILP (MILP) (Muggleton et al. 2014b) . In this paper our approach is in some sense a development of the meta-interpretive learning approach to grammar learning (Muggleton et al. 2014b) . Although this does not use probabilistic inference, the MILP approach MetaBayes does, combining meta-interpretive learning with Bayesian inference (Muggleton et al. 2014a ). However, in comparison with our approach, where probabilistic sampling is defined at the object level in terms of a probabilistic transition system, in MetaBayes it is at the meta-level, essentially by converting the meta-interpreter to a stochastic logic program that is executed to generate clause refinements stochastically.", "We now list some important points of similarity and difference between our approach and MILP in general. First, although both MILP and the work here use a meta-interpreter, the motivations are different. For MILP, the meta-interpreter is intended to be a general-purpose mechanism for providing higher-order templates for learning first-order logic programs. Here, the meta-interpreter is necessary for generating the operational semantics of general transition systems. Since transition systems can be written as logic programs (as seen in the paper), it follows that the meta-interpreter used by a MILP system can, in principle, also be used for system identification. It is therefore unsurprising that the inspiration for the meta-interpreter here lies in the one described in Muggleton et al. (2014b) (however, see next) .", "We conclude with answers to some questions that may arise from the experiments described above. First, concerning the representation: Are we retricted to qualitative values? In all experiments, Boolean values were used to indicate the presence or absence of a sufficient quantity of a chemical. Is this a requirement of the representation? Clearly not, as shown early in the paper (Example 9), in which tokens are integer-valued. We would however expect the transition relation to become increasingly non-deterministic as the cardinality of values allowed increases This can, in turn, lead to multiple proofs, not all of which may make sense, biologically speaking. The most effective antidote for this is again more domain-knowledge. Are we restricted to Petri nets? Again, early examples in the paper show how we are able to represent a variety of automata (including Turing machines) with the representation of transition systems. In fact, with the extension to probabilistic transition systems, we should, in principle also be able to represent different kinds of probabilistic automata. The system identification procedure we use, for example, is in effect extracting a model from the transition probability matrix of a Markov model. Our specific choice of Petri nets here is motivated by its ability to represent a wide variety of biological networks (see Koch et al. 2011) . Are we restricted to logic programs? In principle, no. The procedures in Figs. 1, 2, 3 and 4 provide a complete specification of the approach we have used. The implementation as a logic program is especially natural, given the built-in facilities for theorem-proving and backtracking. Next, concerning the usefulness to Biology: Will the approach scale-up? The experiments here are on real, but modest-sized networks. They have been chosen to highlight some specific features of biological networks, namely: routine metabolic reactions, catalysed reactions, cascades, feedback loops and pathway reuse. We have also seen how domain-knowledge can be naturally included at the object-level, as part of the definition of the transition relation. The experimental results here suggest that construction of large networks is likely to proceed hierarchically (the Yeast network for example, contains several smaller sub-networks, abstracted to the level of a transition), and with strong domain knowledge to constrain proofs by the meta-interpreter. What about \"real\" biology? A criticism of the paper is this: all experiments are reconstructions of networks from simulated data. How useful is this? There are in fact two separate questions here: (a) Broadly speaking, how useful to biologists are re-constructive experiments of the kind reported here?; and (b) Specifically, will the approach work with real experimental data? Standard practice in computational biology to verify that an identification algorithm works as expected is to use controlled experiments where algorithms are applied to reconstruct known networks from simulated data. Quantitative evaluations of the reconstruction performance of such algorithms are a necessary step to assess the algorithm's utility. But clearly, the results of such experiments may not be sufficient to guarantee that the technique will work with experimental data, which can often be noisy, missing or both. In this paper experiments have included such problems with the data, and the results act as a guide of what can be expected when the approach is used in practice. The question of whether the method can be used to identify biologically meaningful networks from experimental data on real systems can only be answered in conjunction with biologists. This we leave for future research."], "rq": [" concerning the representation: are we retricted to qualitative values?"]}
{"intro": ["With the rapid development of positioning services and location acquisition techniques, massive spatiotemporal trajectory data can be easier obtained, and mining or analysing trajectory data is very important in many different applications, such as transportation management (Wasesa et al, 2017) , mobility study (Ting, 2016) , path discovery, movement behaviour analysis (Tang et al, 2016) , and mobile communication management. A trace of object is the continuous moving in geographical space, while the corresponding trajectory is a sample of location points that the moving object passes. The visual analytics solutions show great potential as they allow users to explore the data and improve mining process and results (Schreck, 2009) , and there are various methods to manage, process and mine trajectory data (Leonardi et al, 2017) . However, we believe that, there are several uncertainties in the sampling process of trajectory data, on the other hand, map creation, as well as geolocation in the trajectory data, has historically involved much artistry. In fact, the GPS doodle pioneered by Stephen Lund (Stephen, 2017) has attracted considerable attention. Inspired by artworks and other sources, we try to handle the uncertainties of trajectory data and then focus on its artistic visualization, that is, the aesthetic effect of the traces of moving objects. As the previous references (Tobias, 2013 and Sungye et al, 2013 ) done, we are thus not interested in precise and correct depictions of the world but instead in altering maps to satisfy a specific personal aesthetic. There are many other works related to this topic in nonphotorealistic rendering and computational aesthetics, including image-guided Voronoi aesthetic patterns (Wu and Zhang, 2016) , stroke-based technique (Hertzmann, 2003) , imageguided fracture (David, 2005) and frayed cell (Michael et al, 2014) . Nevertheless, none of these methods have been involved into trajectory data, and how to generate an aesthetic effect of trajectory data is still a challenging problem. In this context, we proposed an artistic visualization algorithm for trajectory data based on cloud model (VTC for short). Our intentions are twofold: (1) how can the uncertain trajectory be produced using a cloud model-based algorithm? (2) how different types of strokes can affect the aesthetic qualities of the rendered results? Cloud model is a cognitive model (Li et al, 2009) , which can realize the bidirectional cognitive transformation between qualitative concept and quantitative data based on probability statistics and fuzzy set theory. There are various successful applications using cloud model (Wang, 2014 , Wu et al, 2015a , Wu et al, 2015b ). This novel model has been attracted several attentions from researchers, which is different from the traditional statistical methods and fuzzy set methods, and cannot be considered as randomness compensated by fuzziness, fuzziness compensated by randomness, second-order fuzziness or second-order randomness. The cloud model-based rendering consists of three main steps, including uncertain line representation using the extended cloud model, uncertain trajectory approximation with varying shadow, and then generating an aesthetic image with the artistic style. Our method satisfies the property of uncertainty, simplicity, and effectiveness: (1) The proposed method is involved as a novel technique with uncertainty using cloud model. (2) The proposed method is very simple, and only the uncertain representation of lines and the drawing of shadows are introduced into the traditional method. (3) From the artist perspective, the focus is on creativity and iterative process, rather than the result image. The proposed method is a dynamic generation one by one, and correspondingly various styles of result images can be obtained by using various types of strokes. The rest of the paper is organized as following: Section 2 firstly provides an introductory explanation of cloud model, then proposes the extended cloud model for uncertain line and uncertain trajectory. Section 3 describes the VTC algorithm. Section 4 shows the experimental results and discussion. Finally, the conclusion is drawn in Section 5."], "relatedWork": [], "rq": [" (1) how can the uncertain trajectory be produced using a cloud model-based algorithm?", " (2) how different types of strokes can affect the aesthetic qualities of the rendered results?"]}
{"intro": [], "relatedWork": [], "rq": [" solids example of figure 10 is processed using the model shown in figure 11 . tcl commands are italicized. as software is integrated into larger systems, accountability issues become increasingly more important. commercial software is often validated and tested before it goes to market. however, does the validity of these codes remain intact in larger architectures?"]}
{"intro": [], "relatedWork": [], "rq": [" it also introduces a new challenge: how to choose the right answer based on the volunteer contribution?"]}
{"intro": [], "relatedWork": [], "rq": [" by other words the problem of the strong regularity of a is equivalent to the question: can we multiply (in the sense of 0) the rows of a by constants in such a way that every column maximum will then be achieved in only one row and the maxima of any two different columns will lie in different rows?"]}
{"intro": [], "relatedWork": [], "rq": [" participants were asked questions such as: what do you see about the bg control in this period?"]}
{"intro": [], "relatedWork": [], "rq": [" (1) what is the physical nature and the molecular basis of protrusion, retraction, and adhesion?", " (2) how are these three processes coordinated to achieve the observed shapes and movements of crawling cells?"]}
{"intro": [], "relatedWork": [], "rq": [" a natural question to ask is: can a communicate at rate at least r to b?"]}
{"intro": ["One way to realize (an approximation of) continuous verification is to use biometrics. Biometrics verification is appealing because several of them that are easily incorporated into ordinary computer usage are passive (that is, they do not require the active cooperation of the user and are therefore not distracting) and they obviate the need to carry extra devices for authentication. 1 In a sense, they are always on one's \"person\" and perhaps a little safer than using external devices which can be separated from their carrier more easily. However, a single biometric may be inadequate for passive verification either because of noise in the observation samples or because of the unavailability of an observation at a given time. For example, face verification cannot work when frontal face detection fails because the user presents a nonfrontal pose. To overcome these limitations, researchers have proposed the use of multiple biometrics and have demonstrated increased accuracy of verification with a concomitant decrease in vulnerability to impersonation [2] . The use of multiple biometrics has led to the investigation of integrating different types of inputs (modalities) with different characteristics. Kittler et al. [3] experimented with six combination (also known as fusion, integration) methods for face and voice biometrics, using the sum, product, minimum, median, maximum, and majority vote rules. These rules are also popular for combining classifiers in general pattern classification problems and have been extenstively studied in the literature. See, for example, [4] , [5] ."], "relatedWork": ["To be sure, the idea of continuous verification/authentication is not new. Interest in this technology has been increasing in recent years, however. For example, Klosterman and Ganger [8] examined the differences between biometrics and traditional passwords/tokens and argued for the suitability of biometrics for continuous authentication. They enabled a Linux system to continuously authenticate its user via the Linux Pluggable Authentication Module (PAM) [9] . Because of high-computational cost, however, the actual authentication decision is made by a separate machine. Only a single modality, face, was used for authentication.", "In response to the 11 September 2001 attacks, Carrillo proposed two designs that use continuous biometrics authentication to safeguard the aircraft cockpit against unauthorized control [10] . The designs differ in where the actual verification decision is made: either on board the aircraft or at distributed locations offsite. Carrillo's proposals allow for, and argue for, the use of multimodal biometrics. She also enunciated new procedures for biometrics authentication on the flight deck. However, Carrillo did not implement an actual system; hers was a design proposal only.", "In [6] , Altinok and Turk experimented with using voice, face, and fingerprint biometrics for continuous authentication. They articulated two key issues in continuous authentication, namely, the need for integration across both modality and time, and the requirement that the system must be able to determine \"authentication certainty\" at any point in time, even in the absence of observations. However, their work focused only on multimodal fusion and did not study the consequences of ephemeral misclassification on system performance in a real system on realistic workloads. Moreover, their experiments used only simulated individuals because of the difficulty of getting real users."], "rq": [" our initial concern was overhead: how much more cpu time was needed to handle continuous verification?"]}
{"intro": [], "relatedWork": [], "rq": ["experiment: why are workers consistent?"]}
{"intro": ["This raises a natural question: is it possible to solve minimum set cover in sub-linear time? This question was previously addressed in [28, 33] , who showed that one can design constant running-time algorithms by simulating the greedy algorithm, under the assumption that the sets are of constant size and each element occurs in a constant number of sets. However, those constant-time algorithms have a few drawbacks: they only provide a mixed multiplicative/additive guarantee (the output cover size is guaranteed to be at most k \u00b7 ln n + n), the dependence of their running times on the maximum set size is exponential, and they only output the (approximate) minimum set cover size, not the cover itself. From a different perspective, [20] (building on [15] ) showed that an O(1)-approximate solution to the fractional version of the problem can be found in O(mk 2 +nk 2 ) time 1 . Combining this algorithm with the randomized rounding yields an O(log n)-approximate solution to Set Cover with the same complexity."], "relatedWork": [], "rq": ["this raises a natural question: is it possible to solve minimum set cover in sub-linear time?"]}
{"intro": ["Evaluating people experience means being able to evaluate engagement, emotions, pleasure, etc. Notions that by definition are difficult to express and measure. Traditionally selfassessment, helped with questions by professionals, is used and further analyzed by psychologists. For instance the SelfAssessment Manikin (SAM) [2] is widely used to measure the pleasure, arousal, and dominance associated with a person affective reaction. It is a non-verbal pictorial assessment technique to ease interpretation and coherence between different subjects. However, there are several bias and limitations that make such methods unusable for real applications where the subjects are experiencing different events in various locations and live conditions. Recently, wearable devices to measure physiological responses have been made available, with corresponding signal processing and algorithms to infer affective reaction from those signals. The main advantage of this measure is its unconscious and objective nature. Three main signals have been used for emotion detection: the facial expressions [11] , the body gestures [13] , and some biological reactions from the autonomic nervous system, such as skin conductivity, heart rate, skin temperature [3] or pupillary response and eye blinking. Such methods have been experimented for various entertainment applications: music [5] , movies and video [14] , [4] , [17] , or advertisement [10] . However, there is not so much trials done for paintings. The most relevant is probably the eMotion mapping museum experience [16] , [15] . Probably because art appreciation addresses deep cognitive information such as religion, culture, education, history, etc. Besides how to capture emotional reactions, the second issue raised is how to represent them to reflect time (when it happened), space (where was the subject), intensity (how strong the emotion was), type (what was the valence of the emotion), and variability (how does it compare between individuals and different experiences). Interesting attempts have been proposed for well-being applications, especially with smartphones, such as the Moodscope [9] , an online platform aimed at tracking ones mood throughout a certain period of time. In general the user is prompted to rank his feelings towards different emotional states, ideally on a daily basis. A simple 2D graph curve indexed by days is used to represent the mood score. MoodJam (http://moodjam.com/) adds patterns of colors and words to describe people mood and share it with others. Other similar apps have been proposed linked or not to social networks, but the representation is still very simple, using only simple colored patterns or scales, representative of people's mood. It also requires the user to manually set his own feelings. Besides the bias of this interpretation, and the difficulty to associate words and colors to an emotion, it may not necessarily represents the truth. More complex representations have been proposed for music experiences by Krcadinac [6] and Kaushal Agrawal in Data Visualization Mood of the Artist (http://www.kaushalagrawal. com/moodoftheartist.php). The book Emotional Cartography by Christian Nold [12] , provides an unique collection of essays discussing the desire to map emotions, by visualizing intimate biometric data and emotional experiences. It is a very relevant work providing \"a tangible vision of places as a dense multiplicity of personal sensations\". In this paper we want to answer two main questions: how to measure the quality of a museum experience? and how to visualize the resulting affective experience? For that we propose a new way to visualize the emotional experience, based on physiological responses induce by pieces of art (paintings in that case). In particular we provide a capture and playback system of the journey, highlighting the emotion intensity (arousal) for each painting. Different analytic visualization methods are also proposed to evaluate individuals, groups, rooms, and paintings specifically. The first part details the affective detection method and how values are extracted from the visitors' body responses. The second part describes the different representations used. Finally the third part provides the results of the experiments done with real visitors of the museum of arts of Lyon, France."], "relatedWork": [], "rq": [" in this paper we want to answer two main questions: how to measure the quality of a museum experience?"]}
{"intro": [], "relatedWork": [], "rq": [" a simple counting argument helps us to assess the expressiveness of a model producing a representation: how many parameters does it require compared to the number of input regions (or configurations) it can distinguish?"]}
{"intro": [], "relatedWork": [], "rq": ["1. is there a decision procedure to determine whether x \u2208 h n lies in the canonical fundamental domain for a not-necessarily-geometrically finite group h?"]}
{"intro": [], "relatedWork": [], "rq": [" the only question is: how significantly does the quality of the approximate match degrade?"]}
{"intro": ["The recent interest in multi-touch interaction has led to a plethora of gesture-based interaction techniques, e.g. [8, 24, 26, 32, 34, 36] , and multi-touch studies, e.g. [13, 22, 23, 31, 35, 37] . At the same time, there is a growing use of interactive surfaces in public exhibition spaces such as museums [10, 11, 13] , galleries [28] , and urban spaces [23] . Previous findings have indicated that public walk-up-and-use displays benefit from multi-touch interaction by providing pleasurable and playful experiences [11, 13, 18, 23] . However, the design of multi-touch gestures for such scenarios is still a significant challenge due to short interaction times and diverse audiences with varying expectations toward technology. This is fueling the demand for a better understanding Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. of multi-touch gestures to inform the design of systems that do not require elaborate instructions or prior practice.", "Previous lab-based studies have indicated interesting trends in people's preferences for certain gestures to accomplish particular actions [35] . However, to observe how gestures emerge as part of people's individual and social interactions around the digital display, we found it important to study the use of multi-touch gestures in a real-world walk-up-and-use context. We focused on the following questions: (1) What characterizes multi-touch gestures that people apply in walkup-and-use scenarios?, (2) How do gesture types differ between different visitor groups such as adults and children?, and (3) What factors influence the choice of multi-touch gestures in walk-up-and-use scenarios? To investigate these questions, we conducted a field study at the Vancouver Aquarium where we observed a general public of all ages as they interacted freely and spontaneously with a third-party multitouch tabletop exhibit (see Fig. 1 )."], "relatedWork": [], "rq": [" (1) what characterizes multi-touch gestures that people apply in walkup-and-use scenarios?", " (2) how do gesture types differ between different visitor groups such as adults and children?", " and (3) what factors influence the choice of multi-touch gestures in walk-up-and-use scenarios?"]}
{"intro": [], "relatedWork": [], "rq": ["1. is it true that fx(pc, k) = o(fx(cir, k))?"]}
{"intro": ["In this paper, we introduce a Bayesian approach to learn an oscillating ODE model from time series data. In contrast to Bayesian networks, ODEs are particularly suited to model the dynamic behavior of a system. Thus, we are not only able to reveal interactions between network components, but also learn their dynamic behavior. To account for the probabilistic nature of gene regulation and the noise in the measurement process, our ODE model is embedded into a probabilistic framework. Then, network inference is formulated as an optimization problem of the posterior distribution. This method is able to reconstruct main regulatory interactions and outperforms maximum likelihood estimation (MLE), as we have shown in [22] . However, there is one problem with differential equations that are based on chemical reaction kinetics: The region in the parameter space corresponding to periodic behavior is usually rather small, and the system tends to converge to a globally stable fixed point for most parameter sets [11] . Several two component models have been proposed to explain the periodic expression of genes involved in the cell cycle (see, e.g. [6, 26] ). It is assumed that a relatively independently acting core mechanism of only a few components is responsible for this oscillating behavior. We include such a core model into our learning process and proceed in two steps: Parameters for the core network are learned in advance. For this, we use the Generalized Reduced Gradient (GRG2) algorithm, a non-linear optimization algorithm, which is implemented in the Microsoft Excel program [16] . Subsequently, this core network is extended by further components and interactions with a Bayesian learning approach. Here, the optimization problem was solved using conjugate gradient descent as described in [20] . Details of the implementation can be found in [13, 14] . A specifically designed prior distribution over interaction strengths favors sparse networks and thus restricts the search space drastically. Our method was applied to a simulated network of seven components and to a real world dataset of the yeast cell cycle. We were able to reconstruct many regulatory interactions and to model the periodic behavior of the system. This paper is structured as follows: Our methods are explained in Section 2: We introduce a differential equation model for gene regulatory networks, which is embedded into a probabilistic framework. A Bayesian approach to infer model parameters is presented. Periodic behavior is assumed to originate from a specific core mechanism, which is learned in advance. Section 3 shows results for simulated data and a dataset of the yeast cell cycle. The last Section 4 concludes with a discussion of the results and an outlook."], "relatedWork": [], "rq": [" this observation leads to an interesting question: how does a cell stabilize the regulation of its cell cycle?"]}
{"intro": [], "relatedWork": [], "rq": ["6) why do pus consist of existing words, and especially of function words?"]}
{"intro": ["Sleep-related disorders such as sleep deprivation [6] and sleep apnea can be studied and diagnosed, and interventions can occur using ubiquitous sensing technologies [1] . Sleep clinics typically use Polysomnography (PSG), a test conducted to study sleep and to diagnose different forms of sleep disorders [4] . To date, PSG is considered as the most accurate method for diagnosing sleep-related problems and considered the gold standard in clinical sleep medicine. However, it suffers from the fact that it is expensive, complex, time-consuming, and uncomfortable for the users [4] . In this early work, we aim to model a sleep classification system using an unobtrusive Ballistocardiographic (BCG)-based heart sensor signal collected from Dozee 1 (Fig. 1) , a commercially available non-contact, unobtrusive pressure-sensitive sensor sheet. Here, we ask: How can a sleep classification system be modelled using BCG sensor data, in order to achieve a performance comparable with PSG?"], "relatedWork": [], "rq": [" we ask: how can a sleep classification system be modelled using bcg sensor data, in order to achieve a performance comparable with psg?"]}
{"intro": [], "relatedWork": [], "rq": ["4) are decreasing rapidly. hence we may obtain a good asymptotic estimate of jn n;n j by only considering a constant number of terms from the sum in eq. (24). let c n i = c n (i) (2 n?"]}
{"intro": [], "relatedWork": [], "rq": ["problem 4. is the set of all finitely presented metabelian groups recursively enumerable?"]}
{"intro": [], "relatedWork": [], "rq": [" for example: what is the entire range of coordination problems that can be solved by this approach?"]}
{"intro": ["As summarized in Table 1 , predictive physics simulation is not a new concept, and it has been increasingly utilized by recent artificial intelligence (AI) and animation research (e.g., [11, 32] ). However, research has focused on nonplayer characters or enabling simple high-level control of a simulated avatar, e.g., making a bipedal character walk in a desired direction. This has obvious applications in action games such as God of War [27] or Uncharted [20] , which presently need vast quantities of animation data to enable such high-level control. However, there are also various games such as Angry Birds [25] and QWOP [2] , where the player directly controls simulations on a low level. In such games, predictive simulation seems less explored. This observation prompts our primary research question: What novel possibilities and challenges emerge from combining predictive simulation with low-level direct control of simulated characters?"], "relatedWork": [], "rq": [" this observation prompts our primary research question: what novel possibilities and challenges emerge from combining predictive simulation with low-level direct control of simulated characters?"]}
{"intro": [], "relatedWork": [], "rq": ["1) is nonzero so that also in this case rational distances x; y; f ?"]}
{"intro": ["It is difficult to overstate the ubiquity of DTW. It has been used in robotics, medicine [5] , biometrics, music/speech processing [1] [27] [41] , climatology, aviation, gesture recognition [3] [38] , user interfaces [16] [22] [29] [38] , industrial processing, cryptanalysis [7] , mining of historical manuscripts [15] , geology, astronomy [20] [31] , space exploration, wildlife monitoring, etc. As ubiquitous as DTW is, we believe that there are thousands of research efforts that would like to use DTW, but find it too computationally expensive. For example, consider the following: \"Ideally, dynamic time warping would be used to achieve this, but due to time constraints\u2026\" [5] . Likewise, [3] bemoans DTW is \"still too slow for gesture recognition systems\", and [1] notes, even \"a 30 fold speed increase may not be sufficient for scaling DTW methods to truly massive databases.\" As we shall show, our subsequence search suite of four novel ideas (called the UCR suite) removes all of these objections. We can reproduce all the experiments in all these papers in well under a second. We make an additional claim for our UCR suite which is almost certainly true, but hard to prove, given the variability in how search results are presented in the literature. We believe our exact DTW sequential search is much faster than any current approximate search or exact indexed search. In a handful of papers the authors are explicit enough with their experiments to see this is true. Consider [28] , which says it can answer queries of length 1,000 under DTW with 95% accuracy, in a random walk dataset of one million objects in 5.65 seconds. We can exactly search this dataset in 3.8 seconds (on a very similar machine). Likewise, a recent paper that introduced a novel inner product based DTW lower bound greatly speeds up exact subsequence search for a wordspotting task in speech. The authors state: \"the new DTW-KNN method takes approximately 2 minutes\" [41] ; however, we can reproduce their results in less than a second. An influential paper on gesture recognition on multi-touch screens laments that \"DTW took 128.26 minutes to run the 14,400 tests for a given subject's 160 gestures\" [38] . However, we can reproduce these results in under 3 seconds."], "relatedWork": ["Our review of related work on time series indexing is necessarily superficial, given the vast amount of work on the topic and page limits. Instead, we refer the interested reader to two recent papers [6] [28] , which have comprehensive reviews of existing work. It has now become common (although not yet routine) to see papers indexing/mining datasets with millions of objects. For example, Jegou et al. have demonstrated very fast approximate main memory search of 10 million images [17] . However, this work and much of the current work that addresses multi-million object datasets focus on approximate search, whereas we are only considering exact search here. Moreover, we are interested in datasets which are five to six orders of magnitude larger than anything else considered in the literature [6] . Thus, comparisons to related work are very difficult to do meaningfully."], "rq": ["7. what can we say about the best match for the full q?"]}
{"intro": ["Because the human-like shapes are more or less unique in the real world, they may provide a powerful clue for human detection and tracking [8] , [11] , [34] , [38] . The difficulty of analyzing human shapes lies in the fact that the local shape nonrigidity has a large number of degrees of freedom thus having very complicated uncertainties, which makes rulebased methods unsuitable. Thus, learning-based methods are generally adopted for learning the shape distributions from training data. If the uncertainty is simple, parametric methods such as Gaussian models or Gaussian mixture models can do a good job, e.g., in face shape [5] . Unfortunately, because of the local nonrigidity, the uncertainty in human shapes is too complex to be sufficiently modeled by reasonable Gaussian mixture models. On the other hand, nonparametric methods, e.g., by using exemplars [11] , can be quite flexible. However, a huge set of exemplars is needed to represent the concept of the human-like shapes in order to accommodate the possible variations. As a trade-off, because the Gibbs distribution is flexible to capture a large variety of densities, it can naturally be employed for this task. This idea has been exploited for modeling the face deformations [20] , where the face is represented as a random vector and an inhomogeneous Gibbs distribution can be learned from training data. Although this model is complicated and needs quite complex training, its excellent performance suggests that the Gibbs distribution is useful for characterizing the large and complex variability of the human-like shapes."], "relatedWork": [], "rq": [" but it does leave a problem: how sensitive is the trained model to the alignment errors?"]}
{"intro": ["The 3-d problem is especially important. When the dimension is 4 or more, the consensus is that O(log n+k) query time is not possible with near linear space [18] . Furthermore, via a standard lifting transformation, the 3-d halfspace range reporting problem includes as special case 2-d circular range reporting: preprocess n points in the plane so that the points inside a query circle, of any radius, can be reported quickly. In turn, this problem is closely linked to 2-d k-nearest-neighbors search: preprocess n points in the plane so that the k exact nearest neighbors to a query point, for any k, can be reported in arbitrary order quickly, where distances are measured in the Euclidean metric. Both 2-d problems are among the most natural proximity problems imaginable, and have been studied extensively since the early days of computational geometry [19, 21] . Table 1 summarizes previous work. The results in the early entries are perhaps portrayed a little unfairly, because basic knowledge in the field was still being developed at the time; for example, Bentley and Maurer's space requirement automatically improves to O(n 3 ) by known combinatorial bounds on order-k Voronoi diagrams, and Chazelle and Preparata's space requirement is actually O(n log 2 n log log n) by Clarkson and Shor's bounds on (\u2264 k)-sets [14] . By the early 90s, with the work by Matou\u0161ek [18] and its precursors, powerful tools such as cutting lemmas and partition theorems have been identified to tackle range searching problems in general, even in higher dimensions [3] ; in fact, the above table entries on Matou\u0161ek's results were not explicitly stated in his paper [18] , which focused on halfspace range reporting in dimensions larger than 3. However, despite these advances, for 3-d halfspace range reporting or 2-d circular range reporting, the current best data structures with O(log n+k) query time require superlinear O(n log log n) space, as found by Chan [8] or Ramos [20] or P. Agarwal (personal communication) ."], "relatedWork": [], "rq": [" : is the o(log r) crossing number bound in the 3-d shallow partition theorem optimal?"]}
{"intro": [], "relatedWork": ["Pspace-complete b Same as for 1-LTL\\X c The hardness result holds for controlled clique parameterized topologies process quantifiers of the same type (all existential or all universal). Furthermore, this is also the case if one allows enhanced versions of these quantifiers that specify that the processes quantified are different, and/or are neighbours (or not) in the topology (see [2] for a definition of these enhanced quantifiers). This allows one, for example, to express mutualexclusion: \u2200i = j. G(\u00ac(critical, i) \u2228 \u00ac(critical, j)). For the case of full k-indexed LTL\\X where alternation of universal and existential quantifiers is allowed, many of the corresponding upper bounds are still unknown, and represent another direction for future work. We now briefly describe what needs to be done to get this extension. All the upper bounds concerning 1-indexed LTL\\X were stated with respect to homogeneous parameterized topologies. 13 Lemma 3 shows that, for 1- indexed LTL\\X, such systems can be simulated by cliques. However, looking at the proof of the lemma, it is not hard to see that this simulation actually works irrespective of the specification logic. Indeed, in the controllerless case we actually get that the set of runs of the cliques is exactly equal to the set of runs of the homogeneous topologies; and in the controlled case this is also true except for a slight technical mismatch between the structure of global configurations in these two types of systems-due to the fact that the single controller of a controlled clique simulates (using a product process template) all the controllers specified by the homogeneous parameterised topology skeleton. However, this technical mismatch is syntactic in nature, and is easily overcome by mapping each coordinate in the state of the unique clique controller to the corresponding controller vertex in the homogeneous topology. Also note that runs of a given topology G in the homogeneous parameterized topology are simulated by runs of a clique topology G of the same size or smaller; conversely, all runs of a simulating clique topology G correspond to runs of topologies in the parameterized homogeneous topology that are larger by at most a constant factor (namely, the number of controllers in the skeleton of the homogeneous topology minus 1). Armed with the above observations, extending our upperbounds from 1-indexed LTL\\X to the universal and existential fragments of k-indexed LTL\\X now requires the following. First, we can easily extend the construction in the proof of Lemma 3 to have the controller of the clique simulate not only the controllers of the homogeneous topology, but also any other k nodes of k different types. Combining this with the observation made in Lemma 2 that, due to symmetry, in a homogeneous system the executions of all processes of a given type are exactly the same, we reach the following conclusion: we can replace reasoning about properties of the set of all runs of a homogeneous parameterised system projected onto processes of k different types with reasoning about the 1-executions of the unique controller of a parameterized clique topology, projected onto the relevant k simulated nodes of interest. Moreover, this reduction incurs only a constant blowup (assuming k and the communication alphabet are fixed). Observe that in case we started with a homogeneous parameterized topology with no controllers (and k types of interest), we do not have to simulate it with a controlled clique-topology. Instead, we can simulate it with a clique topology with two types: one type that is the disjoint union of all the process templates (as in the basic construction in Lemma 3), and one which is the product of the k process templates of interest (similar to the controller casebut not designated as a controller, i.e., allowing one to have many nodes of this product type). Thus, in all cases we can reduce questions about universal and existential k-indexed LTL\\X formulas with respect to a homogeneous parameterized topology to a question about a 1-indexed LTL\\X formula with respect to a clique topology of the same type (controlled or uncontrolled), with a constant blowup."], "rq": [" (2) can be simulated by a synchronous transition using the message inc (2), where the process at vertex v that is running the controller sends inc(2) and updates it's state from i j to i j+1 (simulating the 2cm move to the next instruction), and some vertex w running a memory process with a 0-valued counter 2 bit receiving inc(2) and updating this bit to 1. simulating a jz instruction is slightly more involved since there is no direct way for the controller to query all memory processes for their bit values. however, if all the bits of counter i in all memory processes are 0, then none of these processes is in a state with an outgoing local transition labeled by dec(i)?"]}
{"intro": [], "relatedWork": [], "rq": [" a natural question then is: is every triangulation of a sphere the boundary complex of some simplicial polytope?"]}
{"intro": ["In each of the above examples, the preference for the presuppositionally stronger sentence (and the oddity of the weaker one) is derived as follows. The sentencepairs are contextually equivalent, and as such, a cooperative speaker who obeys MP would use the presuppositionally stronger alternative when the presupposition holds. A speaker's choice of the weaker variant therefore invites an inference, an antipresupposition, that the speaker believes that the relevant presupposition is not met in the context of utterance. However, our common knowledge about the uniqueness of the sun, the duality of human hands, etc. ensures that the presuppositions of the sentences above are met in all typical contexts. The perceived oddity of the presuppositionally weaker sentences derives from the fact that the anti-presuppositions triggered by their use (e.g. that the speaker believes there is more than one sun) run counter to our expectations (about the speaker's knowledge 1 ) about the world. We also find purported MP-effects involving additive presupposition triggers (\"obligatory additivity effects\" henceforth) (Amsili & Beyssade 2009; Chemla 2008; Singh 2011) . 2 Unlike the canonical cases of MP in (1) and (2), competition in (3-5) seems to be between the additive expression and its absence, raising questions about the nature of these effects. Are they in fact MP effects? (3) a. Dana went to a party. Lee went to a party, too."], "relatedWork": [], "rq": [" a question that arises is: why did mp-items incur longer decision times than fillers, whereas with additives, decision times were as fast as with the fillers?"]}
{"intro": [], "relatedWork": [], "rq": [" the reader may well ask: why do i have to decide on a \"reasonable\" prior?"]}
{"intro": ["One might think that the picture would be different for the public ontologies of the Semantic Web. The general spirit of the Semantic Web wants data to be shared across application and community boundaries 3 . However, even the ontologies of the Semantic Web are not available for arbitrary re-publication. Table 1 shows some popular ontologies mentioned together with their licenses. None of the ontologies is available in the public domain. All of them require at least an acknowledgment when their data is re-published. It is considered dishonest to sell or re-publish the data from an ontology elsewhere without giving due credit to the original.", "Some data sets are not freely available at all (see Table 1 ). The Wolfram Alpha data set 4 , for example, can be queried through an API, but cannot be downloaded. Its terms of use prohibit the systematic harvesting of the API to re-create the data set. Any re-publication of a substantial portion of such data constitutes a breach of the terms of use. Similar observations apply to trueknowledge 5 or the commercial version of Cyc [9] . In all of these cases, the extraction and systematic dissemination of the data is prohibited. This raises the issue of how we can detect whether an ontology has been illegally re-published. We call a person who re-publishes an ontology (or part of it) in a way that is inconsistent with its license an attacker. The attacker could, e.g., re-publish the ontology under his own name or use parts of the ontology in his own ontology without giving due credit. We call the source ontology the original ontology and the re-published ontology the suspect ontology. We want to solve the problem of ownership proof: How can we prove that the suspect ontology contains part of the original ontology? Obviously, it is not sufficient to state that the suspect ontology contains data from the original ontology. This is because ontologies contain world knowledge, that anybody can collect. Take the example of an ontology about scientists: the attacker could claim that he also collected biographies of scientists and that he happened to produce the same data set as the original ontology. A similar argument applies to ontologies that have been derived from public sources. YAGO [15] and DBpedia [2] , e.g., have both been extracted from Wikipedia. An attacker on DBpedia could claim that he also extracted data from Wikipedia and happened to produce a similar output. One might be tempted to assume that we could simply publish the original ontology with a time stamp (in the style of proof of software ownership). For example, we could upload the ontology to a trusted external server. If someone else publishes the same data later, then we could point to the original copy. However, this does not prove our ownership. The other publisher could have had the data before we published ours. The fact that he did not publish his data cannot be used against him."], "relatedWork": ["In [4] , the authors introduce named graphs as a way to manage trust and provenance on the Semantic Web. Named graphs, however, cannot guard against the misuse of ontologies that are publicly available.", "Quite a number of approaches have targeted semi-structured data [13] and relational databases [1, 14, 7, 8, 12] . These works provide one way to prove ownership of ontologies. Most of them are blind, i.e., they do not require the original data set for detection. However, all of these approaches presume that the schema of the data is known. This is not necessarily the case on the Semantic Web. Some ontologies (such as DBpedia or YAGO) have hundreds of relationships. An attacker just has to map some of them manually to other relationships to obscure the theft. We will develop approaches that can still identify the suspect data, but in a non-blind way. Furthermore, the classical methods work by voluntarily altering data. Therefore, we call these methods modification approaches in the sequel. Such approaches could, e.g., change the birth year of Elvis Presley from the year 1935 to the year 1936. While the introduced errors are only gradual for numerical data, they are substantial for categorical data. Such an approach could, e.g., change Elvis' nationality from American to Russian. Apart from the fact that an ontology owner will find it controversial to voluntarily alter clean data, such an approach will also decrease the precision of the ontology with respect to the real world. Furthermore, the altered facts can lead to contradictions with other data sets. This is not only annoying to the user, but can also allow the attacker to detect and remove the altered facts. Therefore, we present an alternative approach in this paper, which works by deleting carefully selected facts. Since the Semantic Web operates under the open world assumption, the absence of true information is less harmful than the presence of false information. If O contains the mark, then it is assumed that O has indeed been derived from O * . However, the watermarking protocol may erroneously say that O has been derived from O * , even though O just contains the mark by chance. For example, O may be a totally unrelated ontology. In this case, O is called a false positive. Watermarking protocols are designed so that the probability of a false positive is provably below a confidence threshold \u03be. The parameter \u03be is called the security parameter of the protocol and is typically in the order of \u03be = 10 \u22126 ."], "rq": [" we want to solve the problem of ownership proof: how can we prove that the suspect ontology contains part of the original ontology?"]}
{"intro": ["using Lean (Hines et al. 2018) . However, construction productivity has remained stagnant or regressed, and thus the sector has looked to LC as an antidote to the ills of the sector (Koskela 1999) and as a means of delivering the requisite value that clients have been longdemanding (Koskela 1992; Ballard 2000; Hamzeh et al. 2009) ."], "relatedWork": [], "rq": ["question 1: what differences exist between individual trades' ppc?"]}
{"intro": ["Auditing energy consumption in buildings is a common practice for facilities managers (FMs) in medium to large organisations, and legally mandated in countries such as the United Kingdom (UK). Energy audits seek to identify inefficiencies in building design and operation to reduce running costs and ensure standardscompliance [1] . Sensor toolkits have emerged as a key tool to support audits, however, their roles in this are yet to be reported in academic literature. In this article, we seek to understand the design of sensor toolkits for effective building management. We investigate how sensor toolkits are used by FMs to augment their existing audit practices with additional data. We study the application of an environmental sensor toolkit, BuildAX 1 , which sits at a mid-point of typically sensed variables, with flexibility and robustness for rapid deployment and reusability, addressing the needs of the auditors who participated in our study."], "relatedWork": [], "rq": [" this raises two questions for ubicomp and hci researchers in future work: how do we better support such standards, and how can we build technologies that question the standards and policies themselves?"]}
{"intro": ["In the real-world, however, graphs can be both structurally large and complex [Ahmed et al. 2014; Leskovec and Faloutsos 2006] as well as noisy [He and Kempe 2014] . These pose a significant challenge to graph mining techniques, particularly in terms of performance [Ahmed et al. 2017 ].", "Recently, new methods have emerged to address the challenges facing relational (graph) learning problems by incorporating attention to improve graph deep learning methods. An attention mechanism aids a model by allowing it to \"focus on the most relevant parts of the input to make decisions\" [Velickovic et al. 2018] . Attention was first introduced in the deep learning community to help models attend to important parts of the data [Bahdanau et al. 2015; Mnih et al. 2014] . The mechanism has been successfully adopted by models solving a variety of tasks. Attention was used by Mnih et al. [2014] to take glimpses of relevant parts of an input image for image classification; on the other hand, Xu et al. [2015] used attention to focus on important parts of an image for the image captioning task. Meanwhile Bahdanau et al. [2015] utilized attention for the machine translation task by assigning weights that reflected the importance of different words in the input sentence when generating corresponding words in the output sentence. Finally, attention has also been used for both the image as well as the natural language [Kumar et al. 2016] question answering tasks. However, most of the work involving attention has been done in the computer vision or natural language processing domains."], "relatedWork": [], "rq": [" (2) can walks effectively capture all task-relevant information from a graph, especially if there are complex structural patterns involved?"]}
{"intro": ["A natural question that is of relevance to the CHI community is then: what should the specifics of such an alert be? Indeed, various aspects of handover [20] and in-car alerts [44, 54] have received attention in the literature. Currently alerts, such as in the Tesla model S, include a brief alert and immediately handover control to the driver. However, given that the driver may not have the proper situational awareness to be able to immediately take the proper action, immediate alerts as such can potentially result in fatal outcomes. Moreover, with higher levels of automation (e.g., level 3 in [47] ) such alerts are even more crucial as the car is monitoring the environment for large time intervals and drivers might have disengaged (cf. [16] )."], "relatedWork": [], "rq": ["a natural question that is of relevance to the chi community is then: what should the specifics of such an alert be?"]}
{"intro": ["Aesthetics is defined as the science of the senses, exploring the beauty and art as fiction [1] . Methodological basis for art is formed by employing applied spheres of research, cultural tradition, and certain beauty criteria. So, the aesthetics are inherent in the process of an artistic creation and as a result creating emotional feelings, although, contemporary art can have a value by not achieving criteria of beauty. However, the authors, in accordance with the provision of art \"measurement\" opportunity, set by aesthetic, functional, environmental coherence and other criteria of material environmental design, defines the aesthetic term as corresponding to the requirements of beauty criteria and functional integrity [34] ."], "relatedWork": [], "rq": [" 2. what is the minimum distance from your home you would prefer national significance monument (in meters)?"]}
{"intro": [], "relatedWork": [], "rq": [" a second question concerns the dimension of synchrony: is synchrony an all-or-none condition (synchronous versus nonsynchronous)?"]}
{"intro": ["However, it remains unknown whether there is a limit to what can be encoded into traditional static crowdsourcing workflows, and in particular why many complex goals have remained a struggle to achieve [32] . Workflows for complex goals such as invention [69] [70] [71] , writing [1, 30, 33, 48, 60] and product development [39, 52, 72] today only succeed in limited scenarios and with specific constraints. Challenges include poor early results that derail later stages [44] , uncoordinated contributions that make inconsistent changes [9, 33] , and workers who do not have sufficient global context to make effective decisions [16, 30, 36] . Crowd-written blogs, articles, and stories, for example, repeat content and have an inconsistent voice [1, 9, 30, 36] . Other domains struggle with similar issues. Why are such challenges so pernicious?", "To date, researchers and practitioners have sought to expand the boundary of complex crowd work without a guiding theory of what will work and what may not. Our work clarifies that crowdsourcing workflows will remain a good fit when contingencies are unlikely and adaptation is unnecessary. However, if the work is the kind where even an expert often reflects, revises, and adapts, then workflows will struggle to succeed. We offer design changes to crowdsourcing platforms that may ameliorate these issues: for example, a panic button that allows workers to call a requester's attention to a task if a contingency has arisen, and a revise button that allows a worker to return a task to an upstream stage in the workflow with comments. More aggressively, alternative crowdsourcing approaches may succeed: those which sacrifice some automation for improved ability to adapt, for example reflection-based [30] or organization-based [64] approaches."], "relatedWork": [], "rq": [" workflows like find-fix-verify [9] are attractive in part because they run predictably, at scale, and without intervention. enabling adaptation increases coordination costs, trading off some of these qualities. the open question is: are these tradeoffs worth making?"]}
{"intro": ["Social media has been considered as a means to disseminate health information [2, 6] and as a way to provide online social support for health issues [3, 28, 59] . Mental health is traditionally stigmatised by society [4] , meaning that those suffering from such illness may find it harder to disclose details of their disorders [21] . However, the reduced social presence afforded by the Internet may assist individuals to disclose details about their mental health [42] . In the work presented here, we investigate the degree to which people are using social media to discuss their mental health conditions, and how exactly they are doing this. Our work makes particular reference to insomnia and to the microblogging social media service Twitter.", "Insomnia is one of the most prevalent mental health symptoms reported to health professionals in the UK [39] and is estimated that insomnia affects 30% of the world's population at some point during their lifetime [27, 43] . However, it is estimated that only a small proportion of insomnia sufferers actually seek medical help other than over-the-counter sleep aids [29] . One of the challenges, therefore, facing sleep experts is that of dissemination of accurate information regarding healthy sleep routines and the promotion of sleep therapy."], "relatedWork": ["Twitter posts have recently been extensively data mined to identify political and economic trends [54, 38, 7] , track natural disasters [46] and diseases [5] , and to measure consumer opinion [48] . However, whilst this type of research can tell us about the different subjects that Tweeters are discussing, it doesn't tell us how Tweeters are using Twitter. We are interested, from an HCI perspective, in the different purposes in which social media is being used in order that subsequent designs and affordances of such services can be improved. Knowledge of purpose will inform the development and design of social media and, with particular regards to insomnia related use, can provide useful information for researchers looking to implement online mental health interventions."], "rq": ["2) is much larger than in the non-specific group (mean difference = 3.40, sd = 7.76). this is a significant interaction (f(1)=1069.94, p <.001) albeit with a very small effect size (partial \u03b7 2 = .03). when we look at the use of time in conjunction with tense, a correlation analysis shows us a stronger relationship between time and present tense in the non-specific tweets (r = .166, p <.001) than in the insomnia tweets (r = .112, p <.001). a fisher\\'s r to z transformation shows that this difference is significant (z = 5.25). this strong use of present tense with regards to time could be an artefact of twitter itself. when a user publishes a tweet using the twitter website they are asked \"what\\'s happening?"]}
{"intro": ["Of much interest to us are the non-convex penalty of Huang et al. (2011) and the convex penalty of Jacob et al. (2009) . Given a pre-defined set of possibly overlapping groups of variables G, these two structured sparsity-inducing regularization functions encourage a sparsity pattern to be in the union of a small number of groups from G. Both penalties induce a similar regularization effect and are strongly related to each other. In fact, we show in Section 3 that the penalty of Jacob et al. (2009) can be interpreted as a convex relaxation of the non-convex penalty of Huang et al. (2011) . These two penalties go beyond classical unstructured sparsity, but they are also complex and raise new challenging combinatorial problems. For example, Huang et al. (2011) define G as the set of all connected subgraphs of G, which leads to well-connected solutions but also leads to intractable optimization problems; the latter are approximately addressed by Huang et al. (2011) with greedy algorithms. Jacob et al. (2009) choose a different strategy and define G as the pairs of vertices linked by an arc, which, as a result, encourages neighbors in the graph to be simultaneously selected. This last formulation is computationally tractable, but does not model long-range interactions between features. Another suggestion from Jacob et al. (2009) and Huang et al. (2011) consists of defining G as the set of connected subgraphs up to a size k. The number of such subgraphs is however exponential in k, making this approach difficult to use even for small subgraph sizes (k = 3, 4) as A sparsity pattern forming a subgraph with two connected components is represented by gray nodes. Right (b) : when the graph is a DAG, the sparsity pattern is covered by two paths (2, 3, 6) and (9, 11, 12) represented by bold arrows."], "relatedWork": [], "rq": ["these observations naturally raise the question: can we replace connected subgraphs by another structure that is rich enough to model long-range interactions in the graph and leads to computationally feasible penalties?"]}
{"intro": ["The use of documents in advisory and service encounters is an all-round routine: medical personnel fill out paper forms during admission (Berg 1996) , policemen take notes when recording a case (Sellen and Harper 2002) , supervisors go through documents together with their students (Svinhufvud and Vehvil\u00e4inen 2013) , and financial advisors take notes and explain things by drawing on paper during financial advisory encounters. Despite the technological development and infusion of modern technologies into services, for instance through online service provision channels, face-to-face encounters have remained paper-based. The persistent presence of paper in the advisory encounters is not in itself a problem. On the contrary, we show how essential paper may be for the choreography of interactions during an encounter. However, post-crisis policies (EU 2014; CH 2015; DE 2016) oblige financial institutions to support their service encounters with digital tools: Choosing appropriate products, documenting the advisory process and outcome, and educating clients is increasingly less feasible via conventional tools such as brochures, paper and a pen. Further, the banks wish to use the information captured in advisory sessions for their marketing. Thus, numerous design and research projects have been launched to develop dedicated IT to support financial advisory encounters (Schwabe and Nussbaumer 2009; Heyman and Artman 2015; Kilic et al. 2015) . Many studies propose replacing paper-based interactions with interactive IT-based elements. Nonetheless, such applications have had little commercial success, particularly in retail banking (Schwabe and Nussbaumer 2009; Heyman and Artman 2015) . We argue that the reason for this lack of success includes the misunderstanding of the paper's roles in an advisory encounter: paper's functions go far beyond a medium for note-taking and the visualisation of information. While focusing on mortgage advisory encounters at a bank, we describe paper practices and illustrate how paper is used to establish and preserve a specific social order, to make impressions on a client, and to impose a structure in the cooperative, face-to-face interactions between advisor and advisee."], "relatedWork": [], "rq": [" they have left many questions unanswered: what does it mean when someone puts a post-it on a blackboard?"]}
{"intro": [], "relatedWork": ["Property testing. Our algorithms are related to Property Testing algorithms on dense graphs (Goldreich, Goldwasser, & Ron, 1998) , and in particular, are inspired by the CLIQUE-testing algorithm in that paper. Such algorithms are designed to decide whether a given dense graph has a certain property or whether many edge modifications should be performed so that it obtains the property. In a manner that is similar to the approximate solutions studied in this paper, many testing algorithms can be modified so as to obtain approximate solutions to the corresponding search problems. However, none of the known property testing algorithms, or their extension to approximation algorithms, directly apply to our problem. In particular, the most general family of graph properties studied in Goldreich, Goldwasser, and Ron (1998) does not capture our definition of clustering which allows for overlapping subsets of vertices. Other related work on approximation algorithms and testing algorithm on dense graphs includes (Arora, Karger, & Karpinski, 1995; Frieze & Kanan, 1999; Alon et al., 2000) ."], "rq": [" now we have to address a new problem: how do we decide which subset is the good seed?"]}
{"intro": ["Owing to the development of new mobile and wireless technology, attention around wearable devices have been increasingly paid. Among many types of wearable devices, wearable smart watch has been often mentioned because it is familiar like our wrist watches and at the same time has various functions such as health monitoring, message, weather forecasting, and navigation. As those functions have been varied, the popularity of wearable smart watches has been increased and welcomed as a newbridge that brings technologies to the people (Isacson, 2015) . However, as wearable smart watches are getting complicated in terms ofsuch functions, more cognitive load is required to use the product. What makes it worse is that the user should operate functions with a couple of buttons and the tiny touch screen of the wearable smart watch. As an alternative, natural user interface, which means interface that mainly operates with natural interactions through expressing gesture, acting or noticing the surroundings by sensors or corporal parts (Valli, 2008) , is an emerging topic for human computer interaction technology. It has been developed with the aims of incorporating the wider scope of multimodal interaction (Liu, 2010) . There have been some studies in which it was investigated how natural user interface is applicable with wearable smart watches. For instance, Knibbe et al. (2014) attempted to see the possibility of natural user interface with smart watches by extending the interactive surface for a smart watch to the back of the hand in order to reduce screen occlusion by enabling off-device gestural interaction. In another study done by Chi, Chen, Liu,and Chu(2007) , our everyday objects in our living environment such as writs watch and fridge augmented with digital technology were investigated tosee whether augumented interaction matches or conflicts with its original (i.e. natural) interaction. Also, designs of wearable smart watches that aim to provide natural interaction may not properly fit, or even it might be unnatural to wear a smart watch on the wrist (Han, Han, & Kim, 2015) .Besides them, there would be many other issues regarding delivering better user experience with wearable smart watches.", "Therefore, this study attempts to figure out issues related to natural user interface of wearable smart watches. Based on the findings, it also aims to develop guidelines which can provide a better understanding of natural user interface of wearable smart watches with design practitioners. In order to achieve the goals, three research questions were formulated: 1) what are the people's needs and wishes with wearable smart watches? 2) how can wearable smart watches ideally offer natural user experience considering the needs and wishes, and 3) in which way user interface can be designed in order to deliver natural interaction? www.aodr.org 47 2. Literature Review Natural User Interface Natural interaction deals with the issues about skills of people used to interact with the real world and manipulate the traditional objects, considering ways of using current available technology without cognitive loads (Valli, 2008) . In the past, researchers in the field of HCI have developed interface styles to provide natural interaction called WIMP(Window, Icon, Menu, and Pointer). The styles, however, was hard to be said as natural interaction, because keyboard or mouse itself was already far from natural way that human interacts ( Based on the above studies, it was found that the main concept in natural interaction is knowledge and skills of the physical world involved in human computer interfaces. We considered the notion of reality based on interaction in our study because the framework, put forward by Jacob et al. (2008) ,explains such interfaces that normally build on people's understanding and skills of the everyday in the physical world. They argued that there are four classes of skills in the physical world, which are exploited by reality based interaction designs, because they allow people to take advantage of their real world skills in interacting with smart computing devices, reducing cognitive load and the required mental efforts. Furthermore, they claimed that this may enhance learning, improve performance and foster extemporization and exploration, since people are not knuckled down to learn interfacespecific skills and expertise. These four classes of skills are described as follows (Jacob et al.,"], "relatedWork": [], "rq": [" 1) what are the people's needs and wishes with wearable smart watches?", " 2) how can wearable smart watches ideally offer natural user experience considering the needs and wishes, and 3) in which way user interface can be designed in order to deliver natural interaction?"]}
{"intro": [], "relatedWork": [], "rq": ["6) is not the only factor that determines how constraining it is, or how difficult it will be to process. instead, one should look at its usage in different context (i.e., specificity of the connective usage in the natural text). for example, based on the measurements presented in the figure 3 we would expect a relatively high constraining effect of the connectives such as for example and instead. note however that these predictions strongly depend on the discourse relation sense inventory and the discourse relation hierarchy. in particular, it is important to ask in how far computational linguistics resources, like the pdtb, reflect the inference processes in humans -in how far are the sense distinction and hierarchical classification cognitively adequate?"]}
{"intro": ["Despite this rich body of work, there are few guidelines on how to encode specific information using the different modalities. Although it has been shown that, for instance, rhythm is an effective audio and tactile parameter, the mapping of information to this parameter in experiments is usually random. For example, in Brown et al. [4] Tactons were designed to represent calendar alerts. However, this may not be the best type of information to represent with those particular Tactons. The question now is how to make use of this research to provide an alternative to all of the different types of visual information provided by different mobile applications. Therefore, this paper discusses the possible meanings that can be conveyed by Earcons [3] and Tactons [4] . How can audio and tactile icons be designed to optimise congruence between crossmodal feedback and the type of information this feedback is intended to convey? Congruence is a relationship between objects that implies agreement, harmony, conformity or correspondence (American Heritage Dictionary). In terms of this research, we define congruence as an intuitive match or harmony between the designs of feedback from different modalities and information types. For example, if we have a set of system warnings, confirmations, progress updates and errors: what audio and tactile representations best match the information or type of message? Is one modality more appropriate at presenting certain types of information than the other (e.g. [10] )?"], "relatedWork": [], "rq": [" progress updates and errors: what audio and tactile representations best match the information or type of message?"]}
{"intro": ["We use our findings as a platform to reflect on opportunities, challenges and areas of future research in the development of programmable money and related financial technologies. Specifically, we reflect on the ever-closer union of money and data, which will see money and finance proliferate across digital services. Following in the steps of previous HCI research there is the opportunity for digital monies to become more playful and expressive, rather than simply a matter of convenience. As such we surface how programmable money could provide much greater individual control and configurability of one's money, but only if users are supported in realizing the meaning and opportunities of one's 7 https://ec.europa.eu/info/law/payment-services-psd-2-directive-eu- 2015-2366_en financial data. However, we also point to areas of concern if money that is programmed by others can be used as an instrument of power and compliance. We conclude that HCI research in this context should therefore be sensitive not only to individual interactions with new forms of money, but the infrastructures of which they are a part."], "relatedWork": [], "rq": [" second: what happens when data from these other services now has real implications for your money?"]}
{"intro": [], "relatedWork": [], "rq": ["1) is unavailable to me, but compare l. bloomfield\\'s review (1932) . 3 \\'bei der neigung der griechen zur verhauchung eines \u03c3 zwischen zwei vocalen ist die erhaltung des sibilanten bei gleicher stellung im aorist verwunderlich\\' (curtius 1876: 277). 4 \\'aoriste wie ,\u03ad \u03c0\u03c1\u03b1\u03be\u03b1, ,\u03ad \u03b3\u03c1\u03b1\u03c8\u03b1, , \u03ae\u03bb\u03c0\u03b9\u03c3\u03b1 von consonantischen st\u00e4mmen waren in hinreichender anzahl vorhanden, um den klang des \u03c3 als einen f\u00fcr diese tempusbildung bezeichnenden dem sprachgef\u00fchl einzupr\u00e4gen\\' (curtius 1876: 278). 5 \\'erstens: das bei den consonantischen verbalst\u00e4mmen bleibende \u03c3 hat nicht erhaltend auf den entsprechenden spiranten bei den vocalischen stammen eingewirkt, sondern vielmehr restaurierend; zweitens: ein anderer erkl\u00e4rungsgrund f\u00fcr das \u03c3 in ,\u03ad \u03bb\u03c5\u03c3\u03b1, ,\u03ad \u03c3\u03c4\u03b7\u03c3\u03b1 u. s. w., als der, dass es durch die form\u00fcbertragung von den aoristen consonantischer st\u00e4mme wiederhergestellt sei, braucht nicht und kann nicht geltend gemacht werden\\'. (osthoff 1878: 325 emphasis in original) 6 \\'im griechischen ist sowol einfaches urspr\u00fcngliches s zwischen vocalen als auch das s von sj ausnahmslos ausgefallen.\\' (osthoff 1878: 325 emphasis in original) 7 an einem bestimmten tage mittags 12 uhr sei der ganz genaue zeitpunkt gewesen, wo in der antehistorischen sprache des alten griechenlands der spirantenschwund des \u03c3 zwischen vocalen seinen endgiltigen vollzug erreichte, ganz unmittelbar darauf, eine secunde etwa nach 12 uhr oder auch in v\u00f6llig ebendemselben momente mit dem verklingen des letzten intervocalischen \u03c3, sprach einer ein ,\u03ad \u03bb\u03c5\u03c3\u03b1, indem ihm dabei aoriste wie ,\u03ad \u03c4\u03b5\u03c1\u03c8\u03b1 im sinne lagen: darf man sagen, in diesem ,\u03ad \u03bb\u03c5\u03c3\u03b1 habe das \u03c3 den spirantenschwund \u00fcberdauert, oder ist es nicht vielmehr ein wenn auch nach noch so verschwindend kurzer zwischenzeit wieder aufgefrischtes oder wiederhergestelltes, also notwendig ein junges \u03c3?"]}
{"intro": [], "relatedWork": [], "rq": ["architecture: is the query system standalone, or does it exist in a client-server architecture?"]}
{"intro": [], "relatedWork": [], "rq": [" raise a serious question: how does video technology influence teaching and learning?"]}
{"intro": ["We consider small world graphs as defined by Kleinberg [7] , i.e., graphs obtained from a d-dimensional mesh, for some fixed d \u2265 1, by adding long-range links chosen at random according to the d-harmonic distribution, i.e., the probability that x chooses y as long-range contact is h(x, y) = 1/(Z x \u00b7dist(x, y) d ) where dist() is the Manhattan distance in the mesh (i.e., the distance in the L 1 metric), and Z x is a normalizing coefficient (cf. Sects. 5.1 and 5.2 for more details). This model aims at giving formal support to the \"six degrees of separation\" between individuals experienced by Milgram [14] , and recently reproduced by Dodds, Muhamad, and Watts [5] (see also [1] ). In a social context, professional as well as leisure occupation, citizenship, geography, ethnicity, and religiousness are all intrinsic dimensions of the human multi-dimensional world, playing different roles with possibly different impact degrees [6] . Each of these dimensions should be used as an independent criterion for routing in the social graph. In this context, one would thus expect that the more criteria used the more efficient the routing should be. Surprisingly however, Kleinberg's model does not reflect this fact, in the sense that greedy routing has the same performance whether the number of mesh dimensions considered is one, two, or more. Indeed, Kleinberg has shown that greedy routing in the n-node d-dimensional mesh augmented with long-range links chosen according to the d-harmonic distribution performs in O(log 2 n) expected number of steps, i.e., independently of d. (This bound is tight as it was shown in [3] that greedy routing performs in at least (log 2 n) expected number of steps, independently of d). Kleinberg has also shown that augmenting the d dimensional mesh with the r -harmonic distribution, r = d, results in poor performance, i.e., (n \u03b1 r ) expected number of steps for some positive constant \u03b1 r . Furthermore, it is shown in [2] that, in the 1-dimensional mesh augmented according to any probabilistic distribution, greedy routing performs in (log 2 n/ log log n) expected number of steps, and this lower bound is conjectured to hold in higher dimensions."], "relatedWork": [], "rq": ["we address the following question: is there some additional \"topological awareness\" that could be given to nodes so that greedy-like routing performs in less than (log 2 n) expected number of steps in the augmented d-dimensional mesh, at least for d > 1?"]}
{"intro": ["Amstrong [6 -8] drew attention to the fact that indi erence relations are not transitive because the human mind is not necessarily capable of perfect discrimination, and introduced the notion of semiorders into economic theory. Yet semiorder was axiomatized in a more precise way by Luce [14] who introduced a new numerical representation for semiorders with a constant error. However, one limitation of these studies is that they all worked with constant errors. This idea was developed in many publications, the most recent of which are Fishburn [13] and Pirlot and Vincke [15] . Agaev and Aleskerov [3] and Aizerman and Aleskerov [4] take into consideration generalized models of interval choice in which the error functions are dependent on the feasible set of alternatives. Two cases of numerical representation with speci\u00ffcally de\u00ffned error functions are analyzed in this paper. The \u00ffrst case of error functions depends on the set of feasible alternatives. This is an additive function that depends on the alternatives separately. The second case of error functions depends on compared alternatives x and y. We also assume that this function is multiplicative and we consider the cases when the error function (x) depends on the utility function u(x)."], "relatedWork": [], "rq": [" the question arises: is it possible to \u00ffnd a class of binary relations which is a proper subset of the semiorders such that the choice functions rationalizable via these binary relations can be represented as in (3)?"]}
{"intro": [], "relatedWork": [], "rq": ["(1) what do the vgi portal administrators mean by a certain tag such as \"highway\" tag?"]}
{"intro": [], "relatedWork": [], "rq": [" the question can be of course reformulated in the following way: is the graph g r-partitionable?"]}
{"intro": [], "relatedWork": [], "rq": ["08 are laudable exceptions), there are now also corpus linguists who pretty much argue for trying di\ufffderent ways to modify existing measures and pick whatever yields results that intuitively (!) appear best and then sell that functionality as part of an unvalidated commercial web-based package. \ufffdese facts are troubling because such validations are so necessary as studies di\ufffder with regard to which, say, measures of attraction yield the best results: krug (1998) \ufffdnds string frequency to be most predictive; gries et al. (2005) \ufffdnd p fisher-yates to be best; wiechmann (2008) gets the best results with minimum sensitivity, etc. \ufffdus, do we as corpus linguists just go on using mi (or t or \u2026) just because we're supposed to focus on the discourse only and because the wordsketch engine makes that easy?"]}
{"intro": [], "relatedWork": [], "rq": [" capture the properties of the resulting estimator itself: how does it behave as a function of sample size?"]}
{"intro": ["GPUs are high throughput devices which use fine-grained multi-threading to hide the long latency of accessing off-chip memory [1] . Threads are grouped into fixed size batches called warps in NVIDIA terminology. The GPU warp scheduler chooses a new warp from a pool of ready warps if the current warp is waiting for data from memory. This is effective for hiding the memory latency and keeping the cores busy for compute-bound benchmarks. However, for memory-bound benchmarks, all warps are usually waiting for data from memory and performance is limited by off-chip memory bandwidth. Performance of memory-bound benchmarks increases when additional memory bandwidth is provided. Figure 1 shows the speedup of memory-bound benchmarks when the offchip memory bandwidth is increased by 2\u00d7, 4\u00d7, and 8\u00d7. The average speedup is close to 2\u00d7, when the bandwidth is increased by 8\u00d7. An obvious way to increase memory bandwidth is to increase the number of memory channels and/or their speed. However, technological challenges, cost, and other limits restrict the number of memory channels and/or their speed [2] , [3] . Moreover, research has already shown that memory is a significant power consumer [4] , [5] , [6] and A promising technique to increase the effective memory bandwidth is memory compression. However, compression incurs overheads such as (de)compression latency which need to be addressed, otherwise the benefits of compression could be offset by its overhead. Fortunately, GPUs are not as latency sensitive as CPUs and they can tolerate latency increases to some extent. Moreover, GPUs often use streaming workloads with large working sets that cannot fit into any reasonably sized cache. The higher throughput of GPUs and streaming workloads mean bandwidth compression techniques are even more important for GPUs than CPUs. The difference in the architectures offers new challenges which need to be tackled as well as new opportunities which can be harnessed."], "relatedWork": ["Frame buffer and texture compression have been adopted by commercial GPUs. For example, ARM Frame Buffer Compression [11] is available on Mali GPUs. AFBC is claimed to reduce the required memory bandwidth by 50% for graphics workloads. NVIDIA also uses DXT compression techniques for color compression [12] . However, these techniques only compress color and texture data and not data for GPGPU workloads. Furthermore, the micro-architectural details of most GPU compression techniques are proprietary.", "Recent research has shown that compression can be used for GPGPU workloads [13] , [14] , [15] . Sathish et al. [13] use C-Pack [8] , a dictionary based compression technique to compress data transferred through memory I/O links and show performance gain for memory-bound applications. However, they do not report compression ratios and their primary focus is to show that compression can be applied to GPUs and not how much can be compressed. Lee et al. [15] use BDI compression [9] to compress data in the register file. They observe that the computations that depend on the threadindices operate on register data that exhibit strong value similarity and there is a small arithmetic difference between two consecutive thread registers. Vijaykumar et al. [14] use underutilized resources to create assist warps that can be used to accelerate the work of regular warps. These assist warps are scheduled and executed in hardware. They use assist warps to compress cache block. In contrast to this, our compression technique is hardware based and is completely transparent to the warps. The assist warps may compete with the regular warps and can potentially affect the performance. Compared to FPC, C-PACK and BDI, we use entropy encoding based compression which results in higher compression ratio."], "rq": [" the second decision is: what is the best sampling duration?"]}
{"intro": ["In many cases, articles, and reports, IS research resolved that nothing occurs in a vacuum, social actors are always involved. Researchers brought the social issues to the foreground, and dismissed technological accounts as overly deterministic, ignorant of context, oblivious to human agency, and blind to interpretations. Within this approach, however, some have asked: what have we done with the technological? We have pushed it into the background, into the shadows awaiting representation, intervention, and mobilization. This research note argues that we can, and should when relevant, endeavor to understand the technological within our research. Building from the work of Orlikowski and Iacono (2001) , Monteiro and Hanseth (1996) , and the ideas from the literature on society and technology, this article presents a means of capturing the technological within socio-technological discourses at moments of interest."], "relatedWork": [], "rq": [" some have asked: what have we done with the technological?"]}
{"intro": [], "relatedWork": [], "rq": ["5) is cand preceded by by which is attached to opinion/comm?"]}
{"intro": ["Much efforts has been made for ontology matching. Methods such as Edit Distance [13] , KNN [3] and Bayesian similarity [26] , have been proposed to calculate the similarity between elements (e.g., concepts). However, most of existing works aim to find the ontology matching in a completely automatic way, although the complete automation is infeasible in many real cases [24] . One promising solution is to involve user interactions into the matching process to improve the quality of matching results [24] . However, regarding the large size of ontologies, random user interactions are really limited for helping ontology matching. Now, the question is: is there any way to minimize the amount of user interactions, while maximize the effect (accuracy improvement) of interactive efforts? Or a more specific question: given a fixed number of user interactions, can a system \"actively\" query the user so as to maximize spread of the interactions?"], "relatedWork": [], "rq": [" the question is: is there any way to minimize the amount of user interactions, while maximize the effect (accuracy improvement) of interactive efforts?"]}
{"intro": [], "relatedWork": [], "rq": [" as exemplified by a causal connective that relates in fact to a speech act rather than to a proposition: are you busy now?"]}
{"intro": [], "relatedWork": [], "rq": [" loss signaling mechanism: what mechanism does the destination use to signal the source of unacceptable loss?"]}
{"intro": [], "relatedWork": [], "rq": ["6] are used to specify scenarios as sequences of message interactions between objects or processes. however, mscs often do not tell the complete story. their interpretation can be ambiguous; for instance, does an msc describe all behaviours or a set of sample behaviours of a system?"]}
{"intro": ["Using wood in machines is not a new idea. For instance, it is used in chassis of cars [1] . It found also applications in the design of buildings due to its stiffness and low cost [10] . It is also used in Robotics for the design of mock-ups and proof-1 A u t h o r ' s v e r s i o n of-concept prototypes [15, 14] . However, a detailed study of these works shows that wood is never used in critical parts ensuring accuracy. Indeed, the wood performance / dimensions will vary with the atmospheric conditions / external solicitations and with the conditions they have grown [10, 17] ; thus, new robot design issues appear: How to be sure that an industrial robot made with wood can be accurate and stiff even if wood properties vary?", "A first attempt to introduce wood in industrial robot design was presented in [12] . The results showed that the approach was valid enough to compete with usual materials. However, this study did not deal with the aforementioned issues.", "Indeed, the accuracy issue can be treated through proper control approaches: external sensors combined with proper controllers can be used to accurately control the platform pose [4] . Regarding the robot stiffness, we believe that this issue can be handled through robust design approaches [18] . However, these design approaches must be fed with stiffness models which are able to predict the variability of the robot deformations due to the variability in the material properties."], "relatedWork": [], "rq": [" new robot design issues appear: how to be sure that an industrial robot made with wood can be accurate and stiff even if wood properties vary?"]}
{"intro": [], "relatedWork": [], "rq": [" we still did not have an answer to our empirical research question: how many of the bot-bot reverts in our dataset are instances of genuine conflict like the one between anomiebot and cyberbot ii, and how many were cases of routine, even collaborative work like fixing double redirects?"]}
{"intro": ["Our approach differs from traditional computational dialectology that was developed at the end of the 19 th century, in the wake of the introduction of quantitative methods, by Carl Haag (cf. Haag 1898) . The basic principle of those methods, commonly subsumed under the term 'dialectometry', is that a large quantity of datasets (the usually hundreds of maps from dialect atlases) is aggregated. The strength therein is that seemingly \"unsystematic\" aberrations of single maps are minimised, while only one kind of information becomes dominant, the one that is common to a large number of maps (see Nerbonne 2009 ). This procedure leads to good results if one intends to distinguish between whole dialects (cf. Nerbonne & Kretzschmar 2003 , Goebl 2006 . However, the detailed information contained in the individual maps is lost in the process, i.e. we may know how dialects are distributed, but we do not know why. Therefore, the objective is to group maps in a way that preserves their individual spatial and linguistic information, or, in other words to build clusters of identifiable maps that share similar spatial patterns of variants."], "relatedWork": [], "rq": ["a and figure 4b are similar, but the map in figure 4c is dissimilar to them because the green area has changed its location considerably. however, in our setting, all three maps are similar because we ignore the locations of the structures and consider only abstract spatial properties. this is an advantage if one is interested in more universal, language internal patterns and parameters of language change rather than distributions based on cultural phenomena that are necessarily connected to specific locations (cities, political borders, trade routes etc.). there is a variety of algorithms to perform cluster analysis but they all have a crucial point in common: how many clusters are naturally contained in a dataset?"]}
{"intro": [], "relatedWork": [], "rq": [" (1) can modelling function as 'critical modelling?", " and (2) can it encourage appraisals of design actions motivated by alternative value systems?"]}
{"intro": [], "relatedWork": [], "rq": [" to guide the design of our system for communicating such feedback during the editing process we asked practitioners: what are the benefits figure 1 . a typical editing workflow proceeds as follows: first, (0) the team meets synchronously to talk about the plan for the video and gathers the footage before or after this meeting. after this, (1) the video author (e.g. editor, producer/editor) creates the first cut, (2) a reviewer (e.g., client or executive producer) provides feedback, (3) the author views critiques, then (4) revises the video, and (5) the reviewer views the edits. the team then repeats steps 2-5 until both parties find the result satisfactory, or they reach a predetermined number of iterations. /danin my experience, the termination criterion is when time runs out! did you really find evidence that editors pick a predetermined number of iterations?"]}
{"intro": ["There are many forms of caregiving that are often undervalued and unseen within society. Caregivers of many kinds share similar burdens and experiences, such as prioritizing others' well being above their own, feelings of guilt for taking time for self-care, and care-related stress [12, 32, 81, 82] . However, caregivers are also a diverse population juggling both their own unique needs and the unique needs of those they care for. In particular, the number of informal caregivers who are caring for family members with Alzheimer's Disease (AD) is rising dramatically in the United console) and in which a myriad of user activities may serve as input (e.g., step count while grocery shopping, vacuuming etc). Use of a mobile platform, such as a smartphone, enables the creation of more accessible interventions by removing locational and temporal constraints often present with console-based exergames [92] . With pervasive exergames, the activities that people do throughout their day (and in varied settings) serve as input to the game experience [25, 77] . These games have a great potential to more effectively address barriers to PA that caregivers face, such as the unpredictability and frequency of caregiving responsibilities, as they offer and encourage a wide range of PA experiences [49, 98] . However, work is needed that characterizes how pervasive exergames can be made accessible, useful, and engaging for the AD caregiver population, given the significant challenges that they face.", "Additionally, research has explored how technological innovations can support various caregiver populations with their caregiver responsibilities [37] , and the potential for technology-mediated social support amongst caregivers (e.g., through online support communities [87] ). However, despite the fact that caregivers' needs are interdependent [12] , prior work has typically explored the design of technologies that address discrete caregiver needs (e.g., self-care through PA or increased social connectedness). Therefore, research is needed to examine the specific barriers that AD caregivers face to wellness, and how these barriers can be addressed in concert. Our work seeks to address this research gap through a formative study exploring how pervasive games can jointly address barriers to PA and social connectedness in the AD caregiver population. Taking this approach allows us to explore the broader question of how technology can simultaneously address multiple caregiver needs; such an approach has the potential to be an effective and efficient means of enabling impactful engagements with health technologies in a population that is significantly overburdened. Our work is guided by the following research questions:"], "relatedWork": [], "rq": [" that is: what is the minimal level of engagement needed to support change?"]}
{"intro": [], "relatedWork": [], "rq": ["4, what is the lowest point on earth?"]}
{"intro": ["Attracted by the conceptual simplicity of deep learning (DL) and the huge success in many fields of application, many papers have recently proposed and analyzed ways to benefit from neural networks (NNs) in communications. Therefore, almost any conventional signal processing block has been replaced individually by NN-blocks such as equalization [1] , [2] , channel coding [3] , [4] , detection [5] , or multiple-input multiple-output (MIMO)-detection [6] up to DL enhanced iterative decoding algorithms [7] . However, with the introduction of end-to-end learning autoencoder systems [8] , [9] , the system design paradigms change from optimizing individual sub-components with specific task towards systems that learn to communicate without the need of any conventional signal processing block.", "In classical communication system design, it is assumed that most of the physical effects a transmitted signal suffers from are well known and can be conventionally corrected one by one due to an approximated channel/system model. However, minor effects such as temperature changes, nonlinearities and tolerance ranges in hardware components, mostly caused by hardware insufficiencies, or other unusual channel parameters (e.g., extreme weather conditions) often cannot be fully considered in those models and, thus, are not optimally compensated. As an alternative approach, NN-enhanced communication promises that the real physical channel with all its known and unknown parameters can be formulated as a hard-to-describe black-box. A neural networkbased receiver (autoencoder systems [8] , where transmitter and receiver are learned together) can be trained such that it This work has been supported by DFG, Germany, under grant BR 3205/ 6-1. compensates all effects of this black-box without requiring a detailed description or model. This is one promising advantage of deep neural networks in communications.", "In this work, we focus on the second approach, i.e., we investigate the possibility of finetuning the system on-the-fly. As already mentioned before, the principal benefits of this approach are adaptivity with respect to effects not considered during the design, and initial training simplicity as not all possible effects of the future system need to be considered during the design. However, to ensure that the receiver neural network can handle channel alterations over time it requires to perform a finetuning step periodically, i.e., the receiver has to update its weights during run time. This can be simply done by the stochastic gradient descent (SGD) algorithm (see [10] ), but also leads to a fundamental practical problem of NNbased communication systems: How can the receiver update its weights without knowing what was originally transmitted, i.e., without having a labeled data set?"], "relatedWork": [], "rq": [" but also leads to a fundamental practical problem of nnbased communication systems: how can the receiver update its weights without knowing what was originally transmitted, i.e., without having a labeled data set?"]}
{"intro": ["Visual systems have perfected the art of sensing through billions of years of evolution. As an example, with roughly 100 million photoreceptors absorbing light and 1.5 million retinal ganglion cells transmitting information [2, 3, 4] , a human can see images in three-dimensional space with great details and unparalleled resolution. Anatomical studies determine the spatial density of the photoreceptors on the retina, which limits the peak foveal angular resolution to 20-30 arcseconds according to Shannon theory [1, 2] . There are also other imperfections due to nonuniform distribution of cells' shape, size, location, and sensitivity that further constrain the precision. However, experiment data have shown that human can achieve an angular separation close to 1 arcminute in a two-point acuity test [5] . In certain conditions, it is even possible to detect an angular misalignment of only 2-5 arcseconds [6] , which surpasses the virtually impossible physical barrier. This ability, known as hyperacuity, has baffled scientists for decades: what kind of mechanism allows human to read an undistorted image with such a blunt instrument?"], "relatedWork": [], "rq": [" has baffled scientists for decades: what kind of mechanism allows human to read an undistorted image with such a blunt instrument?"]}
{"intro": ["A line of recent work has considered adversarial examples in the context of linear classifiers [14] and neural networks [22, 10, 17, 18, 15] , much of which has been focussed on developing increasingly sophisticated attacks. How to defend reliably against these attacks, however, is unfortunately less clear. While a number of defenses have been initially proposed [19, 16] , these have mostly been attacked and broken by subsequent work [3, 2] . Thus, with a handful of exceptions [7, 23] , there is an overall lack of general understanding about the foundations of designing machine learning algorithms that are robust to adversarial examples. In this context, several intriguing questions remain: what makes classifiers vulnerable to adversarial examples? When do robust classifiers exist? What kind of defenses can we employ against these examples and in what situation? This work takes a first step towards addressing these challenging questions."], "relatedWork": ["Motivated by applications such as autonomous driving, adversarial examples have received a great deal of recent attention [10, 18, 22, 17] . However, most of the work has been empirical in nature, and has largely focussed on developing increasingly sophisticated attacks [3, 15] . While some recent defenses have been proposed [19, 16] , many of them have been attacked and broken by subsequent work [3, 2] . Currently, the only reliable defense that appears to resist adversarial examples successfully is data augmentation -where the classifier is trained on an additional set of adversarial examples in addition to the usual training set.", "In terms of theory, [7] is the first work to provide quantitative bounds on the robustness of classifiers; they analyze the robustness of linear and quadratic classifiers when the added perturbation is random as well as semi-random. [23] performs an elegant mathematical analysis of robustness in linear classifiers, and establishes conditions under which these classifiers are highly susceptible to adversarial examples. None of these works however, separate out the causes of vulnerability into distributional and finite sample; moreover, they also do not consider nearest neighbors. [1] provides an attack method for nearest neighbor classifiers, and shows that their robustness decreases with increasing intrinsic dimensionality of the data.", "Finally, beginnning with the work of Fix and Hodges [8] , there has been a long line of work on the convergence and consistency of nearest neighbor classifiers and their many variants [5, 21, 13, 6, 4, 12, 11] ; none of these works however have addressed robustness to adversarial examples. In terms of asymptotic results, [5] shows that provided some regularity conditions hold, and provided k \u2192 \u221e and k/n \u2192 0 as n \u2192 \u221e, the error rate of the k-nearest neighbor classifier converges to that of the Bayes Optimal classifier. For k = 1, the error rate converges to at most twice the error rate of the Bayes Optimal.There has also been much follow-up work on convergence rates of nearest neighbors [13, 6, 4, 12, 11] ; however, the rates have been found to be highly dependent on the underlying data distribution. We also remark that our robust nearest neighbor algorithm is motivated by the algorithm in [12] ; however, our robustness analysis is novel."], "rq": [" several intriguing questions remain: what makes classifiers vulnerable to adversarial examples?"]}
{"intro": [], "relatedWork": [], "rq": [" we conclude: what about active attacks?"]}
{"intro": ["With these assumptions, software architecture design becomes essentially a search problem: find a combination of applications of the general solutions that satisfies the quality requirements in an optimal way. However, given multiple quality attributes and a large number of general solutions, the search space becomes huge for a system with realistic size. This leads us to the more refined research 3 problem discussed in this paper: to what extent we could use heuristic search methods, like genetic algorithms (GA) (Mitchell, 1996) , to produce a reasonable software architecture automatically for certain functional and quality requirements?"], "relatedWork": ["Search-based software engineering applies meta-heuristic search techniques to software engineering issues that can be modeled as optimization problems. A comprehensive survey of applications in search-based software engineering has been made by Harman et al. (2009) . Recently, there has been increasing interest in software design in the field of search-based software engineering. A survey on this subfield has been conducted by R\u00e4ih\u00e4 (2009) . We will now briefly discuss the most prominent studies in the field of search-based software design. Bowman et al. (2008) study the use of a multi-objective genetic algorithm (MOGA) in solving the class responsibility assignment problem. The objective is to optimize the class structure of a system through the placement of methods and attributes within given constraints. So far they do not demonstrate assigning methods and attributes \"from scratch\" (based on, e.g., use cases), but try to find out whether the presented MOGA can fix the structure if it has been modified. Simons and Parmee (2007a; 2007b) take use cases as the starting point for system specification. Data is assigned to attributes and actions to methods, and a set of uses is defined between the two sets. The notion of class is used to group methods and attributes. This approach starts with pure requirements and leaves all designing to the genetic algorithm. The genetic algorithm works by changing the allocation of attributes and methods. However, no design choices beyond class structure are made, leaving the end result simpler than what is the goal with our approach. Amoui et al. (2006) use the GA approach to improve the reusability of software by applying architecture design patterns to a UML model. The authors' goal is to find the best sequence of transformations, i.e., pattern implementations. Used patterns come from the collection presented by Gamma et al. (1995) . From the software design perspective, the transformed design of the best chromosomes are evolved so that abstract packages become more abstract and concrete packages in 18 turn become more concrete. This approach only uses one quality factor (reusability), and also a more refined starting point than what is used in our approach. Seng et al. (2005) describe a methodology that computes a subsystem decomposition that can be used as a basis for maintenance tasks by optimizing metrics and heuristics of good subsystem design. GA is used for automatic decomposition. If a desired architecture is given, and there are several violations, this approach attempts to determine another decomposition that complies with the given architecture by moving classes around. Seng et al. (2006) have continued their work by searching for a list of refactorings, which deal with the placement of methods and attributes and inheritance hierarchy."], "rq": [" problem discussed in this paper: to what extent we could use heuristic search methods, like genetic algorithms (ga) (mitchell, 1996) , to produce a reasonable software architecture automatically for certain functional and quality requirements?"]}
{"intro": ["implying that computing the Pfa an is hard for GapL. However, no matching upper bound was known till now. It is known that the square of the Pfa an of an oriented graph is equal to the determinant of a related matrix. Thus, computing the Pfa an correct up to the sign is known to be in NC. This is, however, not adequate to imply a GapL algorithm as (1) we do not know if GapL is closed under square roots (of positive integers), and (2) this does not give the correct sign of the Pfa an. This does not even imply any NC algorithm for the sign. The Pfa an (including the sign) can be computed in polynomial time [3, 6] using cross-eliminations (akin to Gaussian elimination for determinants) and choosing pivot elements carefully, see also [24] . However, this method su ers the same drawback as Gaussian elimination of not lending itself to e cient parallelization. In [6] the authors explicitly state that current techniques which allow determinant computations to be performed in NC do not appear to generalize to Pfa ans, and that no NC algorithm for Pfa ans is known. The subsequent determinant algorithm of [14] uses techniques that do generalize to Pfa ans, yielding the algorithm described in this work. 3 Our algorithm is thus the \u00ffrst to place Pfa ans inside NC, and more precisely, in GapL. It follows that computing integer Pfa ans exactly characterizes the class GapL."], "relatedWork": [], "rq": ["a related question that immediately arises is: what is the complexity of planarity testing itself?"]}
{"intro": ["However, each of these has problems. Hand-crafted preprocessing is frequently used, but is time-consuming and requires in-depth domain knowledge. Writing a custom learner is possible, but is labour-intensive. Relational learning techniques tend to be very sensitive to noise and to the particular clausal representation selected. They are typically unable to process large data sets in a reasonable time frame, and/or require the user to set limits on the search such as refinement rules (Cohen, 1995) ."], "relatedWork": ["There are some general techniques that can be applied to temporal and structured domains. The best developed technique for temporal classification is the hidden Markov model (Rabiner, 1989) . HMMs have proved themselves to be very useful for speech recognition, and are the basis of most commercial systems. However, they do suffer some serious drawbacks for general use. Firstly, the structure of the HMM-similar to that of a finite state machine-needs to be specified a priori. The structure selected can have a critical effect on performance. Secondly, extracting comprehensible rules from HMMs is not at all easy. Thirdly, even making some very strong assumptions about the probability model, there are frequently hundreds or thousands of parameters per HMM. As a result, many training instances are required to learn effectively. Recurrent neural networks and Kohonen maps have also been used to solve temporal and sequence problems (Bengio, 1996) , but suffer similar problems."], "rq": [" is lacking in one regard: what does \"approximately\" mean?"]}
{"intro": [], "relatedWork": ["Video alignment scenarios where cameras are moving bear a certain similarity to the problem of robot localization based on video data. In essence, video synchronization can be viewed as a by-product of vision-based simultaneous localization and mapping (SLAM) [22] , [23] . Approaches to video-based SLAM attempt to link novel frames to previously recorded ones that were captured from nearby viewpoints. The methods in [22] and [23] use visual words representations [6] to extract similar frames from a database of landmark images. The BoW paradigm is also used in [12] , where the video alignment is applied to detect copied video material, and in [5] to retrieve images of similar but different scenes. In particular, the latter proposes a flowbased alignment, called SIFT-flow, to spatially register corresponding frames. However, while SIFT-flow and similar methods can be used for computing general nonrigid alignment between two images [5] , they are sensitive to visual occlusions and too computationally demanding to allow for robust and fast video alignment."], "rq": [" this raises an obvious question: how \"global\" should an algorithm be to resolve uncertainty?"]}
{"intro": ["When an advisor and an advisee engage in service interaction, they rely on a range of mutual expectations and obligations. Many of those expectations or obligations outgrow the institutional identities of the interactants [19] , others result from the social character of the encounter [49, 58] . In order to respond to the expectations and obligations, humans enact rituals. Some of the typical social rituals involve greetings, farewells, introductions [37] . Meanwhile, technology has become an integral part of many rituals. For instance, when agreeing for an appointment date, most humans pick up their mobile phone and it feels natural; many people even expect that others will use some tools when making an appointment. However, in face-to-face interaction, computers can also generate annoyance: consider the situation that occurs within service encounters, where the advisor is typing things on a keyboard and looks at the screen, while the client is sitting on the other side of the table and waiting; if this situation takes more time than the client expects, he 1 will likely get nervous or the advisor will engage in behaviors to bridge the waiting time [40] . In the best case, the situation will require a recovery behavior to put the interaction on the right track [13] . In the worst case, the high-touch, naturally unfolding encounter will turn into a technology-driven conversation, where the computer dictates behaviors and content [51, 57] . In other words, the visible and explicit structure of information technology (IT) can dominate the structure of an interaction ritual. CSCW research provide evidence, that the interpersonal conversation and communication in professional encounters can become less effective and enjoyable due to IT [28, [40] [41] [42] 67] . The literature suggests a tension between using a computer during service encounter to save, process, or visualize the data (high-tech) and the ritualized, individual and human, character of the service encounter (high-touch) [2, 74] . This paper questions this view and provides evidence that high-tech and high-touch in advisory services can be compatible. At the same time, it stresses the role of rituals, which should be considered a key source of inspiration and knowledge for the design of IT for institutional settings."], "relatedWork": [], "rq": [" object constancy contradicts the experiential learning and multimodal approach paradigms: how to make things constant while allowing for manipulation of values?"]}
{"intro": ["Due to the limited space, the present paper will focus on semantic and lexical collocations in a Chinese-English dictionary, excluding grammatical ones. However, there is only limited literature on the subject. Siepmann (2005 Siepmann ( , 2006 critically reviews different approaches to collocation, and tries to synthesize recent advances in collocational theory into a coherent framework for lexicological theory and lexicographic practice. He concludes that \"the traditional dictionary-making process should be turned on its head: rather than starting from an alphabetical framework it should proceed from a bilingual or multilingual onomasiological research base (Siepmann, 2006) .\" Svens\u00e9n (2009) discusses in more details the approaches to collocational information in a L1-L2 dictionary. He argues that in a L1-L2 dictionary, \"all types of relevant collocations need to be included, transparent or not, directly translatable or not (Svens\u00e9n, 2009) .\" Atkins and Varantola (1997) made an empirical study on dictionary use, and found that dictionary users looked up collocations of L2 words (11%) more frequently than they looked up grammar of L2 words (4%). He (2008) did a similar test on the use of Chinese-English dictionaries, and reached the same conclusion that dictionary users look up collocational information more frequently than they look up grammatical information. This shows the importance of collocation in a Chinese-English dictionary. However, no other literature is found to discuss collocational information in a Chinese-English dictionary. Therefore, the present paper attempts to answer the following questions: 1) How is collocational information treated in current Chinese-English dictionaries?"], "relatedWork": [], "rq": [" 1) how is collocational information treated in current chinese-english dictionaries?"]}
{"intro": [], "relatedWork": [], "rq": [" so a natural question arises: is there any effective approach for an isp to find its optimal price, assuming the isp can estimate some necessary information about the system, e.g., the happiness weighting coefficients of peers ( ), capacities and unit prices of the private links?"]}
{"intro": [], "relatedWork": [], "rq": ["1, is named after the human body\\'s autonomic nervous system. the autonomic nervous system regulates body systems without any external help; likewise, an autonomic computing system controls the functioning of computer systems without user intervention. the main goal of autonomic computing is to make managing large computing systems (such as grids) less complex. online (vol. 9, no. 3), art. no. 0803-mds2008030001 an autonomic grid can configure, reconfigure, protect, and heal itself under varying and unpredictable conditions and optimize its work to maximize resource use. you can find applications, challenges, and various methods that have been proposed to work toward autonomic grids elsewhere. 8 examples of autonomic grid projects include the ibm optimalgrid 7 and automagi. 8 knowledge grids. a knowledge grid is an extension to the current grid in which data, resources, and services have well-defined meanings that are annotated with semantic metadata so both machines and humans can understand them. the aim is to move the grid from an infrastructure for computation and data management to a pervasive knowledge-management infrastructure. examples of knowledge grid projects include ontogrid (www.ontogrid.net/ontogrid/index.jsp), inteligrid (www.inteligrid.com), and k-wf grid (www.kwfgrid.eu). several communities are working to realize knowledge grids, including the semantic grid group (www.ogf.org/gf/group_info/view.php?"]}
{"intro": [], "relatedWork": [], "rq": [" is: how many samples are needed for the estimates e * (f ) to be accurate, for all f \u2208 f ?"]}
{"intro": [], "relatedWork": ["In healthcare, a multitude of neuropsychological tests have been developed and used by clinicians for assessing cognition. Mini Mental State Examination (MMSE) [46] , Montreal Cognitive Assessment (MoCA) [29] and Addenbrooke's Cognitive Examination-III (ACE-III) [15] are widely used for cognitive impairment screening and monitoring in clinical settings. Currently, these assessment tools are typically paper-based and not designed for self-administration [15] . Thus, it is infeasible to run the tests frequently to monitor changes in cognitive functions over time due to learning effects [46] , costs and resource requirements around availability of qualified clinical staff to administer them. These can adversely affect the ability of clinicians to detect the early signs of decline in cognitive functions, potentially delaying diagnosis and treatment as well as undermining the effectiveness of medication or other interventions [37] . These limitations have led to a call for alternative approaches. Taking advantage of computerised assessments may provide more precise test results, better control of stimulus presentation and ease of administration to assess cognitive abilities. One of the most widely used computerised tools for cognitive measurements in clinical research is the Cambridge Neuropsychological Test Automated Battery (CANTAB). The CANTAB system includes various tests measuring a range of cognitive functions, e.g. executive function, attention, memory and decision making [55] . It has been extensively used to assess cognitive functions in older people [32] , athletes with exposure to repeated brain injuries [7] , for paediatric neuropsychological assessment [25] , HIV dementia patients [34] and alcohol drinkers [14] . However, trained clinical personnel are required for protocol administration of CANTAB.", "\"Serious games\" have recently gained increasing research attention for their potential to improve sustained participation in continual assessment and therapy by incorporating elements of fun and user engagement in their design [10, 11, 26, 27, 47] . Serious games are those designed for some additional purpose beyond pure entertainment, such as: training, marketing, communicating, assessing and/or enhancing cognitive and physical health [33] . For instance, a tabletop-gaming platform in the Eldergames project was developed to improve cognitive functions in older adults. Their findings showed that their interactive tabletop games were wellaccepted with regard to usability and reported to create a positive experience [11] . However, this approach requires space for equipment setup and is thus not feasible for running in a large-scale experiment. In contrast, the ubiquitous computing power of modern mobile devices offers promising solutions for data collection and processing for cognitive assessment and monitoring outside clinical settings. Mobile versions of serious games have been developed to simulate common daily activities such as cooking [27] and supermarket shopping [54] , in order to assess and help improve cognitive functions among people with mild cognitive impairment (MCI). To complete tasks in the game scenarios, a multitude of cognitive processes were involved, e.g. object recognition, attention, visual search, memory and executive functions. By comparing in-game task performance and classic cognitive assessments, e.g. MMSE and TMT [45] , their findings demonstrated significant correlations between variables in the games and results from standard cognitive measures. Unlike the simulation-based designs that artificially represent real-world scenarios in such games, replicating a popular casual game Whack-a-Mole presents more game-like attributes and reduces the feelings of being tested [47] . A Go/No-Go discrimination task has also been incorporated into serious games to measure cognitive inhibition. The significant correlations between median response time and cognitive test scores suggested that this in-game feature could be used as a predictor for cognitive status. A recent systematic review found that the use of gamified tasks can help improve drop-out rates in longitudinal studies including a reduction in test anxiety [26] . Because of their entertaining nature, these game-based assessments were reportedly well received by the users even in older adults. In spite of common misperceptions, a systematic review reported that older adults enjoyed video games and benefited from game-based cognitive intervention [20] . This was supported by a recent report demonstrating that 23 per cent of the U.S. gamers were 50 years and older [41] . Hence, these studies have emphasised the potential of serious games as highly engaging cognitive assessments to monitor changes in cognition outside of a clinical environment for populations with cognitive disorder across age groups.", "It is important to note that all these studies have explored hand movement in non-time-dependent tasks, such as handwriting. However, gameplay hand movement is closely related to user reactions on game stimuli. The characteristics of touch gestures in games are highly dependent on the time the user perceives stimuli in the game and the limited time they have to perform a specific gesture. Therefore the shape, speed and length of a gesture can be different, depending on the time it takes to perceive a game trigger. For example, a slow response time in identifying a game object that a player needs to interact with (e.g. in \"Fruit Ninja\" spotting a fruit that is about to move out of the screen) could result in a faster and more erratic gesture in order to complete the gesture in the reduced time available. Previous studies have shown that mental fatigue [21] and age [8] adversely affect the speed of processing resulting in slower reaction time. This means that in certain games, faster and more erratic gestures could be an indicator of slower response time to visual stimuli, and therefore indicative of cognitive decline."], "rq": [" rq1: are the swipe length and shape of touch gestures related to changes in cognitive performance?", " and rq2: is the speed of touch gestures related to changes in cognitive performance?"]}
{"intro": ["An effective posture language, however, must also face the limited human capacity of retaining a large number of chordcommand mappings. People move back and forth between mobile device, tablet, and desktop interfaces: input gestures should be easy to remember even when not constantly performed or practiced. Some previous studies proposed natural gesture sets [20, 30] . 'Natural' refers to a mapping between gesture and command, which is rooted in language, culture, or experience in the physical world. Since natural gesturecommand mappings use prior knowledge, it can improve the memorization of such mappings. However, abstract gesture or posture sets and abstract domain-specific commands do not have such desirable properties: how should we design chordcommand mappings in such cases?"], "relatedWork": [], "rq": [" abstract gesture or posture sets and abstract domain-specific commands do not have such desirable properties: how should we design chordcommand mappings in such cases?"]}
{"intro": ["Indeed, nowadays, smartphones do not only provide internet access; besides, they are provided by a countless number of sensors. Microphones and digital cameras are the most common ones; however, they are being equipped with new sensors: accelerometer, gyroscope, compass, magnetometer, proximity sensor, light sensor, GPS, and so forth [1] . Taking advantage of these features, developers have created new amazing app in order to improve user smartphone experience [2] . The embedded sensors allow the device to adapt to environment conditions, use of battery, lighting conditions, and sound. For example, light sensor controls screen brightness in order to preserve battery life. When the user is using the smartphone in a dark place, the screen brightness is reduced.", "Activity recognition aims to perceive which activity is taking place. In these applications, high classification accuracy is always desired. Daily, human beings make ordinary actions such us a cooking, reading or watching TV, chatting 2 International Journal of Distributed Sensor Networks with other people or on the phone, and driving [3] . The ability of activity recognition seems so natural and simple for us; however, actually it requires complicated functions of sensing, learning, and inference for computers [4] .", "Traditionally, activity recognition is carried out through video systems like those described in [5, 6] . However, recent researches in activity recognition show that microelectro mechanical systems (MEMSs) are becoming another way to face this problem [7] . They can return a real-time measurement of acceleration along the x-, y-, or z-axis to be used as a human motion detector.", "In general, placing more accelerometers on different body positions improves pattern recognition performance [8] . At the same time, a wearable system must be inconspicuous and operate during long periods of time [9] . However, people are reluctant to wear strange devices over the body. In this case, smartphones are especially well-suited to accomplish this task since they have integrated MEMS and people consider them as friendly devices. Smartphones may obtain and process physical phenomena from embedded sensors (MEMS) and send this information to remote locations without any human intervention [10] ."], "relatedWork": [], "rq": ["5] is described social network site as: web-based services that allow individuals to (1) construct a public or semipublic profile within a bounded system, (2) articulate a list of other users with whom they share a connection, and (3) view and traverse their list of connections and those made by others within the system. the nature and nomenclature of these connections may vary from site to site. each sns is implemented with specific features; however, all of them have a common point which consists of visible profiles. daily, sns users share their personal information, and sns manage as uncountable gigabytes of useless user information. why do not we use these data to obtain user context information?"]}
{"intro": ["Our proof is self-contained. Bardakov and Gongopadhyay use a result of Akhavan-Malayeri and Rhemtulla [3] , which in turn uses a result from the unpublished PhD thesis of Stroud [22] , details of which may also be found in [21] . However they established more, namely that free abelian-by-nilpotent groups have nite palindromic width. In their sequel [4] , Bardakov and Gongopadhyay have investigated lower bounds for the palindromic width of nilpotent groups and abelian-by-nilpotent groups. Also using work of Akhavan-Malayeri [2] concerning the nature of commutators, E. Fink [15] claims that the wreath product of a nitely generated free group with a nitely generated free abelian group, and hence also the wreath product of any nitely generated group with a nitely generated free abelian group, has nite palindromic width."], "relatedWork": [], "rq": ["1. is there a group with nite generating sets and such that pw( , ) is nite, but pw( , ) is in nite?"]}
{"intro": ["On the one hand, the purposefully design trustless mining protocol [6] [41] [46] does not require a third-party entity to authorize transactions but merely miners' consensus, which in turn supports people's trust in blockchain [41] [57] . On the other hand, the emerging social organization of mining practices brings forward issues of trust among miners such as the risk of 51% attack [15] [19] or of selfish miners [16] [25] [47] [55] , explored mostly within the security research area. Relevant HCI works on blockchain and its trust related issues have started to emerge [33] [48] [49] . We agree with the argument that blockchain offers a unique perspective to explore trust as its characteristics contrast with the centralized, regulated, and nonanonymous traditional transaction systems which have informed the existing HCI models of trust [23] [45] . However, apart from modeling-based security research on mining, we know little about miners' practices from their first-person perspective, and how the specific blockchain's characteristics impact on their trust. To address this gap, we report on interviews with 20 Bitcoin blockchain miners about their mining practices and related trust challenges, in order to explore the following research questions: 1. Which are miners' motivations for bitcoin mining? 2. Which are Bitcoin blockchain's' characteristics impacting on miners' trust and its dimensions? 3. Which is the social organization of mining practices: are there different approaches and types of miners? 4. Which are the main trust challenges and how do people attempt to mitigate them?"], "relatedWork": [], "rq": [" which is the social organization of mining practices: are there different approaches and types of miners?"]}
{"intro": [], "relatedWork": [], "rq": [" their message is in general rather simple-geared towards the questions a potential adopter might ask: what is this technology and how can it benefit my business?", " however: how can we to explain the wide adoption of erp across many different kinds of organisation worldwide?"]}
{"intro": ["The two variants of the game can be totally different in general. However, they coincide when the graph is a cycle. This case has been studied as the so-called pizza game: vertices are seen as slices of a pizza. In 1996 Brown asked whether Alice has a strategy to collect at least 1 2 of the weight of any pizza. This can be easily confirmed for pizzas with an even number of slices: color alternately the slices with two colors and secure the heavier color. At first glance the case of pizzas with an odd number of slices looks better for Alice as she gets one slice more than Bob. Curiously things can get worse for her: there are examples where she can get only 4 9 (see Figure 1 ). Winkler [1] conjectured in 2008 that Alice can secure at least 4 9 of any pizza, and this has been proved by two independent groups of researchers. Theorem 1.1 ( [2, 3] ). Alice can secure at least 4 9 of a pizza. As for pizzas, we measure Alice's outcome on a given graph as the fraction of the total weight of the graph that Alice can guarantee herself regardless of Bob's strategy. A natural question arises for both variants of the game: Is there a common constant c > 0 bounding from below Alice's outcome on any graph? The answer is: No. Simple examples show that such a lower bound cannot exist for either variant even if the game is played on trees instead of general graphs. The interesting point is that the parity of the number of vertices plays an important role here. In particular, Alice can guarantee herself a positive constant gain on trees with restricted parity of the number of vertices (odd for game T, even for game R)."], "relatedWork": [], "rq": [" a natural question arises for both variants of the game: is there a common constant c > 0 bounding from below alice's outcome on any graph?"]}
{"intro": ["Compared to the methods just remarked, the leading role in eQTL mapping is played by the statistics-based methods, which, among others, consist of contingency table-based techniques [7, 8] , Bayesian approaches [2, 9, 10] and regression-based methods [11] [12] [13] . Similar to MI, the contingency-table-based test statistic can also be trans formed to a X 2 distributed random variable under the null hypothesis of no association [5] . This approach, however, can only treat discrete phenotypes, and is therefore of lim ited service in practice. To overcome such limitation, other methods that based on Bayesian rule can be resorted to, with advantages of avoiding the difficult and often intractable mathematical treatments [15] . Nevertheless, the methods within Bayesian framework require the assignments of sub jective a priori probabilistic terms, which might output mis leading results. Due to the solid theoretical foundation and mathematical tractability, the linear regression models with assumption of Gaussian distribution of data are perhaps the most widely used methods in eQTL analysis. However, the linearity and normality assumed in the linear-regression based methods are hardly justifi ed by physical evidence. Besides, it is well known that the linear regression meth ods are very sensitive to outliers. Even a single outlier can distort severely the regression slope and thus result in prob lematic conclusion. In addition to the shortcomings just mentioned, nearly all the above methods rely on algorithms based on resampling techniques that are computationally demanding. Aiming at overcoming these limitations, in this project we propose to develop a new statistical framework for eQTL mapping based on a new 3-c1ass ROC (receiver operating characteristic) approach. The rationale for using a 3-c1ass ROC is that underlying each eQTL mapping, there is a 3-class classifi cation problem-if the gene expression of a certain gene can be classifi ed according to the class la bels (AA, AB and BB) of a SNP locus, then this constitutes an eQTL [cf. Fig. l(a) ]."], "relatedWork": [], "rq": [" now an important question arise: how large is \"large\" of the sample size so as to ensure the validity of normal ap proximation when doing the hypothesis test in practice?"]}
{"intro": ["Although much progress has been made in the theories, methods and experiments that support affective computing over the past several years [4] , the problem of detecting and modeling human emotions in aBCIs remains largely unexplored [3] . Emotion recognition is the primary and important phase for aBCIs. However, emotion recognition based on EEG is very challenging due to the fuzzy boundaries and differences in individual variations of emotion. In addition, we cannot obtain the 'ground truth' behind human emotions in theory, that is, the true label for an EEG corresponding to different emotional states, because emotion is considered as a function of time, context, space, language, culture, and races [5] .", "Many previous studies have focused on participantdependent and participant-independent patterns and evaluations of emotion recognition. However, the stability of patterns and performance of models over time has not been fully exploited, and they are very important for real-world applications. Stable EEG patterns are considered as neural activities related to critical brain areas and critical frequency bands that share commonality across individuals and sessions under different emotional states. Although task-related EEG is sensitive to change due to differences in cognitive states and environmental variables [6] , we intuitively consider that the stable patterns for specific tasks should exhibit consistency among repeated sessions of the same participants. In this paper, we focus on the following issues of EEG-based emotion recognition: What is the capability of EEG signals for discriminating between different emotions? Are there any stable EEG patterns of neural oscillations or brain regions for representing emotions? What is the day-to-day performance of the models based on machine learning approaches?"], "relatedWork": ["One of the goals of affective neuroscience is to examine whether patterns of brain activity for specific emotions exist and whether these patterns are to some extent common across individuals. Various studies have examined the neural correlations of emotions. It seems that processing modules for specific emotion do not exist. However, neural signatures of specific emotions, as a distributed pattern of brain activity [23] , may exist. Mauss and Robinson [24] proposed that the emotional state is likely to involve circuits rather than any brain region considered in isolation. To AC researchers, identifying neural patterns that are both common across participants and stable across sessions can provide valuable information for emotion recognition based on EEG.", "Studies on the internal consistency and test-retest stability of EEG date back to many years ago [6] , [31] , [32] , especially for clinical applications [33] . McEvoy et al. [6] proposed that under appropriate conditions, task-related EEG has sufficient retest reliability for use in assessing clinical changes. However, these previous studies investigated the stability of EEG features under different conditions, for example, a working memory task [6] . Moreover, in these studies, stability and reliability are often quantified using statistical parameters such as intraclass correlation coefficients [32] , instead of the performance of pattern classifiers.", "So far, a few preliminary studies on the stability and reliability of neural patterns for emotion recognition have been conducted. Lan et al. [34] presented a pilot study on the stability of features in emotion recognition algorithms. However, in their stability assessment, the same features derived from the same channel from the same emotion class of the same participant were grouped together to compute the correlation coefficients. Furthermore, their experiments were conducted on a small group of participants with 14-channel EEG signals. They investigated the stability of each feature instead of the neural patterns that we focus on in this paper. Till now, no systematic evaluation has been conducted on the stability of activated patterns over time. The performance of emotion recognition systems over time is still an unsolved problem in developing real-world application systems. Therefore, our major aim in this paper is to investigate stable EEG patterns over time using time frequency analysis and machine learning approaches. We should emphasize that we do not study neural patterns under emotion regulation [35] , but rather to study specific emotional states during different times.", "To investigate various critical problems related to emotion recognition based on EEG, we face a serious lack of publicly available emotional EEG datasets. To the best of our knowledge, the only publicly available emotional EEG datasets are MAHNOB HCI [15] and DEAP [13] . The first one includes EEG, physiological signals, eye gazes, audio, and facial expressions of 30 people while watching 20 emotional videos. The DEAP dataset includes the EEG and peripheral physiological signals of 32 participants when watching 40 one-minute music videos. It also contains the participants' rate for each video in terms of the levels of arousal, valence, like/dislike, dominance, and familiarity. However, these datasets do not contain EEG data from different sessions for the same participant, which cannot be used for investigating the stable patterns over time. Because there are no available published EEG datasets for the analysis of the stability of neural patterns for emotion recognition, we develop a new emotional EEG dataset for this study as a subset of SEED. 1 "], "rq": [" we focus on the following issues of eeg-based emotion recognition: what is the capability of eeg signals for discriminating between different emotions?"]}
{"intro": ["manner, for instance relying on a trusted third party and private channels. Secondly, one has to show that the view of every attacker on the real protocol can be matched by a computationally indistinguishable view that comes from the idealized model. This requires a simulator, whose role is to decorate an ideal run with innocous data that makes it look like a real run to any polynomially bounded adversary. This level of generality comes however at a cost, the security proofs being complex and challenging to automate. Security proofs in symbolic models. On the other hand, significant progress has been made in the field of automated verification of security protocols in formal (or symbolic) models [7, 8] . However, even symbolic definitions of simulation-based security, e.g. [9, 10] or [11, 12] (in applied pi-calculus), are still a challenging task for such methods, which are tailored for basic properties like secrecy, authentication or privacy. Indeed, recent work aiming to automate verification for multi-party computation protocols is either relying on additional manual input [13, 12] , or only captures properties of correctness [14] . For Yao's protocol in particular, we also lack symbolic models for the required cryptographic primitives of garbled computation and oblivious transfer. Overall, we do not yet have the models that could be given directly to a verification tool and ask the basic question: is a particular two-party computation protocol secure or not? We propose such models for Yao's protocol."], "relatedWork": ["The model of Backes et al [14] considers multi-party computation functionalities abstractly, allowing to reason about their use in larger protocols, without necessarily representing the cryptographic primitives that realize the functionality. Their framework comes equipped with a computational soundness result and is applied to the case study of an auction protocol [32] . The security properties that are specified and verified automatically (relying on type-checking) are limited to robust safety, which can be related to our property of result integrity. Dahl and Damg\u00e5rd [13] also propose a formal framework for specifying two-party computation protocols and present the automated verification with ProVerif of an oblivious transfer protocol based on homomorphic encryption [28] . They formulate a definition of simulationbased security in applied pi-calculus and show it to be computationally sound. In order to obtain an input for ProVerif, they have to find a simulator and additionally massage the resulting processes manually. On the other hand, our methodology does not require the explicit construction of a simulator and our models can be readily given as input to automated tools. Our case study is also different, relying on garbled circuits rather than homomorphic encryption, and on oblivious transfer as a sub-protocol to compute any function the two parties may agree on. However, we do not provide a soundness result, and the relation of our models to simulation-based security remains an open question. In that direction, we can also explore extensions of our models into a general framework allowing the verification of other protocols, for two or multiple parties, and relying on various cryptographic primitives.", "A formal model for oblivious transfer in applied pi-calculus is presented by Dahl and Damg\u00e5rd [13] . Their specification is a process modeling a particular protocol, whereas we propose a more abstract equational theory. This equational theory is however specific, because it only models the oblivious transfer of garbled values. It would be possible to propose a generic equational theory for oblivious transfer, but it may cause problems for automated verification -it remains a problem for future work. Conversely, the model of Goubault et al [34] aims to capture formally the probabilistic aspect of some oblivious transfer protocols."], "rq": [" we do not yet have the models that could be given directly to a verification tool and ask the basic question: is a particular two-party computation protocol secure or not?"]}
{"intro": [], "relatedWork": [], "rq": [" it is difficult to find the true reason among many possible reasons: is it because a subject\\'s particular eye property fooled the eye tracker?"]}
{"intro": ["A natural and perhaps the most obvious question to ask concerning trades is what are the equivalence classes obtained? For a given parameter set t, v, k, )~ the underlying set may be taken to be either the collection of all pairwise non-isomorphic designs or alternatively the collection of all realisations of that design on a fixed base set V. To our mind the former is the more interesting. With regard to Pasch switching in Steiner triple systems, it is well known that switching any quadrilateral in the cyclic STS(13) will result in the non-cyclic STS(13). Gibbons [3] studied the situation when v=15 and showed that 79 of the 80 pairwise non-isomorphic STS(15)s form a single equivalence class. The remaining system is anti-Pasch, i.e. contains no quadrilaterals. Recently the present authors [5] have extended Gibbons' work to the switching of other n-cycles both on the class of all pairwise non-isomorphic STS(v) and also the class of all realisations for v=7,9,13 and 15. Some of the results are unexpected! In this paper we will be solely concerned with Pasch switching on the class of all pairwise non-isomorphic anti-Pasch STS(v), for fixed v. The situation for v=7 and 13 (the unique STS (9) is anti-Pasch) and Gibbons' result for v=15 naturally raises the question of whether all such systems form one equivalence class for larger values of v. The answer is an emphatic 'No' as was shown in [4] . The method used in that paper was to construct complete equivalence classes containing only a very small number of, in practice precisely two, systems. We make the following formal definition: a pair of twin Steiner triple systems of order v is two STS(v)'s, each of which contains precisely one Pasch configuration which when switched produces the other system. Twin Steiner triple systems were exhibited for v=19, 21, 25, 27 and 31, and on the basis of this admittedly rather flimsy but nevertheless compelling evidence we conjectured that twin STS(v) exist for all admissible v>~ 19. We believe that this result is likely to be quite difficult to prove if only because the spectrum of anti-Pasch STS(v) is not yet determined and our empirical evidence suggests that twin systems are somewhat rarer even than anti-Pasch. However, we do present in this paper, a general construction which gives infinite linear classes of twin STS(v)'s. It is also an aim to develop the theory of these systems further and, in response to a question we have been asked, to introduce another class of systems which are very appropriately called 'identical twins'."], "relatedWork": [], "rq": [" since the sts (7) is unique to within isomorphism, every pasch switch must result in an isomorphic copy of the original system. do any other systems exist having this same property?"]}
{"intro": [], "relatedWork": [], "rq": [" (9) are related by conjugation in a + (049). however, if we pass from one flag to a new flag in the other orbit, then the sets of distinguished generators of a+(9) are related by conjugation in a(3) but not in a+ (9) . the above change from y to yk illustrates this case; here the change to new generators is realized by an involutory group automorphism of a + (9). now let c?", " (9) are related by conjugation in a + (9). as above, the change from qi to the k-adjacent flag qik results in a change from 01, ... 9 on_ 1 to the new generators given on the right-hand side of (7) . (note that in the chiral case the left-hand side of (7) makes no sense.) this can be seen either directly, or by relating 9 to the universal (directly regular) n-polytope .z = { pl, . . . , pn_ 1 }. note that for different k's the corresponding sets of generators are related by conjugation in a+(p). it follows that up to conjugation in a+ (9) there are precisely two sets of distinguished generators of a+ (c?"]}
{"intro": ["Among the formally describable things are the contents of all books ever written, all proofs of all theorems, the infinite decimal expansion of \u221a 17, and the enumerable \"number of wisdom\" \u2126 [28, 80, 21, 85] . Most real numbers, however, are not individually describable, because there are only countably many finite descriptions, yet uncountably many reals, as observed by Cantor in 1873 [23] . It is easy though to write a never halting program that computes all finite prefixes of all real numbers. In this sense certain sets seem describable while most of their elements are not.", "The calculation of the subjects of these theorems, however, may occasionally require excessive time, itself often not even computable in the classic sense. This will eventually motivate a shift of focus on the temporal complexity of \"computing everything\" (Section 6). If you were to sit down and write a program that computes all possible universes, which would be the best way of doing so? Somewhat surprisingly, a modification of Levin Search [53] can simultaneously compute all computable universes in an interleaving fashion that outputs each individual universe as quickly as its fastest algorithm running just by itself, save for a constant factor independent of the universe's size. This suggests a more restricted TOE that singles out those infinite universes computable with countable time and space resources, and a natural resource-based prior measure S on them. Given this \"speed prior\" S, we will show that the most likely continuation of a given observed history is computable by a fast and short algorithm (Section 6.6)."], "relatedWork": [], "rq": [" the fourth is 8. what is the fifth?"]}
{"intro": ["Wireless sensors and communication have become an integral part of closed-loop control systems in the recent years. Consider for example, the following simple control application. We wish to regulate the temperature of a room. Multiple sensor nodes are placed at different locations in the room for taking temperature measurements. The measurements are then sent to the controller, which utilizes the sensor observations when issuing new control commands to the heating or cooling elements. Naturally, benefits such as flexible placement, reduced costs on installation, and easier maintenance can be expected if sensor nodes communicate to the controller via wireless channels. This, however, also means that the sensor nodes are now sharing the communication medium, which is lossy and in general also restricted in terms of bandwidth and power. Such information constraints make the networked control systems differ from the classic wired setting. As pointed out in [1] , designing the optimal communication and control policies in this case is no simple task, since the changes in one directly influence the outcome of the other. The controller may have to tolerate large errors and/or delays in sensor observations, while the sensors may have to encode and transmit their measurements subject to strict controller requirements, in addition to those imposed by the wireless channel. The situation may be further complicated by the network topology if we take into account possible multihop and relaying between sensor nodes and sensor/controller. Although many challenges in optimization of communication and control in these scenarios remain open [1] , if we restrict our attention to linear and stochastic plants, and if the control objective is to minimize the linear quadratic Gaussian (LQG) control cost, there are a few nice, established results.", "The findings of [4] is predominantly of a communication nature. The control applied to the plant is limited to time horizon T = 0 and is also linear. In this paper, we further extend the network topology to more than two sensors. The angle of our approach to the multisensor control problem is also rooted in communications. Namely, we ask ourselves the question \"exactly how well can the linear coding schemes perform in such multisensor control system?\". Naturally, the answer is trivial if we restrict the control horizon to T = 0, or the problem can be unmistakably complex if our aim is to simultaneously optimize communication and control for an infinite time horizon. We can, however, simplify our problem at hand by using the divide and conquer strategy. That is, we look at the case when communication (i.e., estimation of state) and control can be separated and then focus solely on the communication aspect of the problem. Note also that the term \"communication\" in this paper is confined to the application and physical layer of the communication protocol stack. Unquestionably, other layers can/will be involved when a more complex network is considered."], "relatedWork": [], "rq": ["the findings of [4] is predominantly of a communication nature. the control applied to the plant is limited to time horizon t = 0 and is also linear. in this paper, we further extend the network topology to more than two sensors. the angle of our approach to the multisensor control problem is also rooted in communications. namely, we ask ourselves the question \"exactly how well can the linear coding schemes perform in such multisensor control system?"]}
{"intro": ["What about lower bounds? If S is a group and all constants involved in multiplications in the computation of the Fourier transform are restricted to size no larger than 2, then it is known that the Fourier transform of an arbitrary f \u2208 CS requires at least 1 4 |S| log |S| operations [2] . This bound does not hold for inverse semigroups in general. In Remark 5.19 we point out an infinite family of inverse semigroups S for which the Fourier transform of f \u2208 CS can be computed in |S| operations. However, the semigroups in this family are all idempotent and therefore have only trivial maximal subgroups. The question then becomes: Are there any interesting inverse semigroups S with nontrivial maximal subgroups whose Fourier transform is sub-O(|S| log |S|) in complexity? As a first step towards answering this question, we exhibit in Section 5.4 two inverse semigroup generalizations S of Z n for which a key step in computing the Fourier transform (which has been O(|S| log |S|) or worse in complexity for previously-considered inverse semigroups) can be completed in O(|S| log log |S|) operations."], "relatedWork": [], "rq": [" the question then becomes: are there any interesting inverse semigroups s with nontrivial maximal subgroups whose fourier transform is sub-o(|s| log |s|) in complexity?"]}
{"intro": ["In this paper we take up the question posed by the results of Sporleder and Lapata (2005) : how much, if any, accuracy reduction should we expect if we choose to use only finite-state derived features, rather than those derived from full contextfree parses? If little accuracy is lost, as their results suggest, then it would make sense to avoid relatively expensive context-free parsing, particularly if the amount of text to be processed is large or if there are real-time processing constraints on the system. If, however, the accuracy loss is substantial, one might choose to avoid context-free parsing only in the most time-constrained scenarios."], "relatedWork": [], "rq": [" : how much, if any, accuracy reduction should we expect if we choose to use only finite-state derived features, rather than those derived from full contextfree parses?"]}
{"intro": [], "relatedWork": [], "rq": [" the moral dilemma of a technology innovator remains troublesome: how can i ensure that the systems i envision will bring greater benefits than the negative side effects that i dread and those that i fail to anticipate?"]}
{"intro": ["R OUTING is a crucial operation in many types of networks from communication networks to transportation networks. For instance, in modern IP-based data networks, shortest path routing is most commonly used. In traditional telecommunication networks, dynamic alternative routing strategies that employ paths that are longer than shortest paths have been also proposed to reduce call blocking probabilities (see, e.g., [1] , [17] ). In wireless networks, due to the unstable channel characteristics, using a single ''shortest'' path (e.g., with link quality as link weights) for routing is often not the best choice; routing strategies that go beyond shortest path routing (see, e.g., [4] , [16] , [22] , [31] and references therein) using multiple paths are often more effective. In the other extreme, in wireless sensor networksVdue to their power and other resource constraintsVpotential-based routing [26] has been proposed, where the source essentially utilizes all (eligible) paths to transmit data to the destination. In [26] , it is shown that such ''all-path'' routing minimizes the total energy dissipation of routing and thus maximizes the network lifetime. Clearly, what routing strategies to employ in a network hinges on what objectives are important in practice, therefore should be optimized. However, from a theoretical perspective, when using multi-path routing that goes beyond a single shortest path, two questions arise: 1) what set of paths should be used for routing? and 2) how traffic should be split (and merged) at any node along the multiple paths, especially when the paths are not all disjoint?"], "relatedWork": [], "rq": [" 1) what set of paths should be used for routing?", " and 2) how traffic should be split (and merged) at any node along the multiple paths, especially when the paths are not all disjoint?"]}
{"intro": ["Standard accounts are designed to ensure invariantly a wide-scope interpretation of NRCs. Basically, there are two ways to do this: Syntactic approaches assume that NRCs outscope the embedding clause because they are attached high in the syntactic tree (at CP-level, McCawley 1982) , or even syntactically orphaned (Safir 1986; Fabb 1990; Espinal 1991) . Alternatively, it has been argued that NRCs are attached locally to their host clause at DP (Del Gobbo 2003; Potts 2005) or at IP level, but that the content they contribute projects, i.e. is interpreted globally, outside the context of the embedding sentence. This happens because that content is semantically (Potts 2005) or pragmatically (AnderBois, Brasoveanu & Henderson 2011; Simons, Tonhauser, Beaver & Roberts 2011 ) not-at-issue, or because NRCs have their own illocutionary force (Koev 2013) . Recent work on NRCs, however, has challenged this view showing that the content of NRCs interacts in multiple ways with the content of the main clause (Nouwen 2007; Arnold 2007; AnderBois et al. 2011; Saeb\u00f8 2011) . Moreover, Schlenker (2013: 7) points out English and French examples where the NRC is interpreted locally in the scope of entailment-canceling operators like if. For instance, (2a), adapted from Schlenker 2013: 7, does not tell us that the dean called the chair, but instead has the local reading in (2c): The NRC contributes to the restriction of the conditional. First experimental results from German confirm the existence of such local readings, but suggest that their availability is dependent on the type of coherence relation established between the NRC and its host clause (Poschmann 2018) . These findings challenge existing approaches to NRCs. No worked-out explanation of this phenomenon has been offered so far. The present paper addresses the following questions: How can we explain the existence of local readings of NRCs? Why do coherence relations affect the scope of NRCs? And why are global readings still preferred most of the time? After a brief empirical overview in section 2, we argue that NRCs are locally attached to their host clause in the syntax, but not necessarily semantically conjoined in that position. Rather they are connected to their host clause by an underspecified coherence relation which must be resolved pragmatically (section 3). That pragmatic inference is guided by the same principles as the inference of coherence relations between independent sentences in discourse. In section 4, we show how the traditional discourse-theoretic distinctions between subordinating and coordinating relations, and between semantic and presentational relations help explain the role played by coherence relations in licensing local readings of NRCs. Finally, section 5 addresses the question of why NRCs project most of the time."], "relatedWork": [], "rq": [" the present paper addresses the following questions: how can we explain the existence of local readings of nrcs?"]}
{"intro": [], "relatedWork": [], "rq": ["-systems face questions such as: how can users retain some flexibility in their self presentations?"]}
{"intro": [], "relatedWork": [], "rq": ["the behavior of noncrossing partitions and restricted permutations suggests the following question: what other combinatorial objects admit a natural partial order which is self-dual and possibly, has other nice properties?"]}
{"intro": ["They mainly preserve and organize online content in highly personalized information spaces. Sharing, appropriation, and collaboration further form the crux of the communication dynamics enabled by digital technologies (Shirky, as cited in Mihailidis & Cohen, 2013) . Dunaway (2011) suggests that learning landscapes in a digital age are networked, social, and technological. Since people commonly create and share information by collecting, filtering, and customizing digital content, educators should provide students opportunities to master these skills (Mills, 2013) . In enhancing critical thinking, we have to investigate pedagogical models that consider students' digital realities (Mihailidis & Cohen, 2013) . November (as cited in Sharma & Deschaine, 2016) , however warns that although the Web fulfils a pivotal role in societal media, students often are not guided on how to critically deal with the information that they access on the Web. Sharma and Deschaine (2016) further point out the potential for personalizing teaching and incorporating authentic material when educators themselves digitally curate resources by means of Web 2.0 tools. This paper provides a conceptual exploration of the potential and necessity of content curation as a core competency in higher education. It contextualizes the incorporation of digital curation in a higher education environment and points out the relevance of the concept of metaliteracy in this process. Digital curation is clarified, followed by a presentation of some digital curation tools and possible applications of digital curation in a higher education environment. Finally, the SECTIONS model is used to investigate the suitability of digital curation tools in a higher education environment, followed by further suggestions for research and educational applications. Wiley and Hilton (2009) point to considerable disengagement between higher education and society, especially since higher education does not sufficiently acknowledge six particular means in which technological innovations changed society. This causes considerable disparity between higher education and people's daily experiences, as presented in Table 1 . Table 1 The Differences Between Higher Education and the Super-System Embedding It (Wiley & Hilton, 2009, p Wiley & Hilton, 2009) . Mobile phones and wireless devices further allow people to move freely between geographical locations and it is possible to access services such as the Web, Facebook, and instant messaging without being bound to a specific venue."], "relatedWork": [], "rq": [" costs: what are the costs involved in each technology?", " speed: is it possible to quickly incorporate this technology into courses?"]}
{"intro": ["ers to \"master academic content regardless of time, place, or pace of learning\" (U.S. Department of Education, 2013). Such programs include \"online and traditional courses, dual enrollment and early high schools, project based and community based learning, and credit recovery (i.e. Diploma Plus)\" (U.S. Department of Education, 2013) . Moreover, this strategy employs various teaching methodologies, while providing learners with the credits for the learning opportunities gained via lifelong experiences or other means by demonstrating adequate skill levels (i.e. competence) (Hyman, 2012) . Learners can access preferred learning methodology that provides them with academic experience to develop and enhance specific skills needed to support their working careers. For example, the ability to make good decisions when faced with complex situations, the ability to research information in order to identify a solution, the ability to set goals and work towards achieving these goals, as well as the ability to communicate well are all skills needed in a number of organizations. Subsequently, learners can assume positions with little training on the job, and greater confidence (Gravill, Compeau, & Marcolin, 2006) . CBL strategy can deliver greater flexibility, efficiency, and lower costs to learners, while at the same time, cultivating greater employability. The aim of CBL is to create various paths to college education, while taking advantage of innovative information system technologies not as part of traditional in-class education but rather as an online skills enhancement tool by itself. Such tools are not reserved only for adults. For example, Generation Z have been using popular CBL to develop their own Science, Technology, Engineering, and Math (STEM) skills including Websites such as Khan Academy (www.khanacademy.org), Reading Plus (www.readingplus.com), or the Stanford's Educational Program for Gifted Youth (EPGY) (epgy.stanford.edu) to name a few. Moreover, over the past several years there has been a growing use of free and open tools such as YouTube, iTunes U, online courses, Web 2.0, Massively Open Online Courses (MOOCs), digital books, and digital notes. While a significant number of research studies has been done on the benefits of computer simulations as a skill development tool, the majority of such studies have focused on using the tools as part of face-to-face courses or to augment traditional learning modalities. However, there is a very limited number of research studies that have been conducted on the measurements of skill enhancements in fully online courses and programs, as opposed to measuring the benefits of such computer simulations as well as competency-based projects to augment in-class courses (Levy, 2005) . Additionally, there are still limited empirical results on the use of computer simulations and competency-based projects being used as part of fully online learning courses. Thus, in this paper we have developed a quasi-experimental research using such instrument on pre-and post-tests to collect the set of 12 management skills from learners attending fully-online courses, including competency-based computer simulations and those that didn't. Therefore, the main research question guiding our study was: RQ: What is the effect of computer simulations and competency-based projects on management skills enhancement in online learning courses?"], "relatedWork": [], "rq": [" rq: what is the effect of computer simulations and competency-based projects on management skills enhancement in online learning courses?"]}
{"intro": ["Persuasive technology is a rapidly evolving area of ubiquitous and wearable computing and is growing in popularity both in academic and private sector research groups. Designers of persuasive technologies use different design strategies in order to persuade users, such as Foggs' seven types of persuasive strategies [18] . However, a large part of the strategies used rely on conscious awareness of the user about the behavior to change. While this has been an effective way to develop persuasive technologies, there are several limitations and potential issues involved, such as the strong reliance on user's motivation and humans limited capacity for self-control [31] . * Both authors contributed equally to this work Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. Many of the current persuasive technologies are heavily impacted by both internal factors and environmental contexts, such as what mood the user is in, where they are, how much stress they are under, or who they are with, which are unpredictable and subject to change. These internal factors can even disrupt people's interaction with the technologies, which may counteract positive aspects of the interventions."], "relatedWork": [], "rq": [" the question then is: how to design a technology that positively influence the nonverbal behavior of the user in real-time without compromising their performance?"]}
{"intro": ["However, the adoption of a matrix representation-based standard API faces considerable challenges considering that algorithm writers are prone to describing their algorithms in terms of operations on vertices and edges despite the equivalence of the two representations [8] . This choice is often made based on the notion that adjacency matrices necessarily lead to a O(N 2 ) space requirement [9] . On the other hand, while the GraphBLAS interface describes the operations using matrices and vectors [10] , their implementation takes advantage of the sparse nature of these objects. This representational dilemma presents the algorithm developer with a crucial question: how should an algorithm expressed using vertices and edges be expressed using linear algebra and implemented in an API such as the GraphBLAS? While several graph algorithms have been studied and implemented using the language of linear algebra [10] , many others remain unimplemented. Without a systematic approach to the translation problem, it is unclear if this lack of algorithmic implementation is a matter of effort or if there are fundamental restrictions with the implementation API."], "relatedWork": [], "rq": [" this representational dilemma presents the algorithm developer with a crucial question: how should an algorithm expressed using vertices and edges be expressed using linear algebra and implemented in an api such as the graphblas?"]}
{"intro": [], "relatedWork": [], "rq": [" is: why is s \u2208 ob(s \u2297 ) not the unit object?"]}
{"intro": [], "relatedWork": [], "rq": ["8) are sumcient for r's believing that q has the simple plan as expressed in (2) . this much is not surprising. in effect q has stated in his query what his plan is--to prevent tom from reading the file by setting the permission on it to faculty-read only--so, of course, r should be able to infer just that. and if r further believes that the system manager can override file permissions and that tom is the system manager, but also that q does not know the former fact, r will judge that q's plan is ill-formed, and may provide a response such as that in (7). there is a discrepancy here between the belief r ascribes to q in satisfaction of clause (ii) of (pl)--namely, that expressed in (15) but what if r, instead of believing that it is mutually believed by q and r that tom is the system manager, believes that they mutually believe that he is a faculty member?"]}
{"intro": [], "relatedWork": [], "rq": ["-21 what exactly does the activation of a chord node represent?"]}
{"intro": [], "relatedWork": [], "rq": [" privacy and security: is the internet safe and can we trust systems to protect us?"]}
{"intro": ["IMAGINE a known 3D polyhedral environment where a set of cameras has to be placed in such a way that every point in the environment is visible. The 2D version is known as the art gallery problem [2] , [3] , [4] and sufficiency results exist for several versions of this problem. For example, b n 3 c cameras can cover any simple polygon. However, such results are inapplicable in robotic and image-based rendering applications where the environments can be very complex with millions of vertices. A further application is placing antennas for line-of-sight broadband communication [5] . Imagine that backbones end at each neighborhood and that communication inside 1km can be achieved with line-ofsight laser beams that can carry from 10Mbps up to 1.25 Gbps bandwidth. Assuming that a consumer can put a receiving antenna at the window of her studio or even on a kiosk in a street, the coverage problem becomes a visibility problem where the cameras become arrays of distributing antennas.", "We are not going to address here the equally important problem of unknown environments or objects related to model building tasks. Several algorithms exist for exploring unknown environments [8] , building upon fundamental results in the on-line traversal of graphs [9] , or on Markov processes for modeling partially known dynamic scenes [10] . Significant contributions have been also achieved in the Next Best View planning problem [11] , [12] , [13] for surface acquisition. However, the results proven in this paper have implications for the unknown case as well: Choosing sensor locations randomly is a method frequently used for sensor placement in unknown environments [14] . The VC-dimension theory enables us to answer the question: How many random samples (sensors) do we need in order to cover a given region? Our results, together with the theory of -nets, provide an upper bound to the answer to this question when omni-directional cameras are used as sensors.", "It is well known [2] that the minimal guard coverage problem is NP-hard. To study the existence of approximation algorithms, we can consider minimal guard coverage as an instance of the set-cover problem. The general version of minimum set-cover cannot be approximated with a ratio better than log n. However, we do not know whether any set-cover instance can be realized as a visibility system. A powerful interface between set-cover and the particular geometric setup is the Vapnik-Chervonenkis (VC) dimension, which enables us to quantitatively bound how general a set system is."], "relatedWork": ["In general, it is possible to consider visibility systems as set systems and camera placement as a set-covering problem [22] . The general version of the minimum set-cover problem cannot be approximated better than a factor of log n. However, as mentioned in the first section, it is not clear that general set-cover instances can be realized by visibility systems.", "On the other hand, it is not clear how to exploit a bounded VC-dimension to obtain an improved approximation algorithm. Approximation algorithms for minimum guard coverage have been considered [22] , [24] , [23] for different versions of the problem. However, there is still a gap between the inapproximability results and existing algorithms.", "As mentioned before, the minimum set-cover of set systems with bounded VC-dimension can be approximated within a logarithmic factor of the optimal value [21] . However, this by itself does not improve on the existing log n approximations, as the optimum can be as big as n=3 [2] . Nevertheless, this algorithm was used in [23] to get rid of the dependency of the approximation factor to the number of samples (rather than the number of vertices). In fact, obtaining a constant approximation algorithm for guard placement in polygons without holes is one of main open problems in the field of art galleries."], "rq": [" the vc-dimension theory enables us to answer the question: how many random samples (sensors) do we need in order to cover a given region?"]}
{"intro": [], "relatedWork": [], "rq": [" verification answers the question: how do we know the transformed design preserves the same behaviour?"]}
{"intro": ["Devising computer systems capable of answering questions about knowledge described using text has 1 Available at http://qangaroo. cs.ucl.ac.uk The Hanging Gardens, in [Mumbai] , also known as Pherozeshah Mehta Gardens, are terraced gardens \u2026 They provide sunset views over the [Arabian Sea] \u2026 Mumbai (also known as Bombay, the official name until 1995) is the capital city of the Indian state of Maharashtra. It is the most populous city in India \u2026 Q: (Hanging gardens of Mumbai, country, ?) Options: {Iran, India, Pakistan, Somalia, \u2026} The Arabian Sea is a region of the northern Indian Ocean bounded on the north by Pakistan and Iran, on the west by northeastern Somalia and the Arabian Peninsula, and on the east by India \u2026 Figure 1 : A sample from the WIKIHOP dataset where it is necessary to combine information spread across multiple documents to infer the correct answer. been a longstanding challenge in Natural Language Processing (NLP). Contemporary end-to-end Reading Comprehension (RC) methods can learn to extract the correct answer span within a given text and approach human-level performance (Kadlec et al., 2016; Seo et al., 2017a) . However, for existing datasets, relevant information is often concentrated locally within a single sentence, emphasizing the role of locating, matching, and aligning information between query and support text. For example, observed that a simple binary word-in-query indicator feature boosted the relative accuracy of a baseline model by 27.9%.", "We argue that, in order to further the ability of machine comprehension methods to extract knowledge from text, we must move beyond a scenario where relevant information is coherently and explicitly stated within a single document. Methods with this capability would aid Information Extraction (IE) applications, such as discovering drug-drug interac-tions (Gurulingappa et al., 2012) by connecting protein interactions reported across different publications. They would also benefit search (Carpineto and Romano, 2012) and Question Answering (QA) applications (Lin and Pantel, 2001) where the required information cannot be found in a single location. Figure 1 shows an example from WIKIPEDIA, where the goal is to identify the country property of the Hanging Gardens of Mumbai. This cannot be inferred solely from the article about them without additional background knowledge, as the answer is not stated explicitly. However, several of the linked articles mention the correct answer India (and other countries), but cover different topics (e.g. Mumbai, Arabian Sea, etc.) . Finding the answer requires multi-hop reasoning: figuring out that the Hanging Gardens are located in Mumbai, and then, from a second document, that Mumbai is a city in India."], "relatedWork": [], "rq": ["5) is the capital city of the indian state of maharashtra. it is the most populous city in india \u2026 q: (hanging gardens of mumbai, country, ?"]}
{"intro": [], "relatedWork": [], "rq": [" a natural question arises: how must \u03b5 be chosen?"]}
{"intro": ["P EOPLE have the remarkable ability to remember thousands of pictures they saw only once [1] , [2] , even when they were exposed to many other images that look alike [3] , [4] . We do not just remember the gist of a picture, but we are able to recognize which precise image we saw along with some of its details [3] - [6] . However, not all images are remembered equally well. Some pictures stick in our minds whereas others fade away. The reasons why images are remembered may be highly varied; some pictures might contain friends, a fun event involving family members, or a particular moment during a trip. Other images might not contain any recognizable monuments or people and yet also be highly memorable [2] , [3] , [5] . In this paper we are interested in this latter group of pictures: what makes a generic photograph memorable?", "Similar to other subjective image properties, memorability is likely to be influenced by the user context and also be subject to some degree of inter-subject variability [7] . However, despite this expected variability when evaluating subjective properties of images, there is often also a sufficiently large degree of consistency between different users' judgments, suggesting it is possible to devise automatic systems to estimate these properties directly from images, ignoring user differences. As opposed to other image properties, there are no previous studies that try to quantify individual, everyday photos in terms of how memorable they are, and there are no computer vision systems that try to predict image memorability. This is contrary to many other photographic properties that have been addressed in the literature such as photo quality [8] , aesthetics [9] , [10] , interestingness [11] , saliency [12] , attractiveness [13] , composition [14] , [15] , color harmony [16] , and importance [17] , [18] . Also, there are no databases of photographs calibrated in terms of the degree of memorability of each image."], "relatedWork": [], "rq": [" in this paper we are interested in this latter group of pictures: what makes a generic photograph memorable?"]}
{"intro": [], "relatedWork": [], "rq": ["q2: how much improvement is achieved by the dynamic approach?"]}
{"intro": ["In many contemporary computing environments, especially those where multiple collocated displays are used collaboratively, such as in the war rooms [1, 2, 7, 14, 27, 28] , and in operating rooms [9, 17] , users and their input devices are often not located directly in front of, or oriented toward, the display of interest ( Figure 1 ). Technical solutions to the problem of allowing multiple participants to make input to multiple displays in such an environment have been examined by Johanson et al. [11] . Unexamined in the literature, however, is a usability problem created in such an environment: how should displays be positioned to optimise their use by multiple participants, and what mapping of pointing-device input to on-screen movement should be employed for any given screen position?", "With the exception of direct-touch and tablet interfaces, it is usually the case that the location of user input is offset from the visual consequences of those actions. In the case of desktop computers, most users appear to easily handle the transformation of mouse movements on a horizontal surface to cursor movements on a vertical display. However, research [4, 13, 23, 24] has shown that performance of motor tasks can incur significant penalties under more dramatically offset input/output spaces, such as rotated mappings of up to 180 o . While these penalties are reduced with practice, they are typically not completely eliminated."], "relatedWork": [], "rq": [" is a usability problem created in such an environment: how should displays be positioned to optimise their use by multiple participants, and what mapping of pointing-device input to on-screen movement should be employed for any given screen position?"]}
{"intro": ["Thus, this research is aimed to portray students' perceptions on their teacher's roles in EFL classroom by using social semiotic approach. The participants are the 11 th grade students in a state senior high school in Banjarmasin. The students express their perceptions on teachers' role through drawings. Clarebout et al (2007) argues that drawings can be used to identify nuances and ambivalences within a person's belief system, indicating they would be useful when studying pupil conceptions. This research strategy is increasingly being used to probe students' feelings about how they teachers accomplish their role in the classroom. This research is going to answer a single research question: How do students perceive their teacher's roles in the EFL classroom viewed from their drawings? 2. LITERATURE REVIEW 2.1 TEACHERS' ROLES To achieve an effective teaching-learning process, teachers ought to know their roles in the classroom. In the process of transferring knowledge, teachers are expected to play particular roles. However, the implementation of the role itself depends on the situation of the educational institution, the students, and the subject matter we are required to teach, and so on. Considering those various situations, teachers are required to play more than one role. According to Harmer (2007) , there are five roles of the teachers: (1) teacher as a controller, (2) Teacher as a prompter, (3) teacher as participant, (4) teacher as resource, and (5) teacher as tutor. Furthermore, those five roles are becoming the primary issue in this research. The writer intends to investigate the realization of those five roles by analyzing students' perception through their drawings 2.2 SOCIAL SEMIOTIC APPROACH In this study, the semiotic sign which will be analyzed is students' drawing. Images show not the thing actually seen but its representations in human consciousness. Turkcan (2013) proposes that semiotic approach has increasingly gained importance so that people, who live in a visual bombardment today, can analyze the codes included in visual culture and understand the form of visual communication. Halliday (1978) states that social semiotic theory provides the basis for the study of semiotic resources other than language (e.g. images, architecture, music, mathematical symbolism, gesture, clothing). Moreover, Aiello (2006) argues that the main aim of social semiotics is to look systematically at how textual strategies are deployed to make certain meanings. He also adds that social semiotics concentrates on practices of meaning-making and considers how we make meaning using various semiotic resources, modes and their affordances. Then, what is developed by Kress and van Leeuwen's (1996) and Van Leeuwen and Jewitt (2001) is how we can use social semiotics as an analytical tool to access these meanings by drawing on the contexts and cultures of its production and how we as individuals assign meanings to our texts through our prior knowledge, exposure and use of semiotic resources and modal affordance in our communicative practices. Kress and Van Leeuwen (1996) have extended this idea to images, using a slightly different terminology: representational meaning, interactive meaning, and compositional meaning."], "relatedWork": [], "rq": [" this research is going to answer a single research question: how do students perceive their teacher's roles in the efl classroom viewed from their drawings?"]}
{"intro": ["Prior research on leadership has typically defined it as an individual trait or individual role [12, 13] . These studies have typically focused on how one individual influences the behavior of other team members to help meet team objectives [57] . However, many problems addressed by virtual teams are often complex and require specialized knowledge to solve that no single individual possesses [45] . In these situations it is difficult to have one person in charge of the overall team. This problem is only exacerbated when these teams work at a distance [2, 3, 35] . As a result, there has been a shift away from traditional hierarchical leadership structures to a more decentralized or shared leadership [13, 40] .", "Shared leadership is the distribution of leadership among team members and is characterized by the sharing of leadership roles [12, 13] . Shared leadership, however, offers the possibility for every team member to be included in team decisions which potentially promises more inclusion and better team experiences. This paper seeks to build on prior literature examining shared leadership in virtual teams [5, 13] . The research question this study attempts to address is: \"How does shared leadership impact the identification, satisfaction and actual performance of virtual teams?\" This study has three goals: 1) To examine the impacts of shared leadership on an individual's willingness to identify with their team in light of their race and gender. Team identification has been found to be an important process variable for the success of virtual teams [9, 45] . However, little research has been done to understand the conditions that facilitate team identification when these teams are racially and gender diverse. 2) Investigate the impact of shared leadership on team satisfaction. Virtual teams are often criticized for being overly sterile and too task focused [44] . As work becomes both more virtual and collaborative, understanding what factors facilitate a positive team work environment becomes increasingly important 3) Examine the impact of shared leadership on the performance of virtual teams. Prior research has only posited positive impacts between shared leadership and virtual team performance [5, 13] ; however, there are reasons to believe that shared leadership could potentially decrease the performance of virtual teams."], "relatedWork": [], "rq": ["2] can we expect the same problems to exist in virtual teams that are geographically separated?"]}
{"intro": ["Cyber-physical systems (CPS) are complex networks of embedded systems, aiming to transit the physical world into the cyber world. 1, 2 Vehicular cyber-physical systems (VCPS) 3 are typical CPS applied to the Internet of Things (IoT). They have been proved to hold great potentials in enhancing vehicle safety 4 and improving traffic efficiency. [5] [6] [7] In addition, some large data services onboard should not be ignored either (e.g. local maps downloading or multimedia files downloading). Therefore, for vehicular users, with an increasing demand for downloading contents from the Internet, VCPS have further been envisioned for making a contribution to passengers comfort by providing content downloading services. [8] [9] [10] [11] Normally, in VCPS, vehicular users are the edge nodes which are far away from the network core (i.e. cloud computing servers). They can download contents from the Internet via the edge of the cloud, RoadSide Units (RSUs). A good user experience can be achieved within the coverage of RSUs which is about 300-1300 m. 12 However, in highway scenarios, RSUs are mainly deployed at service areas or gas stations which are [8] [9] [10] [11] [12] [13] [14] [15] [16] km apart. It is impossible to expect a high number of well-deployed RSUs due to cost restrictions and high maintenance overhead. Therefore, many dark areas exist out of the RSUs' coverage range which may cause intermittent connectivity for vehicular users. In other words, if a vehicular client leaves the coverage of a RSU without finishing downloading contents, it may waste much time to wait for contacting the next RSU.", "Previous work on content downloading in VCPS proposes cooperative downloading methods to utilize edge nodes (i.e. other vehicular users) and reduce the effect on intermittent connectivity in dark areas. 13, 14 It means when a vehicular client drives in a dark area without finishing the whole downloading, these unfinished contents are then delegated to cooperative vehicles and further transmitted to clients by a carryand-forward mechanism 15 in dark areas. However, those works ignore the cooperative intention of vehicles and the optimization of the whole cooperative downloading process. In order to fill such a gap and further develop the tremendous potential of edge nodes, two questions should be taken into consideration: (1) How to motivate vehicles to participate in cooperative downloading? (2) How to maximize the downloading throughput in dark areas?"], "relatedWork": ["Content downloading in VCPS has attracted extensive research attention. There are mainly two schemes for achieving the content downloading service in VCPS: no-cooperative downloading and cooperative downloading. Some researchers focused on no-cooperative schemes. 16, 17 Eriksson et al. 16 improved the downloading performance by optimizing both the connection establishment procedure (QuickWiFi) and the cabernet transport protocol (CTP). In the work by Zhang et al., 17 the authors studied data scheduling issues for content downloading. However, these no-cooperative schemes ignore the tremendous downloading potential of edge nodes on the network. Taking this situation into consideration, cooperative downloading schemes were proposed by Chen and Chan 13 and Trullols-Cruces et al. 14 Chen and Chan 13 proposed a method of combining WiFi with wireless wide area network (WWAN) to achieve the cooperative downloading. Trullols-Cruces et al. 14 addressed the cooperative downloading issue by introducing a delaycooperative Automatic Repeat reQuest (ARQ) mechanism and a vehicular route predictability-based carry-andforward mechanism. Nevertheless, these schemes just proposed a framework for cooperative downloading in different environments but did not give a detail algorithm for cooperative downloading."], "rq": [" (1) how to motivate vehicles to participate in cooperative downloading?", " (2) how to maximize the downloading throughput in dark areas?"]}
{"intro": ["Supervised machine learning algorithms were used for pronoun resolution with good results (Ge et al., 1998) , and for definite NPs with fairly good results (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001) . However, the deficiency of supervised machine learning approaches is the need for an unknown amount of annotated training data for optimal performance. So, researchers in NLP began to experiment with weakly supervised machine learning algorithms such as Co-Training (Blum and Mitchell, 1998) . Among others Co-Training was applied to document classification (Blum and Mitchell, 1998) , namedentity recognition (Collins and Singer, 1999) , noun phrase bracketing (Pierce and Cardie, 2001) , and statistical parsing (Sarkar, 2001) . In this paper we apply Co-Training to the problem of reference resolution in German texts from the tourism domain in order to provide answers to the following questions: Does Co-Training work at all for this task (when compared to conventional C4.5 decision tree learning)?"], "relatedWork": [], "rq": ["5) are highly idiosyncratic and applicable only to one particular domain. mccarthy and lehnert (1995) achieved results of about 86% f-measure (evaluated according to vilain et al. (1995) ) on the muc-5 data set. however, only a defined subset of all possible reference resolution cases was considered relevant in the muc-5 task description, e.g., only entity references. for this case, the domain-dependent features may have been particularly important, making it difficult to compare the results of this approach to others working on less restricted domains. soon et al. (2001) use twelve features (see table 1). they show a part of their decision tree in which the weak string identity feature (i.e. identity after determiners have been removed) appears to be the most important one. they also report on the relative contribution of the features where -distance in sentences between anaphor and antecedent -antecedent is a pronoun?"]}
{"intro": ["In recent years, a great deal of research has been conducted into the design and implementation of tabletop interactive systems; for example, the works of ; Rogers et al. (2004) ; Shen et al. (2003) ; Shen et al (2004) ; Streitz et al. France, [205] [206] [207] [208] [209] [210] [211] [212] [213] [214] [215] [216] [217] [218] [219] [220] [221] [222] [223] [224] (1999); Streitz, et al. (2002) ; and Wu and Balakrishnan (2003) . Especially exciting about this domain is that a tabletop application can be targeted to simultaneous use by multiple users seated around the table. This organization, however, creates a problem unique to the domain: how should on-display objects be oriented?", "Research has been conducted into how participants in non-computer based, tablecentered collaborative tasks make use of orientation (Tang 1991; Kruger et al. 2003) . These investigations have shown that users prefer a straight-on orientation for reading text, and orient objects towards themselves, others, or in-line with shared artifacts to ease reading in different circumstances. They have discovered, however, that in a collaborative setting, a straight-on orientation toward the reader is not always desired or exercised. In fact, orientation is employed as a tool to aide in interaction with other users. Despite this, it seems that designers of collaborative research systems have, in general, opted to attempt to orient text towards the reader, as seen in Bruijn et al. (2001) , Rekimoto et al. (1999) , Shen et al. (2003) , Shen et al. (2004) , and Streitz et al. (1999) . Thus, there is a tension between a desire to allow for the use of orientation as an aid to collaboration, and the designers' assumption that users need to have textual elements oriented towards them. Absent from this previous work is a thorough investigation into the parameters for determining when a solution to the text orientation problem should be applied in the context of tabletop groupware. Although users seem to prefer a \"normal\" orientation of text (Kruger et al. 2003) , and studies in the psychology literature in non-tabletop situations with constrained user head and body movement by Tinker (1972) and Norman (1984, 1985) indicate that readability is compromised when text is oriented, is it possible that there are circumstances where it might be appropriate to ignore this preference in favour of a less preferred orientation that may aide collaboration in other ways? Is orientation so critical for text readability that design elements must be sacrificed in order for it to be addressed? Without empirical data quantifying the extents to which readability is compromised in less-preferred orientations on tabletops, it is difficult to make informed choices when confronted with these tradeoffs. Our present work provides such empirical data via two experiments that examine the effect of text orientation on the performance of tasks common to tabletop collaborative groupware. Based on the results, we hope to provide insights for system designers as to when the issue of text orientation takes precedence, and when it can be safely ignored. Although very infrequently done in the field of human-computer interaction, replication and extension of experimental work is an important aspect of research. Our work also contributes in this regard by re-examining and extending the studies of readability of text orientation by Tinker (1972) and Koriat and Norman (1985) to tasks relevant to the new domain of tabletop displays, and where users' head and body movements are unconstrained."], "relatedWork": [], "rq": [" creates a problem unique to the domain: how should on-display objects be oriented?"]}
{"intro": ["Question: Is there a coloring c of G such that c(v) \u2208 L(v); v \u2208 V ? If such a coloring c exists, then we call c an L-coloring of G, and we say that G is L-colorable. This problem is NP-complete even for interval graphs [1] , for complete bipartite graphs [13] , and for line-graphs of complete bipartite graphs [10] . However, (G; L) is solvable in polynomial time for partial t-trees with \u00ffxed t (in O(|V | \u00b7 k t+1 \u00b7 t) time) [16] . If the number of available colors k = |L(V )| = | v \u2208 V L(v)| is \u00ffxed, then (G; L) (denoted by k-(G; L)) is also solvable in polynomial time for P 4 -free graphs (in O((|V | + |E|) \u00b7 4 k ) time) [16] . de Werra [23] introduced the list coloring problem with cardinalities denoted by (G; L; p):", "Question: Is there a coloring c of G such that c(v) \u2208 L(v); v \u2208 V and such that |c \u22121 (j)| = p(j); j \u2208 L(V )? Thus, the goal is to \u00ffnd a coloring of the graph such that each vertex is assigned a color from its list, and such that there are exactly p(j) vertices of color j. Without loss of generality, we may suppose that L(v) \u2286 {1; : : : ; k} for each vertex v \u2208 V , with k = |L(V )|. We will represent by k-(G; L; p) the corresponding restricted list coloring problem with \u00ffxed number of available colors k = |L(V )|. de Werra [23] proved that (G; L; p) is polynomial for G being a union of disjoint cliques. When G = P n is a path with |V | = n vertices, de Werra conjectured that (P n ; L; p) is NP-complete. The proof of this conjecture is given by Dror et al. [8] . The authors in [8] proved a stronger result, namely that (P n ; L; p) is NP-complete even if |L(v)| 6 2 for each vertex v \u2208 V . However, the problem k-(P n ; L; p) can be solved in polynomial time (O(n k )) by dynamic programming [8] . In the next section, we will show that k-(G; L) reduces to k-(G; L; p), which means that if there exists a polynomial time algorithm for k-(G; L; p), then there exists a polynomial time algorithm for k-(G; L). This reduction, and the proof in [18] demostrating that 3-(G; L) is NP-Complete even for a planar bipartite graph G, show that 3-(G; L; p) is also NP-Complete even when G is a bipartite planar graph. We give a polynomial time algorithm for 2-(G; L; p). In Sections 3 and 4, we give a polynomial time algorithm for k-(G; L; p) (and hence k-(G; L)) for two classes of graphs containing P 4 -free graphs and triangulated graphs. These two results extend some earlier results given from [16, 8] ."], "relatedWork": [], "rq": ["question: is there a coloring c of g such that c(v) \u2208 l(v); v \u2208 v ?", "question: is there a coloring c of g such that c(v) \u2208 l(v); v \u2208 v and such that |c \u22121 (j)| = p(j); j \u2208 l(v )?"]}
{"intro": ["2. How can we efficiently generate (list) all instances of the object? (i.e., Can we develop a fast algorithm to generate all nonisomorphic chord diagrams with n chords?) In response to the first question, three independent papers by Li and Sun [8] , Cori and Marcus [4] , and Stoimenow [13] have derived enumeration formulas for the number of nonisomorphic chord diagrams. In each of these papers, the exact formula is the main result; however, in each case the derivation of the formula uses relatively complex methods. Cori and Marcus use Burnside's lemma (stated in section 3) along with liftings of quasidiagrams; Li and Sun introduce a new object called a generalized m-configuration; Stoimenow uses Burnside's lemma along with two new objects: linearized chord diagrams and generalized linearized chord diagrams. As a secondary result in this paper, we derive an exact formula for the number of nonisomorphic chord diagrams with n chords using simple counting techniques."], "relatedWork": [], "rq": ["2. how can we efficiently generate (list) all instances of the object?"]}
{"intro": ["Predicates require that their arguments be of a given type. However, as is well-known, certain acceptable constructions exhibit a mismatch between the type of the argument, as constructed from a possible paraphrase, and the type that the argument has outside *We are indebted to Anne Abeilld, Nicolas Asher, Michel Aurnague, Andrde Borillo, Annie Delaveau, Jean Marie Marandin, Jean-Pierre Mantel, Alex Lascarides, Patrick Saint-Dizier, Annie Zaenen and our referees for helpful comments, criticisms and suggestions. the construction. This traditionM problem has been recently rephrased within type theory, where types (like e for events, p for material objects, ~\u00a2 for kinds, etc.) classify the domain of entities (cf. [Bach, 1986; Carlson, 1977; Chierchia, 1984] ). Pustejovsky proposes in particular that the mismatch is solved by the operation of \"type coercion\" (cf. Pustejovsky and Anick, 1988; Boguraev and Pustejovsky, 1991] ). In essence, it confers to the predicate the ability to change the argument type. For example, the sequence in (1) is accounted for in the following way:", "(1) John began the book. The predicate associated with begin requires that the argument corresponding to the complement be an event (type e). Since the type associated with book is different (we will suppose it is \"material object\", p) it is coerced to e. Accordingly, (1) is given an event reading, which, in this case, is associated with two possible interpretations: \"John began to read the book\", and \"John began to write the book\". This is an interesting way of looking at the phenomenon, and typing certainly plays a crucial role in building a coercion interpretation. However, the hypothesis of type coercion itself is not supported by linguistic evidence, and is not sufficiently constrained to account for the impossibility of some combinations. Instead of type change on the argument, we propose an enrichment of the semantics of the predicates which give rise to coercion interpretation. Predicates may be finitely polymorphic; for instance, begin combines with arguments of type p as well as of type e. The correct interpretation is obtained at the interpretive level, where it results both from general processes and specific semantic properties of the predicate. When begin has a complement of type It, the interpretation makes use of a morphism between events and objects ([Krifka, 1992] ); this morphism itself is not noted in the grammar, but the result of its being resorted to can be noted, as well as the semantic properties of the item commencer. Thus, the phenomenon will be correctly expressed at the lexical level. More precisely, we will use lexical rules in the HPSG format ([Pollard and Sag, 1987; Pollard and Sag, 1993] ). We illustrate the phenomenon in French and focus on the commencer (begin) example, which is a very clear case of a predicate allowing coercion interpretations. We provide glosses, NOT English translations."], "relatedWork": [], "rq": [" the problem is the following: why does the book allow the interpretation \"to read\" while the symphony does not allow the interpretation \"to listen\"?"]}
{"intro": [], "relatedWork": [], "rq": [" this represents a major challenge if we wish to explore how they can be supplemented or augmented with digital means: is it feasible to gain (some of) the benefits of digital components without detracting from the features that make cards beneficial in collaborative design?", "s degree of usability: is it likely that the idea can be put in a use context and has the qualities to be used and have purpose?"]}
{"intro": ["\u2022 Is it possible to achieve non-interactive secure two-party computation with hardware tokens? Again, this problem is open even for stand-alone security, and impossible in the plain model. The work of Goldwasser et al. [GKR08] constructs (non-interactive) one-time programs using hardware tokens, however in their model, the sender is semi-honest 2 . Thus an equivalent question is: is it possible to achieve one-time programs tolerating a malicious sender? We note that [GKR08] make partial progress towards this question by constructing one-time zero knowledge proofs, where the prover can be malicious. However, in the setting of hardware tokens, the GMW [GMW87] paradigm of using zero knowledge proofs to compile semi-honest protocols into protocols tolerating malicious behavior does not apply, since one would potentially need to prove statements about hardware tokens (as opposed to ordinary NP statements)."], "relatedWork": [], "rq": [" thus an equivalent question is: is it possible to achieve one-time programs tolerating a malicious sender?"]}
{"intro": ["While most HCI research on memory technologies has focused on episodic memory impairments such as those associated with dementia, less work has focused on building technologies for autobiographical memory impairments to help people living with other mental disorders such as depression. Memory impairments in depression are however fundamentally different; their effect is felt not through the loss of episodic memories, but rather difficulties in the retrieval of episodic memories through higher levels of autobiographical memories such as general events and lifetime periods. Another distinct body of work explored computerized interventions for depression such as online Cognitive Behavioral Therapy (CBT). Such interventions address memory impairment, not as the main focus, but rather through a subset of psycho-educational materials concerned with negative thinking patterns, and tools for tracking mood [9, 10, 17] , which provide limited support for the distinctive autobiographical memory impairments associated with depression. Our work aims to bridge these two strands of work to contribute to the design of novel classes of technologies that specifically address memory impairments in depression. We argue that understanding the specific memory impairments in depression, and the new range of challenges they pose, offers a rich opportunity to extend HCI research on memory technologies in new directions. This paper is an initial step towards exploring this space and focuses on the following research questions: 1. How are memory impairments in depression addressed through tailored interventions used in clinical and neuropsychological practice? 2. What is the role of materials in these memory interventions and how are they employed in therapeutic practice? 3. How can therapeutic memory interventions for depression inform the design of novel memory technologies?"], "relatedWork": [], "rq": [" 1. how are memory impairments in depression addressed through tailored interventions used in clinical and neuropsychological practice?", " 2. what is the role of materials in these memory interventions and how are they employed in therapeutic practice?", " 3. how can therapeutic memory interventions for depression inform the design of novel memory technologies?"]}
{"intro": ["There are inherent links between emotion and thermal sensation and so thermal sensation is a key component of the conceptualisation and experience of emotion: physical warmth increases interpersonal warmth [22, 42] and the experience of physical temperatures helps to ground and process emotional experience [23] . Research has started to look at thermal feedback and emotion in HCI, including the measurement of physiological emotional responses [34, 35] , the modulating effect of thermal feedback on perception of emotional media [17] and users' inherent interpretations of thermal changes [25, 43] . The subjective comfort and intensity of different thermal stimuli have also been measured, but not in the specific context of affective computing [18, 44] . However, there is still relatively little work in HCI on thermal feedback and none has yet investigated how thermal sensation maps to common models of emotion, and so how thermal feedback might be classified and utilised to convey different emotions. Being able to convey emotion in HCI is important, to increase engagement, enjoyment and information bandwidth."], "relatedWork": [], "rq": [" this paper sought to answer 1) can thermal feedback convey the full range of emotions and 2) does the range of emotions available better fit a vector model?"]}
{"intro": ["The work of graphic designers in user interface projects was popularised with the Web and desktop interfaces [14] . With the growing understanding that this work can improve users' performance and acceptance of new products, it is now sought in the design of specialised user interfaces, from aircraft cockpits to plant supervision systems. For instance, Figure 1 illustrates a graphic designer's work for air traffic control. This evolution raises an important engineering issue: how can companies design and produce such design-intensive software at reasonable costs? For standalone applications with no complex functional core, tools such as Flash and Director allow designers to produce high quality products. But for more complex software, programmers and designers still have to choose between cost and flexibility. On the one hand, pre-designed widgets offered by interface builders are simple to assemble. However programmers often produce poor layouts, and designers resent the low control over the look and interaction style. On the other hand, graphical libraries offer greater flexibility: programmers can reproduce designs provided by graphic designers within certain limits. But even then the limits are sometimes too strict for designers, and the cost of re-coding their work is too high to support both iterative design and a profitable industry. Efficiently involving graphic designers in the production of interactive software calls for tools that meet the requirements of both graphic design and software engineering. It requires graphical capabilities that match the working methods of designers. It also challenges the architecture of common user interface tools and the notion of predefined widgets, which were introduced solely for programming purposes."], "relatedWork": [], "rq": [" this evolution raises an important engineering issue: how can companies design and produce such design-intensive software at reasonable costs?"]}
{"intro": ["Problem and Motivation. The demand for processing large amounts of data has increased over the last decade. As traditional one-processor machines have limited computational power, distributed systems consisting of hundreds of thousands of cooperating processing units are used instead. An example of such a massive distributed cooperative computation is the SETI@home project [17] . As the search for extraterrestrial intelligence involves the analysis of gigabytes of raw data that a fixed-size collection of machines would not be able to effectively carry out, the data are distributed to millions of voluntary machines around the world. A machine acts as a server and sends data (aka tasks) to these client computers, which they process and report back the result of the task computation. However, these client computers are not trustworthy and might act maliciously. This gives rise to a crucial problem: how can we prevent malicious clients from damaging the outcome of the overall computation?", "In this work we abstract this problem in the form of a distributed system consisting of a master fail-free processor M and a collection of n (powerful) processors, called workers, that can execute tasks; worker processors might act maliciously, that is, they are Byzantine [20] . Since each task returns a value, we want the master to accept only correct values with high probability. Namely, if \u03b5 1 is the probability of accepting an incorrect value, we want a probability of success of at least 1 \u2212 \u03b5 (e.g., 1 \u2212 1/n). However, we assume that the service provided by the workers is not free (as opposed to the SETI@home project). For each task that a worker is assigned, the master computer is charged with a work-unit. Furthermore, processors can be slow, and messages can get lost or arrive late; in order to introduce these assumptions in the model, we consider that there is a known probability d (which may depend on n) of M receiving the reply from a given worker on time. We also consider two types of known bounds on the number of malicious workers: we either consider a fixed bound f < n/2 on the maximum number of workers that may fail, or a probability p < 1/2 of any processor to be faulty (f and p may depend on n). Given the above model, and considering a single task (which returns a binary value) assigned to several workers, our goal is to have the master computer to accept the correct value of the task with probability of success at least 1 \u2212 \u03b5, and with the smallest possible amount of work (number of workers M assigned the task). (The problem and model are presented in detail in Section 2.) Observe that a trivial solution to the above problem when all messages are delivered on time (i.e., d = 1) and there are no more than f < n/2 malicious workers is to have M assign the task to 2f + 1 workers. This guarantees that the correct value is accepted (with probability 1). Note, however, that if f = \u0398(n), then the work is linear on n (which is not desired). Furthermore, if d < 1, there are less than 2f + 1 workers available to execute the task, or we consider a probabilistic model of failures (each processor is faulty with probability p < 1/2), then it is not so obvious how to fully guarantee that a correct value is accepted with high probability. In this work, we develop two non-trivial algorithms for this problem and we show that it is in fact possible to obtain high probability of success with low work (for example, in the above case of d = 1 and linear f , if \u03b5 = 1/n, processor M accepts the correct value with probability at least 1 \u2212 1/n and with work logarithmic on n instead of linear). Furthermore, we provide lower bound results on the work required to achieve high probability of success.", "To our knowledge, the work on voting closest to our model is that of Kumar and Malik [18] , since they define a reliability level that has to be achieved and try to minimize the cost of achieving it. However, they still assume that the deciding agent gets proposals from all the entities. More importantly, they assume that each entity has associated a cost versus reliability curve that defines the cost that has to be invested in that entity in order to have a given probability of the entity proposing the correct value. Then, under this model the strategies are able to tune the failure probability of each voter to optimize the total cost. In our model, the failure probability is given, the master gets to choose how many entities are asked to propose, and the cost is the number of entities chosen.", "Finally, there is an interesting connection between the problem considered in this work and the problems of reliably computing Boolean k-variable functions with noisy Boolean circuits (e.g., [22, 10] ), noisy Boolean decisions trees (e.g., [22, 16, 7, 6] ), and noisy broadcast (e.g., [11, 13, 19] ). Also, the fact that the master has to decide upfront the number of queries connects our model with the model of static noisy Boolean decision trees. In particular, our problem can be viewed as the problem of reliably computing the trivial function of one variable (F (x) = x) with a noisy static Boolean decision tree. However, we have identified several differences between our model and the models considered in the literature for these problems. For example, in their models, a query of a bit always returns an answer (0 or 1) as opposed to our model in which it is possible not to get a reply for a query (either a malicious worker chooses not to reply at all or a message is not received on time). Recent work [23] investigated the reliable computation of Boolean k-variable functions assuming that pfaulty copies of each input bit are received. However, it is assumed that is fixed as opposed to our model where the number of received replies is not fixed."], "relatedWork": [], "rq": [" this gives rise to a crucial problem: how can we prevent malicious clients from damaging the outcome of the overall computation?"]}
{"intro": [], "relatedWork": [], "rq": [" the problem is: how to estimate the session time of a node when it joins the system?"]}
{"intro": [], "relatedWork": [], "rq": [" the important questions are as follows: why did he or she explore a particular problem and not the others?"]}
{"intro": [], "relatedWork": [], "rq": [" the key research challenge drsim addresses is: how do we combine sparse data from different observations to create models that can be used for simulating community's energy consumption behavior and sensitivities to dr programs?"]}
{"intro": [], "relatedWork": [], "rq": [" yet the question is: is it true that the verb take has a number of distinct meanings?"]}
{"intro": [], "relatedWork": [], "rq": [" full details of how this algorithm can be parallelized are available in [5] how much parallelism is there in delaunay mesh generation?"]}
{"intro": ["F ACE explicitly provides the most direct and quickest way for evaluating implicit critical social information. For instance, face could convey a wide range of semantic information, such as race, 1 gender, age, expression, and identity, to support decision making process at different levels. Behavior research in psychology also shows that encountering a new individual, or facing a stimulus of human face normally activates three \"primitive\" conscious neural evaluations: race, gender, and age, which have consequential effects for the perceiver and perceived [2] , [3] , [4] , [5] , [6] , [7] , [8] , [9] (see Fig. 1 ). Among which, race is arguably the most prominent and dominant personal trait, which can be demonstrated empirically by its omnirelevance with a series of social cognitive and perceptual tasks (attitude, biased view, stereotype, emotion, belief, etc.). Furthermore, it yields deep insights into how to conceptualize culture and socialization in relation to individual appearance traits, including social categorization [10] , [11] , [12] , association [3] and communication [13] . Therefore, the estimation of racial variance by descriptive approaches for practical purposes is indeed indispensable in both social and computer science. However, while race demarcation drives the intrinsically genetic variation structure of essential facial regions to gather more explicit appearance information, the core question emerges as the computational mechanism underlying this extraordinary complexity. This raises the following fundamental multi-disciplinary conundrum: How does a computer model and categorize a racial face?", "To answer this fundamental question, numerous research consortium and scholars have developed intensive investigations from different angles. For example, psychologists have studied behavior correlations of race perception such as other-race-effect (ORE) and attention model (e.g., [2] , [8] , [10] , [14] , [15] ), which show existence of racially-discriminative facial features such as eye corners or nose tip (further anthropometric survey have confirmed those areas help to discriminate racial groups [16] , [17] , [18] , [19] , [20] , [21] ). Neurophysiologists have shown how race perception influences and regulates cognitive processes such as affection [22] , [23] , [24] , [25] and stereotype [26] , [27] . Computational neuroscientists have built models to simulate and explain 1 . In general English the term \"race\" and \"ethnicity\" are often used as though they were synonymous. However, they are related to biological and sociological factors respectively. Generally, race refers to a person's physical appearance or characteristics, while ethnicity is more viewed as a culture concept, relating to nationality, rituals and cultural heritages, or even ideology. For example, detecting an Eastern Asian from Caucasian crowd is a race recognition task, while visually differentiating a German and a French belongs to ethnic category and thus requires extra ethnographically discriminative cues including dress, manner, gait, dialect, among others. Since there are over 5,000 ethnic groups all over the world [1] , the idea of \"ethnicity recognition\" seems to be both questionable and impractical from current computer vision and pattern recognition point of view. Therefore, considering the soft biometric characteristics of distinctive human population, we prefer to use \"race\" as more suitable category terminology in this article. Nevertheless, for some of the existing papers in literature which have already used the term \"ethnicity\" but were indeed addressing \"race\" related issues, we have also cited and discussed those papers as well, in order to provide a comprehensive and complete survey on this topic. race perceptions (e.g., [3] , [28] , [29] , [30] , [31] ). Other cognitive experiments in [11] , [12] , [32] have also indicated the existence of racial features as a salient visual factor. Among the general public, the validity of racial categories is often taken for granted. Converging lines of investigations provide a sterling evidence of perceptual discrimination relationship (albeit a rather quantitative one) between the implicit racial category model and the explicit face representation. Following these quantitative analysis, computer vision scholars have been motivated to tackle the inherent problem of racial demarcation by building computationally intelligent systems capable of categorizing races.", "In spite of this, derived by burgeoning advances in applications such as security surveillance, human computer interaction, and bioinformatics, an increasing number of efforts have been reported toward race detection and categorization in the community. However, despite overwhelming theoretical principles, algorithms, applications and empirical results, there are relatively few crucial analysis that runs through these repertoire. Indeed, over the past few decades there have been several important work in face-based race perception available, such as [34] , [35] in psychology, and [36] , [37] in neuroscience. However, facing fast development of cutting-edge research and technology, reviewing efforts toward racial feature extraction and analysis cannot rely solely on these existing works due to following reasons: First, subjective cognitive experiments for testing human capability in race recognition cannot compare with automatic race recognition methods. Second, it is well known that automatic race recognition needs to be trained and generalized in a large-scale database, while subjects in traditional experiments can only face limited data displayed on screen. Third, emerging latest 3D scan technologies make 3D facial fiducial data far more convenient for computational recognition approaches rather than human perception, suggesting that existing survey papers may not be suitable for directing new computational research trends. Overall, the increasing contradiction between overwhelming social/scientific demographic data and scarce race mining methods calls for a comprehensive survey on computational race recognition to cover the state-of-the-art techniques and to give scholars a perspective of future advances.", "Aimed at developing a unified, data-driven framework of race classification, the goal of this survey is to provide a critical and comprehensive review of the major recent advances in this field. Since \"race\" is a rather loosely defined and ambiguous term linked with multidisciplinary research areas, such a survey work would unavoidably be involved with a combination of both sole computer vision based analysis and psychological-physiological behavior experimental observation results. Note that while the former is the back bone of this survey, the empirical analysis and experimental support, however, come from the latter. In contrast to most previously published surveys in related fields, we focus mainly on the approaches that are capable of handling both computer vision and cognitive behaviors analysis. It is also noteworthy to point out that for face related research areas, there already exist several representative reviews and surveys, such as face recognition [31] , [38] , [39] , [40] , expression recognition [2] , [38] , [41] , [42] , [43] , age estimation [44] , [45] , [46] , gender recognition [45] , [47] , and even survey of face databases [38] , [48] . However, to our best knowledge, there is no such comprehensive survey on race recognition, which also motivates us to present a review for completing the gap and enriching the research forefront."], "relatedWork": [], "rq": [" this raises the following fundamental multi-disciplinary conundrum: how does a computer model and categorize a racial face?", " 2) how to extract meaningful features from these face regions to train a corresponding classifier or build a model?"]}
{"intro": ["In the last decade, physical layer security (PLS) was drawing a lot of attentions as an alternative technique to achieve secure communication by exploiting the physical characteristics of wireless channels. This work was pioneered by Shannon in [2] and further extended by Wyner in [3] . To improve physical layer security of wireless transmissions, some recent works were proposed by exploiting cooperative diversity [4] . The achievable secrecy rates in cooperative relaying schemes using amplify-and-forward (AF) and decode-and-forward (DF) protocols have been investigated in [5] [6] [7] , respectively. The secrecy outage performance for DF relay beamforming scheme is considered in [8] . Almost all of the above works are under the assumption that the eavesdropper's channel state information (CSI) is known at the transmitter. Clearly this assumption is sometimes impractical, such as the case of passive eavesdropping, where the instantaneous CSI of the eavesdropper's channel is not available at the transmitter and only an estimation of the average channel gains can be performed. The authors of [9] have proposed a joint cooperative beamforming and jamming scheme to enhance the security of an AF cooperative relay network without the CSI of eavesdropper. The proposed relay selection scheme in [10] selects a trusted relay to assist the secondary transmitter and maximize the achievable secrecy rate for cooperative cognitive networks. Considering no knowledge of the eavesdropper channels is available at the transmitter, the authors in [11] investigate the secrecy outage performance of conventional relay selection scheme. Specially, [11] only focuses on the high SNR regime where all the relay nodes successfully decode the source transmission. Different from [11] , the other secrecy performance, that is, the intercept probability and secrecy diversity order, has been derived for several of AF and DF relay selection schemes without any knowledge of the eavesdropper channels in [12] . However, most of previous works focus on general cooperative communication scenario; thus, 2 International Journal of Distributed Sensor Networks the security transmission strategy and analysis framework should be modified a lot in cooperative WSNs. On the other hand, cooperative transmission or virtual MIMO has been explored to increase the energy efficiency and improve the reliability of cluster-based WSNs [13, 14] . To the best of our knowledge, there are very few works making use of cooperative transmission to enhance the wireless security against eavesdropping attack in WSNs. In addition, the above observations raise two questions: (1) can cooperative transmission improve the secrecy performance for cluster-based WSNs?"], "relatedWork": [], "rq": [" (1) can cooperative transmission improve the secrecy performance for cluster-based wsns?"]}
{"intro": ["Which brings us to the topic of this paper: What is the best way to translate path expressions into circuits? Lauer and Campbell have shown how to compile path expressions into Petri nets [7] , and Patil has shown how to implement Petri nets as circuits by using a PLA-like device called an asynchronous logic array [13] . Thus, an obvious method for compiling path expressions into circuits would be to first translate the path expression into a Petri net and then to implement the Petri net as a circuit using an asynchronous logic array. However, careful examination of Lauer and Campbell's scheme shows that a multiple path expression consisting of M paths each of length K can result in a Petri net with K M places. Thus, the naive approach will in general be infeasible if the number of individual paths in multiple path expression is large.", "Perhaps, the work that is closest to ours is due to Li and Lauer [10] who do indeed implement path expressions in VLSI. However, their circuits differ significantly-from ours; in particular, their circuits are synchronous, and synchronization with the external world (which is, of course, inherently asynchronous) is not considered. (This means that the entire circuit, not just the synchronization, must be described using path expressions.) Furthermore, their circuits use PLA's that result in an area complexity of O(N2). Rem [15] has investigated the use of a hierarchically structured path expression-like language for specifying CMOS circuits. Although he does show how certain specifications can be translated into circuits, he does not describe how to handle synchronization or give a general layout algorithm that produces area efficient circuits.", "In contrast, the circuits produced by the construction described in this paper have area proportional to N-log(N) where N is the total length of the multiple path expression under consideration. Furthermore, this bound holds regardless of the number of individual paths or the degree of synchronization between paths. As in [4] and [5] the basic idea is to generate circuits for which the underlying graph structure has a constant separator theorem [8] . For path expressions with a single path the techniques used by [4] and [5] can be adapted without great difficulty. For multiple paths with common event names, however, the construction is not straightforward, because of the potential need for synchronization at many different points on each individual path. Moreover, the actual circuits that we use must be much more complicated than the synchronous ones used in ( [4] , [5] ). Since events are inherently asynchronous in our model, all of our circuits must be self-timed and the use of special circuit design techniques is required to correctly capture the semantics of path expressions."], "relatedWork": [], "rq": ["which brings us to the topic of this paper: what is the best way to translate path expressions into circuits?"]}
{"intro": [], "relatedWork": [], "rq": ["q3: is there a performance difference for certain combinations of mark directions?"]}
{"intro": [], "relatedWork": [], "rq": [" i would like to go back to before you began to study: can you please tell your story about how you think you became interested in cs/it, which then led to your decision to study cs/it?"]}
{"intro": [], "relatedWork": [], "rq": [" now we discuss the first part of question 1: is the list in table 1 complete?"]}
{"intro": ["A group G GL n (2) is said to be linearly representable if it is linearly k-representable for some k 2. If GL n (2) and LS(f ) are replaced by S n and S(f ), respectively then we have the definition of representability by Boolean functions from Ref. [2] . The main problem we are interested in is which subgroups of GL n (2) are linearly representable. It is not difficult to see that every abstract group is isomorphic to some linearly representable group by Theorem 11 in [2] . However, what we are interested in is the following more concrete problem: Given n > 1, which subgroups of GL n (2) are linearly representable by Boolean functions in n variables? This is a generalization of the corresponding problem in the theory of the symmetry group S(f ). In fact we have shown in Theorem F that if S n is embedded in GL n (2) as a subgroup, then G S n is representable in S n by Boolean functions if and only if G is linearly representable in GL n (2) . Thus we may obtain the conclusions on representability of it by Boolean functions from the theorems on linear representability of the group G."], "relatedWork": [], "rq": [" which subgroups of gl n (2) are linearly representable by boolean functions in n variables?"]}
{"intro": ["The paper is structured as follows. In Section 2.1 I review some historical developments in corpus analysis tools from the dual perspectives of power and usability, extending the account previously presented in McEnery & Hardie (2012) , and arguing that the current \"fourth generation\" of concordancers (of which BNCweb and CQPweb are both representatives) have made the greatest progress to date to combine power and usability. However, as outlined in Section 2.2, BNCweb lacks flexibility in the sense that it is bound to a single corpus. The data model used by CQPweb, which achieves cross-corpus flexibility without losing the power and usability that characterise BNCweb, is described in Section 3.1, while some other features of CQPweb's architecture are outlined in Section 3.2; the system's capabilities are overviewed in Section 4. Finally, in Section 5 I attempt a cursory evaluation of CQPweb, in particular comparing what it achieves to the requirements for a future corpus web-interface laid out by Hoffmann & Evert (2006) , but also considering the system's present limitations -especially in terms of flexibilityand giving some indication of additional functionality planned for future versions."], "relatedWork": [], "rq": [" there are numerous practical hurdles to overcome: how is the format of non-expert users' input data to be validated?"]}
{"intro": ["Architecture patterns are an established tool for designing and documenting software architectures. They are proven approaches to architectural design. Numerous architecture patterns have been identified, based on extensive experience with real systems. Because the impact of patterns on software architecture is so critical, it is important to understand better how patterns are used in practice, in real systems. There are several things it is important to learn: How many architecture patterns are usually applied in a system? Is there an optimal number of patterns and what are the effects on the architecture? Which architecture patterns are most commonly used, especially in certain application domains? This can help people understand which patterns to learn and analyze. Architects that have experience in certain domains usually know the most appropriate patterns for that domain even if it is only implicitly. However it is important to make this knowledge explicit: to map the patterns to the domains they are most frequently used, and to reason about this mapping. How can patterns help us understand the impact of an architecture on the system's quality attributes? The consequences of individual patterns are in general documented but combinations of patterns make the impact more complex; which pairs are important? Grady Booch has collected architecture documentation of many software systems [5] . This forms a rich library of diverse systems, representing many different domains. We analyzed the architecture documentation of each of the systems in the library to find the architecture patterns that were apparent from the documentation. We confirmed that patterns are a dominant part of software architectures. We learned which patterns were most common in different domains. We also observed combinations of patterns used, and have begun to study the significance of pattern combinations."], "relatedWork": [], "rq": [" there are several things it is important to learn: how many architecture patterns are usually applied in a system?"]}
{"intro": [], "relatedWork": [], "rq": ["1. what kind of data can be used to answer such question?", " 2. what were the best observation and measurement?"]}
{"intro": [], "relatedWork": [], "rq": [" : are planar graphs without 4-cycles and 5-cycles adaptably 3-colourable (or even adaptably 3-choosable)?"]}
{"intro": ["Recent years certainly have seen several important research outcomes in the area of methodologies for collaborative data gathering. The main question to be answered is: can the potentially enormous quantity of participatory sensing data compensate the typically inferior quality of miniature sensors used by non-experts? Several research efforts have answered that quality-quantity question in the affirmative [25, 6, 10] . But in order to do this, one first and foremost needs to ensure that the potential high data quantity is achieved, typically in the context of a sensing campaign focused in area and/or time. Additionally, organising campaigns is also essential to the people-aspect of PS. Indeed, there is high demand by communities of all sorts -grassroots, institutionled, or research-based -to be able to translate local concerns into actual campaigns for tackling them. However, the knowledge on how to organise campaigns properly is absent from currently existing PS platforms. This is problematic for stakeholders, who are largely dependent on platform owners for insider knowledge on campaign management, but also for platform owners, who are not able to scale up their efforts in sharing this knowledge."], "relatedWork": ["More recently, several articles advocate the necessity of a reconfigurable and reusable framework for mobile sensing applications. Projects such as Epicollect [3] , ohmage [22] , and Sensr [12] propose flexible frameworks for constructing mobile data-gathering tools. However, these frameworks limit the type of data that can be collected (typically form-based), or focus only on the campaign definition phase. Moreover, these so-called reconfigurable PS systems do not ensure that the potential high data quantity is achieved, as participants are expected to contribute freely and without guidance.", "The idea of organising participatory sensing actions into campaigns is an obvious one and indeed has been mentioned as early as 2006 [11] . On the one hand, sensing campaigns are an answer to community concerns which are typically focused in time, place and/or other contextual factors. On the other hand, sensing campaigns are a means to ensure data density in absence of a large enough crowd contributing data. This, in turn, is essential for adequate assessment of the concern at hand. Some PS projects have focused on providing a limited form of campaign management. Examples include the use of location-based triggers [8] , model-based question targeting [14] , and encouraging participants to move towards desired locations [23] . All these approaches, however, lack the reconfigurable aspects of the aforementioned frameworks."], "rq": [" the main question to be answered is: can the potentially enormous quantity of participatory sensing data compensate the typically inferior quality of miniature sensors used by non-experts?"]}
{"intro": ["The plug-in entropy estimators rely upon techniques for density estimation as a key first step. The most popular density estimator is the simple image histogram. The drawbacks of a histogram are that it yields discontinuous density estimates and requires an optimal choice of the bin width. Too small a bin width leads to noisy sparse density estimates (variance), whereas too large a bin width introduces oversmoothing (bias). Parzen windows have been widely employed as a differentiable density estimator for several applications in computer vision, including image registration [32] . Here, the problem of choosing an optimal bin width translates to the optimal choice of a kernel width and the kernel function itself. The choice of the kernel function is somewhat arbitrary [29] , and furthermore, the implicit effect of the kernel choice on the structure of the image is an issue that has been widely ignored. 1 The kernel width parameter can be estimated by techniques such as maximum likelihood. Such methods, however, require complicated iterative optimizations and also a training and validation set. From an image registration standpoint, the joint density between the images undergoes a change in each iteration, which requires reestimation of the kernel width parameters. This step is an expensive iterative process with a complexity that is quadratic in the number of samples. Methods such as the fast Gauss transform [33] . 1. Parzen showed in [19] that sup jp\u00f0x\u00de \u00c0 p\u00f0x\u00dej ! 0, wherep and p refer to the estimated and true density, respectively. However, we stress that this is only an asymptotic result (as the number of samples N s ! 1) and, therefore, not directly linked to the nature of the image itself, for all practical purposes. reduce this cost to some extent, but they require a prior clustering step. However, the fast Gauss transform is only an approximation to the true Parzen density estimate, and hence, one needs to analyze the behavior of the approximation error over the iterations if a gradient-based optimizer is used. Also, as per [29, Section 3.3.2] , the ideal width value for minimizing the mean squared error between the true and estimated density is itself dependent upon the second derivative of the (unknown) true density. Yet another drawback of Parzen-window-based density estimators is the well-known \"tail effect\" in higher dimensions, due to which a large number of samples will fall in those regions where the Gaussian has a very low value [29] . Mixture models have been used for joint density estimation in registration [15] , but they are quite inefficient and require a choice of the kernel function for the components (usually chosen to be Gaussian) and the number of components. This number again will change across the iterations of the registration process, as the images move with respect to one another. Wavelet-based density estimators have also been recently employed in image registration [9] and in conjunction with mutual information (MI) [21] . The problems with a wavelet-based method for density estimation include a choice of wavelet function, as well as the selection of the optimal number of levels or coefficients, which again requires iterative optimization.", "Direct entropy estimators avoid the intermediate density estimation phase. While there exists a plethora of papers in this field (surveyed in [1] ), the most popular entropy estimator used in image registration is the approximation of the Renyi entropy as the weight of a minimal spanning tree [16] or a K-nearest neighbor graph [5] . Note that the entropy used here is the Renyi entropy as opposed to the more popular Shannon entropy. The drawbacks of this approach include the computational cost in construction of the data structure in each step of registration (the complexity whereof is quadratic in the number of samples drawn), the somewhat arbitrary choice of the parameter for the Renyi entropy, and the lack of differentiability of the cost function. Some work has been done recently, however, to introduce differentiability in the cost function [27] . A merit of these techniques is the ease of estimation of entropies of high-dimensional feature vectors, with the cost scaling up just linearly with the dimensionality of the feature space.", "None of these techniques take into account the ordering between the given pixels of an image. As a result, all of these methods can be termed sample based. Furthermore, most of the aforementioned density estimators require a particular kernel, the choice of which is extrinsic to the image being analyzed and not necessarily linked even to the noise model. In this paper, we present an entirely different approach in which the density estimate is built directly from a continuous image representation (as opposed to an arbitrary kernel on the density). Our approach here is based on the earlier work presented in [24] , the essence of which is to regard the marginal probability density as the area between two isocontours at infinitesimally close intensity values. A similar approach to density estimation has also been taken in the work of Kadir and Brady [13] . In our work, we have also presented a detailed derivation for the joint density between two or more images and also extended the work to the 3D case, besides testing it thoroughly on affine image registration for varying noise levels and quantization widths. Prior work on image registration using such image-based techniques includes [24] , [23] , [10] , and [14] . The work in [10] , however, reports results only on template matching with translations, whereas the main focus of [14] is on the estimation of densities in vanishingly small circular neighborhoods. The formulas derived are very specific to the shape of the neighborhood. Their paper [14] shows that local MI values in small neighborhoods are related to the values of the angles between the local gradient vectors in those neighborhoods. The focus of this method, however, is too local in nature, thereby ignoring the robustness that is an integral part of more global density estimates. There also exists some related work by Hadjidemetriou et al. [12] in the context of histogram-preserving locally continuous image transformations (the so-called Hamiltonian morphisms), which relates histograms to areas between isocontours. The main practical applications discussed in [12] are histograms under weak perspective and paraperspective projections of 3D textured models."], "relatedWork": [], "rq": ["1. how does our method compare to histogramming on an upsampled image?", " 2. how does one choose the optimal number of bins or the optimal interpolant for our method?", " 3. what are the limitations of our technique?"]}
{"intro": [], "relatedWork": ["The idea that content is determined by coherence relations is of course not new, and has been implemented for example in (Hobbs et al., 1993) , which also mentions in passing the problem of resolving fragments in context. However, this 'Interpretation as Abduction'-theory (IAT) differs from our approach in a number of important aspects. First, unlike IAT's weighted abduction where conflict among the clues to interpretation is handled by the extraneous logical machinery of weights, in our theory conflict is resolved automatically by the logical consequence relation itself. Secondly, Hobbs et al. don't consider the syntactic constraints on the resolution of fragments that we discussed above. In fact, they seem to regard fragments as 'syntactically-ill formed utterances', and so do not make a difference between well-formed and ill-formed fragments. In principle, further constraints could be added to the ITA framework, but at the cost of having to re-assign weights so that the results of inference are always as intended, and no princples or regulations are given in (Hobbs et al., 1993 ) about how to do this.", "In our opinion, our compositional approach has certain advantages. First, the grammatical analysis of fragments is uniform; contextual variation in their meaning is accounted for in the same way as it is for other anaphoric phenomena, via inferences underlying discourse update. This yields the second advantage: resolving fragments is fully integrated with resolving other kinds of underspecification (although we have not shown this here; cf. (Schlangen and Lascarides, 2002) ). Third, the interaction between grammar and pragmatics is straightforward: pragmatics enriches information coming from the grammar. In G&S's approach the grammar has to 'decide' on the speech act that has been performed (the grammar-rules are specific for eg. answering, clarification); something that is normally seen to be a defeasible process. Hence, even in G&S's approach a pragmatic module is required, which then has the task of filtering out unwanted parses. Fourth, we have available a strong theory of contextual interpretation which can explain the reasoning behind the resolution of examples like (2) (although we have not shown here in detail how); the functional application used by G&S seems too weak to do this. Fifth, our compositional approach allowed us to straightforwardly extend an existing wide-coverage grammar. This contrasts with the non-compositional approach which through its demands for making contextual information available entails that standard parsers cannot be used without modifications. Lastly, the separation of the grammar and resolution components means that in theory our grammar can be used with different resolution strategies; however, we have not systematically explored that."], "rq": ["61 are marked as syntactically well-formed and 387 as ill-formed. table 4 shows a comparison of the original erg with our extended version containing the fragment rules, with respect to the average number of parses per sentence. as these data show, the fragment rules introduce some new ambiguity, but on average less than one more parse per item. we conclude from this that adding the fragment-rules doesn\\'t lead to an explosion of readings that would render the grammar practically unusable. what this evaluation doesn\\'t tell us, however, is whether the additional readings (of what is meant to be full sentences) are erroneous or not. the problem is that \\'fragmenthood\\' is not a syntactic criterion, and so some strings that can be analysed as sentences can also be analysed as fragments. e.g., \"leave\" can be both an imperative sentence and a vp-fragment (e.g. in the context of the question \"what did john make sandy do?"]}
{"intro": [], "relatedWork": [], "rq": [" it can push the researcher to confront issues of belonging and expertise: to what extent can the researcher present himself or herself as a member of the design group being studied?"]}
{"intro": ["Although the naive Bayes algorithm makes some unrealistic probabilistic assumptions it has been found to work remarkably well in practice [4, 3] . Roth [10] develops a partial answer to this unexpected behavior using techniques from learning theory. It is shown that naive Bayes and other probabilistic classifiers are all \"Linear Statistical Query\" classifiers; thus, PAC type guarantees [12] can be given on the performance of the classifier on future, previously unseen data, as a function of its performance on the training data, independently of the probabilistic assumptions made when deriving the classifier. However, the key question that underlies the success of probabilistic classifiers is still open. That is, why is it even possible to get good performance on the training data, i.e., to \"fit the data\" 1 with a classifier that relies heavily on extremely simplified probabilistic assumptions on the data? This paper resolves this question and develops arguments that could explain the success of probabilistic classifiers and, in particular, that of naive Bayes. We start by quantifying the optimal Bayes error as a function of the entropy of the data. We develop upper and lower bounds on this term, and discuss where do most of the distributions lie relative to these bounds. While this gives some idea as to what can be expected in the best case, we would like to quantify what happens in realistic situations, when the probability distribution is not known. Quantifying the penalty incurred due to the independence assumptions allows us to show its direct relation to the distributional distance between the true (joint) and the product distribution over the marginals used to derive the classifier. This is used to derive the main result of the paper which, we believe, explains the practical success of product distribution based classifiers. Informally, we show that almost all joint distributions with a given set of marginals (that is, all distributions that could have given rise to the classifier learned) 2 are very close to the product distribution on the marginals -the number of these distributions goes down exponentially with their distance from the product distribution. Consequently, the error incurred when predicting using the product distribution is small for almost all joint distributions with the same marginals."], "relatedWork": [], "rq": ["2] can be given on the performance of the classifier on future, previously unseen data, as a function of its performance on the training data, independently of the probabilistic assumptions made when deriving the classifier. however, the key question that underlies the success of probabilistic classifiers is still open. that is, why is it even possible to get good performance on the training data, i.e., to \"fit the data\" 1 with a classifier that relies heavily on extremely simplified probabilistic assumptions on the data?"]}
{"intro": ["In view of the inescapability of Plato's Problem 2 , the minimal grounding point raised above has always been of particular significance: acquirers demonstrably go beyond the finite input to which they are exposed in a range of, for the most part, surprisingly consistent ways; similarly, the nature and content of individual exposure also varies greatly, once again seemingly mostly not to the detriment of the essential uniformity of adult grammars. During the Minimalist era, the rich UG assumption and, thus, its potential as a solution to Plato's Problem has, however, been drawn into question: the objective in this context is to populate UG with only the grammar-shaping content that cannot be ascribed to more general cognitive principles. More specifically, Chomsky (2005) proposes the so-called Three Factors Model, represented in (2): (2) UG + PLD + general cognitive factors \u00e0 Adult Grammar (= an I-language)", "To my mind, this Three Factors model has not received the serious and systematic attention that it deserves. In part, this follows from the vastness of the questions about its individual components -the Three Factors -on which there is currently very little, if any, real consensus. Consider, for example, the question of what a minimal UG should contain. Researchers who would today describe themselves as \"generative\"/\"Chomskyan\" range from those, on the one hand, who would identify only (feature-blind) Merge (the basic combinatorial operation which produces Recursion; cf. Hauser, Chomsky & Fitch 2002 and many subsequent researchers 3 ) to those, on the other, who assume richly specified cartographic or even nanosyntactic structures (see i.a. Shlonsky 2010 , Cinque 2013 , Rizzi & Cinque 2016 on the former, and i.a. Caha 2009 , Starke 2009 , and Baunaz, De Clercq, Haegeman & Lander 2018 on the latter). An informal survey of generative colleagues of all ages also suggests that a great many remain committed to the necessary correctness of Chomsky's (2001: 10) proposal that UG 'specifies the features F that are available to fix each particular language L'. This would, however, entail a much richer UG than the Merge-only entity assumed in Hauser, Chomsky & Fitch (2002) , and work following that line of thinking. To the extent that parameters are still assumed to be a useful way of thinking about (the limits on) crosslinguistic variation 4 both synchronically and diachronically, we also see significant unclarity regarding the nature and origins of minimalist parameters, with some researchers assuming a high number of innately specified choice-points (cf. i.a. Westergaard 2009 , and the work of Richie Kayne more generally), and others assuming these to be (in part) emergent in different ways (cf. i.a. Dresher 2009 Dresher , 2014 in the domain of phonology; Gianollo, Guardiano & Longobardi 2008; Longobardi 2017, and Longobardi 2018 for the proposal that specific parameters in fact reflect a limited number of innately specified parameter schema, and Rizzi 2014 Rizzi , 2015 for a proposal in the same spirit; see also i.a. Zeijlstra 2008; Biberauer 2018 et seq.; Roberts 2012 Roberts , 2019 Wiltschko 2014; Ramchand & Svenonius 2014; and Roberts 2015, 2017 on different types of specifically emergent parameters), and perhaps the majority leaving aside explicit consideration of this \"bigger picture\" question. In relation to third factors, the picture is more rather than less opaque; see Mobbs (2015) for overview discussion. Finally, systematic consideration of the form that the 'triggering' input takes has barely advanced beyond the by now long-standing recognition that 'PLD' cannot be taken to mean \"everything the child hears\". Thus discussions like Evers & van Kampen (2008) , Gagliardi (2012) , and Lidz & Gagliardi (2015) highlight the difference between 'input' and 'intake', 5 while Fodor & Sakas (2017) provide a useful overview of work to date on so-called 'triggering input'."], "relatedWork": [], "rq": ["3) what (all) did he (all) say (all) that he (all) bought (all)?"]}
{"intro": ["Studies show that seven of the top ten industries with the highest growing rates in the USA for 2017 are construction related (Sageworks2017), and the pace of expansion in the global construction industry is expected to continue growing through 2021 with an average of 2.8% 1(Construction Intelligence Center2017). But how to measure success in the construction industry? And is value to the end user a factor frequently considered? In fact, success has been previously closely tied to only three main parameters: cost, time & quality. Other criteria has been added such as safety, functionality and satisfaction including user expectation and satisfaction (Chan and Chan 2004) .However, many construction companies rarely implement new knowledge from recent research to assess different parameters related to the user's perspective in their work. Such companies ignore the fact that input from end-users is important as to learn from previous projects to continuously improve and apply this new knowledge to design future projects (Vischer 2009) . One way to improve current practices is to implement Post-Occupancy Evaluations (POE) in order to assess the operation requirements of existing buildings; generate new knowledge about the human use of space and give feedback on key decisions made during the design and construction phase. Since the 1960's universities have been a main part of the POE exercise, given the fact that each university has its own design and construction standards and that there is no general design standard for higher education as a whole (Tookaloo and Smith 2015) . Value for end users in higher education, is a building that creates optimal conditions for teaching, learning, and research (Spiten, Haddadi, St\u00f8re-Valen, & Lohne, 2016) .", "However, POEs have a few short comings. It might show a focus on short-term thinking to achieve direct financial profits rather than long-term benefits to clients and society. Other issues include liability and accountability issues where participants in a project fear that POE will only focus on the negative aspects, holding different parties (architect, contractor, structural engineer, etc.) accountable and responsible for defects. Finally, another important issue is the lack of policies and legislations that demand the use of POE regularly. Eventually, applying POE enables a wider perspective and encourages owners to investigate the needs of the users, and offer satisfying building design quality in return. Conducting research in different types of buildings (educational, residential, healthcare, etc.) enables a deep insight about the needs of the end users (Watson et al 2014) . When talking about user's satisfaction, Ornstein and Ono (2010) define various ways of obtaining information including interviews with key persons and POEs through questionnaires with scales of values to measure users' satisfaction levels regarding the respective environments."], "relatedWork": [], "rq": ["17 are construction related (sageworks2017), and the pace of expansion in the global construction industry is expected to continue growing through 2021 with an average of 2.8% 1(construction intelligence center2017). but how to measure success in the construction industry?"]}
{"intro": ["A key outcome of this work has been the observation that collaborative caching usually outperforms locally optimised algorithms [6] , [8] - [10] . This is primarily caused by the nature of ubiquitous ICN caching, where nearby caches will often wastefully store the same objects [9] . To remdy this, a simple collaborative algorithm between two nodes may involve sharing their resources and strategically caching distinct objects [5] , [11] . There is therefore a convergence towards the need for collaborative algorithms in any future ICN deployments. In tandem, we are witnessing a fragmentation of cache ownership in the live Internet, with large content providers deploying separate infrastructures (e.g. Google, Facebook and Akamai). A more extreme example of this fragmentation is within the expanding number of wireless community mesh networks; these are constructed by groups of individuals who each contribute wireless routers mounted on their property on a city-wide scale. In a community network, every router/cache would be operated by a separate individual. Hence, we predict that future ICNs will use caches that are provisioned not just by network operators, but also various distinct stakeholders at strategic in-network locations. These observations, however, have the potential to undermine the key tenets of caching in ICNs: What if caches operated by separate entities pursue different policies that do not include collaboration, the storage of competitor's content or the serving of specific users? As of yet, nearly every ICN collaborative algorithm proposed has assumed fully cooperative nodes [5] , [6] , [9] - [12] ."], "relatedWork": ["Game theory is an effective tool to analyse the effects of individual behaviours on a system. Recent work like [4] , [30] - [33] apply game theory to study in-network caching problem. The caching problem is modelled as non-cooperative, pure strategic games and the equilibrium is analysed. In all the formulations, this line of research shares a common utilitarian view and only concerns about the global optimum which is an indicator of the efficiency of a cache system. Fairness, however, is unfortunately overlooked by the prior work. Without coordination, selfishness, driving nodes to act alone to pursue self-interest, is studied in [4] , [30] , [32] regarding how it impacts the equilibrium and system efficiency (measured by Price of Anarchy). Moreover, these work also show that the global optimum is seldom achieved due to lack of coordination and nodes' inherent selfishness. Nonetheless, most prior work are limited by various over-simplifications for the purpose of theoretical analysis (e.g. single or a small set of objects, complete graph, global knowledge and etc. ), which brings scalability and applicability into question in a practical setting and algorithm implementations."], "rq": [" have the potential to undermine the key tenets of caching in icns: what if caches operated by separate entities pursue different policies that do not include collaboration, the storage of competitor's content or the serving of specific users?"]}
{"intro": ["In this paper, we will deploy a novel diagnostic for probing the logical makeup of how-and why-questions, involving the modification of wh-phrases with additive else (Romero 1998; Isac & Reiss 2004; Harris 2014; Schwarz in press ). For both how-and why-questions, the application of this diagnostic leads us to a finding that may at first seem unremarkable, viz. that depending on the question nucleus' content, Hamblin answers may or may not be pairwise incompatible. However, we will see that the particulars of the data pattern that emerges are surprising, leading one to wonder how to predict from the nucleus' content whether or not a question's Hamblin answers are pairwise incompatible, and, relatedly, how the newly discovered logical typology of how-and why-questions might relate to grammatical or notional typologies that have been proposed in previous literature (e.g., Higginbotham 1993; Oshima 2007; Saeb\u00f8 2016) ."], "relatedWork": [], "rq": [" then the felicity of (4) is after all not a threat to oshima's hypothesis that hamblin answers to how-questions are pairwise incompatible. this analysis of (4) as a response to (1) is furthermore amenable to a modification that still avoids abrus\u00e1n's objection and that almost achieves full compatibility with the particulars of oshima's proposal. saeb\u00f8 (2016) argues at length that a howquestion can be grammatically ambiguous between a question about manners, which are expressed by adverbials like slowly, and a question about methods, which are expressed by by-phrases like by pushing hard. yet a conjunction like slowly and by pushing hard is surely a felicitous response to a question like, say, how did she open the door?"]}
{"intro": ["In addition to the performance benefits, cache coherence protocol implementations in multiprocessor systems can also exploit synchronization information, leading to more scalable protocols. Indeed, with synchronization operations exposed, coherence need only be enforced lazily at synchronization boundaries via self-invalidation [6] , [7] , [8] , [9] . Using selfinvalidation, instead of relying on eager invalidations, is beneficial, as it no longer requires maintaining a sharing vector and associated data structures for maintaining the list of sharers. Although there have been numerous approaches to optimize the cache and directory organization to maintain the list of sharers more efficiently [10] , [11] , [12] , [13] , [14] , [15] , for coherence protocols that exploit synchronization information, the sharing vector can be completely eliminated. Motivation: Conveying data/synchronization information from the language level to the hardware level, however, requires a compatible hardware memory consistency model that also clearly distinguishes between data and synchronization operations. One such model, enabling an efficient mapping from the language to the hardware level, is Release Consistency (RC) [16] . In fact, a number of recent lazy coherence protocols [17] , [18] , [19] , [20] , [21] , [22] target variants of RC."], "relatedWork": ["RCtso has not been explicitly mentioned in the literature, although variants of the RC memory model have been formally defined in the literature [27] . In particular, the RCpc consistency model, also relaxes the release to acquire ordering; in contrast to RCtso, however, RCpc but does not require multi-copy atomicity among releases and acquires. While not explicitly referred to as RCtso, Intel Itanium implements what we consider RCtso [43] .", "Note also that recent Intel processors have introduced hardware transaction extensions (including XACQUIRE, XRELEASE) [45] . However, these are for a different purpose, namely lock elision [41] . Our problem is orthogonal, as we are interested in weakening the memory consistency model; in this instance we argue that since TSO reads and writes already have acquire and release semantics respectively, exposing relaxed memory operations is the right approach. It is worth noting that in the same way as hardware transaction extensions were introduced in a backwards compatible way, we propose reuse of a null prefix on memory operations to introduce more relaxed ordinary memory operations. Consistency directed coherence: Several recent works [17] , [18] , [19] , [20] , [21] , [22] target relaxed memory consistency models, typically RC or Weak Ordering [5] ; DeNovo [18] and DeNovoND [22] follow a programmer-centric approach (SC for DRF). These works introduce a number of optimizations for enhancing the performance of relaxed consistency protocols. Notably, optimizing higher-level synchronization primitives (locks, barriers, etc.) [18] , [22] , [46] would help improve latencies and reduce misses, as polling behaviour could be avoided. These optimizations, however, are orthogonal to our proposal, as we stuck with implementations of current operating system and standard library vendors. Unfortunately, none of these approaches can directly be applied to existing architectures with stricter models.", "Coherence for GPUs has become a recent topic of interest, to more efficiently support wider ranges of workloads. GPUs are typically programmed using higher level languages (e.g. OpenCL), and the vendor is responsible for a correct mapping to the hardware level. As such, the system-centric memory consistency model of GPUs has not been readily exposed. However, recent proposals for coherent memory systems on GPUs propose RC [48] , [49] .", "As referred to in previous sections, TSO-CC [24] is the most closely related protocol; however, we eliminate the per cache line timestamp requirement by relying on RCtso explicitly distinguishing synchronization and data operations. Data structures in eager protocols: Numerous works attack the cache coherence problem by optimizing the data structures and cache & directory organization to maintain coherence state -in particular the list of sharers more efficiently via: hierarchical directory organizations [13] , [15] ; sharing vector compression [12] , [50] ; variable size sharing vectors [14] ; or optimizing directory utilization [11] , [14] ."], "rq": [" 3) is far more sensitive, with an average increase of 125% compared to mesi. interestingly, the network traffic as well as l1 misses (fig. 4) are heavily correlated, yet often with much less noticeable effects on execution times, as the out-oforder cores can hide miss latencies well. introducing the optimizations of rc3 provides an average improvement over rc-base of 12% in terms of execution times, and 57% in terms of network traffic. rc3 reduces redundant acquires via the transitive reduction optimization, and most of the difference can be attributed to the consequent reduction of self-invalidations: compared to rc-base we note a reduction of self-invalidations by 800% on average. however, why does rc3 perform poorly in the first place with respect to selfinvalidations?"]}
{"intro": [], "relatedWork": [], "rq": [" (2) how is the .~;emanti-(:ally annotated library of t)i(:tures created'?", " and (3) what selection algorithm is employed to retrieve all optimally approt)riate illustration for a given part of the, kb frolll the library?", " 2. how is the library created?"]}
{"intro": [], "relatedWork": [], "rq": ["problem 12 is conjecture 4 equivalent to conjecture 2, or moreover to conjecture 1?"]}
{"intro": ["Machine learning methods have been applied to deal with various multi-media and computer vision tasks. Traditionally, linear models such as sparse representation (SR) [30] and collaborative representation (CR) [44] have drawn much attention and gained promising results in image classification. Lately, nonlinear deep learning [12] models, e.g., 1 https://github.com/zengsn/research ResNet [8] and VGG [24] , have produced state-of-the-art results in many image-based tasks, including face recognition, object detection, video tracking, etc. Linear sparse models can be utilized to improve deep neural networks [34] . On the other hand, more and more conventional methods took deep features as input to gain more promising classification results [2, 42, 43] . However, recent studies showed that deep features from neural networks are usually designed for SVM-like classifiers [13] . Using deep features as sole input in non-SVM classical models could be dubious. For this problem, we believe that the technique of linearly representing images can be applied to enhance nonlinear deep models. age classification. We name it DeepCWC for short. The contribution of this work includes: 1) proposing a new classifier to integrate features from linear and nonlinear models, 2) giving an analysis on how black-box deep features work in a sparse classification model, 3) conducting image classification experiments on different CNN models and convolutional layers inside them, to demonstrate the performance of DeepCWC in a consistent and comprehensive way. The proposed method produces promising results on face and object recognition. In particular, it ranks first in recognition (97.66%) on the Fashion-MNIST dataset."], "relatedWork": [], "rq": [" we first need to answer another question: what is the relationship between collaboration and sparsity?"]}
{"intro": [], "relatedWork": [], "rq": ["it is now time to turn to our main question: is the cfc testable?"]}
{"intro": [], "relatedWork": [], "rq": [" r[0] is the target of eq. 1, ''?"]}
{"intro": [], "relatedWork": ["Due to the numerous applications in computer vision and image processing, during the past two decades, subspace clustering has been extensively studied and many algorithms have been proposed to tackle this problem. According to their mechanisms of representing the subspaces, existing works can be roughly divided into four main categories: mixture of Gaussian, matrix factorization, algebraic, and spectral-type methods. The mixture of Gaussian based methods model the data points as independent samples drawn from a mixture of Gaussian distributions. So subspace clustering is converted to the model estimation problem and the estimation can be performed by using the Expectation Maximization (EM) algorithm. Representative methods are K-plane [3] and Q-flat [37] . The limitations are that they are sensitive to errors and the initialization due to the optimization mechanism. The matrix factorization based methods, e.g., [8] , [12] , tend to reveal the data segmentation based on the factorization of the given data matrix. They are sensitive to data noise and outliers. Generalized Principal Component Analysis (GPCA) [38] is a representative algebraic method for subspace clustering. It fits the data points with a polynomial. However, this is generally difficult due to the data noise and its cost is high especially for highdimensional data. Due to the simplicity and outstanding performance, the spectral-type methods attract more attention in recent years. We give a more detailed review of this type of methods as follows."], "rq": [" 2. is it possible to give a unified proof of the block diagonal property by using common properties of the objective f ?", " 3. how to design a soft block diagonal regularizer which encourages a matrix to be or close to be k-block diagonal?"]}
{"intro": ["Given n fixed destination points in the plane with integer coordinates (aj ,hi J. deter-mine\u00b7the optimum location (~;y) of a single source poinl r thal is quotient or the k,h root of preceding j)\u00b7s and the last j)1J is a. location of a plant, with the objective of minimizing the sum oi transportation C;::'S\u00b7'S from the plant to sources of raw materials and to market centers. Hence this problem for n points has also come to be known as the Generalized Weber problem. In the recognition version of this problem we ask if there exists (x;y) such that for given integer L, if IiC=l\u2022.n V(x aj)2+(y bj)'l S L? This problem is not even known to be in NP. Since on guessing a solution one then attempts to verify if .kJ \"1..'1 VC; S I. ?, in time polynomial in the number of bits needed to express certain rational numbers Ct\u2022... c~and L. However no such polynomial time algorithm is known [GGJ761, [Gr84] .", "Hence We are able to show that th,e Generalized Weber problem, is not soivable by radicals over Q for n>5. For .. the Line-restricted V/eber problem, where the optimum solution is constrained to lie \u00b700. a certain given line. a much stronger result holds. We show that the Line-restricted Weber problem. in general, is not solvable by radicals over Q. for n;::: 3. A similar result is also shown to apply to the 3-Dimension version of this problem, for n;:: 4. A proof of the impossibility of straight-edge and compass constructions for the Generalized Weber problem (but not the Linerestricted .-case) appears in [Me73] . however nothing was known about the non. expressibility of the solution by radicals."], "relatedWork": [], "rq": ["1j is a. location of a plant, with the objective of minimizing the sum oi transportation c;::\\'s\u00b7\\'s from the plant to sources of raw materials and to market centers. hence this problem for n points has also come to be known as the generalized weber problem. in the recognition version of this problem we ask if there exists (x;y) such that for given integer l, if iic=l\u2022.n v(x aj)2+(y bj)\\'l s l?"]}
{"intro": [], "relatedWork": [], "rq": ["0) can help here. 4. tell people the time, not how to build clocks. in \"built to last\" (collins & porras, 1994) , the authors suggested that building clocks to tell time is far more important than telling people in time. we found the opposite to be true-clients wanted interesting insights, and early on did not care how we found them. because many of the insights we discovered generalized well across multiple clients, it was easy to show a graph that depicts how online spending correlates with distance from their physical stores (the farther you live from the nearest retailer\\'s physical store, the more money you spend on the average purchase) than to explain how we found it. over time we started to develop standard reports that are available out-of-the-box. these reports include interesting findings and highlight insights that make a difference to the business. 5. define the terminology. our clients often ask questions such as: what is the difference between a visit and a session?"]}
{"intro": ["Space partition-based geolocation algorithm is less time consuming and easy to implement. As shown in a previous paper, 30 the algorithm is able to geolocate 50% of users in less than 40 m and the average geolocation accuracy is about 51 m. However, actual tests show that, affected by the location protection strategy update of WeChat, there is no stable correspondence between the reported distance and actual distance. The algorithm is difficult to geolocate target users with high precision under the current conditions. To analyze the correspondence between the reported distance and actual distance and, furthermore, improve the geolocation accuracy for WeChat users in actual environment, a geolocation algorithm based on the relation between the reported distance and actual distance is proposed."], "relatedWork": [], "rq": [" (1) how to select the reasonable target reported distance as the beginning of the geolocation?", " (2) how to delimit the initial target space that covers the location of the target user with high probability?", " (3) how to find the right subspace with high success rate?"]}
{"intro": [], "relatedWork": [], "rq": [" this pair of orders also provided a negative answer to the following second question of sands: is every finite order p uniquely determined-up to isomorphism-by the family (p \u2212 \u2191 ] p x) x\u2208v (p ) ?"]}
{"intro": ["The concept problem-solving method (PSM) is present in a large part of current knowledge-engineering frameworks (e.g. GENERIC TASKS [Chandrasekaran et al., 1992] ; ROLE-LIMITING METHODS , [Puppe, 1993] ; KADS and CommonKADS ; the METHOD-TO-TASK approach [Musen, 1992] ; COMPONENTS OF EXPERTISE [Steels, 1990] ; GDM [Terpstra et al., 1993] ). Libraries of PSM are described in [Benjamins, 1993] , [Breuker & Van de Velde, 1994] , [Chandrasekaran et al., 1992] , and [Puppe, 1993] . In general a PSM describes which reasoning steps and which types of knowledge are needed to perform a task. Such a description should be domain and implementation independent. Problem solving methods are used in a number of ways in knowledge engineering: as a guideline to acquire problem-solving knowledge from an expert, as a description of the main rationale of the reasoning process of the expert and the knowledge-based system, as a skeletal description of the design model of the knowledge-based system, and to enable flexible reasoning by selecting methods during problem solving. However, a question that has not been answered clearly is the relation between PSMs and efficiency of the problem-solving process. Most descriptions of PSM frameworks do point to PSMs as being somehow related to efficiency, however no framework makes this relation explicit. Others claim to have no concern for efficiency since their PSMs are only used to capture the expert's problem-solving behavior. But one must be aware that experts also have to solve the task given their real-life limitations. In fact a large part of expert-knowledge is concerned exactly with efficient reasoning given these limitations. The conceptualization of a domain from an expert differs from the conceptualization of a novice as the former reflects the learning process which yields to efficiency in problem solving. According to us the main point of a PSM is: providing the desired functionality in an efficient fashion . In general, most problems tackled with knowledge-based systems are inherently complex and intractable, i.e., their time complexity is NP-hard (see e.g. [Bylander, 1991] , [Bylander et al., 1991] , and [Nebel, 1995] ). 2 A PSM has to describe not just a realization of the functionality, but one which takes into account the In N. Shadbolt et al. (eds.) , Advances in Knowledge Acquisition, Lecture Notes in Artificial Intelligence (LNAI), no 1076, Springer-Verlag, 1996. constraints of the reasoning process and the complexity of the task. The constraints have to do with the fact that we do not want to achieve the functionality in theory but rather in practice . When this relation between PSMs and efficiency is ignored or kept as an implicit notion, both the selection and design of PSMs cannot be performed in an informed manner. Besides the efficiency in terms of computational effort of a PSM, there are further aspects which can influence design decisions of appropriate PSM: The efficiency of the entire knowledge-based system, the optimality of the combined problem solver user and system (e.g. minimizing the number of tests a patient has to suffer from in medical diagnosis), and efficiency of the development process of the knowledge-based system. 3 After stating the claim that PSMs provide functionality in an efficient way, the next question then is: how could this be achieved if the problems are intractable in their general form? In our view, the way problem solving methods achieve efficient realization of functionality is by making assumptions . The assumptions put restrictions on the context of the PSM, such as the domain knowledge and the possible inputs of the method or the precise definition of the functionality (i.e., the goal which can be achieved by applying the PSM). These restrictions enable reasoning to be performed in an efficient manner. The role that assumptions play in the efficient realization of functionality suggests that the process of designing PSMs must be based on these assumptions. [Akkermans et al., 1993] and [Wielinga et al., 1995] introduce a general approach that views the construction process of PSMs for knowledge-based systems as an assumption-driven activity. A formal specification of a task is derived from informal requirements by introducing assumptions about the problem and the problem space. This task specification is refined into a functional specification of the PSM by making assumptions about the problem-solving paradigm and the available domain theory. Further assumptions are introduced in the process of defining an operational specification of the method. A task is decomposed into declaratively described subtasks and the data and control flow between these subtasks are defined. We will use this approach as a general framework and try to make it more concrete. Our focus lies thereby on assumptions which are related to efficiency of a PSM. We propose to view the process of constructing a PSM for a given function as the process of incrementally adding assumptions that enable efficient reasoning. Summarizing, we want to make the following claims in this paper:"], "relatedWork": [], "rq": [" the next question then is: how could this be achieved if the problems are intractable in their general form?"]}
{"intro": [], "relatedWork": [], "rq": ["-dcigroup and z 16 is not a 6-ci-group. a natural question is, for a positive integer m, which cyclic groups are m-dci-groups, and which cyclic groups are m-ci-groups?"]}
{"intro": ["One of the objectives of Human-Machine Interaction (HMI) is to provide systems that users can use in an intuitive way. In the context of qualitative spatial reasoning, the notion of proximity is one of the foundational concepts in daily human cognition studied by researchers during the last decades. Several authors proposed tools to reason about proximity and solutions which can be automated and integrated in geographic information systems (GIS). The goal is to reduce the semantic gap between quantitative data in GIS (metric distance) and qualitative data (proximity) as used by humans (Cohn and Renz, 2007) . Such works used advanced qualitative techniques such as fuzzy sets and fuzzy logic as well as conceptual notions such as influence and impact areas. Empirical experiments were also conducted. However, spatial distance, on which most of these works based their solutions, is not the only factor that influences human reasoning about spatial proximity. Actually, proximity relations have two characteristics: they are context dependent and uncertain. For example, the means of transportation used to travel from Paris to London may change the traveler's perception of distance (context-dependence). When a person parks a car, she does not need to know the exact distance of the empty space between two cars (uncertainty). A suitable model of spatial proximity should consider both characteristics in order to be closer to the human apprehension of proximity."], "relatedWork": ["Reasoning with spatial proximity is a research area which has been addressed by the qualitative spatial reasoning community, adopting different perspectives such as geography, cognitive science, linguistics and others (Yao and Thill, 2007) . A large number of prior works used fuzzy logic and qualitative techniques to deal with spatial proximity because it has inherent fuzziness (Robinson, 1990) . While reasoning with proximity, human beings may also consider metric distances and other parameters called contextual information. In the following subsections, we present an overview of some of the works which used uncertainty techniques and contextual information or a combination of both aspects. Guesgen (2002) used fuzzy sets and associated each set with a qualitative spatial relation. The idea behind Guesgen's approach is to interpret qualitative proximity relations between spatial objects as restrictions of spatial linguistic variables such as near and far. Each linguistic variable is associated with a fuzzy set. The proximity relation is therefore represented by a membership degree of each of these fuzzy sets using a membership function. For example, the relation \"the object A is near to\" may be interpreted by a near membership value which is associated with each near relation between object A and surrounding objects such as B, C and D. Generally, the nearest an object is to A, the highest its membership value is. A software implementation used Euclidian distance between two objects to implement Guesgen's formalism. A Java-based implementation allows a user to define a \"nearness\" factor which will be used to specify other proximity relations. Then, the user can qualify the distance between two spatial objects using proximity relations. (Brennan and Martin, 2006) stated that most of fuzzy based proximity formalisms proposed in the literature suffer from a major shortcoming: membership functions are not clearly defined. To overcome this limitation, they used Gahegan's approach (Gahegan, 1995) who proposed a method to identify spatial proximity using three factors: the absolute distance, the relative distance between two spatial objects and the combination of both. An absolute distance may be a spatial relation such as very close, close and far. Relative distance may be a spatial relation such as closest or farthest. The combination of both absolute and relative distances was defined by Gahegan to reason about spatial relations using a fuzzy union operator. Since Gahegan did not use experimental data to validate his approach, Brennan and Martin (2006) proposed an approach to evaluate membership functions and showed how they can be combined using fuzzy logic operators. To this end, they used the absolute distance membership function \u07e4 \u0be6 \u123a\u202b,\u0723\u202c \u202b\u0724\u202c\u123b proposed by Gahegan. This function is presented in Table 1 where A and B are spatial objects, Dist(A,B) is the absolute distance between A and B . Max is the maximum distance between all the places in the data set and it is used to normalize the value of Dist (A,B) . For relative distance membership function, they used the function \u07e4 \u123a\u202b,\u0723\u202c \u202b\u0724\u202c\u123b proposed in (Worboys, 1996) and given in Table 1 where reldis(A,B) is relative distance between A and B which is calculated using the distance between A and B divided by the mean of distance between A and each object in the data set. The result given by the Brennan and Martin's experiments demonstrated that the absolute distance and relative distance membership function can be used separately and generate linear distributions. However, combination of both metrics by union gives clustered distributions and may not be relevant for proximity reasoning whereas fuzzy intersection gives better results. The authors proposed to use this option and implemented it in a GIS. However, they neither expressed the meaning of fuzzy intersection in terms of spatial proximity,, nor did they justify the use of fuzzy logic in general. Moreover, how to apply such an approach to qualitative spatial reasoning was not obvious. (Brennan and Martin, 2006) Later, (Brennan and Martin, 2012) proposed a conceptual framework to qualitatively represent spatial proximity and to enhance the capacity of spatial reasoning systems using contextual information. They consider contextual information as a key element in any model of spatial proximity. For example, a degree of proximity to an object may vary if the object is meant to be seen or reached. To reason about spatial proximity Brennan and Martin introduced the notion of impact area which is a generalization of the influence area introduced by (Kettani and Moulin, 1999 ). An influence area is a portion of space surrounding an object: it has an interior and exterior border such that the borders of the influence area and the border of the object have the same shape (Kettani and Moulin, 1999) . Euclidean geometry has been used by Kettani and Moulin to calculate the width of an influence area. Brennan and Martin proposed a more generic approach motivated by the fact that spatial proximity is not just a metric measure. Proximity is rather context dependent. Furthermore, other spatial relations such as topological and directional relations have some unified views within the research community. Hence, they introduced the notion of impact area which involves contextual information to qualify spatial proximity. The impact area of an object takes into account both the nature of the object and its surrounding environment. Some examples in (Brennan and Martin, 2012) demonstrate how impact area is more generic than influence area, and how this notion uses contextual information in proximity analysis. Contextual information is defined as any \"information collated by an expert who is expected to incorporate all relevant factors into the impact area\". Figure 1 illustrates the difference between influence area and impact area for two couples of objects. Objects A1 and B1 are water tanks and objects A2 and B2 are radio towers. The distance between A1 and B1 is equal to the distance between A2 and B2. The influence area of water tank is equal to the influence area of radio towers because all objects have the same shape and size and because they are located at the same distance from each another. If we consider the functionality of the towers (range of frequency) and the surrounding Cliff, the impact areas will be different. Figure 1 . An example of difference between influence areas (Kettani and Moulin, 1999) and impact area (Brennan and Martin, 2012) The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, Volume XL-2, 2014"], "rq": ["agent tries to answer to the following question: what is the proximity relation between object a and object b?"]}
{"intro": [], "relatedWork": [], "rq": ["2. what counts as \"empirical\" in cognitive linguistics?", "2. what counts as \"empirical\" in cognitive linguistics?"]}
{"intro": [], "relatedWork": [], "rq": [" this is the challenge we address: can we offer the power, expressive creative freedom and the ability to support data dynamics without requiring people to learn to code?"]}
{"intro": ["An extensive survey was conducted by Lih Lih [2013] , where many of the results on EQUITABLE COL-ORING of the last 50 years were assembled. Most of them, however, are upper bounds on the equitable chromatic number. Such bounds are known for: bipartite graphs, trees, split graphs, planar graphs, outerplanar graphs, low degeneracy graphs, Kneser graphs, interval graphs, random graphs and some forms of graph products."], "relatedWork": [], "rq": [" question: can g be partitioned in k cliques of size r and n\u2212rk r\u22121 cliques of size r \u2212 1?"]}
{"intro": [". We develop a representation [1] for scenes containing relocatable occluders. Specifically, the scene is a composition of depth-ordered layers of graphical models. These models can be composed on-the-fly to form a layered global scene model. . We propose a solution to a specific problem that makes use of this new representation: tracking of pedestrians in a parking lot crowded with parked vehicles. We also note what the paper is not about. This paper is not about a new appearance descriptor for person-tracking. In fact, in our example application, due to the very small apparent size of people and severity of occlusions, we employ binary images generated by background subtraction. This paper is not about learning a static occlusion map. Methods for learning such maps [40] , [63] , [16] are complementary to our approach. This paper is not about free-space tracking, for which off-the-shelf algorithms of [49] , [47] , [12] can be applied. This paper is not about highlevel activity recognition; the output of the algorithm is a sequence of estimates of vehicle and pedestrian locations. However, the mapping of pedestrian estimates from the image plane to locations around parked vehicles would provide valuable cues to an activity-recognition system."], "relatedWork": [], "rq": [" the key advantage of the rj mcmc formulation in [1] is that it models uncertainty in the positions of targets; this uncertainty is represented as samples in the markov chain at each time step. however, we have found that, in practice, the specifics of the rj mcmc death move and severe occlusions tend to yield a set of samples that is too diffuse to provide a definitive answer to the question: where is each person in the scene?"]}
{"intro": ["These cognitive challenges have been addressed by current research. In the psychology research literature, the cognitive capabilities of individuals and the role of the physical environment thereupon are discussed extensively [cf. 27, 28, 31] . Research on memory aid systems highlights suitable technical support to recall information [cf. [11, 15, 16, 21] . However these current research discussions address mostly the individual level or single-user scenarios, and insights on the cognitive aspects in mobile collaborations and how the individuals' cognitive capabilities influence their collaborative behavior are scarce. To provide appropriate support, it is important to understand the role of the OOS-OOM phenomenon in mobile collaborations. This brings us to our first research question: RQ1: How does forgetting influence mobile counseling sessions?", "The cognitive aspects in collaborations are broadly discussed in research on computer supported collaborative work (CSCW). Researchers provide high-level insights on the concept of collaborative memory [2, 24] and extensively discuss how to support collaboration partners to create and use a collaborative memory [1, 2, 19] . However, current research focuses mostly on stationary scenarios, and insights on the novel challenges and their influence on group members' cognitive capabilities and the creation of a collabora-tive memory are lacking. Current research on mobile memory aid systems addresses the individual's cognitive capability, and presents diverse solutions to facilitate memory cue creation [15, 16, 21] . However, research insights on suitable support accounting for multiple users are scarce. Both the collaborative situation of the clientconsultant relationship and the mobile setting give rise to new challenges for developing suitable support systems. We thus asked ourselves how an appropriately designed \"memory aid\" could support actors to create and use a collaborative memory in their mobile collaboration. After reviewing related literature on psychology research, CSCWresearch and memory aid systems, as well as discussing different design options, it was \"working with pictures\" that turned out to be a surprisingly useful concept for responding to the characteristics of mobile collaborations and to bring information back into the minds of the actors. Thus, in this study we further pursue the second research question"], "relatedWork": ["The actors' cognitive processes not only comprise the individual's internal mental processes, but also their interactions with others as well as with the physical artifacts in the surrounding environment. In this context, the changing environmental context can influence individual's cognitive capabilities considerably. In psychology research literature, researchers discuss extensively the individual's cognitive processes and the role and effects of the environmental context on an individual's cognitive capabilities [cf. 3, 8, 10, 27, 28, 31] . Studies report on different effects (e.g., environmental reinstatement [28] , where changing the physical environment causes individuals to remember fewer memories) and propose measures to reduce the resulting context-dependent forgetting (e.g., multiple-learningcontext-technique [27] , where individuals learn information in different environmental contexts rather than one). Whereas these studies provide useful insights on how to enhance a human's cognitive capabilities, they mainly discuss them on the individual level, when reporting on several actors and how they influence each other's memories, discussions remain mostly conceptual. These interactions between actors can enhance individual's cognitive capability, e.g., by either cross-cueing [9] , where information from others trigger memories of an individual that s/he would not have remembered alone, or impair it, e.g., by the \"outshining\"-effect [28] , where an individual's memories are \"outshone\" by easier-to-access information. However, current research studies barely provide insights how corresponding effects influence actors' behaviors in collaborations.", "Current research on counseling support systems investigates consultants' and clients' collaborative work practices from different perspectives. Novak [17] describes the hampering effects of information overload in shared problem solving, where actors' ability to make decisions deteriorates due to the presence of too much information. Novak and Schwabe [18] highlight the impact of sticky information that is bound to a specific location (physically, mentally) aggravating the exchange of information. Nussbaumer et al. [20] discuss the information asymmetry between consultants (as experts) and clients (as laypersons), and report on the negative effects, thwarting collaborative interactions. Schmidt-Rauch and Nussbaumer [25] give insights into actors' collaborative task of co-creating the value of counseling and show how to design appropriate support to help them becoming more equal co-creators. In their solutions, these researchers give insights into collaborative work practices in counseling collaborations, showing how technical support systems should be designed to support users appropriately. However, they rarely consider the individuals' cognitive aspects in storing and recalling information, and do not discuss how they influence the individual's collaborative behavior. Furthermore, they focus mostly on stationary scenarios, disregarding the actors' physical environment and its role and effects within the consultant-client collaboration."], "rq": [" rq1: how does forgetting influence mobile counseling sessions?"]}
{"intro": [], "relatedWork": [], "rq": [" software engineering philosophy must thus ask the question: what is the characteristic feature of a software engineering 'principle', and are we really confronted with (genuine) principles when practical software engineers speak about such?"]}
{"intro": ["In this paper, we use the analysis of scrip systems from [18] to understand how robust scrip systems are, and how to optimize their performance. We compute the money supply that maximizes social welfare, given the number of agents. As we show, the behavior mimics the behavior in the babysitting coop example. Specifically, if we start with a system with relatively little money (where \"relatively little\" is measured in terms of the average amount of money per agent), adding more money decreases the number of agents with no money, and thus increasing social welfare. (Since it is more likely that an agent will be able to pay for someone to work when he wants a job done.) However, this only works up to a point. Once a critical amount of money is reached, the system experiences a monetary crash: just as in the babysitting coop example, there is so much money that, in equilibrium, everyone will feel rich and no agents are willing to work. We show that, to get optimal performance, we want the total amount of money in the system to be as close as possible to the critical amount, but not to go over it. If the amount of money in the system is over the critical amount, we get the worst possible equilibrium, where no agent ever volunteers, and all agents have utility 0. This means that, for a system designers' point of view, there is a significant tradeoff between efficiency and robustness."], "relatedWork": [], "rq": [" we consider a fundamental question faced by system designers: what is the optimal amount of money and how does it depend on the size of the system?"]}
{"intro": [], "relatedWork": [], "rq": [" [3] is referenced in this paper. however, these languages mainly support modeling of the business processes themselves, but say nothing or very little about the business process environment and semantics. they don't answer -why this process is used, how it is linked to other processes and enterprise demands, how we can control and improve it. certainly, some of these languages can be used as a framework for modeling the above mentioned higher-level concepts in an indirect way. the question is -what are general concepts to which we should pay attention to describe all aspects of an enterprise, and which is the best way?"]}
{"intro": [], "relatedWork": [], "rq": ["7. is it true that q(f) ::s; 2w(f) for any family f of double intervals?"]}
{"intro": ["From the beginning, the fields of human-computer interaction and computer-supported cooperative work have embraced the idea that human and machines, i.e., a computer, can engage in a productive relationship. Licklider's vision of a 'man-machine symbiosis' [41] and Engelbart's 'augmenting human intellect' proposal [14] have influenced how scholars have conceptualized this partnership profoundly. These ideas have been re-evaluated more recently because of the technological advancements. Regarding the concept of 'human-computer integration' [15] , for example, the authors describe an interaction and integration continuum on which software can travel. Interactive intelligent systems are AI 1 -supported systems, with which people interact when selecting songs, reading news or searching for products. Such systems should be examined by considering both the human and the system. Jameson and Riedl call this a 'binocular view' of interactive intelligent systems because the system's design includes algorithm design with interaction design, on the one hand, and a combined evaluation of a system's performance and human behavior, on the other hand [33] . However, existing research provides little guidance on how we should design interactive intelligent systems. Should a task be carried out by a human or a computer? What is an appropriate level of interaction vs. integration? Is human labor more preferable than automated action? How can we make an informed decision about allocating the task to either one or the other? How can we evaluate our decision? A proposal of a method for elaborating this spectrum is still missing.", "The authors have experienced this issue in the context of information extraction from ideas in the research area of collaborative ideation. In addition to complete manual approaches for making sense of an idea's content [4] , algorithmic approaches were proposed which describe the content of the ideas statistically (e.g., [6] ). However, both perspectives (manual vs. automatic) have their limitations; thus, the question is whether a 'sweet spot' that emphasizes the advantages of both perspectives exists and how such a 'sweet spot' can be identified. A 'sweet spot' defines a compromise between the often contradictory evaluative criteria of an interactive vs. an intelligent system. It is a carefully balanced configuration of both parts, the intelligent and the interactive, by considering the objectives of the system's stakeholders and the context of use. The design of an interactive intelligent system might focus only on the learning part of the system, for example in the area of reinforcement learning [1] . However, without considering the human perspective, that design might diminish the human's trust in that system and, therefore, the applicability of the system. In the context of our use case of collaborative ideation, the 'sweet spot' defines the compromise between minimizing human effort in the annotation task (human perspective) and maximizing the necessary quality of the idea annotation (system perspective). A 'sweet spot' represents one configuration of the interactive intelligent system, i.e. a specific task allocation between both partners. Based on research from the area of human factors, we propose a method that allows for the configuration of an interactive intelligent system by defining and evaluating different levels of automation (LoA). Each configuration represents a certain LoA which involves a carefully designed technical realization (in terms of involved computation) and a specific amount of human involvement necessary."], "relatedWork": ["From the beginning, the development of software has been driven by the question: 'What can be automated?' [2] . The goal was often to automate as many functions as technically possible [11] . However, instead of looking at these systems from a machine-centered perspective, we should consider a human-centered perspective that changes the underlying question from 'What can be automated?' to 'What should be automated?' [57] . We do not want to argue the ethical or philosophical part of this question, but rather ask for a methodical perspective on designing an interactive intelligent system that helps to answer this question explicitly. Even though Jameson and Riedl propose a 'binocular view' of interactive intelligent systems that integrates algorithm design with interaction design and evaluates the system and the human integratively [33] , existing research provides little guidance on how such design and evaluation can be carried out in this context. We found this reoccurring question of automation, i.e., allocating tasks either to humans or to machines, in the area of robotics, ergonomics and human factors in our literature review. Following Parasuraman et al. [49] , 'automation refers to the full or partial replacement of a function previously carried out by the human operator'. One of the first attempts to describe task allocation originates in Fitts's MABA-MABA ('Men are better at; Machines are better at') lists [16] . With the latter, Fitt aimed to support design decisions regarding whether a system's function should be carried out by either a machine or a human. The lists define a fixed set of skills and abilities that can be clearly attributed to either a human or a machine. However, these lists motivated one to compare human and machine capabilities instead of creating hybrid task allocations where human and machine activities intertwine; system engineers were thinking of replacing one with the other (mostly humans by computers). These led to a number of taxonomies -so-called LoA -with the first proposed by Sheridan and Verplank [52] . They differentiate ten possible LoA in human-computer decision-making processes. The lowest levels describe a human who carries out the whole process up to the point when the computer takes over for realizing the decision. On the highest level, a computer carries out the task completely autonomously. In between, they define a continuum which details the different options of possible task allocation precisely. The research community on automation has reflected on this seminal work intensively (e.g., [13] , [37] ) and this finally led to a taxonomy proposed by Parasuraman and colleagues [48] . This taxonomy (cf. Figure 1 ) was accompanied by a framework for examining automation design issues for specific systems. It provides principled guidelines for automation design by defining a series of steps that are performed iteratively. The framework suggests four classes of functions: Information acquisition, information analysis, decision and action selection, and action implementation. Designers should evaluate possible LoA within each of these functional domains by studying the consequences of its associated human performance. Parasuraman and colleagues suggest a non-exhaustive set of evaluative criteria, such as reliability, operating cost and safety. Based on the evaluation, a system designer can recommend an appropriate upper and lower boundary on the level of automation that defines the maximal and minimal level required.", "Dekker and Woods [11] , however, argue that these lists of automation levels convey the impression that such systems can be designed just 'abracadabra.' A major issue is the underlying assumption that technology can replace humans. The goal of automation is conceptualized as a 'substitution problem': A fixed workflow in which selected tasks are replaced (by automation), which leads to, amongst others, less labor, fewer errors and higher accuracy. Johnson and colleagues [34] suggest that the design of the 'automated algorithm' process should be different for all joint human-computer activities which are not clearly separable functions. Thus, designers should support the interdependence of joint human-computer activities.", "However, except for the framework of Parasuraman and colleagues [48] , all approaches mentioned provide no clear guidance for designing interactive intelligent systems. Thus, the LoA framework finally informed our proposal of a method for designing and evaluating the human-machine configurations which we introduce in the next section."], "rq": [" the underlying question is: how can we evaluate the effectiveness of the different configurations?"]}
{"intro": [], "relatedWork": [], "rq": [" the question this paper hopes to answer is: is our quantum neural network robust to noise and to decoherence?"]}
{"intro": ["In this paper, we consider a classical problem: how to approximate a finite set D in R m for relatively large m by a finite subset of regular low-dimensional objects in R m . In applications this problems arises in many areas: from data visualization (e.g., visualization of the differences between human genomes) to fluid dynamics. A typical data approximation task starts with the following question: whether the dataset D is situated near a low-dimensional affine manifold (plane) in R m ? If we look for a point, straight line, plane, ... that minimizes the average squared distance to the datapoints, we immediately come to the Principal Component Analysis (PCA) which is one of the most seminal inventions in data analysis (Jolliffe, 2002) . The nonlinear generalization of PCA remains a challenging task.One of the earliest attempt suggested in this direction was the Self-Organizing Maps (SOM) (Kohonen, 2001 ) with its multiple generalizations and implementations such as Growing SOM (GSOM) (Alahakoon et al, 2000) . However, unlike classical PCA and k-means, the SOM algorithm is not based on optimization of any explicit functional (Erwin et al, 1992) .", "However, the main drawback of all described methods of approximation is sensitivity to outliers and noise, which is caused by the very nature of Euclidean distance (or quadratic variance): data points distant to the approximator have very large relative contributions. There exist several widely used ideas for increasing an approximator's robustness in the presence of strong noise in data such as substituting the Euclidean norm by the L 1 norm (e.g. Ding et al (2006) ; Hauberg et al (2014) ) and outliers exclusion or fixed weighting or iterative reweighting during the construction of data approximators (e.g. Xu and Yuille (1995) ; Allende et al (2004) ; Kohonen (2001) ). In some works, it was suggested to utilize \"trimming\" averages, e.g. in the context of the k-means clustering or some generalizations of PCA Cuesta-Albertos et al (1997); Hauberg et al (2014) )."], "relatedWork": [], "rq": [" we consider a classical problem: how to approximate a finite set d in r m for relatively large m by a finite subset of regular low-dimensional objects in r m . in applications this problems arises in many areas: from data visualization (e.g., visualization of the differences between human genomes) to fluid dynamics. a typical data approximation task starts with the following question: whether the dataset d is situated near a low-dimensional affine manifold (plane) in r m ?"]}
{"intro": [], "relatedWork": [], "rq": [" for example: what are the minimum necessary and sufficient requirements, such that a dynamic system may exhibit chaotic behavior?"]}
{"intro": [], "relatedWork": [], "rq": [" she still has no idea about the price because she does not know the type of the variable: is it british pounds, us dollars, euros\u2026?"]}
{"intro": ["As an alternative to overcome these di culties, xed-degree graphs such as the cube-connected cycles (CCC) 3] and the star-connected cycles (SCC) 4] have been proposed. An n-CCC graph is formed by substituting each node of an n-cube with a ring of n or more nodes. Accordingly, an n-SCC graph is formed by substituting each node of an n-star with a ring of (n ? 1) nodes. Fixed-degree interconnection networks may however present a longer diameter, which a ects the performance of the system if proper communication techniques and well-devised algorithms presenting locality of operation are not used."], "relatedWork": [], "rq": [" steps 9] is used for the ccc graph. we assume that each ring in the ccc graph contains n nodes, resulting in a graph of size n2 n . in addition, the diameter of an n-ccc graph is given by 2n + bn=2c ?"]}
{"intro": [], "relatedWork": [], "rq": ["rq1: what is the effect of asking for contributions?"]}
{"intro": ["Research examining the links between individual differences in cognitive skills and gaming expertise have yielded varying results. In these studies, participants are typically recruited from college campuses (majority of studies cited in this manuscript) and are separated into two groups of individuals based on gaming experience. Those who are defined as gamers typically play greater than five hours a week whereas non-gamers are defined as those who play less than an hour a week [1, 6, 8, 10, 11, 13, 18, 19, 32] . There is evidence that gamers outperform non-gamers in basic perceptual skills such as integrating information that is presented in multiple modalities [13] , detecting changes in visual stimuli [10] , and visuospatial acuity [19] . Differences have also been reported in visual attention capabilities with gamers being faster at finding a target amongst a set of distractors [8] and deploying attention to a select set of stimuli amongst distractors [6, 18] . There is also mixed evidence that differences in gaming experience extend to visual working memory capacity defined as the number of chunks of visual information that can be retained [12] . Gamers outperformed non-gamers when given an N-back task where participants had to judge whether the current stimulus from a stream of randomly presented visual images was the same or different than the image N-back (e.g., two images back) [11] . Additional evidence of gaming experience effects has been observed with the speed of retrieving information in working memory [32] . However, there have also been null effects when examining the latent variables underlying working memory capacity [39] . A meta-analysis by Powers and colleagues [34] found that when there is an advantage for gamers over non-gamers on cognitive measures the effect size is typically small, which is consistent with mixed results in previous research.", "International gaming tournaments that have been established with the rise of eSports offer an opportunity to recruit a large number of gamers that range in gaming expertise. Some of the most popular tournaments, in regard to attendance and viewership, are those of MOBA games such as League of Legends and Defense of the Ancients II (Dota 2). This genre of game is typically defined as being a free-to-play game where gamers control a hero they select and, alongside a team of other gamers and their heroes, battle an opponent team with the goal of destroying their home base. The tournaments are composed of brackets where teams compete against each other to win money and recognition. They are typically held in indoor arenas to fit both the teams competing and the large number of fans that attend to view the competition. Much like professional sporting events such as the NCAA College Basketball Championship, MOBA tournaments attract dedicated gamers who are interested in the game and vary in gaming expertise themselves. These fans are typically familiar with the basic mechanics of a game and are typically players of the game themselves. This offers the chance to sample gamers who are familiar with a particular game and vary from novices to professional gamers. However, recruiting participants in a tournament environment is much different than a laboratory environment. To our knowledge, running a cognitive research study at a gaming tournament has yet to be done although the value of it has been considered theoretically [23] . This leads to a basic but critical question: can valid and reliable data be collected at a MOBA tournament?"], "relatedWork": [], "rq": [" this leads to a basic but critical question: can valid and reliable data be collected at a moba tournament?"]}
{"intro": [], "relatedWork": [], "rq": [" this is an ongoing challenge in language intervention: how to be therapeutic while maintaining naturalness (ukrainetz, 1998 (ukrainetz, , 2006 . slps can make many reasonable choices concerning therapy targets, procedures, and contexts. because most choices can be justified, the focus should be more on the quality of the therapy. does the slp present a theoretically coherent approach or a random eclectic mix?"]}
{"intro": ["There is an increasing body of literature on how readers browse and choose print books (Reutzel et al. 1998; Hinze et al. 2012) , and some studies comparing overall usage of print and ebooks (Littman et al. 2004; Christianson et al. 2005) . Thus far, however, no-one has yet examined how readers select ebooks. It is similarly unclear whether the search-dominant tools on offer for ebook seeking affect selection behaviours. In this paper, we both examine ebook borrowing patterns, and compare these to print book borrowing patterns in the same library. Our aim is to understand the influence of book format and information seeking interfaces on book selection behaviour. We extend the usage log analysis techniques we created in earlier work (McKay et al. 2014; McKay et al. 2015) for print books to generate an understanding of both print and ebook use in our test collection. Understanding the differences between print and ebook provides cues as to how physical and digital library design affects borrowing, and is likely to provide insight into user needs for ebook or combined libraries moving forward. It is clear that the ability to browse affects borrowing patterns, but what about ebook constant availability, and conversely their lack of browsing facilites? In this paper we examine print and ebook borrowing patterns to understand reader needs and interaction patterns, and draw lessons for library design."], "relatedWork": [], "rq": ["print and ebooks: are they the same?", "print and ebooks: are they the same?"]}
{"intro": [], "relatedWork": [], "rq": ["our account still contains an open question: why is it that insertion of also in the every condition degrades the stimuli?"]}
{"intro": [], "relatedWork": [], "rq": [" lyons suggests that the markedness relation may differ from the simple hyponymy relation by its potential of being reflexive: is that doqadog or a bitch?"]}
{"intro": [], "relatedWork": [], "rq": [" 1) how many backup service graphs should be maintained for a stream session?"]}
{"intro": ["It is well established in the literature that the semantics of a lexical item is understood, in part at least, by the association of the item to the other lexical items used around it. This position has been captured most famously by Firth's well cited claim that \"you shall know a word by the company it keeps\" (Firth 1957:11) , but more recently shown in Hanks (2013) . However new words are formed in the English language regularly and this means that, in theory, they are candidates for providing a non-historical perspective on their meaning in use. In other words, it is not possible to know what company a new word keeps. Any newly-formed word is formed for a purpose and therefore it is reasonable to assume that the word will bear some meaning that is related to its context of use.", "While not yet appearing in most dictionaries, it does appear as an entry on Wikipedia and it was named the word of the year 2016 by Collins Dictionary. However, it has been used extensively in print and social media in the UK and in Europe. BREXIT is a blend of Britain or British with exit. Its formation was influenced by analogy to the term Grexit, which is also a relatively recently blend (Greek + exit), although older, and which was coined in response to the Greek debt crisis in 2010. Both Grexit and Brexit capture the meaning of 'an exit from the European Union'. This clearly expresses an event-oriented meaning but while exit can be seen as a nominalization, as will be discussed below, the lexeme BREXIT is not a nominalization in the strictest sense since it is formed by an adjective-noun blend rather than the more typical deverbal nominal transpositions (i.e. the nominal is formed without recourse to nonnominal meanings or items, cf. Mackenzie 2007) . In systemic functional linguistics (SFL) nominalization is much broader than lexical derivation and is rather treated as a means of conceptualising and encoding our experience but the concept of nominality plays a central role in this (see Halliday, 1966 and Fontaine 2015, for example) . In discussions of the membership of the United Kingdom in the European Union, we might ask whether media reports are reporting on this membership congruently, which in SFL terms means that things are represented by nouns and events are represented by verbs."], "relatedWork": [], "rq": [" in considering the nominal status of the neologism brexit we can ask two questions: is the mapping between the semantics and the grammar congruent?"]}
{"intro": [], "relatedWork": [], "rq": [" a question to be raised here: how is linguistic competence acquired or stored in the brain?"]}
{"intro": ["We identify that container-like understandings of urban tourism space [51] , as represented in the tourist-historic city model [12] [13] [14] , resemble the traditional framing of space as a \"natural fact\" [64] in the CSCW community. Both approaches, however, are lacking satisfying explanations of how tourism space can emerge and develop in residential neighborhoods. The tourist-historic city model solely takes into account tourism-related facilities and infrastructure as defining elements of tourism space. Such structures are often non-existing in new urban tourism areas like Kreuzk\u00f6lln, one of our case-study neighborhoods. Still, this neighborhood without any major sights is becoming a tourism hotspot [33, 55] . Against this background, we follow a constructionist understanding of urban tourism space [51, 68, 136] . We argue that tourism space, like tourist sights [84] , is socially constructed through representations [99, 102] and performances [15, [45] [46] [47] 74] . That means we no longer regard tourism facilities and infrastructure as the central elements defining tourism space. Instead, people are the major agents who transform places and landscapes into tourist destinations. They attach meanings and values to places and objects [37, 102] , produce written, oral, or pictorial representations of them, and thus contribute to the discourse of how places or objects are to be perceived. Finally, following the performative turn in tourism studies [74, 75] , we argue that places need to be enacted through \"bodily performances\" [15, 75] . Practices, such as picture taking or collectively \"gazing\" [122] upon a building, are necessary to enact places in a touristic manner [76] . Taking these theoretical considerations into account, we analyze how two different Berlin neighborhoods, Kreuzk\u00f6lln and City West, are socially constructed in Airbnb listings. The following three questions guided our research: RQ1: How are the two neighborhoods Kreuzk\u00f6lln and City West constructed as tourism spaces in Airbnb listings? RQ2: How does the space construction differ between these two neighborhoods? RQ3: How do the neighborhood descriptions differ between Airbnb hosts and the destination management and marketing organization (DMO)?"], "relatedWork": [], "rq": [" rq1: how are the two neighborhoods kreuzk\u00f6lln and city west constructed as tourism spaces in airbnb listings?", " rq2: how does the space construction differ between these two neighborhoods?", " rq3: how do the neighborhood descriptions differ between airbnb hosts and the destination management and marketing organization (dmo)?"]}
{"intro": [], "relatedWork": [], "rq": [" the goal is to explain the economy of unstressed do: why does it appear only when it is needed?"]}
{"intro": ["Although the future of web services looks very promising, there are still some challenging problems. Web service selection is one of them. The focus of current web service techniques is on the functional aspects of services. A service provider publishes its service function description by which a service consumer can find the service. But in a redundant open system, a service consumer faces a dilemma in having to making a choice from a bunch of services offering the same function. At this time, a service consumer needs to know not only what a service can do, but also how well a service can do, evaluated according to some quality of service (QoS) metrics. Current web service technology can not support QoS or other non-functional aspects of a service. A service consumer is forced to either make a selection manually at design time depending on some external information resources or it has to make a random choice, which is a blind choice. In an open environment where anyone can publish services, a consumer may select a poor quality, time-consuming, expensive, or even harmful service. Therefore, mechanisms are needed to help consumers to distinguish good from bad services. Various trust and reputation mechanisms have been proposed and implemented to guide choice in many other open systems, like e-commerce, peer-to-peer, and multi-agent systems. Recently trust and reputation mechanisms have also been applied to web service systems [13, 16, [18] [19] [20] [21] . Since trust and reputation mechanisms have several advantages over other web service selection methods (see Section 2), they are getting more attention from web service researchers. In this review paper, we are going to discuss current trust and reputation solutions for web service selection and explore further opportunities for developing trust and reputation mechanisms for web services by examining systematically trust and reputation mechanisms proposed for other open systems. Two web service selection scenarios and the currently used web service selection methods are presented in the next section. Section 3 discusses the existing trust and reputation mechanisms for web service selection. Section 4 introduces a typology to classify trust and reputation systems proposed for other open systems. Section 5 discusses future directions for using trust and reputation in web service selection. The main application of web services is in business-to-business interactions. The scenarios where a business uses a web service can be classified into two types: direct selection and mediated selection. Direct selection means that the client gets directly the result of the service, i.e. a computation result, for example, a weather report from the web service provider, as shown in Figure 1 A. In this scenario, the selection of web service is mainly determined by the properties of the web service itself. The mediated scenario is to use a web service to get (indirectly) a result from another service, called \"general service\" in Figure 1 B to differentiate it from the intermediary web service. For example, a consumer uses a flight booking web service like Expedia.com to get a flight service (the general service) from an airline company like Air Canada. For the scenario B, the major part of selecting a web service is decided by the general service properties, for example, the quality of the flight service. The properties of the intermediary web service (the flight booking service in the example) only play a small part in the web service selection since the web service is just a tool to acquire the general service. A provider may advertise a service with a QoS description. Therefore the most common solution for service selection depends on the QoS information from service providers [16] . Although the provider is supposed to deliver the service with the described quality, it is not an agreement or obligation. A provider can deliver a service not according to its published QoS description. A provider may also exaggerate its capability of providing good QoS on purpose to attract consumers to get more benefits, such as money. A consumer is vulnerable to inaccurate QoS information. Therefore, in order to get a service with a guaranteed quality, a consumer can negotiate with a provider to make an agreement, called a Service Level Agreement (SLA) which specifies the quality that a service should meet. A SLA may also include the methods of how to measure different QoS metrics. A third party may be involved to supervise the service and do the justice. The SLA expresses an obligation of a service provider, who may have to pay a penalty when the service is not delivered according to SLA. However, making a SLA comes with a cost, such as time, expenses. This method relies on the establishing of a common ontology so that providers and consumers have the same understanding of various QoS metrics."], "relatedWork": [], "rq": [" 3) how can dishonest feedbacks or unfair ratings be detected?"]}
{"intro": ["To tackle these issues one might be tempted to \"scientise\" PD (compare discussion with respect to design in Gaver, 2012) . However, PD takes a fundamentally different metaphysical stance, which distinctively sets it apart from the engineering tradition of building interactive technology. Any attempt to retrofit PD with a (post-)positivistic perspective would necessarily make it look scientifically weak, supported by fuzzy data and arbitrary in terms of its conclusions. Instead of seeing the practitioner as an objective observer who inquires about an absolute reality and the best possible solution, PD sees knowledge generation as a dialogic process that is mediated by values and strongly situated. The philosophy that underpins the ideas and concepts of PD are deeply rooted in the postmodern tradition, including phenomenology and Marxism (Ehn, 1989) , and demand a different epistemological position as well as methodological approach. So, instead of imposing a positivistic philosophy, we propose that PD needs to build on its own philosophical groundings to argue for its qualities and contributions."], "relatedWork": [], "rq": [" at its core the issue seems strikingly similar to that of ar: how could localised and highly contextual, creative design practices yield scientific knowledge of appropriate rigour?"]}
{"intro": ["Hausa is a West Chadic SVO tone language spoken by about 35 million people (mainly) in Nigeria and the south of Niger. Its temporal system, which is the subject of this study, is commonly classified as aspect-prominent. However, the descriptive literature also identifies one tense category which is supposed to be encoded in the morpheme z\u0101 (Newman 2000; Jaggar 2001) . The starting point of the following study is the observation that in Hausa, finite matrix sentences such as (1) can in principle get past, present and future interpretations. 1 (1) H\u00e0wwa H\u00e0wwa tan\u00e0 3SG.F.PROG w\u00e0s\u0101. play 'H\u00e0wwa is / ?was / ??will be playing.'", "As indicated by the question marks, these interpretations are not all equally acceptable for native speakers when presented out of the blue. Hence, the matter of investigation is how temporal interpretation is restricted in Hausa, i.e.: (i) What reference time locations are licit under what circumstances? (ii) (How) do aspect morphology and contextual information influence the location of the reference time? (iii) How is future time reference obtained? Following Reichenbach (1947) , Klein (1994) and many others, we assume that temporal interpretation involves a threefold distinction between the time at which a sentence is uttered (= UT for \"utterance time\"), the time at which an eventuality is located (= ET for \"eventuality time\") and the time about which the speaker makes a claim (= RT for \"reference time\"). According to Klein (1994) , tense relates UT and RT, thus restricting the time interval that the speaker's claim is confined to with respect to the utterance time. Aspect defines a relation between RT and ET, which essentially means defining a temporal viewpoint on an eventuality. The paper is structured as follows: Section 2 identifies two major approaches to the analysis of languages without overt tense morphology, namely one according to which temporal interpretation is restricted by covert tense morphology (Matthewson 2006) and one under which aspectual and contextual information alone determine the location of the reference time of a sentence (Smith & Erbaugh 2005; Smith et al. 2003 Smith et al. , 2007 ; see also Tonhauser 2011b) . Section 3 is the core part of the paper. After giving a brief overview of the tense-aspect-mood (TAM) system of Hausa as it is described in the reference grammars, the pertinent data is presented. Based on these data, we arrive at the conclusion that a tenseless analysis in the spirit of Smith et al. 2003 Smith et al. , 2007 accounts for the observations most appropriately, since temporal interpretation is fully predictable from aspect, context and some basic principles of pragmatic reasoning. Subsection 3.1 deals with past and present interpretations specifically. Since, however, special attention is given to future time reference, this is discussed in a separate subsection (3.2). Finally, section 4 summarizes the findings and gives an outlook on further research and conceivable cross-linguistic generalizations."], "relatedWork": [], "rq": [" finite matrix sentences such as (1) can in principle get past, present and future interpretations. 1 (1) h\u00e0wwa h\u00e0wwa tan\u00e0 3sg.f.prog w\u00e0s\u0101. play 'h\u00e0wwa is / ?"]}
{"intro": [], "relatedWork": [], "rq": ["research question 1: how do researchers define social presence in highly cited social presence research?"]}
{"intro": ["Automatic assessment of translation quality is a challenging problem because the evaluation task, at its core, is based on subjective human judgments. Reference-based metrics such as BLEU (Papineni et al., 2002) have rephrased this subjective task as a somewhat more objective question: how closely does the translation resemble sentences that are known to be good translations for the same source? This approach requires the participation of human translators, who provide the \"gold standard\" reference sentences. However, keeping humans in the evaluation loop represents a significant expenditure both in terms of time and resources; therefore it is worthwhile to explore ways of reducing the degree of human involvement.", "To this end, Gamon et al. (2005) proposed a learning-based evaluation metric that does not compare against reference translations. Under a learning framework, the input (i.e., the sentence to be evaluated) is represented as a set of features. These are measurements that can be extracted from the input sentence (and may be individual metrics themselves). The learning algorithm combines the features to form a model (a composite evaluation metric) that produces the final score for the input. Without human references, the features in the model proposed by Gamon et al. were primarily language model features and linguistic indicators that could be directly derived from the input sentence alone. Although their initial results were not competitive with standard reference-based metrics, their studies suggested that a referenceless metric may still provide useful information about translation fluency. However, a potential pitfall is that systems might \"game the metric\" by producing fluent outputs that are not adequate translations of the source. This paper proposes an alternative approach to evaluate MT outputs without comparing against human references. While our metrics are also trained, our model consists of different features and is trained under a different learning regime. Crucially, our model includes features that capture some notions of adequacy by comparing the input against pseudo references: sentences from other MT systems (such as commercial off-the-shelf systems or open sourced research systems). To improve fluency judgments, the model also includes features that compare the input against target-language \"references\" such as large text corpora and treebanks."], "relatedWork": [], "rq": [" have rephrased this subjective task as a somewhat more objective question: how closely does the translation resemble sentences that are known to be good translations for the same source?"]}
{"intro": [], "relatedWork": ["An alternative to post-processing are methods for selecting a set of basis vectors a priori. This includes sampling randomly from the training set in the NYSTROM method (Williams and Seeger 2001) , greedily minimizing reconstruction error (Smola and Sch\u00f6lkopf 2000) , and variants of the Incomplete Cholesky factorization (Fine and Scheinberg 2001; Bach and Jordan 2005) . However, these selection methods are not part of the optimization process, which makes a goal-directed choice of basis vectors difficult. In fact, all but Bach and Jordan (2005) ignore label information, and all methods are limited to selecting basis vectors from the training set.", "Another set of methods are basis pursuit approaches (Keerthi et al. 2006; Vincent and Bengio 2002) . They repeatedly solve the optimization problem for a given set of basis vector, and then greedily search for vectors to add or remove. The Cutting-Plane Subspace Pursuit method we propose is similar in the respect that it iteratively constructs the basis set. However, the construction of the basis set is part of the optimization algorithm itself, and the cutting-plane model makes it straightforward to add basis vectors that are not in the training set. It is not clear how to efficiently add such general basis vectors in other basis pursuit approaches.", "The method most closely related to ours was proposed in Wu et al. (2006) . They treat the basis vectors as variables in the SVM optimization problem, and solve the resulting nonconvex program via gradient descent to a local optimum. However, training efficiency is a bottleneck in this approach and they focus only on small datasets in their evaluation. We will consider datasets that are several orders of magnitude larger. Furthermore, we will provide theoretical results giving insight into the quality of the CPSP solution."], "rq": ["5) can be computed as sums of kernel evaluations. the o( c ) bound on the number of iterations (joachims 2006; joachims et al. 2009 ) holds independent of whether a kernel is used or not, but how does the time complexity per iteration change when moving from a linear to a kernelized svm?"]}
{"intro": [], "relatedWork": [], "rq": ["4) is also consistent with the reinforce formulation. for this strategy one maintains an adaptive estimate ?"]}
{"intro": ["The connection between graph theory and matroid theory was first noticed by Whitney [1] , who defined a matroid in order to generalize the idea of linear dependence. Sim\u00f5es-Pereira [2] introduced a notion of matroidal family. Among known examples of such families we find the collections of cycles and bicycles which give well-known cycle matroid [4] and bicircular matroid [5] . Cycles and bicycles are graphs with, respectively, one and two fundamental cycles. Thus, it is interesting whether we can define, for given n \u2265 3, a matroidal family that consists of graphs from C n , a family of all graphs with n fundamental cycles. One of the results of this paper is a theorem which explains why it is impossible. However, we may put a question the other way. Given n \u2265 3, can we describe graphs, such that their edges form R. Kawa matroids with circuits from C n ? Positive answer to this question is the main result of this paper."], "relatedWork": [], "rq": [" 3, can we describe graphs, such that their edges form r. kawa matroids with circuits from c n ?"]}
{"intro": [], "relatedWork": [], "rq": ["q1: how to get rid of stuffy nose?", " q2: what is the best way to prevent a cold?", " q3: how do i air out my stuffy room?", " q4: how do you make a nose bleed stop quicker?"]}
{"intro": ["Is the basic mechanism behind presupposition projection fundamentally asymmetric, or does it allow for symmetric effects? That is, when processing presuppositions, can we take into account only material that precedes (either in time, linear order, or hierarchical order) a presupposition trigger, or can we also access material that follows the trigger? This is a basic question for the theory of presupposition, which also bears on broader issues concerning the sources of asymmetries observed in natural language: are these rooted in superficial asymmetries of language uselanguage use unfolds in time, which we experience as fundamentally asymmetric-or can they be directly referenced in linguistic knowledge and representations? 1 In this paper we aim to make progress on these questions by exploring presupposition projection across conjunction. Projection across conjunction has typically been taken as a central piece of evidence that presupposition projection is asymmetric (i.e. presuppositions can be filtered by material to their left, but not by material to their right). Following recent work, we argue that the evidence, however, is much less clear than is commonly accepted once we take into account independent issues about redundancy (Schlenker 2009; Rothschild 2011; Chemla & Schlenker 2012) . Similarly, intuitions about right-to-left filtering in conjunction are muddied by the possibility of presupposition suspension. The question of the influence of order on projection across conjunction thus remains open. Building on recent work by Chemla & Schlenker (2012) and , we investigate this question by using experimental inference tasks to ascertain whether asymmetries persist once we control for redundancy and suspension. In our results, we find that it does: they provide strong evidence for left-to-right filtering of presuppositions across conjunctions, but no evidence for right-to-left filtering."], "relatedWork": [], "rq": [" which also bears on broader issues concerning the sources of asymmetries observed in natural language: are these rooted in superficial asymmetries of language uselanguage use unfolds in time, which we experience as fundamentally asymmetric-or can they be directly referenced in linguistic knowledge and representations?"]}
{"intro": ["Note that, just like in (1)-(2), the degree constructions in (3) are compatible with a situation where Athos' height exceeds Porthos', but they somehow introduce an additional entailment. For example, for the comparative of inferiority in (3a) and the analytic comparative in (3b) to be felicitously uttered, Athos and Porthos have to count as 'short' in the context. In turn, (3c) gives rise to the inference that Athos and Porthos count as 'tall' in the context. This type of context sensitivity, by which gradable predicates are interpreted relative to a contextually-provided standard of comparison has been called 'evaluativity' (Rett 2008 (Rett , 2015 Breakstone 2014 among others) . Note that no analogous inference arises in any of the sentences in (1), (2a) or (2b). For instance, (1) and (2b) involve the adjective tall, but the two sentences can be felicitously uttered in contexts where Athos and Porthos are short 1 . In essence, the above paradigm raises the following questions: how can we derive suitable meanings for the evaluative sentences in (3) while preserving our assumptions about the semantics of degree morphemes? Is there a way of predicting which degree constructions are evaluative? One of the main accounts of evaluativity relies on the idea of a markedness competition (Rett 2007 (Rett , 2008 . 2 At the core of this approach is the idea that degree constructions come in pairs composed of a marked and an unmarked member. The unmarked construction precludes the marked construction whenever they express the same meaning. The markedness-based competition account further argues that the presence of an optional evaluativity operator can break the competition, thereby forcing the evaluative construal of a degree construction to surface. However, the explanatory scope of this account is limited by the fact that it leaves unexplained the source of markedness and consequently, the way competitors are determined is not fully predictable.", "In several other degree constructions however, only the evaluative reading is available, suggesting that the non-evaluative construal is blocked. The key idea behind Rett's (2007 Rett's ( , 2008 ) theory of evaluativity is that this blocking effect is due to a markedness competition. In the comparative paradigm, Rett considers three types of markedness triggers: the polarity of adjectives (the negative adjective is marked whereas the positive adjective is unmarked), the type of comparative operator (less is marked, whereas the degree operator -er is unmarked) and the shape of the comparative (analytic comparatives are marked whereas synthetic comparatives are unmarked). On this account, when a comparative lacks a non-evaluative reading, is it because its non-evaluative parse is blocked by a less marked competitor that expresses the same meaning. To illustrate an example of markedness competition, let us look at the two parses generated for the negative-antonym less-comparative, provided in (8a) and (8b). "], "relatedWork": [], "rq": [" the degree constructions in (3) are compatible with a situation where athos' height exceeds porthos', but they somehow introduce an additional entailment. for example, for the comparative of inferiority in (3a) and the analytic comparative in (3b) to be felicitously uttered, athos and porthos have to count as 'short' in the context. in turn, (3c) gives rise to the inference that athos and porthos count as 'tall' in the context. this type of context sensitivity, by which gradable predicates are interpreted relative to a contextually-provided standard of comparison has been called 'evaluativity' (rett 2008 (rett , 2015 breakstone 2014 among others) . note that no analogous inference arises in any of the sentences in (1), (2a) or (2b). for instance, (1) and (2b) involve the adjective tall, but the two sentences can be felicitously uttered in contexts where athos and porthos are short 1 . in essence, the above paradigm raises the following questions: how can we derive suitable meanings for the evaluative sentences in (3) while preserving our assumptions about the semantics of degree morphemes?"]}
{"intro": ["Our discussion is grounded in a qualitative analysis of interview data from a pro-environmental crowdsourcing study called Close the Door (CTD) [28] . In CTD participants used mobile phone apps to collect data about whether shops in a city kept their doors open or closed in cold weather. The study was designed to explore the effects of competition on participants through the use of leaderboards, badges and pay-for-results financial incentives. Whereas high performing participants were clearly motivated by competition, analysis of semistructured interviews with participants shows that may low performing participants were demotivated by competition. However, it also became clear that some participants, in particular those performing at a mid-level, were using the leaderboard in an unexpected way. For example, one participant stated the leaderboard \"was an indication obviously on how much other people were using it and I wanted to make sure I was sort of in the middle or top half rather than the bottom end.\" The CTD study did not make explicit use of normification strategies, however attitudes such as this indicate the potential for motivational approaches based on normification to encourage those who are not motivated, or indeed demotivated, by competition. This paper makes several contributions to CSCW research. It provides a review of three major psychological theories on the behavioural influences of norms. Following this we review the impact of normative theories on prior research in crowdsourcing and citizen science, and categorise the ways in which these theories have been used. We then provide a qualitative analysis of attitudes of participants in the CTD study. Three broad motivational/demotivational factors are identified:"], "relatedWork": [], "rq": [" the question therefore arises: what techniques can we apply to maximise the effectiveness of both high and low level contributors?"]}
{"intro": [], "relatedWork": [], "rq": ["q: what is a prism?"]}
{"intro": [], "relatedWork": [], "rq": [" he does not explicitly explain in what way his operationalization can be interpreted as gradual: how to interpret successful insertion into one, two, or all three of the gapfilling sentences types?"]}
{"intro": ["Previous studies tackled this challenge in various ways. Some recovered visibility as well as the 3D structure in haze and underwater [38] , [39] , [40] under distant natural illumination. However, application fields operating in highly turbid media use artificial illumination sources at short distances, be it underwater or in the human body. However, artificial lighting usually causes a strong backscatter. Backscatter can be modulated and then compensated for in image postprocessing. Prior modulation methods require acquisition of long image sequences by structured light [21] , [23] , [31] or time-gating [6] , [9] , [14] , [47] , [49] . Nayar et al. [32] required many frames as well, to achieve quality results. Such sequences may lengthen the overall acquisition time. Moreover, such systems may be complex and expensive.", "To counter these problems, we look at wide-field (not scanning) illumination with a small (or no) baseline, where the backscatter is modulated by polarization. Preliminary studies [10] , [11] , [24] indicated that backscatter can be reduced by polarization. However, we go further. By postprocessing, we remove residual backscatter that is not blocked by optical means. Moreover, a rough estimate of the 3D scene structure may be obtained from the acquired frames. The acquisition setup is a simple modification of instruments used routinely in such media: simply mounting two polarizers, one on the light source and another on the camera. The acquisition process is instantaneous, i.e., requiring only two frames, rather than scanning. In this paper, we describe and demonstrate each step separately.", "Some prior methods used polarization in scattering media. Some assumed a negligible degree of polarization (DOP) of the objects [38] , [39] , [41] , [50] . Others assumed the contrary, i.e., the object reflection is significantly polarized rather than the backscatter [52] . However, here we allow both the backscatter and the object reflection to be partially polarized. Thus, our analysis unifies and generalizes the mentioned previous methods."], "relatedWork": [], "rq": ["1) is approximately linear at short distances, yielding a good distance resolution. however, (21) saturates very quickly, thus losing the capacity of proper recovery. again, when the resolution is in the magnitude of the noise, the reconstruction may become fruitless. what are the typical saturation distances?"]}
{"intro": ["Multiprocessor System-on-Chip (MPSoC) development is known to be a complex task due to the combination of software and hardware challenges. On the software side, target applications are increasingly complex and demanding in computation power. On the hardware side, modern architectures, however powerful they may be, often cannot be fully exploited due to a lack of well-suited programming models and tools. For instance, in the field of video decoding, the major algorithms, namely the ubiquitous H.264/AVC [1] and its successor HEVC [2] , exhibit a very high level of complexity and dynamism (i.e. variability and unpredictability) which requires both flexibility and computing power on the part of the platform. The former can only be achieved by software, while the latter is clearly more amenable to hardware, hence the need for heterogeneous architectures. But such mixed hardware/software implementations are notoriously difficult to program efficiently [3] . To ease the development process, a number of paradigms have already been proposed, including dataflow and Kahn process networks which lend themselves well to streaming applications. The latter requires very little knowledge about the application but can be costly to execute, while the former needs more information in exchange for efficient execution. Therefore, in the context of mapping dynamic streaming applications onto heterogeneous MPSoCs, an important question remains: how to get the best of both worlds?"], "relatedWork": [], "rq": [" an important question remains: how to get the best of both worlds?"]}
{"intro": [], "relatedWork": [], "rq": [" this main objective generated the research question for present study: what problems or barriers do first year engineering students find when reading texts at pakistan?"]}
{"intro": ["We can add syntactic information to the SCFG rules by parsing the parallel training data and projecting parse tree labels onto the spans they yield and their translations. For example, if house was parsed as a noun, we could rewrite Rule 1 as N \u2192 maison ; house But we quickly run into trouble: how should we label a rule that translates pour l'\u00e9tablissement de into for the establishment of? There is no phrase structure constituent that corresponds to this English fragment. This raises a model design question: what label do we assign to spans that are natural translations of each other, but have no natural labeling under a syntactic parse? One possibility would be to discard such translations from our model as implausible. However, such non-compositional translations are important in translation (Fox, 2002) , and they have been repeatedly shown to improve translation performance (Koehn et al., 2003; DeNeefe et al., 2007) ."], "relatedWork": [], "rq": [" house but we quickly run into trouble: how should we label a rule that translates pour l'\u00e9tablissement de into for the establishment of?", " this raises a model design question: what label do we assign to spans that are natural translations of each other, but have no natural labeling under a syntactic parse?"]}
{"intro": ["Statistical language modeling is concerned with determining the probability of naturally occurring word sequences in human natural language. Traditionally, the dominant motivation for language modeling has come from the field of speech recognition (Jelinek, 1998) , however statistical language models have recently become more widely used in many other application areas, such as information retrieval (Lafferty & Zhai, 2001 ), machine translation (Brown et al., 1992) , optical character recognition, spelling correction, document classification , and bio-informatics (Durbin et al., 1998) .", "Several techniques for combining language models have thus also been investigated. The most commonly used method is simple linear interpolation (Chelba & Jelinek, 2000; Jelinek & Mercer, 1980; Roark, 2001; Rosenfeld, 1996) , where each individual model is trained separately and then combined by a weighted linear combination, where the weights are trained using held out data. Even though this technique is simple and easy to implement it does not generally yield effective combinations because the linear additive form is too blunt to capture subtleties in each of the component models (Rosenfeld, 1996) . Another approach is based on Jaynes' maximum entropy (ME) principle (Jaynes, 1983) . This approach has several advantages over other methods for statistical modeling, such as introducing less data fragmentation (as in decision tree learning), requiring fewer independence assumptions (as in naive Bayes models), and exploiting a principled technique for automatic feature weighting. The major weakness with maximum entropy methods, however, are that they can only model distributions over explicitly observed features, whereas in natural language we encounter hidden semantic (Bellegarda, 2000; Hofmann, 2001 ) and syntactic information (Chelba & Jelinek, 2000) which we do not observe directly.", "One way to encode constraints over hidden features in a maximum entropy model is to first pre-process the training corpus to obtain explicit values for all of the hidden featuressuch as recovering syntactic structure by running a parser, or recovering semantic content by using a latent semantic indexer-and then incorporating statistics over explicitly measured features as additional constraints in the model (Berger, Della Pietra, & Della Pietra, 1996; Khudanpur & Wu, 2000; Rosenfeld, 1996) . However, doing so explicitly is not always possible, and even if attempted, sparse data problems almost always immediately arise in such complex models. Consequently, the perplexity improvements or word error rate reductions obtained are often minimal. In this paper we address the question: is it possible to exploit the hidden hierarchical structure of natural language within a maximum entropy method without resorting to explicit preliminary parsing or semantic analysis?"], "relatedWork": [], "rq": [" in this paper we address the question: is it possible to exploit the hidden hierarchical structure of natural language within a maximum entropy method without resorting to explicit preliminary parsing or semantic analysis?"]}
{"intro": [], "relatedWork": [], "rq": ["5] can be detected from text. therefore, we consider sympathy (is the tweet sympathetic or not towards the affected individuals?"]}
{"intro": [], "relatedWork": [], "rq": ["8) is concerned. if dpt is intended (as in 19), the do must be pied-piped along l-verb movement. here the possibility that all of the amalgamated head (l-verb + little v\u00ba) and the do move to null t\u00ba and then to the left periphery, as in sentence (19) the do cannot move as part of the amalgamated head (l-verb + little v\u00ba) to t\u00ba for its sharp violation for sentence derivation principles and structure preservation rules (cf. myers, 1991; it\u00f4 & mester, 1993; chomsky, 1995 chomsky, , 2008 . indeed, we cast doubt on the suggested correlation obtained between whether t\u00ba is overtly filled or not and the movement of the do along with l-verb to the left periphery. in either way, dpt is available. therefore, how dpt (l-verb + the do) is licensed in syntax?"]}
{"intro": [], "relatedWork": [], "rq": ["rq4. how is sleep debt related to mood?"]}
{"intro": ["The accelerated development of science during the past century and the development of mass communication are followed by growing interest of the public in terminology, especially in several past decades. Specialized vocabularies of all fields of interest become part of laypersons' lives as a part of popular culture because of increasing use of scientific terms in mass media [1] , [2] . Naturally, in monolingual dictionaries of Czech the number of terms steadily increases. 1 However, as it will be argued in this article, concepts of terminology processing in monolingual dictionaries of Czech vary and so does the level of their usability and user-friendliness."], "relatedWork": [], "rq": ["now we come to the crucial part of our paper: what do domain labels say to dictionary users?"]}
{"intro": [], "relatedWork": [], "rq": [" the dot product of the two prisms shown in figure 19 is a type 2 cubic graph of cyclic-edge connectivity 4, and is class 1. however, we have not succeeded in finding any type 2 cubic graph of cyclic-edge connectivity 4 which is class 2, even with a square. therefore, we can extend cavicchioli's problem to all snarks. question 2. what is the smallest type 2 snark?"]}
{"intro": [], "relatedWork": ["In this context, a popular approach consists of flattening the manifold via its tangent space. The best-known example of such an approach is Principal Geodesic Analysis (PGA) [42] , [43] . PGA and its variants, such as [44] , [45] , [46] , have been successfully employed for various applications, such as analyzing vertebrae outlines [47] and motion capture data [44] . PGA can be understood as a generalization of PCA to Riemannian manifolds. To this end, the widely-used formulation proposed in [43] identifies the tangent space whose corresponding subspace maximizes the variability of the data on the manifold. PGA, however, is equivalent to flattening the Riemannian manifold by taking its tangent space at the Karcher, or Fr echet, mean of the data. As such, it does not fully exploit the structure of the manifold. Furthermore, PGA, as PCA, cannot exploit the availability of class labels, and may therefore be suboptimal for classification.", "Different from the methods that flatten the manifold, via either a tangent space, or an RKHS, [50] directly employs notions of Riemannian geometry to perform nonlinear DR. In particular, [50] extends several nonlinear DR techniques, such as Locally Linear Embedding (LLE), Hessian LLE and Laplacian Eigenmaps, to their Riemannian counterparts. Take for example the case of LLE [23] . Given a set of vectors fx x i g p i\u00bc1 ; x x i 2 R D , the LLE algorithm determines a weight matrix W W 2 R p\u00c2p to minimize a notion of reconstruction error on fx x i g p i\u00bc1 . Once the weight matrix W W is determined, the algorithm embeds fx x i g p i\u00bc1 into a lower dimensional space R d ; d < D, where the neighboring properties of fx x i g p i\u00bc1 are preserved. The neighboring properties are encoded by W W and the embedding takes the form of an eigen-decomposition in the end. As shown in [50] , the construction of W W can be generalized to the case of an arbitrary Riemannian manifold M by using the logarithm map. Hence, for a given set of points on M, an embedding from M ! R d can be obtained once W W is appropriately constructed. In [50] , the authors showed that the embedded data was more discriminative than the original one using several clustering problems on M. In principle, the Riemannian extension of LLE (and of the other nonlinear DR algorithms discussed in [50] ) can also be applied to classification problems. However, they are limited to the transductive setting since they do not define any parametric mapping to the low-dimensional space."], "rq": [" a natural question arises: how can popular dr techniques be extended to curved riemannian spaces?"]}
{"intro": [], "relatedWork": [], "rq": ["the idea of independent derivations is extremely dubious: what type of phonological representations are we going to build indepen- dently of syntax?", ": how can the system establish the order between the transferred chunks?"]}
{"intro": ["Visual relationship detection (VRD) [10] , [11] , [12] , [13] , [14] , [15] , [16] , [17] , [18] focuses on recognizing the potential relationship between pairs of detected objects, in which the output is often formulated as a triplet in the form of (subject, predicate, object). Generally, it is not sufficient to interpret the input image by only recognizing the individual objects. The visual relationship triplet, in particular the predicate, plays an important role in understanding the input images. However, it is often hard to predict the predicates since they tend to exhibit a long-tail distribution. In most cases, for a same predicate, the diversity of the subjectobject combinations is often enormous [19] . In other words, compared with the individual objects, the corresponding predicates capture more general abstractions from the input images."], "relatedWork": [], "rq": [" 1) how many regions are enough for most visual semantic information pursuit applications?", " 2) how to efficiently compute the higher-order pseudo-marginals given the decomposed regions?", " 3) how to properly propagate contextual information between different regions?"]}
{"intro": [], "relatedWork": [], "rq": [" some difficulties complicate this second objective: how might an instructor intuit when, precisely, students have understood the material sufficiently?"]}
{"intro": [], "relatedWork": [], "rq": [" scrutiny of the concordance output prompted one student to ask: why are there so many occurrences of focus in the present perfect?"]}
{"intro": [], "relatedWork": [], "rq": [" it is difficult to discern what these numerical scores mean: are they positive enough to claim real community engagement or do they demonstrate the existence of review systems for users?"]}
{"intro": ["In this paper, however, we will consider the following related problem, which is obtained by retaining the k-geodecity requirement in the above characterisation of Moore digraphs, but allowing the diameter to exceed k: what is the smallest possible order of a k-geodetic digraph G with minimum out-degree \u2265 d? A k-geodetic digraph with minimum out-degree \u2265 d and order M (d, k)+\u01eb is said to be a (d, k, +\u01eb)-digraph or to have excess \u01eb. It was shown in [6] that there are no diregular (2, k, +1)-digraphs for k \u2265 2. In 2016 it was shown in [5] that digraphs with excess one must be diregular and that there are no (d, k, +1)-digraphs for k = 2, 3, 4 and sufficiently large d. In a separate paper [7] , the present author has shown that (2, k, +2)-digraphs must be diregular with degree d = 2 for k \u2265 2. In the present paper, we classify the (2, 2, +2)-digraphs up to isomorphism and show that there are no diregular (2, k, +2)-digraphs for k \u2265 3, thereby completing the proof of the nonexistence of digraphs with degree d = 2 and excess \u01eb = 2 for k \u2265 3. Our reasoning and notation will follow closely that employed in [3] for the corresponding result for defect \u03b4 = 2."], "relatedWork": [], "rq": [" but allowing the diameter to exceed k: what is the smallest possible order of a k-geodetic digraph g with minimum out-degree \u2265 d?"]}
{"intro": [], "relatedWork": [], "rq": [" and the central question raised by the high number of nonexplicit cbs found in naturally occurring texts remains unaddressed: how can we characterize coherence in a text in which cbs are so often inferable and thus in which transition types are often indeterminate?", " 21 what would be the ramifications of this move for the theory?"]}
{"intro": ["In our experiments, we investigate a restricted version of the map building problem, in which a human operator tele-operates the robot through its environment. In particular, we assume that the operator selects a small number of significant places (such as intersections, corners, dead ends), where he pushes (with high likelihood) a button to inform the robot that such a place has been reached. The approach, however, can be applied to the problem of landmark-based map acquisition (using one of the many landmark recognition routines published in the literature, such as (Borenstein, Everett, & Feng, 1996 , Choset, 1996 , Kuipers & Byun, 1991 , Matari\u0107, 1994 ). Thus, the paper phrases the approach in the language commonly used in the field of landmark-based navigation. The general problem addressed in this paper is: How can a robot construct a consistent map of an environment, if it occasionally observes a landmark? In particular, the paper addresses situations where landmarks might be entirely indistinguishable, and where the accumulated odometric error might be enormous."], "relatedWork": ["Over the last decade, there has been a flurry of work on map building for mobile robots (see e.g., (Chatila & Laumond, 1985 , Leonard, Durrant-Whyte, & Cox, 1992 , Rencken, 1993 , Thrun, 1998 ). As noticed by Lu and Milios (1997) , the dominating paradigm in the field is incremental: Robot locations are estimated as they occur; the majority of approaches lacks the ability to use sensor data for revising past location estimates. A detailed survey of recent literature on map building can be found in (Thrun, 1998) . The approach proposed there, however, is also incremental and therefore incapable of dealing with situations such as the ones described in this paper.", "To the best of our knowledge, the topic of collaborative multi-robot mapping has previously only been studied by Lopez and colleagues (Lopez & Sierra, 1997) . Like ours, their approach models the uncertainty of a robot's location explicitly, and it also takes this uncertainty into account when building maps. However, their approach lacks a method for localization. As the uncertainty grows larger than a prespecified threshold, mapping is simply terminated, thereby imposing tight, intrinsic bounds on the size of environments that can be mapped. Due to the lack of a localization component, robots cannot localize themselves in another robot's map."], "rq": [" the general problem addressed in this paper is: how can a robot construct a consistent map of an environment, if it occasionally observes a landmark?"]}
{"intro": ["Based on an observation in the fourth semester students of English department at IKIP Mataram who programmed Writing subject, it was found that the disability of majority students to present correct topics for their paragraph which are often in complete sentences rather than phrases. The students' paragraph topic statements are not appropriately stated yet; they are mostly not in response to the topics proposed. The students' supporting sentences are not clearly elaborated to explain further about the topic statements. And, the concluding sentences are not precisely stated to end writing their paragraphs. Next to them, the use of punctuation which signed the order of the sentence was also inappropriate. However, these difficulties can be tolerated since it is a productive skill that is more complicated that it seems at first, and often seems to be the most difficult of the skills since it has a number of micro skills such as using orthography correctly, spelling, and punctuation (Orwig in Armana, 2011) .", "Ideally a good paragraph is that it considers a topic to be discussed. The paragraph topic directs discussion on what ideas which are going to be further explored in a paragraph. The topic of discussion is commonly stated in a phrase and not in a complete sentence. Next to them, a good paragraph is composed of three main components; topic sentence, supporting sentence, and the concluding sentence. Then the question is that how these difficulties can be minimized or even eliminated within writing learning process that students have better paragraph writing ability. This simple question can be optionally solved by applying Process based Approach as the approach to teach students in learning to write English paragraph. Process based Approach is a teaching of process triggering students to create their own ideas step-by-step to result in a comprehensive organization of paragraph. Therefore, to provide for an appropriate approach in teaching writing will be able to lead students into having higher sense of creativity in learning to write English, particularly in learning to write English paragraph. Process based approach, based on the elaboration on the teaching approach, is claimed to be an effective teaching approach to pump up students' creativity in learning to write. Seeing the phenomena above, the researchers have in depth interest to research a process based approach to teaching writing entitled:\"Process based Approach towards Students' Writing English Paragraph Ability\". The use of process based approach is expected to be effective steps for students in the efforts of creatively developing their English paragraph writing ability. The main research question of the study is: Is Process based Approach effective towards students' English paragraph writing ability at the fourth semester students of English Department at IKIP Mataram in the academic year of 2016/2017? Oshima (2006: 2) defines paragraph as a group of related sentences that discuss one (and usually only one) main idea. A paragraph can be as short as one sentence or as long as ten sentences. The number of sentences is unimportant, however, the paragraph should be long enough to develop the main idea clearly. Further, Irawati, (2013: 5) states that paragraph is the basic unit of composition. Paragraph usually consists of several sentences, they are topic sentence, supporting or developing sentence and concluding sentence. Thus, it can be concluded that paragraph is a unit of composition consisting of several related sentences; topic sentence, supporting sentence, and concluding sentence which are coherently composed and usually developing one main idea."], "relatedWork": [], "rq": [" the main research question of the study is: is process based approach effective towards students\\' english paragraph writing ability at the fourth semester students of english department at ikip mataram in the academic year of 2016/2017?"]}
{"intro": ["The clustering coefficient is also an important issue in the development of network models. So far, various models have been proposed in order to simulate the behavior of large and complex networks in the real world [1, 2, [9] [10] [11] [12] [13] . Among them, the preference attachment model proposed by Barab\u00e1si and Albert [9] is one of the most well-known and widely used models because it exhibits a scale-free degree distribution, which is another important property that can be observed in many networks in the real world. However, it is known that the clustering coefficient of the Barab\u00e1si and Albert model is very low [14, 15] . Therefore, based on this model, many authors have developed scale-free network models with tunable clustering coefficient [16] [17] [18] [19] . In most of these models, the clustering coefficient can be controlled in a certain range by a user-specified parameter. On the other hand, some authors [4, 5, 20, 21] used the 2-switch [22] , which rewires two edges simultaneously without changing the degree of each vertex, to increase or decrease the clustering coefficient of a network. In particular, Fukami and Takahashi [21] have recently shown experimentally that the clustering coefficient of graphs generated by the Barab\u00e1si and Albert model can be increased to around 0.8 by applying the 2-switch repeatedly.", "As explained above, the importance of the clustering coefficient is widely recognized in the literature. However, properties of the clustering coefficient itself have not been discussed much. To see this, let us consider the following fundamental question: What is the most clustered graph for the given number of vertices and edges? This question was first raised by Watts [23, 24] . He considered the connected caveman graph as a candidate solution and derived a general formula for its clustering coefficient. However, it is still not clear whether the connected caveman graph has the highest clustering coefficient or not."], "relatedWork": [], "rq": [" let us consider the following fundamental question: what is the most clustered graph for the given number of vertices and edges?"]}
{"intro": [], "relatedWork": [], "rq": [" : what makes one poem more aesthetically appealing than another?"]}
{"intro": [], "relatedWork": [], "rq": ["0) is simpli ed in that the basis vectors become v k =\u00ea k , k = 1; : : :; n. because of the reversed depth-rst ordering, this corresponds to starting at the leaves of the tree (not shown in fig. 2d) ; subsequent unit vectors are selected for orthogonalization only after their descendants. by construction, a unit vector\u00ea k is automatically a-conjugate to any unit vector which is not its direct descendant or direct ancestor. hence, the gram-schmidt projection step only needs to be e ected against the columns of x k?"]}
{"intro": ["However, some studies [1, 4] indicate that switching focus between two separate views slows users downs due to increased motor and mental effort. Due to this constant attention switching, the time needed to complete a given set of tasks increases, when using such two-view interfaces. Little research related to the analogous problem in parametric CAD modeling exists. Naturally arising questions include: is there a way to combine the information present in both views into one single view, thus avoiding attention-switching and possibly improving task completion times in parametric CAD modelers? What are the usability-related characteristics of such \"integrated\" graphical interfaces? And are users actually more satisfied when using such \"integrated\" graphical interfaces?"], "relatedWork": [], "rq": [" naturally arising questions include: is there a way to combine the information present in both views into one single view, thus avoiding attention-switching and possibly improving task completion times in parametric cad modelers?"]}
{"intro": ["T HE task of 3D motion segmentation aims to separate tracked feature points (in our case a sparse set of trajectories in a video sequence) according to the respective rigid 3D motion. Various geometric models have been used in the 3D motion segmentation problem to model the different types of cameras, scenes, and motion. In this problem as commonly set forth, the underlying models are generally regarded as applicable under different scenarios and these scenarios do not overlap. For instance, when the underlying motion contains translation and the scene structure is non-planar, the fundamental matrix is used to model the epipolar geometry [1] , [2] . When the scene-motion is degenerate, i.e. close-to-planar structure and/or vanishing camera translation, the homography is preferred [3] , [4] . However, the real world scene-motions are in fact not so conveniently divided. They are more typified by near-degenerate scenarios such as a scene that is almost but not quite planar, or a motion that is rotation-dominant but with a non-vanishing translation. In such cases, imposing a false dichotomy in deciding an appropriate model would pose difficulty for subsequent subspace separation. For instance, it is well-known [5] , [6] , [7] in the case of a scene with dominant-plane, it is easy to find inliers belonging to the degenerate configuration (the plane), but the precision of the resulting fundamental matrix is likely to be very low. Most of the inliers outside the degenerate configuration will be lost, and often the erroneous fundamental matrix will pick up outliers (e.g. from other motion groups). Since this is not a purely planar scene, using", "Computer Engineering, National University of Singapore, Singapore E-mail: alex.xun.xu@gmail.com & eleclf@nus.edu.sg Z. Li is with the PonyAI, US E-mail: lzhuwen@gmail.com homography in a naive manner might fail to group all the inliers together too, resulting in over-segmentation of the subspaces. It is also not hard to establish-from a glance of the motion segmentation literature-that of the various models, the fundamental matrix model is generally eschewed, due to the lack of perspective effects in the Hopkins155 benchmark [8] . However, it is never clearly articulated if the numerical difficulties arising from degeneracies in such approach present insuperable obstacles. And no one has put his/her finger on the exact manner how the resulting affinity matrix is ill-suited for subspace clustering: is it solely due to the degeneracies or are there other factors? Considering that in many real-world applications say, autonomous driving, perspective effects are not uncommon, it surely follows that we should come to a better understanding of the suitability of fundamental matrix (or for that matter, the homography model) as a geometric model for motion segmentation. This, we contend, is far from being the case. For instance, does it follow that if we use the fundamental matrix for wide field-of-view scenes (such that strong perspective effects exist [9] ), like those found in the KITTI benchmark [10] , we will get better performance than those using homography? We have in fact as yet no reason to believe that this will be the case, judging by the way how the best performing algorithm is based on homography model [4] , outperforming those based on fundamental matrix [2] . Empirically, we observed that this superiority still persists for the MTPV62 dataset and individual Hopkin sequences that have larger perspectives (though admittedly still moderate in the latter). Indeed, from the results we obtained on the KITTI sequences that we adapted for testing motion segmentation in real-world scenarios, the superiority of the homography-based methods is again observed. Thus, one might naturally ask what factors other than degeneracies are hurting the fundamental matrix approach? And why is the homography matrix approach holding its own in wide perspective scenes, when"], "relatedWork": ["Finally, we briefly mention works based on alternative definitions of motion segmentation [33] , [34] , [35] , [36] , [37] , [38] . These works segment motion groups entirely in the 2D domain, and as such, cannot deal with the motion discontinuities arising from background surfaces residing at different depths. Some works such as [36] , [38] do not attempt to separate the differently moving foreground objects, being interested only in a binary foreground-background separation. The datasets related to the above works,including FBMS [33] , either avoid strongly camera translating sequences, or if it exists, provide a different groundtruth label of motion clusters (i.e. the background is not grouped as one cluster). These datasets are thus not appropriate for evaluating 3D motion segmentation approaches. Multi-View Clustering Multi-view learning aims to synergistically exploit the information derived from multiple modalities/views to achieve better learning. The affinities induced by H and F here are seen as two different views. To avoid confusion with multi-frame motion segmentation, we use the term \"multimodel clustering/motion segmentation\" hereafter. A detailed review on the current status and progress can be found in [39] . While extensive efforts have been dedicated to supervised multiview/model learning [40] , that for the unsupervised case, in particular, clustering, is much less touched. We focus on the spectral framework for clustering [14] , under which there are roughly two genres of multi-view approaches. The first kind discovers an optimal combination to aggregate multiple affinity matrices (kernels) for spectral clustering [41] , [42] , [43] . However such combination is often non-trivial to discover. Alternatively, studies have been carried out on discovering a consensus on multiple kernels. In particular, the co-regularization scheme [44] was proposed to force data from different views to be close to each other in the embedding space for clustering. Few if any of the existing approaches can guarantee superiority to the simple approach-kernel addition. In this work, we start our evaluation with this simplest baseline and then reveal its relation with the co-regularization schemes. We also evaluate a custom-built version incorporating a subset constraint that preserves the true hierarchical structure of the affinity matrices induced by different geometric models. Model Selection This refers to the problem of estimating the number of clusters. We focus primarily on the model selection works in motion segmentation. One approach selects the number of models based on the multiplicities of the zero eigenvalue of the Laplacian matrix [14] , or in the noisy case, using the gap heuristics [11] , the silhouette index [45] or soft thresholding [15] . As opposed to direct spectral analysis, another line of works have been developed for motion segmentation [2] , [4] . They first oversegment, i.e. estimate a higher number of clusters, and then a merging scheme is applied to obtain the final model parameter. These approaches achieved competitive performance on motion segmentation tasks; however, they lack a strong theoretical underpinning, e.g. the degree of over-segmentation is rather arbitrarily determined. The SCAMS approach [13] offers a principled characterization of the trade-off between various terms, specifically, a data fidelity term akin to our reconstruction error, and model complexity in terms of the rank and L 0 norm of the reconstructed affinity matrix [13] . Our work is similar in spirit in that we also have a global objective function, but the difference lies in that our data fidelity term is normalized, and instead of using rank and matrix norm to characterize the goodness of the clusters, we adopt the classical criteria of intra-cluster coherence and intercluster dissimilarity. These two well-known criteria introduced by Shi and Malik [46] have been shown to be well-approximated by the rate of loss of relevance information [47] , defined in the Information Bottleneck clustering method [48] as a representation of model complexity. Model selection was also considered in the 2D motion segmentation work [33] by exploiting a 2D spatial smoothness constraint. However, such assumption is often not true in 3D motion segmentation tasks."], "rq": ["her finger on the exact manner how the resulting affinity matrix is ill-suited for subspace clustering: is it solely due to the degeneracies or are there other factors?"]}
{"intro": [], "relatedWork": [], "rq": [" study coalition formation with a focus on the optimization activity: how do computational limitations affect which coalition structure should form, and whether that structure is stable?"]}
{"intro": ["In many applications, f is relatively small compared with N , however M can also be a small number. Thus the condition M \u00bf2f + 1 imposes a severe limit on the number of detectable faults. For instance, in case M = 3, only one corrupted page is allowed even though the \u00ffle has thousands of pages. In this paper, we replace the restriction M \u00bf2f + 1 with a much weaker assumption that for each page the majority of copies are correct. Let us point out that this assumption is needed for all detection schemes allowing identical errors and not assuming the existence of an incorruptible site, since otherwise we may not be able to distinguish between the correct pages and the corrupted pages and thus detection failure occurs. Under this assumption, we [8] came up with a non-optimal detection scheme for M \u00bf4 that requires the transmission of at most (M \u2212 1)min{N; f} + min{1 + f ; 1 + (M + 1)=2 }min{N \u2212 f; f} signatures."], "relatedWork": [], "rq": [" we close with a natural question: what is the best lower bound for m = 3?"]}
{"intro": [], "relatedWork": [], "rq": [": is well equipped for this task, but constraining equations involve looking at the structure of other possible parse trees. (in this respect they are reminiscent of the feature specification defaults of gpsg.) the approach of the present paper has been driven by the view that (a) models capture the essence of lfg ontology, and, (b) the task of the linguist is to explain, in terms of the relations that exist within a single model, what grammatical structure is. most of the discussion in kaplan and bresnan (1982) is conducted in such terms. however constraining equations broaden the scope of the permitted discourse; basically, they allow implicit appeal to possible derivational structure. in short, in. common with most of the grammatical formalisms with which we are familiar, lfg seems to have a dynamic residue that resists a purely declarative analysis. what should be done?"]}
{"intro": ["In this article, we study authentication of classical information in this quantum-secure model. Here, the adversary is granted quantum query access to the signing algorithm of a message authentication code (MAC) or a digital signature scheme, and is tasked with producing valid forgeries. In the purely classical setting, we insist that the forgeries are fresh, i.e., distinct from previous queries to the oracle, so that the security definition does not become vacuous. When the function may be queried in superposition, however, it's unclear how to meaningfully reflect this constraint that a forgery was \"unqueried\" without ruling out natural, intuitive attacks. For example, consider a uniform superposition query. Simply measuring the output state to get a forgery-a feasible attack against any function-should not be considered a break. On the other hand, an adversary who uses the same query to discover some structural property (e.g., a superpolynomial-size period in the MAC) should be considered successful. Examples like these indicate the difficulty of the problem. How do we correctly \"price\" the queries? How do we decide if a forgery is fresh? Furthermore, how do we do this in a manner that is consistent with these examples, and many others? This problem has a natural interpretation that goes well beyond cryptography: What does it mean for a classical function to appear unpredictable to a quantum oracle algorithm? 1 Previous approaches. The first approach to this problem was suggested by Boneh and Zhandry [5] . They define a MAC to be unforgeable, if no adversary can use q queries to the MAC to produce q + 1 valid inputoutput pairs except with negligible probability. We will refer to this notion as \"BZ security\" (and k-BZ for the case where the adversary is permitted a maximum of k queries). Boneh and Zhandry prove a number of results about this notion, including that it can be realized by a quantum-secure pseudorandom function (qPRF)."], "relatedWork": [], "rq": [" this problem has a natural interpretation that goes well beyond cryptography: what does it mean for a classical function to appear unpredictable to a quantum oracle algorithm?"]}
{"intro": ["projected data rates for 4G wireless data communication are expected to increase 50 times over current 3G standards [27] . These performance and energy demands are in direct conflict with an increasingly important characteristic, postprogrammability. High performance and low power can often be achieved using hardwired solutions, e.g., applicationspecific integrated circuits or ASICs. Modern embedded systems generally employ ASICs for the most compute-intensive tasks. However, a programmable solution offers several key advantages. First, software implementations allow the application to evolve in a natural way after the chip has been manufactured due to changes in the specification, bug fixes, or the addition of new features. Second, multi-mode operation is enabled by running multiple different applications or variants of applications on the same hardware. Third, time-to-market of new devices is lower because the hardware can be re-used. And finally, chip volumes are higher as the same chip can support multiple products in the same family.", "A key question that this paper investigates is: How much programmability is really required in a design? Programmability is generally thought of as a binary issue -either a design is programmable or not. Programmable designs support a wide range of applications while hardwired designs support a single algorithm implementation. An important insight is that semiprogrammable solutions may be enough for many embedded designs. For example, video coding standards are typically developed years ahead of time by industrial consortiums [14] . These standards go through many rounds of development and adjustment, but the core algorithm kernels often evolve at a relatively slow rate. At the same time, domain-specific hardware is often essential to achieve the necessary performance and energy efficiency. And, this customized hardware is neither appropriate nor efficient for applications outside the domain. Therefore, providing universal programmability may have little practical value. Our approach is to push programmability into a highly customized hardware substrate to retain the high performance and energy efficiency of an ASIC, while offering a limited degree of post-programmability. The starting point is a stylized loop accelerator (LA) that is customized for a single application loop nest [23, 9] . The LA is a direct hardware realization of a modulo scheduled loop [22] . Each LA has a specialized datapath, including function units, register files, and interconnect, and a simple controller driven by the initiation interval of the schedule. We generalize the structure of the base LA template to create a semi-programmable solution, termed a programmable LA or PLA. However, the PLA datapath is still highly specialized with point-to-point interconnect, fixedcapability function units, and limited storage to retain its inherent efficiency characteristics. Such a platform cannot execute an arbitrary loop. Rather, the programmability objective is to map loops with similar computation structure onto a common hardware platform, such as two loops from the same application domain or a single loop that has undergone small to modest changes in composition."], "relatedWork": [], "rq": ["a key question that this paper investigates is: how much programmability is really required in a design?"]}
{"intro": [], "relatedWork": [], "rq": [" one question explored in this paper is: what other families of ranges have \u03b5-samples with size near-linear in 1/\u03b5?"]}
{"intro": ["Prior feature-based methods for this task (Kambhatla 2004; Zhou et al., 2005) employed a large amount of diverse linguistic features, varying from lexical knowledge, entity mention information to syntactic parse trees, dependency trees and semantic features. Since a parse tree contains rich syntactic structure information, in principle, the features extracted from a parse tree should contribute much more to performance improvement for relation extraction. However it is reported (Zhou et al., 2005; Kambhatla, 2004 ) that hierarchical structured syntactic features contributes less to performance improvement. This may be mainly due to the fact that the syntactic structure information in a parse tree is hard to explicitly describe by a vector of linear features. As an alternative, kernel methods (Collins and Duffy, 2001) provide an elegant solution to implicitly explore tree structure features by directly computing the similarity between two trees. But to our surprise, the sole two-reported dependency tree kernels for relation extraction on the ACE corpus (Bunescu and Mooney, 2005; Culotta and Sorensen, 2004) showed much lower performance than the feature-based methods. One may ask: are the syntactic tree features very useful for relation extraction? Can tree kernel methods effectively capture the syntactic tree features and other various features that have been proven useful in the feature-based methods?"], "relatedWork": ["The features used in Kambhatla (2004) and Zhou et al. (2005) have to be selected and carefully calibrated manually. Kambhatla (2004) use the path of non-terminals connecting two mentions in a parse tree as the parse tree features. Besides, Zhou et al. (2005) introduce additional chunking features to enhance the parse tree features. However, the hierarchical structured information in the parse trees is not well preserved in their parse treerelated features."], "rq": [" one may ask: are the syntactic tree features very useful for relation extraction?"]}
{"intro": ["How can we increase work quality in micro-task platforms? Increasing payment can improve individual performance, but paying more actually reduces quality after a certain point because it incentivizes speed [12] . Requesters can also increase quality by aggregating answers from multiple workers [2, 16, 23] , filtering spammers by inserting test problems with known solutions [19, 37] , or using intelligent redundancy to simultaneously filter and aggregate [18] . Moreover, providing context about the rationale for a task can be a powerful motivator [5] . This paper explores the value of providing real-time assessment to help motivate and teach online workers to produce high-quality results. In many micro-task platforms, requesters and workers remain largely anonymous to each other, and little direct interaction occurs between them. We hypothesize that task-specific feedback will help workers in micro-task markets perform better, learn over time, and persevere longer. Concrete expert feedback can help people understand and strive toward key metrics in a task domain [28, 29, 31] . Assessments also provide motivation by making workers cognizant that other people are judging their work [1] . However, external feedback requires the time of someone with sufficient domain knowledge. Alternatively, a well-structured self-assessment may be able to provide guidance without the additional overhead [34] . Selfassessments can help workers see how their work aligns with key performance criteria and may be less discouraging than external assessments [7] ."], "relatedWork": [], "rq": ["specificity: how detailed should feedback be?", "specificity: how detailed should feedback be?"]}
{"intro": [], "relatedWork": [], "rq": ["the second issue in evaluating our hypothesis is one of measurement: how can improvements in usability be determined?"]}
{"intro": [], "relatedWork": ["However, FCM misses geographical factor in its design. For example, consider that residential area Type 27 is the same and behaves in the same way wherever it happens to be located. But what happens if Type 27 areas respond differently depending on their map locations? To overcome this shortcoming, Feng and Flowerdew [4] proposed an extension to the fuzzy clustering technique, which provides for the ex post facto adjustment of the cluster membership values based on \"neighbourhood effects\". The neighbourhood effects incorporate geography into the model. The neighbourhood effects formula adjusts the cluster membership as shown in equation (1), A is a factor to scale the \"sum\" term to the range 0 to 1.", "However, Feng and Flowerdew's neighbourhood effects have some limitations. Firstly, they ignore the effects of areas which have no common boundaries. Secondly, they exclude the effects of population -a key Geo-Demographic consideration. To overcome these limitations, a modified version of the cluster membership adjustment was proposed that incorporates a spatial interaction effect model [5] . Originated from the principles of geographical spatial interaction [2] by incorporating a basic spatial interaction model into the weighting of the memberships as well as included the neighbourhood effects in fuzzy clustering algorithm, this makes cluster centers \"geographically aware\"."], "rq": [" consider that residential area type 27 is the same and behaves in the same way wherever it happens to be located. but what happens if type 27 areas respond differently depending on their map locations?"]}
{"intro": ["T HIS paper adresses the problem of statistical unsupervised image segmentation and its main purpose is to deal with nonstationary images. The nonstationarity considered in this paper is understood in the following probabilistic sense. In the classical Markov field context, the distribution of the hidden field is defined by some functions specified on cliques; a field will be considered nonstationary when these functions depend on the position of the cliques in the set of pixels. Therefore, nonstationary fields will produce, on average, nonstationary images in which the visual aspect of the spatial organization of different labels varies with pixels. Of course, a nonstationary field can produce a stationary image and a stationary field can produce a nonstationary image. However, real images are often nonstationary and, thus, on average, using nonstationary fields in different statistical processing should give better results. While doing so, an important problem arises: How do we estimate the parameters when they vary with pixels? Answering this question via the recent \"triplet Markov field\" model is the main purpose of the paper. Starting from the classical hidden Markov fields (HMF), we propose different extensions allowing one to deal with nonstationary images and possibly non-Gaussian correlated noise. The classical HMF and Bayesian segmantation based on them can be of outstanding efficiency when dealing with the difficult-and importantproblem of unsupervised image segmentation. Hundreds papers have been written on the subject since the seminal articles [14] , [22] and a rich bibliography can be seen in [11] , [16] , [30] , [32] , [37] , among others. In such models, we have the hidden Markov field, X \u00bc \u00f0X s \u00de s2S , and the observed one, Y \u00bc \u00f0Y s \u00de s2S , and the problem is to estimate X \u00bc x from Y \u00bc y. The first models, in which X is a Markov field and the random variables \u00f0Y s \u00de are independent conditionally on X, can give good results in many situations; however, they turn out to be too simple when considering very complex images (nonstationary, textured, strongly noisy, . . . [25] ). A pairwise Markov field (PMF) model has then been proposed, which consists of directly considering that the pair Z \u00bc \u00f0X; Y \u00de is a Markov field [34] . This implies that both conditional distributions p\u00f0yjx\u00de and p\u00f0xjy\u00de are Markovian: The former fact allows one to better model complex noises and the latter one still enables one to apply Bayesian segmentation. Afterward, triplet Markov fields (TMF) were proposed in which one introduces a third random field, U \u00bc \u00f0U s \u00de s2S , and assumes the Markovianity of the triplet T \u00bc \u00f0X; U; Y \u00de [1] , [35] . This third field can have some physical interpretation or not; however, when the set of its values is not too large, analogous Bayesian processing can still be used to estimate X \u00bc x from Y \u00bc y. Different ways of defining such TMF are described in [1] , along with a parameter estimation method making possible unsupervised Bayesian segmentation."], "relatedWork": [], "rq": [" an important problem arises: how do we estimate the parameters when they vary with pixels?"]}
{"intro": ["While the use of data-driven LECs provides a paradigm shift in the ability to create adaptive systems, it also presents challenges in testing and assurance. For example, there are no established analogues to path coverage-based testing mechanisms for components designed with neural networks. There has been ongoing research in designing tools for automated testing [6] , [7] of Deep Neural Network driven systems; however, they are limited by the exhaustive test case scenarios they support, and hence, may not be able to detect all the edge cases. In addition, existing verification tools [8] can only handle some types of activation functions, and Neural Network of limited complexity.", "However, Simplex Architectures do not provide a method to combine two unverified controllers. Applying a simplex strategy in such a scenario may not improve the systems safety (for such scenarios we introduce weighted simplex strategies). Additionally, it does not consider the different operational modes and contexts of the working environment in performing the arbitration, which could be crucial for the systems performance. Biswas, Gautam, et al. [13] have shown that mode detection is a crucial problem and datadriven anomaly detection methods should be context sensitive. They also do not provide any confidence metric that can be used to evaluate the decisions of the LEC if safety violations occur. Such diagnostic capabilities are crucial in safety-critical systems. We seek to address the following research questions: ) Can we use an online simplex supervisor that can learn from past actions (experience) and augment the control actions taken by the LEC to improve safety? 2) Can we provide a confidence metric about the safety of current actions at system level in real-time, given all the past actions?"], "relatedWork": [], "rq": [" 2) can we provide a confidence metric about the safety of current actions at system level in real-time, given all the past actions?"]}
{"intro": [], "relatedWork": [], "rq": ["q6: what is the effect of our clause evaluation time-out on the complexity and quality of the models for each of the algorithms?"]}
{"intro": [], "relatedWork": [], "rq": ["0] can be used. can this be formalized to a method for removing vertices not in generating faces?"]}
{"intro": [], "relatedWork": [], "rq": ["this question is very interesting from the point of view of the semantic map methodology and can be reformulated in this way: is the same semantic map valid both for grammatical morphemes and word formation patterns?"]}
{"intro": ["Rational approximations are rather seldom used for approximating functions in the common math libraries, because division is much slower than multiplication. There are no hardware-oriented methods for rational functions, with the notable exceptions of the E-method [7, 8] or approaches which use conventional arithmetic operations and tablelookups [13] . The E-method, although intended for hardware implementation, can be viewed as a general approach for software implementations. However, the method, as reviewed shortly, is not directly applicable to any rational function.", "where \u2206 is the size of the overlap between the selection intervals where the choice of the digit is made. The E-method requires that the above defined bounds \u03be, \u03b1, and \u2206 satisfy . Those bounds are restrictive, but in practice they present no difficulties for polynomials since there are scaling techniques to achieve the required bounds. However, this is not the case for rational functions in general. In certain cases a scaling can be found but not for all rational functions. To remove this limitation we propose to derive rational function approximations, called simple E-fractions, which are products of a power of 2 by a fraction that satisfies the bounds (2) and (3). In [4] , we give a more general definition of E-fractions, where some additional scaling is allowed."], "relatedWork": [], "rq": ["there remains one problem: how to choose the points?"]}
{"intro": ["A more recent proposal was the work of Anshel et al. [4] , which can use essentially any nonabelian group as the platform. In their original paper, the authors adopted braid groups. However this choice made the protocol susceptible to various attacks, some of them quite successful (e.g., [13] ; see also [33] for a survey)."], "relatedWork": [], "rq": ["looking for instantiations of lhn: what makes lpn/lwe hard?"]}
{"intro": [], "relatedWork": [], "rq": ["1. how to handle more than two streams?", " 2. how to handle learning in high dimensional spaces?", " 3. how to handle long term temporal dependencies?"]}
{"intro": ["The business goals, required functionalities and quality attributes are also considered as the aspects influencing the design and specification of reference architectures [7] . The definition of a reference architecture, however, is not a direct response to a need for a system. Rather, it is an estimation that the existence of a reference architecture will facilitate the work of a design team in multiple projects or, even more, the work of multiple design teams in a specific domain. The usage of a reference architecture (we call this \"success of a reference architecture\") is determined by its design qualities and the \"good\" estimation of the assignees (i.e., the stakeholders deciding to design the architecture) for the initiation of the design project in a proper context and with goals properly reflecting this context. Thus, for designing an effective architecture the assignees should consider not only the systems-to-be-designed when defining goals but also the context in which the architecture is defined and the goals of the architecture itself."], "relatedWork": ["The literature on reference architectures is scarce. Definitions and brief explanations on reference architectures are provided in [6] and [26] . In [14] , the authors acknowledge the lack of clear understanding of reference architectures and provide a multidimensional classification of reference architectures. The classification is, however, ad-hoc and mainly based on practical experiences. In [21] , an overview of reference architectures is presented. Similar to [14] , the authors concentrate on the description of the many facets of reference architectures. The goal of the paper is to \"create guidelines for the content of a reference architecture and the process to create and maintain it\". The dimensions discussed in [21] are less explicit compared to our work and have a descriptive goal. The dimensions discussed in our paper are clearly structured and are defined to serve as a framework for the classification of the level of congruence of goals, context, and design of reference architectures. In [21] , the authors acknowledge the need for congruence of different dimensions for the definition of successful reference architectures but do not state requirements with respect to these dimensions.", "The design of reference architectures with congruent business goals, functionalities and qualities for software product line architectures is addressed in [25] and [1] . However, this work is limited for the concrete context of product families discussed in Type 2 reference architectures (see Section 3.1)."], "rq": ["g1: why is it defined?"]}
{"intro": ["However, historically, interrupt handling was first implemented in hardware processors before Unix even existed in order to support manual interruption of long-running (or badly behaving) programs without shutting down the entire computer. The circuits in hardware that save the state of a task, restore the state of another task and switch control to it, were only a necessary feature when the main computation tasks and \"operator\" tasks like shells or debuggers needed to share the same processor. Unix then fortuitously piggy-backed on this hardware feature to implement time sharing and system calls, and this arrangement has persisted to this day. This begs the question: what if... What if cores were so cheap and so numerous that time sharing of single hardware threads wasn't a concern? What if cores were cheap and numerous because they were small, and the reason for that is that we could drop hardware support for interrupts and privileged code in all but a few of them? (1) Could we run Unix on that, i.e. reuse existing software as-is on the simple cores modulo recompiling the source code? (2) Then, what would be the benefits? (3) These are the three main questions answered in this article. We first provide an answer to (1) by summarizing our perspective on the current architectural landscape in section 2 and describing precisely in section 3 why manycore chips would benefit from improved support in Unixlike systems. In section 4 we then describe a positive answer to (2) by defining a general machine model, AM 3 , able to support Unix on different many-core architectures. Section 5 then describes a proof-of-concept implementation on top of an emerging many-core architecture which was designed without support for interrupts. To answer (3), we start in section 5.4 by showing experimentally the benefits of our own implementation on common system tasks. We then compare our approach to related work in section 6. We discuss in section 7 the other indirect benefits that we have discovered and directions for future work, and conclude in section 8."], "relatedWork": [], "rq": [" this begs the question: what if... what if cores were so cheap and so numerous that time sharing of single hardware threads wasn\\'t a concern?"]}
{"intro": [], "relatedWork": ["We review related work from several areas: large scale feature selection, decentralized optimization and consensus-based learning, and discovery of feature subsets by ILP engines. Large scale feature selection Techniques for selecting from a (large) but finite set of features of known size d 11 have been well-studied within the machine learning community, usually under the umbrella-terms of filter-based or wrapper-based methods (see for example, John et al. 1994; Liu and Motoda 1998) . While most of the early work was intended for implementation on a single machine, several algorithms have been proposed to enable feature-selection from massive datasets (Garcia et al. 2006; Lopez et al. 2006; Sun 2014) . Singh et al. (2009) propose a framework for handling feature selection for logistic regression by developing a new forward feature selection heuristic that ranks features by their estimated effect on the resulting model's performance. Zhao et al. (2012) describe an algorithm that selects features based on their ability to explain data variance. Zhou et al. (2014) present a framework for Parallelizable Feature Selection (PFS) which is inspired by the theory of group testing. Group testing is a combinatorial search paradigm where the goal is to identify a small subset of relevant items from a large pool of possible items. The feature selection problem in the group testing framework applies a \"test\" to a set of features which produces a score designed to measure the quality of the features. From the collection of test scores the relevant features are supposed to be identified. PFS has several similarities to the algorithm proposed in this paper -notably, the set of features at each node can be viewed as a collection of relevant features for group testing; the process of local function evaluation can be mapped to the \"test\" required in group testing. The end product, however, in the two algorithms is fundamentally different-in PFS, all feature sets are identified in advance without knowing the scores of other tests and a final subset of features is discovered; nodes executing the DFE algorithm have updated scores of the local features after gossiping with neighbors. The updated scores help learn approximations of the global objective and nodes independently reach a consensus."], "rq": [" we are interested in the following question: is it possible to arrive at a consensus model, starting from multiple models using different feature-sets (which may have common elements). as we shall see, when each node constructs a local linear model, the iterations of a consensus-based algorithm (described next), result in all nodes in the network exchanging feature weights and moving to different states, but finally converging on the optimal weights for all the features. 3 on the face of it, this would appear to contradict the fischer, lynch, paterson (flp) result (fischer et al. 1985) that asserts the impossibility of reaching consensus in a distributed system. the consensus problem described in flp involves an asynchronous system of processes, some of which may be unreliable. the question then is: how can the reliable processes have a consistent view of the system?"]}
{"intro": ["This is proved by using the fact that region R 1 is defined to include the region of large real q, and then requiring the expression (1.4) to match the general expression (1.3) and satisfy the property that the coefficient of the leading term, q n , is unity. For some families of graphs, the c j and a j are polynomials in q. However, there are also families of graphs, such as the L y = 3 cyclic and M\u00f6bius strips of the square and kagom\u00e9 lattices [36, 39] (see below for notation) which have nonpolynomial algebraic a j 's with polynomial c j 's, and families of graphs, such as certain strip graphs of regular lattices with free transverse and longitudinal boundary conditions [30, 31] , as well as the L y = 2 M\u00f6bius strips of the triangular lattice and asymmetric homeomorphic expansions of the L y = 2 M\u00f6bius square strip [36, 39] for which some a j 's and their respective c j 's are both algebraic, nonpolynomial functions of q. When the a j 's are algebraic nonpolynomial functions of q, this happens because these are roots of an algebraic equation of degree 2 or higher. There are then two possibilities: first, the roots may enter in a symmetric manner, and, because of a theorem on symmetric polynomial functions discussed below, their coefficients are all equal and are polynomial functions of q."], "relatedWork": [], "rq": ["9. is b compact (which in our context is synonymous with the property of boundedness) in the q plane, or does it extend to complex infinity in some directions?"]}
{"intro": [], "relatedWork": [], "rq": [" s: is it the case that: your answer to s is 'yes' ?"]}
{"intro": ["In the new situations, users play more and more importance to intelligence of sensor network systems because of the growing actual demands. But it is very difficult to implement the intelligence due to seriously limited resources on nodes and the lack of historical sensed data. This is particularly true in the healthcare monitoring applications based on WSNs. Fortunately, system detection/identification accuracies and intelligence can be greatly improved if multisource data fusion technologies are fully leveraged. However, if so, some challenges still exist below. (1) How to select and evaluate those multisource sensing parameters to fuse? (2) How to establish novel data fusion model/mechanism to improve detection accuracy? (3) How to leverage limited resources on nodes to automatically recognize complex parameter patterns?"], "relatedWork": [], "rq": [" (1) how to select and evaluate those multisource sensing parameters to fuse?", " (2) how to establish novel data fusion model/mechanism to improve detection accuracy?", " (3) how to leverage limited resources on nodes to automatically recognize complex parameter patterns?"]}
{"intro": ["While previous research (Anari & Ghaffarof , 2013; ATA, 2015; Baker, 2011; Colina, 2003 Colina, , 2008 Colina, , 2009 Colina, , 2015 Doyle, 2003; Drugan, 2013; Hatim & Mason, 1990; House, 1977 House, , 1997 House, , 2015 Kiraly, 2005; Melis & Albir, 2001; Nord, 1991 Nord, , 1997 Nord, , 2005 Williams & Chesterman, 2002) indicated that the evaluation of translation is relevant in three areas of translation, which embrace the evaluation of published translations, the assessment of professional translators' works, and evaluation in translation teaching, the current study focused on the area of Translation Quality Assessment (TQA) using linguistic functional approaches. The relationship between translation and linguistics urges researchers to continue examining how such a connection can impact the quality of translation. Assessing the quality of translation through employing authentic tools is one of the controversial issues in the fields of translation and applied linguistics. The process of quality assessment refers to the process of collecting empirical data to measure how certain standards can be achieved through using multifaceted objective assessment criteria. However, the absence of valid and consistent criteria for evaluating the quality of translation requires further investigation. Although the major component of the translation process is that translation is always discussed, criticized, and evaluated, however, the questions are: what criteria can be used to carry out the assessment process? How can a critic provide an assessment report that is objective and inclusive? Should the assessment of the quality of translation depend solely on a comparison between the original and target texts? What are the bases of such a comparison? What are the objective references for assessing the quality of translation? The current study tried to provide answers to such questions through examining the attempts the that were made to create objective criteria into the evaluation of translation, including the linguistic-based approaches such as Reiss' (1981 Reiss' ( , 2004 objective-relevant criteria, Nord's (1991 Nord's ( , 1997 Nord's ( , 2005 didactic model, Colina's (2003 Colina's ( , 2008 Colina's ( , 2009 Colina's ( , 2015 functional-componential approach, House's (1977 House's ( , 1997 House's ( , 2015 functional-pragmatic model, and Schaffner's (2011) text-linguistic approach. The current research also investigated the theories of equivalence while exploring the application of some functional linguistic models for assessing the quality of translation."], "relatedWork": [], "rq": [" the questions are: what criteria can be used to carry out the assessment process?"]}
{"intro": ["Fixing defects associated with tolerances is time consuming, costly and onerous (Milberg and Tommelein 2005) . In spite of increasing calls for waste reduction and an improved quality of buildings, Forcada et al. (2016) estimate that tolerance-related defects are amongst the most common and recurring defects in construction projects and make up more than 9 percent of the overall number of defects. One of the factors that can help minimise defects is to improve inspection methods used by surveyors and engineers (Yates and Lockley 2002) . More specifically, defects associated with tolerances, called tolerance problems hereafter, can be mitigated by changing the inspection techniques and gaining better control of the magnitude of dimensional and geometric variations (Landin 2010) . Conventional inspection methods use sampling techniques (Phares et al. 2004) , some of them are limited to the need for surface contact (Bosch\u00e9 and Guenet 2014) , and they depend on inspectors' subjective assessments . However, the use of conventional inspection methods has remained time-consuming, laborious, and therefore ineffective, although some of them are relatively accurate (Phares et al. 2004) . As a result, such methods often cannot identify tolerance problems early and comprehensively during the construction process (Akinci et al. 2006) and results obtained from them may not be reliable (Phares et al. 2004) . For instance, when assessing the flatness of concrete slabs by using the total station, only a few points representing the whole surface are collected. The elevations of the collected points are measured to determine their vertical deviations from the nominal elevation . Such method gives an incomplete and sometimes incorrect understanding of the achieved flatness to surveyors because surfaces that have higher deviations than permissible limits may not be controlled (Bosch\u00e9 and Guenet 2014) . Communication of surveying results obtained from conventional methods is another problem area as inspectors may have different approaches to report the results (Anil et al. 2013) . The lack of an effective standard method for communication may result in misinterpretation among project participants (Phares et al. 2004 ).", "The terrestrial laser scanner (TLS) has been proven to be useful for a variety of applications including deviation analyses. Various methods for: (a) data acquisition (e.g., Wilkes et al. 2017) , (b) registration (e.g., Olsen et al. 2009 ), (c) deviation analyses (e.g., Holst and Kuhlmann 2016) , and (d) visualisation/demonstration of deviation analyses (e.g., Anil et al. 2013 ) have been proposed. However, a review of the literature reveals that there is not any current research work that proposes a holistic process consolidating these four independent fields of research for measuring geometric variations. To improve the accuracy of data registration and data analyses, and to improve the interoperability of results, it is suggested to have a formal process specifically for measuring geometric variations using TLS. Such a process must be holistic, that is it should start from data acquisition and extend to visualisation/demonstration of analyses. This is because the accuracy of deviation analyses depends on the way data is collected and registered; the accuracy of deviation analyses is also reflected in visualisation/demonstration. Hence, it is not a sufficient practice to consider these steps independently, especially when using them to measure variations that require a high level of accuracy. Moreover, most of the existing research works in this realm of research are about the assessment of surface flatness, whereas the capability of existing commercially available software for deviation analyses can also be used to assess other types of geometric variations (Nahangi and Haas 2014) . Here the question arises: What method of deviation analysis is most suitable for each type of tolerances? This question can be addressed if types of tolerances are welldefined and they are associated with different methods of deviation analyses."], "relatedWork": [], "rq": [" here the question arises: what method of deviation analysis is most suitable for each type of tolerances?"]}
{"intro": [], "relatedWork": [], "rq": [" we have been able to formulate the tri-crit problem: how to minimize the energy consumed given a deadline bound and a reliability constraint?"]}
{"intro": [], "relatedWork": [], "rq": [" this raises one key challenge of future actuated devices: how much should actuated devices be automated/ be controlled by the user?"]}
{"intro": ["The highly personal and dynamic nature of genomic data raises important HCI questions, including: what are the functional requirements for supporting meaningful engagement of consumers with personal genomic information? How can we design effective interaction with personal genomic information? How can we evaluate the effectiveness of techniques for interaction with personal genomic information? Though prior studies have sought to answer these questions in the context of other types of personal informatics, we assert that interaction with personal genomic data is unique. In other forms of personal data, the dynamic element is the data itself, which is usually sampled at intervals over time with the objective of creating an incremental feedback loop to influence an individual's behavior [18] . Genomic data, on the other hand, are largely stable during a person's lifetime. The certainty of the evidence, its interpretation, and related implications for the user's health, however, often change over time as new medical research exposes new relationships between people's genetic make up and their health."], "relatedWork": ["There are a number of studies that investigate the motivation for and subjective experience of genetic testing, and of using interactive tools to understand results (e.g. [26] [10] [8] ). However, these studies tend not to look at the relationship between this experience and specific design interventions. Direct lay-user engagement with personal genomic information has been relatively understudied in the HCI field. Existing research tends to focus on participants' comprehension of anonymous reports from a variety of perspectives, or on exploring novel interaction techniques for manipulating large volumes of biological data. Lachance et al. [16] examined the informational content, literacy demands, and usability of DTCGT service websites. They find that websites vary widely, and most users would struggle to use these resources effectively. The Other studies have developed ways of interacting with large-scale and complex biological datasets and use them as a platform to explore novel interaction techniques, such as tangible interaction [31] . Systems developed include a tangible interface for designing new DNA molecules [26] , and several tabletop interfaces for interactive visualization of biological datasets, such as DeepTree [4] and PhyloGenie [29] . G-nome Surfer [30] is a tabletop interface for collaborative exploration of genomes; however, it was not designed to support users in the exploration of their own genomic data."], "rq": [" including: what are the functional requirements for supporting meaningful engagement of consumers with personal genomic information?"]}
{"intro": ["The issue of studying human language and how it is acquired started several centuries back and those \"caring\" for such an issue include many philosophers, beginning with Plato and ending with Quine, psychologists beginning with Mendel and ending with Pinker; linguists beginning with Siibawayh, and ending with Chomsky, evolutionists beginning with Darwin and ending with Dennett, biologists beginning with Beddoes and ending with Gould, and many other scientists. Knowledge of language and its acquisition has also been the concern of several approaches such as behaviorism advocated by Skinner, structuralism advocated by Saussure, cognitivism, advocated by Paiget, mentalism advocated by Chomsky, etc. However, the views these theories and approaches have come up with were not satisfactory. For example, Chomsky (1959) severely criticizes Skinner's views and assumptions on language acquisition. In fact, Chomsky was puzzled by Skinner's ideas and assumptions of how http://journals.uob.edu.bh language is acquired. The latter 'equalizes' learning simple tasks by animals, a chimpanzee, for instance, with simple mental capabilities (if any), to learning language by human, a child, for example, who is a very sophisticated creature, with very high mental abilities and capacities.", "The latter provides imprinting as the most striking evidence that there is an innate disposition in bees, for example, when they learn how to point to a particular direction by means of a special dance, or birds when they respond appropriately to a special song. He also stresses that all these patterns of animals' behavior can be learned without the need to be rewarded. However, though menatlism accounts for several facts involved in language acquisition process, it also fails to account for certain phenomena (for criticism on mentalism, see, for example, Shormani, 2014a; Pinker, 1995 Pinker, , 1997 ."], "relatedWork": [], "rq": [" the question is: how does a native speaker of english (in general and john in particular) know that?"]}
{"intro": ["Relatedly, participation patterns in some ARGs reveal how designers can miss the mark for promoting productive collective action and learning, and instead, create an echo chamber of player opinions and attitudes that may appear effective in the ARG's fictional context, but would be ineffectual or even erroneous when applied to \"real-world\" problem-solving situations [31] . For example, players in World Without Oil (WWO) [18] shared personal narratives about their efforts to survive a global oil crisis that added a human interest touch to community efforts; however, they often failed to develop effective sustainability solutions that were grounded in factual, evidence-based research [30] ."], "relatedWork": [], "rq": [" must address: what social roles, toolsets, or other components can be integrated into these experiences to promote and sustain participation?"]}
