{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "BACKGROUND AND RELATED WORK: With the recent developments in many of the sub fields of conversational AI, including machine learning, dialog management and NLU, many different conversational AI systems have emerged [22] .\nBACKGROUND AND RELATED WORK: In industry, this technology has been incorporated into search engines, mobile devices, and personal computers. In search engines such as Google and Bing, conversational AI is used to create the feeling of having a conversation with the search engine, enhancing the experience. In mobile devices and personal computers, one use of conversational AI is to create virtual assistants. Some of the biggest virtual assistants on the market today are Apple's Siri, Google Assistant, Amazon Alexa and Microsoft Cortana [14] . These assistants also have the capability of acting as chatbots where they keep a turn-based dialog (a dialog where the user and the bot take turns in asking and responding to queries) with the user. There also exist conversational interfaces that only focus on this type-dialogbased conversation such as XiaoIce [11] and Replika [11] . These dialogs use what is known within conversational AI as intents and entities to understand the user's goal behind the query. In other words, an intent is what the user wants to achieve with the query, and an entity is the key information for answering the intent.\nBACKGROUND AND RELATED WORK: Recently a number of different platforms have been made available to simplify the creation and integration of conversational interfaces for developers. The most popular ones are: Google's Di-alogFlow (formerly api.ai) 1 , IBM's cloud-based bot service Watson Conversation 2 , Amazon Lex 3 and the Microsoft Bot Framework 4 . These platforms come equipped with several different technologies used for NLU, dialog management, response generation and other aspects [5, 11] .\nBACKGROUND AND RELATED WORK: Since conversational AI is a new field, systematic approaches to overview and categorize it are still in their infancy. Patil et al. [24] makes a general comparison of features and functionalities between some of the commercial platforms, giving an overview of what platform one might choose for developing a conversational AI system. There have also been more specific studies conducted which compare the NLU and conversational abilities of these types of platforms. Canonico and De Russis [18] compare the NLU performance of these platforms have in terms of usability, pre-built intents (a number intents already existing in the NLU tool) context etc. McTear [19] describe the two main conversation models \"oneshot queries\" and \"slot-filling dialogues\". He compares different platforms' ability to handle follow up questions in one-shot query scenarios and their mechanisms for slot-filling (a type of conversation where the bot asks specific questions to fill certain slots to fulfil a user intent). McTear also presents a number of problems that developers may face when creating conversational interfaces with these platforms. One of the main issues is that it might be difficult to know what functionalities a specific platform offers. There is also a difficulty in interpreting what functionalities might be common between platforms since there is no standard terminology. Venkatesh et al. [31] describe a number of metrics that can be used to evaluate the overall performance of a conversational agent based on the annual competition Alexa Prize [25] made for furthering conversational AI. They propose metrics such as conversational user experience, engagement, and conversational depth to measure the conversational abilities of entire conversational AI systems or chatbots [31] . Shawar and Atwell [26] describe metrics to specifically evaluate chatbot systems, a type of conversational AI interface. They argue that metrics for evaluating the abilities of these systems should be done based on the application and its domain and not solely on a standard.\nBACKGROUND AND RELATED WORK: One of the main issues with creating the metrics described above is the understanding of what a good conversation is. Clark et al. [6] discuss that people generally describe conversations with conversational interfaces in terms of their performance and perceive them more as a device to be controlled. Indicating that people have a previous notion of how these systems will behave coming from a perception that infrastructure to support proper human-to-human dialogs do not exist.\nBACKGROUND AND RELATED WORK: The maturity assessment framework presented in this paper takes inspiration from three language proficiency frameworks: Common European Framework of Reference (CEFR, [7] ), American Council on the Teaching of Foreign Languages (ACTFL, [27] ), and the Interagency Language Roundtable (ILR, [1] ). The goal of these frameworks is to assess the language competency of an individual for a particular language. All of these frameworks have a similar structure, distinguishing different, successive levels (e.g., in case of CEFR, a sixitem scale A1-C2), language-relevant skills (e.g., for CEFR, reading, listening, speaking, and writing), and a number of hints for assigning an individual to a level. While the contents of the framework differ, they all share this same basic structure, which we also found useful for inspiring the design of our framework. A number of papers have scientifically investigated these frameworks, studying their validity and the possibility to use them in an automated way [12, 15, 30] .",
    "rq": " rq1: what platforms exist for developing conversational ai systems?\n rq2: what are the features of these platforms?\n rq3: what are the levels of conversational maturity supported by the identified platforms?\n user: what is the weather like today?\n user: what is the time in new york?\n user: can you call mom?"
}
{
    "intro": "INTRODUCTION: Knowing a foreign language has many benefits and is often a requirement for academic and job opportunities [65] . In China alone, over 300 million people are estimated to be learning English [42] . While many inexpensive self-study resources exist for learning to read and write in a foreign language, opportunities to practice speaking skills are much more limited. Traditional classrooms can be inaccessible, expensive, and may offer only sparse practice opportunities to practice speaking [18] . Intense in-person prep courses are often even more expensive and inaccessible. The lack of opportunity to practice spoken English means that many people may pass written requirements and yet struggle to communicate in a way required by academic training or job opportunities [43, 66] . There is a key need to provide effective education at scale for language learners to practice speaking.\nINTRODUCTION: Chatbots are a promising tool to address this. Speech recognition and natural language processesing (NLP) advances have significantly improved chatbot technology, especially in targeted domains [20] . Chatbots can also help simulate the process of talking to another human in a more natural way than synthesized voices and potentially could offer additional benefits such as enhanced distributed cognition and social interaction abilities [35] . New language learners are sometimes shy about practicing their spoken English around other people [51] , but chatbots could provide a friendly, non-intimidating setting for spoken language practice. Chatbots may also be more engaging and fun than more traditional spoken language interfaces, such as listen-and-repeat. Perhaps for these reasons, there is emerging commercial interest in foreign language learning chatbot systems: for example, English Liulishuo is used by 150 million people worldwide [40] .\nINTRODUCTION: Indeed, in other educational settings there is encouraging evidence that chatbots can promote engagement and learning gains [17, 57, 58] . In particular, for factual knowledge learning, prior work [57] found that over a week of optional usage, subjects using a chatbot interface did substantially better on the learning material than those using a classic flashcard app. However, chatbots may also increase the amount of time learners spend with the same material, impacting efficiency of learning.\nINTRODUCTION: This suggests that it is important to evaluate and understand the impact of chatbots on spoken language learning effectiveness and efficiency. In this study, we present the first, to our knowledge, experimental study of a standard listen-and-repeat interface versus a chatbot interface (EnglishBot) for spoken language learning. The learning materials are from the standardized English examinations IELTS [61] and TOEFL [54] , which cover a range of common scenarios international students could encounter in English-speaking universities. We recruited 56 native Chinese college students who had never worked or studied abroad in English speaking countries for more than three months to participate in two six-day betweensubjects studies (one with fixed usage and the other with free usage) comparing EnglishBot against a traditional listen-and-repeat interface. We conducted three assessments to carefully measure users' improvements on memorization of vocabulary, script translation ability, and unrehearsed speaking ability.\nINTRODUCTION: Our results show that compared to a traditional listen-and-repeat interface, EnglishBot was more engaging to interact with and promoted more vocabulary learning and speaking fluency and coherence, but did not affect grammatical range and accuracy, lexical resource , or pronunciation. This work makes three contributions: (1) We design a novel conversational interface for spoken language, combining recent advances in natural language processing and speech recognition. (2) We conduct two novel empirical studies evaluating an AI-powered conversational interface against a traditional listen-and-repeat interface for second language speaking under fixed and voluntary usage conditions. (3) In light of the success of chatbots in promoting engagement, vocabulary learning, and speaking fluency, we present design suggestions based on our findings for building the next-generation of conversational interfaces that are both engaging and effective for language learning.",
    "relatedWork": "RELATED WORK 2.1 Language Learning ITSs: ITSs provide adaptive feedback that depends on the user's response and/or a model of the user's knowledge state, rather than fixed feedback regardless of user input [2, 59] . Prior studies found general ITSs can be as effective as human tutoring [63] , and that use of an ITS in a study was associated with greater achievement in comparison with teacher-led large-group instruction, non-ITS computer-based instruction, and textbooks or workbooks [41] . The inclusion of sophisticated AI modules produced learning improvements of 0.3 to 1.0 standard deviations compared with students learning the same content in a classroom [8] .\nRELATED WORK 2.1 Language Learning ITSs: German Tutor [24, 25] , later renamed to E-Tutor, was one of the first ITSs developed for foreign language learning. The focus was on building grammatical competence for introductory-level adult learners of German as a foreign language. German Tutor used NLP to parse learner responses to various exercises and provided metalinguistic feedback tailored to the learner's specific errors and perceived proficiency level. Empirical studies of German Tutor/E-Tutor found that students attend to feedback provided by the system [22] , and metalinguistic feedback leads to greater learner uptake and self-corrections than simply highlighting mistakes for both grammatical and spelling errors [23, 26] .\nRELATED WORK 2.1 Language Learning ITSs: ITSs for adult Japanese foreign language learners, Nihongo-CALI and BANZAI, have been well-documented in the literature. Through a series of empirical studies, Nagata [47, 48] demonstrated that intelligent metalinguistic feedback provided by the ITSs led to significant improvements on Japanese language tests, as compared to traditional feedback that simply highlighted mistakes. These improvements also held when the ITS was used in a simulated self-study context [49] . The prior work did not use chatbots in their ITSs for language learning.\nChatbots and Student Learning: While ITSs are able to give more detailed and intelligent feedback to learners than traditional systems, historically such systems assume a constrained set of possible answers from the learner. Chatbots provide the promise of more open-ended interaction between system and learner, in which the learner can take more control over the direction of the conversation, and the system can accommodate a very large range of potential answers. Previous work has shown that chatbots can increase learner engagement [21, 31, 32] and are effective for learning many subjects including factual information [57] , math concepts [17, 56] , and physics [14] [15] [16] .\nChatbots and Student Learning: In language education settings, chatbots and conversational agents have been used with learners of all ages. Results have consistently shown that language learners find interacting with intelligent computer agents to be engaging and enjoyable [1, 9, 30, 37, 44-46, 52, 60, 62] . There is some evidence that such systems are more approachable to learners than speaking with human partners, and their use can reduce anxiety about communicating in a foreign language [4, 9, 10, 50, 58] .\nChatbots and Student Learning: Although chatbots and conversational agents may be engaging, their actual effectiveness at improving users' foreign language conversation skills has received less study. Selected studies have shown that learners interacting with these systems can produce similar levels of short-term learning in tests of isolated grammar functions, and general speaking proficiency, as interacting with human partners [33, 34, 64] ; however, long-term learning has not been studied in detail, and comparisons with traditional non-intelligent systems are limited.\nChatbots and Student Learning: Furthermore, the heightened levels of engagement found with chatbots may represent a short-term novelty effect. In one study, English learners' interest in conducting speaking tasks with a chatbot decreased significantly after the first task, whereas interest in conducting the same tasks with a human partner stayed at the same level through three tasks [11, 12] .\nChatbots and Student Learning: Despite limited research on the pedagogical effectiveness of chatbots for foreign language learning, public interest in their development is high. In China, the English learning mobile app English Liulishuo [40] incorporates a conversational agent with adaptive feedback for users to practice dialogues. Given the already high levels of interest in developing chatbots for foreign language learning, it is important to understand whether this increased engagement is also associated with improved foreign language skills in users, as with ITSs, or whether chatbot technology is better suited as a fun activity with limited pedagogical benefit for foreign language learners.",
    "rq": " (3) how does the inclusion of chatbot elements affect user engagement with the interface?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": "try a few of these commands: is it snowing in new york?"
}
{
    "intro": "INTRODUCTION: As information becomes abundant, and access to it becomes more important, we face the problem of choosing between all the available options. Recommendation systems [1] are intended to guide users through the (potentially large) space of available options and support users in selecting the most suitable option. Most recommendation systems, in research as well as in commercial use, are text-and graphicsbased, and presentation of options is done using a display, where users can see many options at a time (when using desk or laptop computers) and/or scroll back (as when viewing options on a mobile phone) to view previously mentioned options. However, there is also great interest in information services that are intended for situations where users' eyes and hands are busy, e.g., in-car voice services, in which user preferences must be elicited and recommendations made using spoken natural language as the main interaction mode. Indeed, spoken dialogue systems (SDS) have been developed to provide information about a wide range of products and services, e.g., restaurant recommendation [7, 10] , and travel information [6] . The naturalness and perceived intelligence of a spoken dialogue interface does not depend only on its ability to recognise and analyse user utterances correctly, but also on the quality of the information presentation module. Recent work on evaluating SDS has shown that dialogue duration is negatively correlated with user satisfaction [9] , and our own analysis of the DARPA Communicator Corpus has shown that the information presentation phase is the primary contributor to dialogue length. Therefore, it is crucial that we gain understanding of how best to design information presentation strategies for spoken recommendations. In this study, two previously introduced approaches to information presentation are compared in terms of their impact on effectiveness and user satisfaction.\nINTRODUCTION: In recommendations based on the summarize and refine (SR) approach, developed by [7] and later extended by [2] , a large number of options is grouped into a small number of clusters that share attributes. Then, the system summarizes the clusters based on their attributes and suggests additional constraints to the user. For large data sets, attributes that partition the data into the minimal number of clusters are chosen, so that a concise summary can be presented to the user to refine. However, [4] argued that there are limitations to this approach. First, many turns may be required during the refinement process depending on the size and complexity of the domain. Second, if there is no optimal solution, exploration of trade-offs is difficult. Finally, the chosen clusters may be irrelevant for the specific user. This approach to information presentation is comparable to that of typical conversational recommender systems [8] which utilize conversations between the user and the system to determine and constantly update user preferences during the dialogue to arrive at a recommendation best matching the user's preferences. However, in contrast to such systems, the SR approach does not create a user model. Table 1 shows an example dialogue based on the SR approach.\nINTRODUCTION: The user-model based (UM) approach [6, 10] employs a user model and decision theory techniques to identify and present a small number of options that best match the user's preferences. Even though this approach to information presentation enables a concise presentation of a small number of options, highlighting the ways in which those options sat- isfy user preferences, it does not scale up to presenting a large number of options. When there are hundreds of potentially relevant options to consider, there may be many that fit the user model. Additionally, users may not be able to provide constraints until they are presented with more details concerning the option space. Finally, since the UM based approach does not provide an overview of the available options, it may lead to the user's actual or perceived missing out on possible better alternatives. More recently, [4] proposed the user-model based summarize and refine (UMSR) approach, which combines the benefits of the UM and SR approaches. The UMSR approach to information presentation employs a user model to reduce dialogue duration by considering only options that are relevant to the user. When the number of relevant items exceeds a manageable number, the UMSR approach builds a cluster-based tree structure which orders the options for stepwise refinement based on the ranking of attributes in the user model. The effectiveness of the tree structure, which directs the dialogue flow, is enhanced by taking the user's preferences into account. In order to provide the user with a better overview of the option space, trade-offs between alternative options are presented explicitly. We hypothesize that this also allows the user to make a more informed choice. Finally, to give users confidence that they are being presented with all relevant options, a brief account of the remaining (irrelevant) options is also provided. Thus, the UMSR approach maintains the benefits of user tailoring, while allowing for presentations of large numbers of options in an order reflecting user preferences. Table 2 presents an example dialogue employing the UMSR approach.\nINTRODUCTION: In an earlier study, researchers found significant preferences for the UMSR approach when participants read transcripts of dialogues [4] . To obtain the user judgments, participants were asked to read and evaluate transcripts of six dialogue pairs. Each pair consisted of one dialogue using the SR approach to information presentation and one using the UMSR approach. Participants were asked to judge each dialogue on the following 4 criteria:\nINTRODUCTION: 1. understandability (\"Did the system give the information in a way that was easy to understand?\"), 2. overview of options (\"Did the system give the user a good overview of the available options?\"), 3. relevance of options (\"Do you think there may be \"I'd like the second flight, please.\"\nINTRODUCTION: flights that are better options for the user that the system did not tell her about?\"), and 4. efficiency (\"How quickly did the system allow the user to find the optimal flight?\").\nINTRODUCTION: [4] found that users rated the UMSR approach significantly more highly on criteria 2-4, and found the UMSR and SR approaches equally easy to understand. This study was replicated using an \"overhearer\" technique in which, rather than reading transcriptions of dialogues, participants listen to dialogues between a \"user\" and a simulated SDS [Moore, p.c.]. Again, users preferred the UMSR approach.\nINTRODUCTION: In subsequent work, we carried out 2 studies to examine the impact of the two different information presentation methods on a secondary task, namely driving [5, 11] . In these experiments, participants actually interacted with what they thought was a spoken dialogue system, and thus we were able to assess the impact of the different approaches on effectiveness criteria such as task duration and completion. We found that the UMSR approach enables more efficient information retrieval in comparison to the summarize and refine approach, and that presenting information with UMSR did not negatively affect driving performance. However, in contrast to results of the previous studies [ [4] , Moore, p.c.] showing significant preferences for UMSR when participants were reading or overhearing dialogues, no differences between user satisfaction ratings of the two presentation methods were observed in the dual task studies. Thus, in order to find out whether the lack of differences between the user satisfaction ratings was caused by the fact that participants were actually conversing with a SDS (as opposed to simply \"overhearing\" or reading the dialogues), or whether the reason was the demanding secondary task, we conducted the following experiment in which participants only interacted with the simulated SDS.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "Introduction: Much of the work in natural language processing relies on intrinsic evaluation: computing standard evaluation metrics such as precision, recall and Fscore on the same data set to compare the performance of different approaches to the same NLP problem. However, once a component, such as a parser, is included in a larger system, it is not always clear that improvements in intrinsic evaluation scores will translate into improved overall system performance. Therefore, extrinsic or task-based evaluation can be used to complement intrinsic evaluations. For example, NLP components such as parsers and co-reference resolution algorithms could be compared in terms of how much they contribute to the performance of a textual entailment (RTE) system (Sammons et al., 2010; Yuret et al., 2010) ; parser performance could be evaluated by how well it contributes to an information retrieval task (Miyao et al., 2008) .\nIntroduction: However, task-based evaluation can be difficult and expensive for interactive applications. Specifically, task-based evaluation for dialogue systems typically involves collecting data from a number of people interacting with the system, which is time-consuming and labor-intensive. Thus, it is desirable to develop an off-line evaluation procedure that relates intrinsic evaluation metrics to predicted interaction outcomes, reducing the need to conduct experiments with human participants.\nIntroduction: This problem can be addressed via the use of the PARADISE evaluation methodology for spoken dialogue systems (Walker et al., 2000) . In a PARADISE study, after an initial data collection with users, a performance function is created to predict an outcome metric (e.g., user satisfaction) which can normally only be measured through user surveys. Typically, a multiple linear regression is used to fit a predictive model of the desired metric based on the values of interaction parameters that can be derived from system logs without additional user studies (e.g., dialogue length, word error rate, number of misunderstandings).\nIntroduction: PARADISE models have been used extensively in task-oriented spoken dialogue systems to establish which components of the system most need improvement, with user satisfaction as the outcome metric (M\u00f6ller et al., 2007; M\u00f6ller et al., 2008; Walker et al., 2000; Larsen, 2003) . In tutorial dialogue, PARADISE studies investigated which manually annotated features predict learning outcomes, to justify new features needed in the system (Forbes-Riley et al., 2007; Rotaru and Litman, 2006; Forbes-Riley and Litman, 2006) .\nIntroduction: We adapt the PARADISE methodology to evaluating individual NLP components, linking commonly used intrinsic evaluation scores with extrinsic outcome metrics. We describe an evaluation of an interpretation component of a tutorial dialogue system, with student learning gain as the target outcome measure. We first describe the evaluation setup, which uses standard classification accuracy metrics for system evaluation (Section 2). We discuss the results of the intrinsic system evaluation in Section 3. We then show that standard evaluation metrics do not serve as good predictors of system performance for the system we evaluated. However, adding confusion matrix features improves the predictive model (Section 4). We argue that in practical applications such predictive metrics should be used alongside standard metrics for component evaluations, to better predict how different components will perform in the context of a specific task. We demonstrate how this technique can help differentiate the output quality between a majority class baseline, the system's output, and the output of a new classifier we trained on our data (Section 5). Finally, we discuss some limitations and possible extensions to this approach (Section 6).",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: We developed a multi-modal dialogue system that takes advantage of the temporal congruence between gaze and spoken deictic references to acquire and update spatial knowledge about objects [1] . Our dialogue system enables the humanoid robot NAO to learn new objects and their positions from the user (Learning), and second, can instruct the user to move objects while tracking this movement (Instructing). For Learning, we combine the user's speech and gaze to infer an objects position. The link between gaze and hand movements facilitates tracking and updating position data for Instructing. We integrated a mobile and a remote eye tracker for our system. The remote eye tracker is of particular interest, as the form-factor is important (e.g., dementia day hospitals require non-obtrusive devices, see also [2] for a similar approach).\nINTRODUCTION: Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). ",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Human gaze is involved in many processes in multi-modal speech-based interaction, such as in disambiguating speech, in joint attention during collaboration and in turn-taking [6] . For instance, there is a strong link between gaze behaviour and spoken language: speakers fixate elements \"less than a second before naming them\" [2] . Further, the coordination of hand-movements involves vision, e.g., when \"directing the hand or object in the hand to a new location\" [4] . To this end, gaze is ideal for incorporating non-verbal cues to Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). human-robot dialogues, especially for complementing spatial information. We developed a multi-modal dialogue system that takes advantage of the temporal congruence between gaze and spoken deictic references to acquire and update spatial knowledge about objects. However, gaze estimation is erroneous [1] and the eye tracker's form-factor is important 1 . We conducted a user study for investigating the impact of different devices, a remote and a hear-worn eye tracker, on user performance. Figure 1 shows the setting of our study including a shared workspace (3 \u00d7 3 grid), the humanoid robot NAO and a display for calibrating the head-mounted eye tracker.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: In recommendation systems, obtaining user information is indispensable in providing services suitable for users. However, a user may not desire to answer the many questions posed by a questionnaire. On the other hand, human professionals, such as sommeliers and waiters, provide enjoyable conversations to the customers, as a result of which, they obtain customer information. They not only ask questions but also provide professional knowledge about food and wine. Dialogue systems that can perform the interaction described above are required to have broad knowledge about food and be able to provide related topics, in addition to generating questions Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). ",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "Related work: The topic of chatbots and conversational answer seeking has recently become quite popular. [10] , where 14 people were interviewed about their experience with an intelligent assistant that they use in their daily life. The authors report on people's experiences, expectations, discuss scenarios of successes and failures of conversational agents. They report that the most frequent types of tasks are relatively simple -weather updates and checking reminders. Our study, on the other hand, focuses on studying similar aspects of user behaviour for a different type of task -for complex search tasks. However, some of their findings overlap with ours. Much work has been done in the area of comparing user interactions with a human and a computer. There are varying opinions on the subject. Edwards et al. [6] found no significant differences in how Twitter users treated a social bot, whether it was perceived as a human or not. In turn, Cl\u00e9ment and Guitton [5] report that the way bots are perceived varies with the role they play. They found that \"invasive\" Wikipedia bots received more \"polarizing\" feedback -both positive and negative -compared to the bots that carried out \"silent helper\" functions. The similar result is reported by Murgia et al. [11] -Stackoverflow bot receives more negative feedback for false answers when its identity as an automatic program is revealed. Another work by Aharoni and Fridlund [1] reports mixed results from participants who underwent a mock interview with a human and an automatic system. The authors report that there were no explicit differences in the interviewer perception described by the participants, although the authors noticed significant differences in people's behaviour -when talking to a human interviewer they made greater effort to speak, smiled more, and were more affected by a rejection.",
    "rq": " rq1: what are the main expectations from a conversational search system?\n rq2: what are the differences between human-tohuman and human-to-computer conversations?\n rq3: what characteristics prevent existing conversational agents from becoming effective tools for complex information seeking?"
}
{
    "intro": "Introduction: The SimCoach project is motivated by the challenge of empowering troops and their significant others in regard to their healthcare, especially with respect to issues related to the psychological toll of military deployment. SimCoach virtual humans are not designed to act as therapists, but rather to encourage users to explore available options and seek treatment when needed by fostering comfort and confidence in a safe and anonymous environment where users can express their concerns to an artificial conversational partner without fear of judgment or possible repercussions.\nIntroduction: SimCoach presents a rich test case for all components of a dialogue system. The interaction with the virtual human is delivered via the web for easy access. As a trade-off between performance and quality, the virtual human has access to a limited set of pre-rendered animations.\nIntroduction: The Natural Language Understanding (NLU) module needs to cope with both chat and military slang and a broad conversational domain. The dialogue policy authoring module needs to support nondialogue experts given that important parts of the dialogue policy are contributed by experts in psychometrics and mental health issues in the military, and others with familiarity with the military domain.\nIntroduction: The dialogue manager (DM) must be able to take initiative when building rapport or collecting the information it needs, but also respond appropriately when the user takes initiative.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Chatbots are regarded as one of the most promising technologies and are increasingly applied in many domains. Because chatbots provide a fast, convenient, and low-cost communication channel, both scholars and practitioners are keen to develop effective chatbots to address the challenges of providing healthcare services. For example, a growing body of research demonstrates how chatbots can be useful for helping people maintain good lifestyles [29, 37] , collecting daily health information to share with healthcare providers [26, 34] , and guiding people to improve their general well-being [28, 32, 63] . For instance, Wang et al. [61] proposed a conversational agent to coach people to relieve their public-speaking anxiety through cognitive reconstruction exercises, and Fitzpatrick et al. 's [17] Woebot system gives step-by-step guidance for users to think through their situation with cognitive behavioral therapy and was found to relieve users' depression. Other recent studies have applied a variety of conversational strategies and structures to promote behavioral change and to persuade chatbot users to act differently [27, 29, 57] . Some of these systems have even been found to outperform human-human interaction in some scenarios. For example, Lucas et al. [36] found that utilizing a virtual agent as an interviewer could promote users' depth of self-disclosure, and Xu et al. [67] concluded that the use of interactive robot agents would probably enhance physical-therapy outcomes. Therefore, these prior works have demonstrated that chatbots can serve as an effective platform for delivering guidance and tutoring people.\nINTRODUCTION: Despite the success of utilizing chatbots to deliver guidance, there are still a number of challenges to overcome. For example, research points out that people easily become disengaged from using a chatbot [48, 58] , hampering them from long-term interventions. Moreover, people may overtrust solutions suggested by chatbots which could be inappropriate [24, 67, 68] . In another study, Luria et al. [38] found that people felt uncomfortable interacting with a chatbot which used the same personality to handle both low-risk (e.g., social chat) and high-risk (e.g., medical purpose) contexts. Thus, the authors suggested to design a chatbot that embodies multiple personalities, each of which are displayed in a unique social presence and have the expertise to focus on a single task.\nINTRODUCTION: Prior studies inspire us to overcome challenges by integrating human support into a chatbot system. More specifically, we may be able to make the best use of both human-based and chatbotbased approaches by co-embodying them into a single system. Indeed, studies have suggested that the integration of human support with chatbot interactions could promote user engagement [48] and efficacy of using self-guided systems. For example, a recent study [34] proposed a mediator chatbot that promotes deep self-disclosure from users and delivers the information to a human expert. More research is clearly needed on how individuals might respond differently to interaction with a chatbot alone vs. one incorporating human support. We are also interested in understanding how such differences affect user experience in the long run. To help fill the gap, we conducted a mixed-methods study with 35 participants. We deployed two chatbot designs, both of which delivered training in journaling skills [22, 25] . The first version of the chatbot guided participants in the journaling skills itself, while the second version integrated a human expert (coach) into its interaction when guiding the participants in the journaling skills. Over a period of four weeks, we tracked changes and differences in how each version impacted users' responses to and perceptions of the chatbot system, as well as their level of compliance with the guidance to practice journaling skills.\nINTRODUCTION: Our work makes several contributions to the CSCW and human-computer interaction (HCI) communities. It is among the first that investigated the effects of integrating a human expert to deliver guidance for practicing journaling skills. Our unique three-phase design of an experimental study with 35 participants contributes novel findings of how chatbot interactions with and without expert guidance elicited user interaction differently over time. More specifically, during the Training phase, participants' actual and perceived engagement with the chatbot providing expert guidance (HC) was significantly higher than that of the participants who interacted with their chatbot alone (OC); however, during the Free-will phase, the OC participants chose to continue practicing journaling significantly more than the HC participants. Second, triangulating system log analysis with interviews and surveys, we provide new insights into how the design of chatbot systems with and without human support affected user experience of such systems both objectively (e.g., in terms of the length and depth of journaling content) and subjectively (e.g., participants' perceived trust and intimacy with the chatbots). Third, our work also presented empirical evidence of using chatbots to practice journaling on improving participants' self-reflection. Since prior work shows better self-reflection could enhance people's awareness of their well-being, our work further provides Exploring the Effects of Incorporating Human Experts to Deliver Journaling Guidance through a Chatbot 122:3 design implications for applying chatbots in the healthcare domain and to support diverse training purposes.",
    "relatedWork": "RELATED WORK: 2.1 Chatbot for Delivering Guidance Conversational agents (e.g., chatbots) are gaining considerable attention in many fields including healthcare [69] and education [64] . Research has shown that chatbots can assist users in tracking and monitoring their behavior (e.g., [37] ) and feelings (e.g., [17] ), which could further be used to solicit social support and self-reflection [29] . Also, many studies designed chatbots to guide healthier habits or ways of thinking [46] , such as better eating habits [37] , exercise [29] , ways of coping with stress [46] , and self-compassion [32] . For example, Park et al. [46] incorporated a motivational interview technique into chatbot conversation to help users cope with stress, and found that their design facilitated conversations that improved self-reflection as well as stress management. Lee et al. [32] designed a dialogue aimed at inspiring users to take care of a chatbot that was portrayed as having had a negative experience, and found that after doing this for two weeks, users' self-compassion increased significantly. Another line of research has shown that chatbots have the potential to help people improve their mental well-being by training their thoughts and behavior [36, 61, 65] . For instance, Wang et al [61] designed a public-speaking tutor using a chatbot system to coach users and reduce their public speaking anxiety. Hence, these studies have shown that chatbots could not only help track users' behavior but could also play a proactive role in training users to learn skills.\nRELATED WORK: Recent advancements in artificial intelligence (AI) are also enabling chatbots and other virtual agents to act more credibly like human beings, including during the provision of self-help information [10, 14] . Prior studies [5, 33] indicated that conversational interaction can increase trust and affect users' acceptance of recommendations from a conversational agent. Thus, the design of the interaction between them is important in enhancing users' willingness to adopt chatbot suggestions. Gabrielli et al. [18] proposed a chatbot-based coaching intervention that successfully helped adolescents learn life skills, such as strategies for coping with bullying, and previous research [24, 67] found that their participants' trust and compliance with physical therapeutic suggestions were both higher when interacting with robot therapy partners than with a human expert. Moreover, research has shown that people tend to apply the social norms of human relationships to their interactions with computer agents. This tendency, known as the Computers Are Social Actors (CASA) paradigm [42] , has informed the design of many computer agents [35, 50, 57] . People may perceive intimacy and companionship with a computer agent [27, 35, 40] , inducing changes in behavior change. For example, Ravichander et al. [50] found that reciprocity occurred in human-chatbot interactions and that a chatbot's self-disclosure encouraged people's self-disclosure. Similarly, recent work by Lee et al. [35] showed that a chatbot's self-disclosure improved participants' perceived intimacy with the chatbot and facilitated their self-disclosures in response to the chatbot's sensitive questions (e.g., failure experiences).\nRELATED WORK: However, several limitations of chatbot-based approaches remain, and in certain situations, chatbot-based approaches may be less beneficial than those provided by humans [40, 41, 48] . For example, Howard et al. [24] has pointed out that some people may trust robots too much, due to over-optimism about the viability of the solutions they suggest, and that this trust becomes a source of risk if robots make clinically suboptimal or inappropriate suggestions. In addition, for healthcare interventions that require long-term engagement [8] , people may easily become disengaged from the use of self-guided systems, due to loss of motivation and/or failure to incorporate those systems' recommendations into their daily lives [48] . Furthermore, an investment model shows that purely computer-based interventions are often much less effective than hybrid ones with some professional human input [16] , in part because the latter tends to inspire their users to execute a higher proportion of their intervention requests.\nIntegration of Human (Expert) Support and Chatbot-based Approaches: In prior works, human support has been provided via a separate communication channel external to the chatbot system, such as phone calls, text messages, and email [1] . For example, there have been two main ways of providing human support to chatbot systems. The first is to deploy chatbots in between human-run sessions, to offer users unbroken access to materials and activities [52, 53, 69] . Studies that have adopted such an approach regard chatbots as supplementary tools to support human expert's intervention; chiefly, by monitoring clients outside of their clinical sessions, and garnering information about them that may result in better treatment (e.g., [28, 48] ). Alternatively, it is possible to design a primarily chatbot-based intervention, augmented by human supporters who promote engagement and provide technical troubleshooting and clinical support when issues arise [41, 48, 60] . Such an approach could be more efficacious than interventions by chatbots unsupported by humans [48, 52] .\nIntegration of Human (Expert) Support and Chatbot-based Approaches: Recently, some researchers have suggested an integration of human support into chatbot interventions [48, 52, 53] . For example, Schueller et al. [52, 53] reviewed prior studies of integrating human experts (e.g., coach and therapist) into behavioral intervention technologies, not chatbot-based, and suggested concepts to guide a deeper integration by capturing the trade-offs between client benefits and the available human resources. Alternatively, some prior studies [15, 31] use conversational agents to encourage users' collaboration and communication between people. Specifically, Kumar et al. [31] designed a chatbot tutoring system which gave guidance for multiple students to facilitate collaborative learning among them. Duan et al. [15] utilized a conversational agent to enhance non-native speakers' confidence in conversation with native speakers. These studies showed that chatbots could help mediate interactions between users, but we further explore the effect of applying chatbots to mediate suggestions for guiding users to learn skills.\nIntegration of Human (Expert) Support and Chatbot-based Approaches: The foregoing review and Schueller et al. 's work [52, 53] calls for an integration of the support provided by chatbots and humans. However, previous works have indicated different designs for implementing multiple personas into a chatbot system. For example, Luria et al. [38] conducted studies to examine multiple personalities for conversational agents under different contexts (e.g., low-risk and high-risk contexts). They found that users preferred to have an additional expert agent guiding a specific complex task instead of interacting with the same agent that handled both simple and complex tasks. Conversely, Chaves et al. [11] found that users reported confusion when they engaged in multiple persona chatbots for an information gathering task in a single communication channel. Therefore, it is not clear if and how adding a human supporter into human-chatbot interactions could impact user experience and outcomes, which motivates our research.",
    "rq": "rq2: how do people perceive their interaction with the chatbot differently between the hc and oc conditions?"
}
{
    "intro": "",
    "relatedWork": "RELATED WORK: We draw on work from studies of information seeking and search, conversational style, and embodied agents.\nSpoken Conversational Search: With the increasing popularity of voice-only interfaces, and increasing sophistication in natural language processing, conversational search has emerged as an active research area. In conversational search we have long-form interaction in natural language, asymmetry of role between user and intermediary, and a (possibly ill-defined) task. This contrasts with past work examining chitchat [16] ; cooperative tasks between peers [1, 2] ; or slot-filling \u0142dialogue\u017e systems [21, 33] .\nSpoken Conversational Search: A few studies have examined the structure information-seeking conversations, mostly between people or otherwise in the absence of running software [e.g. 5, 17, 26, 29, 30, 38] . There have also been attempts to describe what a conversation should look like [4, 11, 28] . Studies with agents have been limited, however, by the agents' inability to hold a lengthy conversation [12] . There has also been very little discussion of conversational style in a retrieval setting.\nConversational Style: James Pennebaker et al. [20, 25] have extensively studied language, or linguistic, style matching (LSM), observing that increased style matching between humans was correlated with increased interest in the partner [20] .\nConversational Style: Tannen [34] draws a distinction between \u0142high involvement\u017e (HI) and \u0142high consideration\u017e (HC) styles in social conversation. Differing from Pennebaker et al.'s LSM, these styles also include non-verbal or paralinguistic features. A high involvement style emphasises enthusiasm and interpersonal involvement: features include rapid speech, volume, questions, overlap, and reference to personal experience. A high consideration style, by contrast, emphasises independence and space and features longer pauses, hesitation, and less paralinguistic effect. These notions are also seen in, for example, Lakoff's \u0142camaraderie\u017e and \u0142distance\u017e strategies [23] . In parallel with Pennebaker, Tannen further suggests that partners with opposing styles will find conversation frustrating.\nConversational Style: There is little work on conversational style in information-seeking contexts, as opposed to social contexts. Tannen's two styles were adopted by Thomas et al. [36] , who used audio recordings and transcripts from the MISC data [37] to demonstrate that style could be characterised and computed automatically. They also found that differences in style have an effect on people's perception of conversations. We draw on this work in the present experiments.\nStyle Matching: There is ample evidence of alignment in human-human conversation: that is, partners in conversation tend to converge on word choice, syntax, and even accents [15] . There is some evidence of a similar effect when people interact with computers as well [8, 22, 27] . As conversational agents become more sophisticated, we believe based on early evidence [18] that linguistic style matching could influence trust and likeability of the agent for the human user. We know from user-centered design studies that people unconsciously apply human social rules when interacting with computers: they prefer systems which appear to manifest personalities similar to their own [24] and agents that are more human-like [7] . Therefore, it is reasonable to assume that conversational systems would be preferred if they exhibit conversation styles that are more similar to that of the user. We test this hypothesis in the present work.\nEmbodied Conversational Agents: Voice-based conversational agents have become ubiquitous in the past few years. However, research into voice-only and embodied systems dates back several decades. Cassell [10] and Bickmore [7] pioneered the use of embodied conversational agents. The research has shown that embodied agents have several advantages over voice or text-only conversational agents. Embodiment helps users locate the source of the interaction and allows for a richer opportunity for the communication of non-verbal cues. An agent that has a physical presence also provides visual affordances, indicating where the user should focus their attention and receive cues from body and hand gestures, eye gaze and other expressions [10] . When artificial conversational agents engage in more natural social behaviours (e.g., using social dialogue), it helps increase user trust [7] . In this work we use an embodied conversational agent for these reasons.\nEmbodied Conversational Agents: The present work builds on the above, but is different in several aspects. We report on spoken conversations which run for several minutes (as opposed to one or two turns); for this, we use an embodied agent and we track and adapt conversational style in real time. In contrast to past work on conversational style, which uses recordings or transcripts of human-to-human conversation, we draw from recordings of conversations with an actual software agent. We also focus on information-seeking conversation, which has been little-studied in this context.",
    "rq": "s style and we might ask: what style is best?\n 10 what is the asking price?\n 11 how soon can i move in?\n 13 is there an attic?\n 14 how many cars fit in the garage?\n our present data lets us ask our rq1: is that in fact true?\n that is: can we use pitch, repetition, and so on to measure the involvement\u015bconsideration dimension in real time?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: According to the 2011 census, 54.6% of the Indian population is engaged in agriculture [10] , but earn only 13.9% of the country's GDP [49] . To address this gross mismatch, the Indian government aims to double farmer incomes in the next five years [38] . It is well believed that access to information and expert advice is crucial in achieving this 170:2 \u2022 M. Jain et al.\nINTRODUCTION: goal [5, 15, 16, 41] . Such information includes the choice of seeds to sow, how to combat specific crop diseases, weather forecast based advisory, and optimal harvesting times. However, farmers in rural India often have limited access to such information [15, 16, 41, 46] . Even when the information is available, farmers are often unable to consume it due to illiteracy, as India has the lowest adult literacy rate in the world [47] .\nINTRODUCTION: Several solutions have been proposed to solve the limited information access issue faced by farmers in the developing world. This includes forums for asking questions to experts and peers [16, 41] , peer education using participatory video [15] , interactive voice response (IVR) systems [12, 43, 46] , and social networks for farmers [27, 35] . Since 2004, the Indian government has been operating the Kisan Call Centre (KCC). The KCC [16] is a toll-free call-centre to answer farmers' queries in 22 local languages daily between 6 am to 10 pm. However, it is difficult for the manually operated call-center to keep up with the massive demand. In June 2014 alone, 1.11 million calls were received by the KCC, out of which over 450,000 (~40%) went unanswered [3] . Thus, it remains an open problem to build a system that satisfies the information needs of rural farmers. Systems meant to serve this population must be usable and acceptable by people with limited literacy, highly scalable, available around-the-clock, responsive, and have a manageable overhead for agricultural experts (referred as agri-experts).\nINTRODUCTION: As a potential solution, in this paper, we introduce an automated conversational agent, or chatbot, to provide farming related information through natural speech interactions. A chatbot offers several benefits that can potentially satisfy the above-mentioned requirements. First, speech is the most familiar mode of interaction that requires little learning or literacy. In fact, audio-based interactions are considered the preferred-sometimes the only usable-interactions for illiterate users [33, 34] . Interactions with an agent should enable farmers to formulate queries as if they were talking to another person. Second, conversational agent systems offer direct information access without the need to navigate complex information paths as often required by graphic user interfaces (GUI), and simplicity is considered a primary design requirement for low-literate users [33] . Finally, from a system point of view, a chatbot is a scalable solution that can be accessed by any user at any time. Moreover, agri-experts can review user inquiries to the chatbot periodically and then continuously expand the chatbot knowledge base without high maintenance cost.\nINTRODUCTION: While recently the HCI community has developed an increasing interest in conversational agents and demonstrated their benefits [20-22, 29, 30, 32, 44] , most prior works focus on literate, technologically-advanced users. In our work, we take the conversational agent on the ubiquitous smartphone to rural farmers in the developing world, broadening its scope to a new demographic. This new user population and usage require answers to questions concerning (a) how to encode farming related queries efficiently in a conversational system; (b) the robustness of speech and language technologies in the local language; (c) the acceptability and usability of chatbot technologies for rural farmers; and (d) interaction modality preferences of the farmer population.\nINTRODUCTION: To answer these questions, we designed FarmChat, a conversational agent to meet the information needs of rural Indian farmers. Recent work suggests that interface design should support semi-literate users differently from illiterate users [13] . For semi-literate users, text can offer faster and unambiguous mode of interaction [13] , while for illiterate users, the appearance of text negatively impacts their performance [34] . Further, text output allows more flexibility to process information and persistent access to messages, which we identified as a design requirement in a formative study. Hence, we built and compared two interfaces of FarmChat: (1) Audio-only (input: speech; output: audio) and ( 2) Audio+Text (input: speech, button; output: audio, text, image).\nINTRODUCTION: Currently, FarmChat supports Hindi-the most widely spoken Indian language-and answers queries about potato farming as a use case for the study. The knowledge base embedded within FarmChat on potato farming was derived by analyzing the query logs from KCC and a formative study with 14 farmers and 2 agri-experts. Thereon, we ran a task-based user study with 34 farmers in villages around the city of Ranchi in the state of Jharkhand in eastern India. From the 626 inputs provided by the participants, we found that farmers appreciated FarmChat's precise and localized responses, showed great interest and trust on the information, and generally found a conversational agent easy to use, thus hinting that a chatbot has the potential to meet their farming-related information needs at scale. We also uncovered some interaction behaviors and issues that are important to consider when developing chatbots for this user population, including over-expectation of chatbot's capabilities, a tendency to imitate phone conversations with a human, and unfamiliarity with standard speech input and messaging interfaces. Moreover, we found that literacy level, prior experience with technology, and profession played crucial roles in the users' preferences between the Audio-only or the Audio+Text interface. We conclude with implications for designing chatbots for low literate and novice technology users.\nINTRODUCTION: Our main contributions are: (1) Proposing a novel speech-based conversational system to meet the information needs of low literate rural Indian farmers, wherein the chatbot intelligence was based on KCC logs and inputs from local agri-experts. (2) Comparing two chatbot interfaces with differing interaction modalities, and inferring that preference for an interface was highly dependent on the farmer's literacy level and prior technology experience.",
    "relatedWork": "RELATED WORK: Our work is mainly informed by three areas of relevant research: UIs for low literate users, technologies to support agriculture related activities, and conversational systems.\nUIs for Low Literate Users: Designing user interfaces for low literate populations is a growing area of research [13, 26, 33, 34, 40] . It spans multiple application domains, including agriculture [11] , health care [18] , citizen journalism [36] , video search [11] and social networking [35] . Research has shown that users with low levels of literacy perform better with user interfaces which use minimal or no text and represent textual information using graphics/photographs and audio [33, 34] . Even interfaces that use graphics liberally, such as a job search web portal for illiterate users [34] , rely on audio to provide descriptions and instructions. Voice as an interaction modality is well-suited for low literate users, as it is a natural means of expression. Voice-only citizen journalism portals [36] , voice-based Q&A forums for rural farmers [41] , and IVR systems [18, 25] are a few successful examples, relying heavily on speech for input and audio for output. With audio as the output modality, researchers have compared speech versus DTMF/keypad for input and obtained contrasting results with low literate users [18, 40, 45] . Interestingly, most low literate users are numerically literate; hence, using numbers (both for input in keypad buttons and output as text) has been found to be acceptable [34] . This has paved the way for multimodal interfaces [11, 35] that embed graphics, voice, and numbers for low literate users.\nUIs for Low Literate Users: Recent research highlighted the interaction differences between illiterate and semi-literate users and suggested to treat the two groups differently in interface design [13] . Comparing them in tasks on Audio+Text versus Text-only interfaces, Findlater et al. [13] found that text was important for semi-literate users since it offers a faster and less ambiguous mode of interaction. Importantly, text allows for opportunistic language learning. Interestingly, social factor also results in the preference of a text-based interface since it avoids the stigmatized perception of illiteracy [26] . For illiterate users though, the appearance of text negatively impacts their task performance [13, 34] . Hence, we designed and compared two FarmChat interfaces: Audio-only and Audio+Text.\nAgriculture-related Technological Solutions: ICT4D research has contributed immensely in developing technological solutions to help farmers in developing regions with their information needs [12, 15, 41, 43, 46] . Farmers are usually located in rural areas with low literacy levels; hence, a majority of the proposed solutions rely heavily on voice as an interaction modality. Two of the widely adopted approaches are automated calls providing agriculture-related knowledge and IVR systems. These are highly scalable solutions, but are limited to providing generic crop-related advice, which may not work for a majority of farmers due to variations in crop, soil type, climate, etc. Automated calls have been adopted by several governments around the world, including India's [37] . Khedut Saathi [39] took automated calls a step further, allowing farmers to forward the received audio message to five other phone numbers. IVR systems use a computer-based back-end with a keypad/voice-based input, and audio output to provide farmers with relevant information related to weather, fertilizers, and market prices [43, 46] . Accessing the hierarchical navigation is a major usability issue with IVR systems [33] . Instead of restricting their system to audio alone, Digital Green [15] focuses on creating videos by farmers and uses human-mediated instruction for disseminating these videos to other farmers. VideoKheti [11] proposed a multimodal method for illiterate farmers to search specific videos.\nAgriculture-related Technological Solutions: Other solutions offer custom advisory services to farmers. The Indian government program KCC [16] allows farmer to dial a toll-free number and get responses to their specific queries. Since the demand for such information is too large to be met by a manually operated system, most of the KCC calls go unanswered because the phone lines are usually busy [3] . Avaaj Otalo [41] proposed a voice forum for asking agriculture-related questions to experts and peers. It is an asynchronous system, meaning that responses are not given in real-time. Furthermore, questions can be answered by anyone, which may result in incorrect answers and distrust towards the system.\nAgriculture-related Technological Solutions: The above work suggests that to fully support the agriculture-related needs of farmers, constant, real-time access to specific information is needed. None of the systems in prior work achieve all of those criteria; FarmChat is an attempt to fulfill these requirements.\nConversational Systems: The last decade has seen rapid growth in conversational agents (also called chatbots). Chatbots have appeared on a variety of mobile and ubiquitous platforms, including phones, smart speakers, VR/AR devices, smartwatches and even operating systems (Siri by Apple and Cortana by Microsoft). The term 'Conversational Agent' has come to mean a wide variety of systems with varying capabilities and purposes, with the underlying assumption that human interactions with the systems resemble normal conversations. Proponents of chatbots embrace their many strengths: e.g., user's familiarity with the conversational interaction, seamless natural-language interface across use-cases, offering direct information access and simplified navigational paths, and the promise of personalized and evolving intelligence [7, 32] . In spite of this rapid growth, chatbots are still in its nascent stage, as 84% of the Internet users have not used a chatbot yet [19] .\nConversational Systems: From an HCI perspective, Licklider's 'Man-machine symbiosis' [31] was one of the earliest discourses that visualized humans interacting with machines in a natural manner. Although the HCI community has studied how conversational agents are used in different settings [22-24, 29, 30, 32, 44, 48] , no prior work has focused on low literate users. The closest to our work is a study with first-time chatbot users [23] , who were technologicallyadvanced literate Indians knowing about chatbots, but had not used them before. Most works evaluating users' experience with chatbots have discovered a gulf between experience and expectation with respect to the capabilities of chatbots [23, 30, 32, 48] . Users were found to be disappointed and even frustrated with the current bots [23, 32] , and most chose to limit the usage of chatbots to simple tasks (e.g., setting alarms) [32] . This suboptimal user experience can be attributed to the high expectations of expert users that these chatbot technologies currently target [23, 32] . In contrast, our work focuses on developing chatbots for low literate novice smartphone users who have little knowledge or preconception of the technology.\nConversational Systems: To summarize, speech-based conversational interface has several potential benefits for our targeted farmers population. Besides little requirement for literacy, it offers a natural and familiar modality that does not require a user to learn new technical concepts or interaction methods. This could be important as rural farmers often have low technology literacy and self-efficacy. Meanwhile, knowledge base of the conversational system can be easily edited or customized by agriculture experts, thus offering a scalable solution for disseminating information and expert advice. However, it still calls for empirical understanding for the acceptability and usability of this new type of technology among the farmers population. Our study set out to fill this gap.\nConversational Systems: Two sources of knowledge informed the development of FarmChat: farmers' information inquiries with the Kisan Call Center (KCC) and findings from a formative study with local farmers and agri-experts.\nConversational Systems: The Government of India has made all logs of calls to the KCC from January 2015 to September 2017 publicly available. In total, this corpus contains data for 8,012,856 calls. Each call log has 11 fields, including the date and time of the call, location, crop (one of the 306 crop types), query, and the answer provided by the KCC agri-expert.\nConversational Systems: For implementing FarmChat, our system was restricted to potatoes since most farmers around Ranchi were engaged in potato farming during the study period. Moreover, farmers are more keen to gain knowledge about the crop they are currently farming [15] . There were 85,852 calls related to potato crop in the KCC dataset. We performed topic modeling using LDA [6] on these calls and found that the top 5 queries for potato farming were pest and disease (52,070 queries), weather (11, 628) , best practices (5, 648) , fertilizer use (4,049), and seeds (3, 646) ; these calls constituted 89.7% of the total potato farming calls. The majority of pest and disease queries (17, 668) were about the late blight disease. FarmChat covers these main areas of questions with curated answers.\nConversational Systems: The KCC dataset does not contain the complete dialogues between the farmer and the KCC expert, but rather a limited summary of the question and the answer provided. To fill this gap and validate that the common questions identified from the KCC dataset apply to the local situations around Ranchi, we conducted semi-structured interviews with 14 farmers (9 male, 5 female) and 2 male agri-experts, in September 2017. We worked closely with a local agriculture NGO (Non Governmental Organization), where the two agri-experts were employed. They helped us recruit the farmers and obtain their consent for participation, following their own internal ethics policies. The farmers were from three different villages situated within 50 miles of Ranchi. Five farmers were literate (can speak, read and write Hindi), three were semi-literate (can speak and read Hindi), and six were illiterate (can only speak Hindi). The definition of different literacy levels have been adapted from previous works [13, 28] . Six of the literate and semi-literate farmers owned a smartphone with Internet access. Both agri-experts had a graduate degree in agriculture and more than 15 years of farming advisory experience. Though five of the farmers and both the agri-experts had heard about the KCC, only two of them have tried calling the service and none of their calls were answered. One of the researchers conducted the interviews. The interviews were conducted in Hindi and took 20 minutes. All sessions were audio-recorded, and were transcribed and translated to English later. Both the farmers and the agri-experts participated in the study voluntarily without compensation. From the interviews, we tried to understand: (1) What are the information needs of these farmers? (2) What are their current approaches to seek that information? (3) What are the concerns and limitations of these approaches?\nConversational Systems: These questions are intended to inform the potential usage patterns of FarmChat. We performed a thematic analysis [2] on the interviews data to identify themes related to the above three questions.",
    "rq": " (1) what are the information needs of these farmers?\n (2) what are their current approaches to seek that information?\n (3) what are the concerns and limitations of these approaches?\nhuman: can see black spots on the leaves, what to do?"
}
{
    "intro": "INTRODUCTION: In recent years, conversational agents or chatbots have received great attention. Since 2016, major tech companies started providing open APIs for building chatbots. This resulted in a massive growth of chatbots especially the chatbots on text messaging platforms. Currently, more than 300,000 bots exist on Facebook messenger [49] cutting across a variety of use-cases-online shopping [44] , casual chatting [5], travel arrangements [45] , etc. So, far, the focus has been largely commercial, e.g. customer support, since the service providers see chatbots as a cost-e ective and e cient way to supplement human-based customer service. However, chatbots also o er opportunities for creating positive social impact [35] . With the ability to reach millions in a ordable ways, chatbots o er unique opportunities to empower those living in the constrained environments. Further, we discuss the ethical implications of introducing breastfeeding education chatbot in the study context and potential societal impact.\nINTRODUCTION: Recently, HCI and CSCW communities have started exploring use of chatbots for variety of problem domains [26, 102, 116] . In India, the access to doctor is limited given the poor ratio of almost 4.8 doctors to 10,000 people [89] . To ll this gap, India uses a large network of community health workers for critical areas of public health such as maternal and child health. We want to explore the use of chatbots in assisting community health worker and mothers in the exclusive context of breastfeeding. Promoting exclusive breastfeeding is a global priority of maternal and child health programs because of its importance in child survival [86] . Timely initiation of breastfeeding and exclusive breastfeeding for the rst six months of child's life helps in preventing 20% newborn death and 13% under-ve deaths [55] . However, despite this knowledge, breastfeeding rates fall short of recommendation [51] .\nINTRODUCTION: Our users, community health workers and mothers residing in under-developed (slums) regions of India, have limited or no access to technological advancements in chatbot research, such as Alexa or Google Home, because of poor economic conditions and limited literacy. While they now possess smart-phone, it is often a low-end smart-phone (< $100) [42] . Moreover, the socio-economical conditions dictate that the phone may be shared within the family, often with the husband or other male members of the family [96] . This makes our users, the rst-time users of chatbots.\nINTRODUCTION: These rst time users of chatbots, in their unique contexts, represent an important class to study and design for. So far, the research around chatbots has been majorly technology driven with limited investigation on user perceptions and contexts of use [88] .\nINTRODUCTION: In this study, we explore the feasibility of using chatbots for breastfeeding education of community health workers and mothers in urbal slum areas of India and understand how they react and perceive chatbot based intervention. We aim to contribute to the HCI and CSCW community by informing design recommendations for chatbot implementation for causes like breastfeeding education in under-developed areas, with users who are newcomers to personal device use. We guide our exploration with the following research questions:\nINTRODUCTION: (1) What is the perception of users towards chatbot based breastfeeding assistance? (2) What kind of information users -CHWs and Mothers -seek from a chatbot? (3) What are the contexts of chatbot use for mothers and community health workers residing in slum areas of India?\nINTRODUCTION: We study users' interaction with the chatbot through a Wizard-of-Oz experiment with 22 participants (12 ASHAs and 10 mothers). We prototyped our chatbot as an interactive questionanswering application and analyzed users' interaction patterns, perceptions, and contexts of use. Our ndings highlight the role of familial unit in breastfeeding practices. We found that the majority of questions sought by users are answerable by a chatbot and value of chatbots for mothers as a rst point of contact. This gives us the con dence that breastfeeding education is a potential application for chatbot intervention. We also discuss implications for the design for future bots and the characteristics they should embody to address users' concern in constrained settings.",
    "relatedWork": "RELATED WORK 3.1 Breastfeeding Practices in India, Support System and Role of CHWs: Breastfeeding is considered to be the best way of providing optimal food to infants. Ideally, breastfeeding should be initiated within 1 hr of childbirth, practiced exclusively for the rst six months and then continued with the addition of adequate complementary foods up to two years of life [51] . Good breastfeeding practice is directly linked with improved survival rates of under-5 and child's health that includes optimal cognitive development and lower risk of obesity and diabetes later in life. Mothers get positive bene ts in terms of reduced risk of haemorrhage and breast cancer [51] .\nRELATED WORK 3.1 Breastfeeding Practices in India, Support System and Role of CHWs: According to the study [80] that examines trends in the young child feeding practices in India in the period of 2006-2016, breastfeeding practices have shown signi cant improvements: Early initiation of breastfeeding nearly doubled from 23% to 42%, and Exclusive breastfeeding increased from 46% to 55%. However, still, a major cohort of children are missing out and the current rates, at best can be described as modest [80] . Several factors play role in dis-continuation of breastfeeding.\nRELATED WORK 3.1 Breastfeeding Practices in India, Support System and Role of CHWs: While it is known that barriers to breastfeeding occur at the social, cultural and political level that are outside of maternal control [24] , there are di erences in contexts of these factors. Prevalence of traditional beliefs and wrong practices among women (both rural and urban areas) are a major deterrent to exclusive breastfeeding [12, 13, 57] . Many families commonly practice prelacteal feeding as a ritual and discard colostrum [71] . For example, in some of the Hindu communities, after birth, a child is welcomed through a ceremony in which a family member writes the word \"OM\" onto the tongue of the child with honey in the hope to wish good qualities to the infant. In some castes, breastfeeding is delayed until fth day due to their belief in ancient medical science. Perception of insu cient milk supply among mothers and caregivers is also a common barrier to exclusive breastfeeding. The study [12] found that nearly 33% women believed their milk supply would be low for their child and introduced animal milk and external food. Another challenge for new mothers is con icting advice from a health practitioner and a family member. Female relatives in the house have a strong in uence on breastfeeding practices.\nRELATED WORK 3.1 Breastfeeding Practices in India, Support System and Role of CHWs: From the country's perspective, to support optimal infant and young child feeding practices (IYCF), various strategies have been established [91] . However, gaps remain at the implementation level. For instance, the Maternity Bene t Act 1961 was amended only recently in 2017 for extending the paid maternity leaves for working women from 12 weeks to 26 weeks [84] . Further, currently, the act is applicable to certain establishments e.g. government and other organized sectors and does not protect women working in unorganized sectors such as household sta , contractual labors etc. [91] . This directly hampers the women belonging to lower socio-economic strata which hold a greater percentage of the country's population [15] . To improve awareness of women on IYCF, there is an emphasis on providing counseling at health centers and through community outreach. ASHA workers, who play a crucial role in connecting to marginalized communities, are seen as an important vehicle of behavioral change and thereby also trained on IYCF to promote exclusive breastfeeding in their post-natal home visitations [74, 99] . However, a range of challenges exists in translating the knowledge into actual practice. Due to over-burdened duties, high engagement in paperwork and sub-optimal training, ASHAs' awareness of being health educators has been found to be low [41, 43] . Studies assessing ASHAs knowledge on breastfeeding practices report that though their knowledge is good, signi cant improvement is required on addressing prelacteal feeds, mothers perception on milk supply, and complementary food [99] . Further, there is a scarcity of studies to report on how mothers are supported beyond the six home visits of the ASHAs that is after 42 days of delivery.\nConversational Agents: Growth and Opportunities: Although the buzz around chatbot might appear to be newer, the history dates back to 1966 when the rst-ever chatter bot named ELIZA was launched [113] . ELIZA was a computer program that could respond in natural language to engage patients in psychotherapy. Using keyword spotting and pattern matching algorithm ELIZA would select the most appropriate responses from a pre-de ned set. Later, more advanced form of the ELIZA developed and became popular e.g. Parry (1972), JabberWacky (1988), ALICE (1995) .\nConversational Agents: Growth and Opportunities: The key idea behind is to provide personalized assistance to the users through the mediation of human-like conversations. Advances in the eld of arti cial intelligence have led the chatbots to pick momentum. They can now learn user experiences and context and based on user history respond better in real-time. These interactions keep on improving as the user interacts more with the bot. Chatbots are increasingly being designed for variety of purposes, e.g. engaging in small talk [2], imitating psychotherapy [65] , coordinating team events [104] , updating on weather [58] , and improving customer service [44] . And thrives on almost all popular digital forums, like messenger apps, social networking sites, and other conversation interfaces e.g. Amazon Alexa, Google Home, etc. In particular bots embodied in messenger apps gained traction in recent times. For instance, the year end 2016, when Facebook launched its messenger bot platform for the developers, marked the chatbots proliferation to 34,000 [66] , which now has reached 300,000 [49] . Examples of some of the recent popular bots include Mitsuku [1], started in China, engages and entertains users by telling stories, jokes, and songs. Similar to this is Microsoft's Xiaoice [2] that is mostly liked for its sense of humor and ability to entertain users based on their mood cues. Beyond the commercial purpose, which has been the large focus till now, there is a growing consensus on the potential of chatbots in creating a positive impact on society [35, 88] . Due to the much wider reach, ease of access and interaction, chatbots can extend the public services. Healthcare sector is one of the promising areas for the chatbots. Low doctor to patient ratios, high cost, and complex procedures increase the gap between professionals and patients. Chatbots can bridge the gap by providing preliminary assistance [23] . As of today, the use-cases of tasks taken over by bots include making doctor's appointment, checking medical history, post-discharge follow-up, information searching, and lifestyle promotion. Few examples of the chatbots are as follows. Babylon Health [3], asks questions on symptoms, provides detailed information and additionally book an appointment with physicians. Florence [4] acts like a nurse that reminds taking pills, track health (body weight, periods ) and help nd a specialist in the nearby area. Safedrugbot [6] is designed to help doctors get information on the side e ects of drugs during breastfeeding [2] [1]\nTechnology and Motherhood: In recent year, HCI research exploring the role of digital technologies for women's health has increased, particularly covering di erent aspects of motherhood. Social attitude towards breastfeeding in public, varies widely across cultures and countries. In UK, where it is mostly perceived to be a personal act, it is di cult for mothers to engage in breastfeeding practice openly outside homes. The study [14] , proposes and develops a mobile application, \"FeedFinder\", to help a woman nd breastfeeding places in nearby area, review and share them with other women. The work report on the challenges in co-designing the application with mothers and design implications for mobile technologies to support public health. On similar lines of understanding the mothers' issues in using technology, [112] presents the development of \"Milk matters\", an application to motivate women in donating surplus breastmilk to nearby milk banks. The paper informs on the design considerations to make while working with mothers: interrupted interactions, elements of distraction and value of positive reinforcement. In many developing countries, the infrastructure to support storage and distribution of breastmilk, often known as Milk Banks does not exist. To address, a recent work [25] , proposed a low-cost solution for milk pasteurization using mobile and sensing technologies. To this emerging line of HCI research around motherhood, [31] contributes by analysis of 1000 submitted ideas of mothers on the improvement of breast pump. The paper highlights the complexity of post-partum phase and suggests using principles of feminist HCI and participatory design in designing supportive technologies. In developed countries, often new mothers face isolation in their initial phase of motherhood, [37] conducted an ethnographic study to understand the role of technology in the emotional and social well being of mothers and highlights the power of technology to provide social connectedness. Applications for expectant and new mothers to share information within their intimate group [50] or communication tools to stay connected with supporting people [62] have been found to be e ective.\nTechnology and Motherhood: Apart from research endeavors, public health services have shown interest in leveraging technology to extend breastfeeding support. For example, Public Health England has launched a chatbot application under the program-Start4life over Facebook Messenger and Alexa platforms to provide 24/7 support [33] . [70] have worked towards developing tools to enhance communication between mothers and hospital sta . Also, commercially, there is an increase in applications targeting motherhood needs. A simple google or app store search on breastfeeding and motherhood leads to various options, which could be either paid or free, available for both android and IOS platforms e.g. Baby Tracker [64] , LactMed [85], LatchMe [39] , MyMedela [8] , Feed Baby-breastfeeding app [90] etc.\nTechnology and Motherhood: While the above-mentioned works are towards women who are digitally literate and belong to urban settings of the developed world, there are others from marginalized groups, for whom we have limited understanding. Recent a ordances to smartphones in developing nations, have made women in under-served contexts to start owning smartphones and explore the digital world, which earlier was a remote possibility for them. These women represent an important class of users for HCI and CSCW communities. We extend the limited but growing body of work on the intersection of technology for women's health and technology use by marginalized women [10, 115] through presenting our insights into the perceptions, experience, and contexts of chatbot application by mothers and community health workers in slum areas of Delhi, India.\nTechnology Based Support to CHWs: Community Health Workers are the last mile actors of healthcare who serve communities often vulnerable populations towards accessing health services and adopting healthy behaviors. In particular, their role in reducing mortality rates for maternal and child health has been signi cant. Thus, increasingly more programs are aiming towards innovating methods to empower CHWs [36] at the global level. Particularly, empowerment through technology integration in the ongoing endeavors has been an important direction .\nTechnology Based Support to CHWs: Since CHWs often work in resource-constrained settings, whereby, infrastructure and technology penetration is limited, e orts have been towards making simple, low-cost and e ective solutions. For instance, because of the high use of phones by CHWs, mostly feature phones, a rich body of work surrounds designing mobile-based solutions. By providing assistance through mobile applications, researchers focus on furthering CHWs performance which gets a ected due to factors including limited supervision, education, and training [41, 97] . For example, mobile applications e.g. SMSs, IVR, e-checklists have been used in improving CHWs adherence to house visits and case management guidelines [11, 28, 29, 54, 72] . In addition to supporting supervision, data collection by CHWs has been facilitated [32] .\nTechnology Based Support to CHWs: Community education has been highlighted to be one of the most challenging tasks for CHWs [41, 92, 97] . Studies have found a number of causing factors. For example, in persuading families CHWs face resistance due to low literacy, compliance with old age traditions and practices [92] . Due to minimum focus on the role of activists by existing training and supervision, CHWs remain unclear on how to execute the role of counselor [97] . Further, Limited, irregular payment of incentives and overburdened duties have been reported to negatively a ect CHWs motivation towards counseling job [41, 97] . In this regard, use of mobile multimedia is increasingly being investigated in multiple ways because it o ers an e ective method of equipping ASHAs with authentic health information which they can use anytime and anywhere in the elds. Ramachandran et al. [92] addressed two challenges of community education as to how to persuade women towards the adoption of healthy behaviors and enhance the motivation of CHWs towards e ective consultation. By providing short videos on health topics and testimonial videos by in uencing actors of the village to CHWs, researchers could observe enhancement in the quality of dialogue between the CHWs and the families and self-e cacy of CHWs. The study by Fiore-Silfvast et al. [34] further con rms the usefulness of such videos in the work ow of CHWs by understanding their perspective on technology adoption.\nTechnology Based Support to CHWs: Distinct from the aforementioned studies which adopt top-down approach in providing content to CHWs, Molapo et al. [77] proposed a software tool for health trainers in rural areas, to create their localized content. The easy to use computer application enabled the trainers to create nontextual content that ful lled their community speci c needs. On the similar line, Kumar et al. [60] demonstrated the impact of community participation in content creation and its use in the adoption of globally approved health practices. Further, there are studies that complement the prior work by understanding the sustainability aspect of mobile videos [75] and the value of involving CHWs directly in designing the content and applications [76] .\nTechnology Based Support to CHWs: Shortage of skilled trainers is a known challenge in upgrading skills of CHWs on a regular basis. A number of studies and projects have thus attempted to focus on using technology-based solutions to provide better learning opportunities for CHWs [17, 117] [53] [7] . The study by Yadav et al. [117] provided a learning platform, Sangoshthi, that enables live interaction between remote CHWs and experts. The study found positive outcomes in terms of signi cant knowledge gains and perceived usefulness as a supplementary learning platform by the stakeholders.",
    "rq": "(1) what is the perception of users towards chatbot based breastfeeding assistance?\n (2) what kind of information users -chws and mothers -seek from a chatbot?\n (3) what are the contexts of chatbot use for mothers and community health workers residing in slum areas of india?\n mother: what should be given to children after 6 months?\n mother: how long should a child be breastfed so his/her stomach gets full. yukti: thank you. getting your answer from my material. yukti: some babies suckle fast and some are slow. as a child grows, their feed duration can change. duration of feed can also depend on when the milk comes in after the child starts suckling. usually, a baby spends 5-10 minutes on a breast, but we recommend that you let the baby decide the duration of the feed. yukti: how did you find the answer to your question, give your rating by pressing one button: good/bad/average. yukti: question: how will i find that my child is full so i can stop breastfeeding?"
}
{
    "intro": "INTRODUCTION: As we are living in an era of artificial intelligence, we are getting more comfortable with the idea of interacting with machines in our everyday life [42] . Artifacts such as Embodied Conversational Agents (ECAs) [10] are specifically designed to facilitate these interactions, providing a natural, human-like user interface capable of reproducing the different modalities of human communication such as speech, gestures, facial expressions or gaze.\nINTRODUCTION: Proc. of the 17th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2018), M. Dastani, G. Sukthankar, E. Andr\u00e9, S. Koenig (eds.), July 10-15, 2018, Stockholm, Sweden. \u00a9 2018 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved. While Bledsoe's dream [7] of seeing humans befriend machines is still a long way off, many efforts are being made to make these ECAs capable of building and maintaining a relationship in the long run with their users. Indeed, this social bond also known as rapport has been shown to increase ECA performance during tasks such as tutoring [33] , health coaching [21] , or museum guidance [6] .\nINTRODUCTION: One particular role that would greatly benefit from rapport building mechanisms is that of personal assistant [2] . Indeed, companies large and small are now moving forward with the vision of intelligent virtual personal assistants such as Apple's Siri, Microsoft's Cortana or Amazon's Alexa. However, these current personal assistants only provide a vocal interface and do not yet allow multimodal input, or provide embodied output to their users. Increasingly, these same companies have begun to investigate adding social chitchat, but their approach is not grounded in human behavior. They lack the social awareness and reasoning that would allow them to sense and generate relevant social language leading to increase rapport with their user.\nINTRODUCTION: In this paper, we report results of a field trial with a semiautomatic socially aware personal assistant that helped participants of a large conference to find relevant sessions to attend and interesting people they should meet. Rather than simply delivering information through a textual interface, or a plain dialogue, we designed our personal assistant to build a relationship with the conference attendees through a multimodal rapport-building dialogue. Conference attendees interacted with our personal assistant during the conference, getting recommendations about sessions to attend and/or people to meet. Our main contribution here is to analyze these interactions and (1) investigate the relationship between the rapport dynamics and the task performance of our personal assistant and (2) propose design guidelines for personal assistants and socially aware ECAs in general.",
    "relatedWork": "RELATED WORK: Since 2011, the Siri's debut, major tech companies have released a number of voice-based intelligent personal assistants, such as Microsoft's Cortana, and Amazon's Alexa. One of the origins of such intelligent personal assistants might be CALO, the Cognitive Agent that Learns and Organizes, a foundation technology of Siri. CALO is able to handle major cognitive tasks, such as task and schedule management [5] [25] , and human communication mediation [39] . Similarly, the RADAR project developed a software-based personal assistant to help users cope with email overload as effectively as a human assistant. The system analyzes text messages received by the user to distill out task-relevant information including new tasks elicited by a message [16] . The major purpose of these research prototypes and commercial products is to support users' cognitive tasks. They only focus on the task aspect of the interaction without taking into account the social cues delivered by their users. Some other work, however, has started to investigate the positive influence of rapport on their agents' task-performance. Rea is a real-estate virtual agent who talks about apartments to rent while building trust and rapport with her users through the use of social language and small-talk [11] . The authors found that extroverted users trusted the social version of Rea more than the version that only focused on the task itself. However, there was no evidence to suggest that users who engaged with social Rea would pay more for a recommended apartment than users who interacted with Rea's task-only counterpart. In [6] , the authors deployed Tinker, a virtual museum guide designed to describe various exhibits to guests and then help them find their way out. Tinker was able to build rapport with its users through a short dialogue using predetermined strategies. The authors reported that Tinker's use of relational behavior improved users' engagement, which consequently improved the amount of information retained by the users about museum exhibits. Ellie [14] is a virtual agent designed to have engaging interactions in which users would feel comfortable sharing and disclosing information. Ellie uses non-verbal behavior and a set of dialogue policies to build rapport with its users. In terms of rapport, the authors discovered that people who interacted with a fully autonomous version of Ellie reported feelings comparable to people who had a face-to-face interview with a human semi-expert. Furthermore, most of the participants (75.8%) who interacted with autonomous Ellie agreed that they were comfortable sharing information with it. Most of these works rely on self-rated rapport score to investigate the relationship between rapport and the agent task-performance. None, however, try to capture the evolution of rapport during the interactions nor do they investigate whether an agent's task performance could affect the dynamics of rapport over time. We try to address these gaps by shedding the light on the following research question:\nRELATED WORK: RQ: \"How does the task performance of a personal assistant affect the dynamics of rapport over the course of an interaction?\"\nRELATED WORK: In this paper, we hypothesize that rapport between a personal assistant and its user is likely to increase if the former achieves high task-performance. On the other hand rapport is likely to decrease if the personal assistant fails to achieve its task.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: As the advance of embodied conversational agent (ECA) technology, there are more and more real-world deployed applications of ECA's [9, 17] . Various kinds of kiosk information systems are used in public places, such as shopping malls, museums, and visitor centers. The typical situation in which such systems are used is that a group of people stand in front of the kiosk and operate it in order to retrieve the information while talking with one another. Therefore, in order to implement an ECA that can serve as an information kiosk in public places, multi-party conversation functionality for simultaneous interaction with multiple users is indispensable.\nINTRODUCTION: In dyadic dialogs where only two participants are involved, it can be assumed that when one participant is speaking then the other one is the addressee in most cases. In multi-party dialogs, however, the distinction among conversation participants' roles, addressee, overhearer, and speaker is necessary. Who is talking to whom about what topic should be identified and traced, the obligation to answer a question when being asked needs to be fulfilled, and whether an utterance has been grounded [3] with the addressee(s), i.e. became a shared belief between the speaker and the addressee(s), needs to be identified. Since there are potentially more interlocutors to acquire dialog turns from and transfer dialog turns to, managing the communication flow in a multi-party conversation is much more sophisticated than a dyadic one. Traum [16] provided the principal literature identifying some of the general issues in realizing multi-party human-agent interactions.\nINTRODUCTION: The conversation style of most contemporary ECA systems are either agent-initiative (e.g. the agent always asks questions and the user answers) or user-initiative (e.g. the user always ask questions and the agent answers), because these interaction designs are simpler and are easier to be implemented. However, the natural conversation occurred between human and human is mixed-initiative, i.e. both of the agent and the user may take initiative during the conversation. In this study, we are proceeding with a project that aims to build an ECA capable of mixed-initiative conversation with multiple users in a typical application, information providing for users' collaborative decision making.\nINTRODUCTION: Considering the case where a real estate agent is serving a recently married couple who are planning to buy a new house, the married couples consider the location, layout, or the price of the houses suggested by the agent, discuss with each other and collaboratively make the final decision. In this task, the decision can be made efficiently if the agent can actively intervene the discussion between the users to provide timely information while the agent is listening to the users' conversation as an overhearer. For example, the agent may suggest new candidates as \"how about this apartment? It is only a five-minute walk to the train station.\" if it hears that the users are discussing about the transportation issue, or \"how about this apartment? There is a super market which is just five-minute away from it.\" if it hears that the users are discussing about meal issue. This kind kind of interaction is nature and frequently occurs in human-human conversation. However, for a virtual agent to deal with this, several functions have to been incorporated to the agent. The agent needs to reason what information to provide from the users' conversation even their demands are not clearly described. The agent also needs to identify the timings when the user may be interested in the information being provided without making them feel disturbed. In order to realize this, we focus on nonverbal information. It has been known that nonverbal information is an important element in a conversation scene. In particular, gaze has been found to have a major impact on multi-party conversation [14] .\nINTRODUCTION: This paper describes the development of the method dealing with the timing finding issue. In order to realize the feature to find the appropriate timings for the agent, at first, a Wizard-of-Oz (WOZ) experiment was conducted for collecting human interaction data. The experiment participants were instructed to collaboratively make decisions in three tasks: travel planning, lecture registration, and part-time job hunting. They did the tasks with the information provided by a virtual agent who is controlled by one of the experimenters from remote (Section 3). Then an analysis was conducted on the collected data corpus to find the eight possible timings and the corresponding user behaviors (Section 4). A method was developed to automatically identify four of the eight kinds of timings only by using nonverbal cues, gaze, body posture, and acoustic information. Finally, the paper is concluded with the evaluation of the performance of the proposed method and the discussion on the results (Section 5).\nIdentification of individual utterances from raw audio: track. First, the speech audio was divided into utterances as the unit for further analysis. The speech was automatically separated using the voice source information from one of the two Kinect sensors. The speaker of each utterance is identified at the same time.\nIdentification of individual utterances from raw audio: As the results, there are averagely 108 user utterances and 49 agent utterances in one session.\nIdentification of individual utterances from raw audio: 2. Manual labeling on the addressee of each utterance.\nIdentification of individual utterances from raw audio: Using the video annotation tool Anvil 5.1 [8] , one of the experimenter annotated the addressee for each utterance identified by the process in Step 1, and used these labels as the ground truth.\nIdentification of individual utterances from raw audio: 3. Manual labeling on body posture. The body posture was labeled to stand in five directions (front, left, right, slightly left, slightly right). Averagely there were 89 posture shifts in each session as the results.\nIdentification of individual utterances from raw audio: 4. Gaze direction. In this study, we used head direction as the approximation of gaze direction.\nIdentification of individual utterances from raw audio: In order to eliminate the difference among individual users (their distance to the screen, personal habits when speaking, etc.) and to prevent overfitting to slight head movements, only three conditions of the participants' gaze targets: gazing at the agent, gazing at the partner (the other participant), or gazing elsewhere were considered relevant to the final results. Precise eye gaze detection usually requires a dedicated hardware which can be very expensive (several dozens of thousand dollars), and is not practical to the systems which are meant for public use. Approximating gaze direction with head direction cannot be very precise. However, regarding to the standing positions of the experiment setting where the view angle between the agent and the other participant is 90 degrees. It should be large enough that the participants have to turn their head to switch their gaze targets between the agent and the partner. Head directions were acquired by using the face tracking software, FaceAPI 1 , which can measure the head position and rotation angles in terms of x, y, and z axes.\nIdentification of individual utterances from raw audio: The head directions of four randomly chosen pairs (or eight subjects) were manually labeled as target values (forward, left, and right for each subject). For example, in the case of the subject who is standing on the left side, he/she is evaluated as looking at the agent if he/she gazes forward, as looking at his/her partner if he/she gazes to the right, or as facing elsewhere if he/she gazes to left. All of the six head pose data and their measurement confidence scores were used as input teaching data. A C4.5 [11] decision tree was then trained with the Weka [5] data mining tool. As a result of 10-fold cross-validation, the classification accuracy of the learned model was 97.2%. Since this is a good enough degree of accuracy, this model was used to automatically label the head direction of the entire corpus (the four manually labeled groups were relabeled with the decision tree). The result was: averagely there were 489 Gaze direction shifts in each session.",
    "relatedWork": "RELATED WORKS: Many studies on the processes of recognition and interpretation in multi-party meetings have been done in the AMI and AMIDA projects [12] and in the projects of DARPA [18, 19] . By applying speech and natural language processing technologies, a number of useful components were developed in these projects, such as speech transcription, speaker diarization, and meeting summarization.\nRELATED WORKS: The management of speaking turns in multi-party dialogs is very related to the goal of this work. The intervention for active information providing is a higher level task that needs to consider the situation and the context of conversation in addition. In the literature of human communication, [7, 4, 13] , it has been reported that in addition to explicit verbal utterances, people also use nonverbal signals such as their gaze, nods, and postures to regulate their conversation, e.g., to show their intention to yield or the willingness to take next speech turn. The speaker looks at the addressee to monitor her/his understanding or attitude, and, the addressee looks at the speaker in order to be able to offer positive feedbacks in return [7, 1] . When yielding his/her turns to speak, the speaker looks at the next speaker at the end of his/her utterances [4] . Takemae [15] provided evidence that the speaker's gaze indicates addressee-hood and plays a regulatory role in turn management. The Japanese spoken dialog system, DUG-1 [10] realized rich turn-taking with rules based on linguistics features. Jonsdottir and Thorisson [6] proposed a system that learns turn-taking behavior on-line with an individual user in less 30 turns by utilizing reinforcement learning techniques. Bohus and Horvitz [2] proposed a turn management framework in the virtual receptionist application. They showed the verbal and non-verbal cues used by the avatar can shape the dynamics of multi-party conversation. However, the application task was relatively simple, a question-answering game with trivial questions.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Motivation. Our health begins with what we eat. Maintaining healthy eating habit is one of the key factors in reducing the risk of chronic preventable diseases, such as coronary vascular diseases and type-2 diabetes, and increasing life expectancy. This generally involves consuming a healthful and balanced diet and practicing portion control on a regular basis. Nevertheless, a lack of motivation, poor self-regulation, and personal biases tend to prevent people from successfully developing and maintaining healthy eating behavior.\nINTRODUCTION: Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. PervasiveHealth '20, May 18-20, 2020, Atlanta, GA, USA \u00a9 2020 Association for Computing Machinery. ACM ISBN 978-1-4503-7532-0/20/05. . . $15.00 https://doi.org/10.1145/3421937.3421960 Subsequently, researchers have looked into mHealth and mobile sensing technologies as a cost-effective way to facilitating healthy eating lifestyle at a population scale. A popular type of mHealth applications (apps) for dietary self-tracking is mobile food journal, such as MyFitnessPal, in which users manually record their food entries by selecting items from a food database. However, previous studies [1, 5] recently identified design flaws in mobile food journal apps which lead to unintended and undesirable user behaviors.\nINTRODUCTION: Specifically, many users were frustrated by the tedious nature of manual data entry effort [5] . Next, most food databases used by mobile food journal apps consist of a combination of commercially available and user-contributed food and nutritional records. Even so, many users were not able to find many food items, such as those from local restaurants [5] , due to poor data coverage. Lastly, unexpected lapses in healthy eating habits of mobile food journal users [1] suggested that mobile food journal apps tend to overly emphasize caloric and weight goals, whereas consumption of a healthful and balanced diet was not used as goals. Consequently, these design flaws limit the practicality and effectiveness of mHealth technologies in supporting healthy eating lifestyle.\nINTRODUCTION: Objectives. To address the aforementioned issues, we present Foodbot, a chatbot-based mHealth application for goal-oriented just-in-time healthy eating interventions. The main contribution of Foodbot is in its open-source design incorporating: (1) a naturallanguage user interface; (2) a large-scale food knowledge graph; and (3) goal-setting and JIT intervention and recommendation, to overcome the burden of manual dietary self-tracking and provide users with a personalized guidance toward a healthy eating lifestyle. Its behavior change design follows common techniques [13] , such as goal and planning and feedback and monitoring.\nINTRODUCTION: As shown in Figure 1a , Foodbot utilizes automatic speech recognition and mobile messaging interface, ubiquitous in most smartphone platforms, to replace traditional food-journal user interface. Users simply dictate what they have eaten through simple dialogue turns. Next, Foodbot relies on a large-scale food knowledge graph to ensure sufficient coverage of food items. A simple key innovation of the food knowledge graph is in its data-driven and open-source approach to constructing an extensive database of locally available food items, sourcing from multiple online food delivery services. Together with the dietary self-tracking data, the food knowledge graph also enables Foodbot to provide personalized food recommendation to the users.\nINTRODUCTION: Lastly, Foodbot allows users to set specific dietary intake goals following evidence-based dietary guidelines [1] , for example, a fish intake goal is at least two servings per week, a water intake goal is about 6-8 glasses per day, etc. After the goals are set, Foodbot will monitor users' progress toward those goals. As needed, JIT notification prompts will be sent to the users to remind them about their progress toward the goals. Users can also directly interact with Foodbot to inquire about their goals and request personalized food recommendations. The goal-tracking dialogue turns following a JIT prompt are shown in Figure 1b .",
    "relatedWork": "RELATED WORK: Early human-computer interaction (HCI) research has pioneered the applications of virtual agents in the health-related domains. Those studies typically focus on designing persuasive agents with human-like qualities for promoting physical activities [2] [3] [4] 15] and healthy eating [4, 12] in various populations. Recently, the widespread adoption of the smartphone & mobile apps and the substantial advances in wearable & mobile sensing, machine learning, and artificial intelligence (AI) technologies have renewed interests in conversational agents (commonly known as chatbot) as a promising solution to a population-scale and cost-effective delivery of personalized health interventions. Several studies have explored the use of chatbot for monitoring and promoting behavior change in a wide variety of healthcare domains [14] . For health promotion, a new wave of data-centric chatbots for physical activities [7, 10] and healthy eating [6] [7] [8] have recently been explored.\nRELATED WORK: For chatbots designed for supporting dietary monitoring and personalized feedback, our Foodbot is most similar to CoachAI platform [6, 7] . Both Foodbot and CoachAI employ similar behavior change techniques. However, CoachAI operates as a telemedicine platform, utilizing chatbot as a front-end user interface for monitoring and delivering interventions, thus requiring full supervision from healthcare practitioners. In contrast, all user interactions in Foodbot are completely automated, making it more scalable for population-wide interventions. Next, CoachAI uses wellness questionnaires to periodically collect overall dietary patterns from users, whereas Foodbot focuses on a food journal approach to dietary self-monitoring, which captures more granular data. Furthermore, Foodbot incorporates JIT mechanism in monitoring and providing personalized feedback.\nRELATED WORK: The usage of a natural-language user interface for dietary selftracking in Foodbot is similar to a nutrition dialogue system in [9] , where a speech and language understanding model was trained to recognize 975 food concepts, including quantity, brand, and description. Foodbot depends on services provided by Google Assistant and Dialogflow, which were further tuned, to recognize 177,700 food entities from natural-language inputs.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: \"Chatbots are the new apps\", Microsoft's CEO Satya Nadella proclaimed back in 2016 [17] . Indeed, over the last few years we have observed the appearance of a large number of respective bots, which enable interaction by human users in natural, written language. The rise is driven by powerful platforms which ease the creation of chatbots, as well as their easy integration into websites and popular messaging apps such Facebook Messenger, Sykpe or Telegram [9] . Overall, the global chatbots market is expected to grow at approximately USD 6 Billion, by 2023 [14] .\nINTRODUCTION: Currently, we see a major application focus of chatbots on customer service interactions: A lot of e-commerce providers offer chatbots integrated in their Websites, or via messaging platforms, which are able to inform customers about products and even trigger a purchase in a dialogue-style manner. Since these chatbots are able to initiate transactions, they are often referred to as virtual customer assistants (VCA).\nINTRODUCTION: Besides these VCA, so-called virtual enterprise assistants (VEA) are of increasing relevance, yet are, so far, a less common and studied application area of chatbots: VEA are chatbots for employees for simplifying interaction with incompany systems. In particular, VEAs are supposed to have great potential regarding efficiency increases and cost reductions for in-company IT help desks. Similar to a chatbot for customer interactions, a specialized VEA could assist with common IT-related troubleshooting tasks. While various available chatbots demonstrate the general technical feasibility, scientifically validated knowledge about the employees' attitudes regarding an in-company chatbot is scarce.\nINTRODUCTION: In this paper, we investigate the acceptance and user experience of such virtual enterprise assistants offering incompany IT support tasks. We are interested in the suitability of a chatbot for typical IT support cases as well as the experience and potential acceptance of users. To gain respective insights, we conducted a user study involving a sophisticated functional chatbot prototype (Figure 1 ). The prototype supported three typical IT support tasks (resetting a password, answering frequently asked questions, releasing an e-mail attachment from quarantine). 12 employees of a bank and hospital were confronted with the chatbot, assessed the supported services and compared their experience with traditional help desk channels. The contributions of this research include valuable insights into the overall user experience of an IT troubleshooting chatbot and validated recommendations for suitable use cases.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "RELATED WORK: Many existing recommendation systems already generate explanations, and several attempts have been made to classify these explanations [21, 22, 37] . In item-based explanations, the system relies on the previous recommendation's outcome to justify the current recommendation: \"I have recommended X because you previously liked/bought Y. \" Feature-based explanations use preferences that were specified by the user during the preference-elicitation process: \"Your interest in Z suggests that you would like X. \" These two different types can be combined: an example of item and feature based explanations can be found in [25] , where the system displays the features of previously liked movies to justify the current recommendation. In the domain of movie recommendations, a system can justify its decision by emphasizing a plot similarity [25] or an overlapping cast [30] . An evaluation comparing featurebased explanations, item-based explanations, and a combination of both shows that the hybrid explanation type was significantly more appreciated by users [30] .\nRELATED WORK: Both item-based and feature-based explanations are machinecentered and thus essentially reveal the system's decision-making process. Although they have a great impact on transparency, these explanations are tightly coupled with the types of features that the recommendation engine is relying on and may lack the persuasiveness and richness that humans often express when they recommend a specific item. Another important question regarding feature-based explanations is whether they should be personalized to match users' preferences. Research indicates that while personalization generally increases satisfaction, it can be detrimental to effective decision-making [33] . This shows how effectiveness and satisfaction aims can be discordant.\nRELATED WORK: Human-based explanations take an alternative approach; here, the system relies on collaborative filtering to reference similar products: \"People who liked X also liked Y.\" One such example is [13] , in which the system recommends social software items such as social groups or communities and justifies its choice by showing the names of people in the group/community, as well as their relationship to the user. This relation could be \"familiar\" if the user was friends with the person, or \"similar\" if both shared similar interests. The authors' experiment shows that when these people were \"familiar, \" users were more satisfied with the recommendations.\nRELATED WORK: Human-based explanations can be merged with feature-based explanations by combining existing reviews with users' preferences [7, 14] to generate explanations: \"You might want to watch X because Bob says that the storyline is amazing and I know that you are highly interested in plot. Here is his review: (...). \" This approach thus uses third-party opinions to justify choices. However, reviews are sometimes extremely long, making them difficult to integrate when conversing with a user.\nRELATED WORK: As recently demonstrated by [17] , researchers would benefit from taking a more human-centered approach for the design of their recommendation systems, i.e., building systems able to express their \"own\" opinions. The authors' recommendation system, which used social conversational strategies such as self-disclosures and reciprocity in its recommendation process, significantly increased users' satisfaction and intention to seek future recommendations.\nRELATED WORK: In this paper, we aim to build a conversational recommendation system that recommends movies by expressing its \"own\" opinions and experience through social explanations. We thus focus on the following research questions: RQ-1: What are the types of social conversational strategies that humans use when they describe a movie they watched to someone? RQ-2: Do social explanations used by a conversational recommendation agent to justify its recommendations influence the perceived quality of both the recommendations and the interaction?",
    "rq": " rq-1: what are the types of social conversational strategies that humans use when they describe a movie they watched to someone?"
}
{
    "intro": "INTRODUCTION: Engagement is the process by which two or more participants establish, maintain, and end their perceived connection [Sidner and Lee 2003] . In face-to-face conversation, to maintain communication, speakers sometimes glance at the listener and check whether the listener is properly engaged in the conversation. On the listeners' side, they display their engagement through verbal/nonverbal behaviors, such as verbal acknowledgement, head nods, and eye gaze. Specifically, eye gaze is useful for demonstrating that the listener is actively engaged in the conversation.\nINTRODUCTION: Recent studies on virtual agents and communication robots have revealed that conversational engagement is fundamental and indispensable in communication between human users and humanoid interfaces [Peters 2005; Sidner et al. 2005 ]. If the user is not fully engaged in the conversation, information presented by the system (agent) will not be properly conveyed to the user. Thus, in order to maintain communication with users, such systems need the ability to perceive users' nonverbal engagement signals, such as eye gaze and head nods. If a system can monitor the user's attitude toward the conversation and thus determine whether he or she is engaged in the conversation, it can then adapt its behavior and communication strategy based on his or her attitude. This is critical for information-providing systems such as explanatory agents, kiosk agents, and instructor agents. If the user is not actively following the agent's information feed, the system cannot construct a reliable user model. While tailoring explanations based on the user's understanding is one of the primary goals of systems that provide information, few studies have considered user engagement as a basis for modeling communication with the user.\nINTRODUCTION: To build conversational agents that are sensitive to user engagement, there are two primary aspects to be considered. First, the system must accurately perceive the user's nonverbal behaviors and estimate his or her engagement based on detected information. Thanks to progress in computer vision and human sensing technologies, accurate measurement of human behavior is now possible in real time. For example, under ideal circumstance, eye trackers can recognize a user's gaze points to an accuracy of 0.5 degrees at more than 100 Hz [Tobii 2013 ]. However, little has been studied about how to investigate such enormous volume of gaze point data to get better understanding of gaze-based communication signals.\nINTRODUCTION: The second aspect is to properly modify the agent's behaviors according to the user's disengagement status. More specifically, the system needs the ability to actively reestablish engagement with the user when it determines that he or she is distracted from the conversation. There are numerous possible ways to do this, such as using an agent's gestures to attract the user's attention, asking the user questions, and changing the conversation topic. We believe that one of the most promising approaches is to consider the user's engagement in dialog management, and to generate probing questions to ask the user such as \"Do you have any questions?\" and \"Do you understand so far?\" These questions will inform the user that the system can perceive his or her attitude. Accordingly, in the sections that follow, we will outline our proposed dialog management mechanism, which works with an eye-tracking system and generates probing questions when the system detects the user's disengagement.\nINTRODUCTION: To summarize, with the goal of creating a conversational agent that can adaptively manage a dialog based on the user's displayed attitude towards the conversation, this article proposes a method of detecting a user's disengagement from attentional behavior and implements a conversational agent that can modify the agent's response according to the user's level of engagement.\nINTRODUCTION: In the following sections, first, in Section 2, we review related research and then in Section 3, describe a Wizard-of-Oz experiment on data collection using an eye Gaze Awareness in Conversational Agents: Conversational Engagement from Eye Gaze 11:3 tracker. Then, in Section 4, an empirical study is described in which we investigate the correlation between human judgment of disengagement and gaze information. These include gaze transition patterns, mutual gaze, gaze duration, pupil size, and eye movement distance.\nINTRODUCTION: Based on analyses of the results, then Section 5 identifies useful information for estimating the user's engagement, and establishes an engagement estimation method that utilizes a decision tree technique. In Section 6, the decision tree is then implemented in a multimodal dialog system as a component of a conversational agent. In Section 7, the performance of the system is evaluated in a user study. Finally, Section 8 discusses ideas for future research.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: In human interaction, humans adapt and adjust their behavior according to the behavior of their interlocutors [6] . For example, a listener nods for indicating agreement with the speaker, gazes at the same object or smiles in response to the interlocutor's smile. In light of this, we aim to model an Embodied Conversational Agent (ECA) able to adapt its behavior according to the user's behavior. Nonverbal behaviors play an important role for maintaining engagement between humans and agent [2, 5, 12, 34] . This is why we are particularly interested in adapting dynamically the agent's nonverbal behaviors to those of its interlocutor.\nINTRODUCTION: To adapt the agent's behavior according to the user's one, we take advantage of the recent advances in the domain of neural networks, specifically a popular type of networks called LSTM. This approach simultaneously encompasses the sequentiality and temporality of non-verbal behavior over time. The designed model adopts a user-in-the-loop approach to constantly predict the behavior of the agent in response to the user's behavior. It takes as input both, the user's and the agent's past behavior and predicts the next agent's behavior. More precisely it predicts agent's smile, head movements, and gaze, behaviors we have considered so far. So our model encompasses the dynamics of both interactants' behaviors and predicts continuously (i.e. on a frame by frame basis) the agent's behaviors.\nINTRODUCTION: To integrate and evaluate our LSTM model, called IL-LSTM Interaction Loop LSTM, we create an interaction system where the agent interacts in real-time with a human user. The system takes as input data from the user (speech and nonverbal behavior), computes what the agent has to say as well as the corresponding animation. It is built upon four modules: (1) User's behavior detection and analysis based on multimodal analysis software EyesWeb [31] . (2) Dialogue management: we have used the dialog manager Flipper to define the dialogue rules (turn taking and verbal content) for the virtual agent [29] . (3) Agent's behavior prediction based on our IL-LSTM (Interaction Loop LSTM) model to predict the behavior of the agent for the next frame taking as input the behavior of both agent and user, during the past frames. (4) Behaviour generation from the predicted agent's behavior using the GRETA-VIB platform [25] . To our knowledge, our model is the first attempt to produce, in real time, behavior of the agent taking into account user and agent's past behaviors as well as the communicative intentions of the agent in order to create an interactive loop between the user and the agent. In our current model, we have considered the behaviors smile, head movements and gaze direction. So our model compute the agent's smile, head movements and gaze direction driven from both agent and user's smile, head movements, and gaze direction as well as agent's communicative intentions. The results showed that users were indeed more satisfied by their interaction with the agent when it adapted its behavior.\nINTRODUCTION: In this paper, we present our IL-LSTM model that continuously generates the agent's behavior that is both responsive to the user's behaviors and is consistent with the agent's intentions. First, we present an overview of related works 2. Then, in Section 3 we describe the database we used to train our model. Our IL-LSTM model is described in Section 4. Then, we present, in Section 5, the architecture of the interactive system implementing the IL-LSTM model. The experimental protocol used to evaluate our interactive system as well as the obtained results are presented in Section 6.",
    "relatedWork": "RELATED WORKS: In this section, we first, investigate what are the multimodal behaviors that maintain engagement. Then, we present an overview of existing models that focus on adapting the agent's nonverbal behavior according to user's behavior",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Every semester, a few hundred new computer science students start their academic studies at the University of Vienna. They all face the first difficult task -the transition from high school to university. Many first-year students struggle with this transition, and some even fail [4, 10] . The transition from a predetermined, structured school system to the independence of a university system leads to students feeling disoriented and often having difficulties motivating themselves [3, 9] . They must also adapt to higher academic demands and different teaching methods [10] . A smooth transition is, therefore, more likely if students have access to the information they need and feel socially connected [3] . Concerning the social aspect of transition, social presence is consistently linked to the motivation of the students and is also supposed to influence motivation [13] . The right support for freshmen in this transition phase can help the university to achieve a significantly higher persistence rate of first-year students [5] .\nResearch Interest: Q1: How can a virtual companion, based on Chatbot technology, support the onboarding process of freshmen?\nResearch Interest: In this context, we will consider the following possible approaches, where a virtual companion could help new students:\nResearch Interest: \u2022 getting to know the faculty building, the university campus, and the vicinity \u2022 collecting knowledge, best-practices, tips & tricks for everyday student life \u2022 network building with other freshmen Q2: Is a Chatbot suited for this problem? Some problems are suitable for conversational interfaces and some are not adequate for them. In this project, we want to find out whether the chatbot technology is a good fit for our use cases.\nResearch Interest: The goal of this project is to develop a concept and to implement it to ease the transition from high school to university for first-semester students, thus minimizing transition difficulties and increasing the retention rate; as a technology-based supplement and improvement to the already existing offers. Another goal is in-depth research on Chatbots and their possible applications. Since the past several years, Conversational Agents (CAs) such as Chatbots continue to experience increasing popularity as an interaction modality alternative or augmentative to classical graphical user interfaces (GUIs), prompted, among other developments, by advances in the fields of Artificial Intelligence (AI) and Natural Language Processing (NLP). The increasing availability and capabilities of this low-threshold modality of interaction with a conversational agent hold transformative potential in many scales and sectors of smart tools for humans, e.g. a potential in democratization of technology access for social good ( [6] ) or as a new basis of exploration of the notion of Companion Technology ( [1] ). It is subject to current debate, what role(s) Chatbots can play as interactional modality, as e.g. approached by [7] in pursuit of a typology of Chatbots (see Figure 1 ).",
    "relatedWork": "",
    "rq": "q1: how can a virtual companion, based on chatbot technology, support the onboarding process of freshmen?\n network building with other freshmen q2: is a chatbot suited for this problem?"
}
{
    "intro": "INTRODUCTION: Effective monitoring of symptoms can help mentally ill patients recognize so-called 'early warning signs' resulting in timely clinical interventions [1] . The most common methods of monitoring and assessing mental illness are based on clinical evaluation including interviews, questionnaires (e.g., the 17-item Hamilton Depression Rating Scale (HAMD) [7] and the Young Mania Rating Scale (YMRS) [22] ) and self-reports reflecting the patient's mental state (e.g., mood, stress or other symptoms). Studies have shown that these methods are subject to individual observer bias, lack of ecological validity due to recall bias, and incomplete assessment of behavior [2, 8] .\nINTRODUCTION: To address these issues, researchers have leveraged the advancement in smartphone technology enabling them to collect Ecological Momentary Assessments (EMA) from the patients in real-time, in real-world settings, over time and across contexts [18] . Smartphone based self monitoring systems like MONARCA [1] , PSYCHE [21] , MORIBUS [17] and other similar systems allow continuous collection of automatically generated objective data on behavioral activities (e.g., mobility, log of text messages and phone calls per day) as well as self-reported subjective data (e.g., mood, sleep length, activity level, stress, and medicine compliance). Given the fact that there is a lack of biomarkers for mental illness [9] , analysis of these data seems to be a highly promising approach to help mentally ill patients manage and cope with their diseases [1] .\nINTRODUCTION: A recent review of the literature suggests that self-reports in these types of systems are often collected from the patients in graphical and textual forms [5] which do not capture emotional nuances such as tone, volume, intonation, and rate of speech present in the human voice. These features of voice reflect the patient's emotional state which is a significant indicator of mental health conditions [15, 19] . Further, variations of speech have been suggested as one of the sensitive and valid measures of mental illness such as bipolar disorder and depression [13] .\nINTRODUCTION: Recently, researchers have analyzed voice features as an objective state marker of the patient's mental health by sampling from the patient's phone calls (e.g. [6] ), interactive voice responses (IVRs) (e.g. [13] ), and smartphone based conversational agents (e.g. [12] ). While these methods of collecting voice samples have been considered efficient, there exists a number of issues in terms of adherence, privacy, data storage and security [6, 20] . For example, Faurholt-Jepsen et al. [6] reported that the participants did not carry their smartphones with them at all times, calling from other devices and thereby not providing voice features during all their phone calls. It can be speculated that this method is not fully acceptable to the participants, perhaps due to the intrusiveness and privacy concerns. Further, the large number of voice features collected in the study proved to be a challenge in terms of computational costs and storage space. Likewise, Torous et al. called for a need to design and employ these technologies focusing on the user's perspective to increase adherence and acceptance [20] .\nINTRODUCTION: With an aim to address these issues, we propose a smart speaker based conversational agent -\"Hear me out\" -that will collect voice features of the patient's self-reports on sleep, mood and activity level in the home settings. We hypothesize that actively asking patients to self-report at his/her comfort while at home is less intrusive than other methods. Therefore, the proposed system is designed to record voice features exclusively when the patient self-reports, which will increase adherence and help reduce the burden of data storage and computational cost.\nINTRODUCTION: In this paper, we describe (1) the proposed system, (2) rationale behind using smart speakers, and (3) the challenges we are facing in the design process. During the workshop, we would like to get valuable feedback on the design of the system and suggestions to overcome our technical and methodological challenges.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": "2% are either uncategorizable (versus 24% for all commands) or questions, mostly about alexa 7% (versus 3.5% for all commands). in this initial period of exploration, only 4% of commands relate to smart home devices (compared with 14.7% for the entire set of commands. a sample of commands from this period include: \"alexa, tell us a joke,\" \"who was santo dumont?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: In the contemporary Internet of Things (IoT) era, people can interact with a multitude of smart devices, always connected to the Internet, in the majority of the today's environments [6] . Smart lamps, thermostats, and many other Internet-enabled appliances are becoming popular in homes and workplaces. Furthermore, by using PCs and smartphones, users can access a variety of online services, ranging from social networks to news and messaging apps. In this complex scenario, the End-User Development (EUD) vision aims at putting personalization mechanisms in the hands of end users, i.e., the subjects who are most familiar with the actual needs to be met [13] . Through visual trigger-action programming platforms such as IFTTT [3] and Zapier [4], users can \"program\" the joint behaviors of their own connected entities, i.e., smart devices and online service, by defining trigger-action (IF-THEN) rules such as \"if I publish a photo on Facebook, then upload it to my Google Drive\", or \"if the security camera detects a movement, then blink the kitchen lamp. \"\nINTRODUCTION: Despite apparent simplicity, previous studies [8, 15, 19, 20] highlighted many interoperability, scalability, and understandability challenges suffered by contemporary trigger-action programming platforms. In such environments, smart devices and online services are typically modeled on the basis of the underlying brand or manufacturer [8] : as the number of supported technologies grows, so do the design space, i.e., the combinations between different triggers (if s) and actions (thens), and users often experience difficulties in discovering rules and related functionality [20] . As a result, trigger-action programming becomes a complex task for people without any previous programming experience [16] . Some previous works, e.g., [8, 13] , tackled the identified issues by proposing to move towards a new bread of trigger-action programming platforms supporting a higher level of abstraction, with abstract and technology-independent rules that can be adapted to different contextual situations. With triggers such as \"when user is sleeping\" and actions such as \"illuminate the room\", users can personalize their connected entities by saving time and reducing errors, without the need of explicitly programming every single involved technology. While this vision seems promising, however, it is yet unclear how to effectively move from abstract users' needs to the real devices and services needed for implementing them. How can a system decide how to \"illuminate\" a room? Is turning the lights on the right choice for the user? Does the user prefer to open the blinds, e.g., because she is interested in saving energy? First, it allows users to communicate their personalization intentions and preferences (a). Then, it analyzes users' inputs, along with contextual and semantic information related to the available connected entities, to recommend a set of IF-THEN rules able to map the abstract users' needs to real connected entities (b).\nINTRODUCTION: In this paper, we present HeyTAP, a conversational and semanticpowered platform able to map abstract users' needs to executable IF-THEN rules. By exploiting a multimodal interface, the user can interact with a conversational agent, either by typing or by voice, to communicate her personalization intentions for different contexts, e.g., to personalize her room's temperature when she is near home (Figure 1a ). By interacting with the agent, the user can also specify her preferences on how to reach the goal of her personalization intention, e.g., convenience and preserving security in Figure 1a . To model such concepts, we extended the EUPont model [7] , a semantic representation for End-User Development in the IoT. We exploited the OWL 1 classes and individuals of EUPont to categorize triggers and actions offered by user's connected entities in terms of provided functionality, and to model contextual information, e.g., the devices and services owned by the user and the relative position. Furthermore, we added classes and restrictions to automatically characterize triggers and actions on the basis of the user's preferences, e.g., to discriminate between energy demanding and privacy invasive behaviors. All these semantic information are used to suggest a set of IF-THEN rules that satisfies the user's needs, i.e., intentions and preferences. The user can finally inspect the recommended rules in the multimodal interface and select one or more of them to personalize her connected entities (Figure 1b ).\nINTRODUCTION: To understand to what extent HeyTAP is able to successfully guide participants from abstract needs to actual IF-THEN rules, 1 https://www.w3.org/OWL/, last visited on January 18, 2020 we ran an exploratory experiment with 8 users. In the study, we challenged participants in freely personalizing a set of connected entities in different contexts. Results confirm the effectiveness of the approach, and show that HeyTAP can successfully \"translate\" abstract users' needs into IF-THEN rules that can be instantiated and executed by contemporary trigger-action programming platforms. Despite participants expressed their personalization intentions with different level of abstractions, in particular, the tool was able to address the 90.63% of the collected needs, by providing IF-THEN recommendations that satisfied the participants. The collected participants' feedback also highlights possible improvements that could inform future works that aim at assisting users in personalizing their smart devices and online services.",
    "relatedWork": "RELATED WORKS 2.1 Trigger-Action Programming: Opportunities and Issues: One of the most popular paradigm to empower end users in directly programming their connected entities is trigger-action [11, 19] . By defining trigger-action (IF-THEN) rules, users can connect a pair of devices or online services in such a way that, when an event (the trigger) is detected on one of them, an action is automatically executed on the latter. Trigger-action programming offers a very simple and easy to learn solution for creating end-user applications [5] , and trigger-action programming platforms such as IFTTT and Zapier are becoming popular [10, 15] .\nRELATED WORKS 2.1 Trigger-Action Programming: Opportunities and Issues: Recently, researchers started to investigate different aspects of these solutions, e.g., through empirical characterization of usage perfomances [18] and large-scale analysis of publicly shared rules [20] . Despite apparent simplicity, indeed, the process of composing IF-THEN rules in trigger-action programming platforms has been found to be a complex task for non programmers [16] , and the expressiveness and understandability of solutions like IFTTT have been criticized since they are rather limited [15, 19, 20] . Barricelli and Valtolina [5] analyzed the most popular end-user tools for personalizing connected entities, including IFTTT, and found that some of them \"offers a too complex solution for supporting end users in expressing their preferences.\" By evaluating thousands of trigger-action rules publicly shared on IFTTT, Ur et al. [19] found that the trigger-action approach can be both useful and usable for end-user development in IoT settings like smart homes, but they also found that the level of abstraction end users employ to express triggers needs to be better explored: many users, indeed, express triggers one level of abstraction higher, e.g., \"when I am in the room\" instead of \"when motion is detected by the motion sensor. \" In another study, Ur et al. [20] found that a large number of users is using IFTTT to create a diverse set of IF-THEN rules, which represents a very broad array of connections for filling gaps in devices and services functionality. According to the authors, however, the continuous growth of supported entities and connections highlights the need to provide users with more support for discovering functionality and managing collections of IF-THEN rules. The analysis emphasizes also the future need of making \"IFTTT rules more expressive. \" Similarly, Huang and Cakmak [15] conducted two user studies to systematically study how different types of triggers and actions, e.g., states vs. events, influence the understandability of trigger-action artifacts. They found users' inconsistencies in interpreting the behavior of IF-THEN rules and some errors in creating programs with a desired behavior.\nTowards a Higher Level of Abstraction: The aforementioned issues are strictly related to the \"low-level\" of abstraction of the adopted representations. Contemporary triggeraction programming platforms, indeed, mainly model smart devices and online services on the basis of the underlying brand or manufacturer, thus opening the way to interoperability, scalability, and understandability issues [8] : to program their IoT ecosystems, users need to know all the involved technologies, and they have to define many different rules even if they perform the same logical operations.\nTowards a Higher Level of Abstraction: To overcome the drawbacks of low-level representations, different previous works [8, 13, 19] envisioned a new bread of triggeraction programming platforms supporting a higher level of abstraction. In the context of smart homes, for example, Funk et al. [12] asserted that we need \"a new approach aimed at first capturing endusers' intentions and potential usage scenarios, then providing this information to a control system that learns to resolve intentions and scenarios for available devices in the context.\" Following this need, Ghiani et al. [13] proposed a novel trigger-action programming platform to let end users personalize the contextual behavior of their IoT applications through trigger-action rules. By exploiting an authoring tool, in particular, users can specify trigger-action rules that indicate the desired specific application behavior for the target contexts of use, e.g., \"when user is sleeping, do turn-off bedroom television. \" Corno et al. [8] , instead, developed EUPont, a high-level representation for IoT personalization that allows users to model abstract trigger-action rules like \"if I enter a closed space, then illuminate it. \" Such rules can be adapted to different contextual situations, independently of manufacturers, brands, and other technical details. Besides describing the model, the authors presented its integration in the architecture of a trigger-action programming platform, and they explored the advantages of using the model in the definition of trigger-action rules thanks to a user study. They found that the usage of a higher level of abstraction allows users to define IF-THEN rules with fewer errors and in less time with respect to existing solutions.\nTowards a Higher Level of Abstraction: While a higher level of abstraction in IF-THEN rules is a promising direction, the identification of the real devices and services to be used to satisfy users' needs becomes crucial. In this paper, we aim at presenting a conversational and semantic-powered platform able to map abstract users' needs to IF-THEN rules that can be executed by available connected entities.\nProgramming the IoT via Conversation and Recommendations: By using popular conversational agents such as Amazon Alexa [1] and Google Assistant [2] it is now possible to interact with a variety of different smart devices and online services via conversation. To the best of our knowledge, however, the only example of a conversational system that allows to personalize connected entities through the definition of IF-THEN rules is InstructableCrowd, a research prototype developed by Huang at al. [16] . InstructableCrowd is a crowd-sourcing system that enables users to create IF-THEN rules based on their needs. By exploiting a custom user interface on their smartphones, users can converse with crowd workers to describe some problems they are encountering, e.g., being late for a meeting. Crowd workers can therefore exploit a tailored interface to combine triggers and actions in appropriate IF-THEN rules that are then sent back to the users' phones.\nProgramming the IoT via Conversation and Recommendations: In our work, we focus on a similar goal by trying to automatically map abstract users' needs to actual IF-THEN rules, i.e., without the help of other users such as crowd workers. The idea is to adopt a semantic-based approach to analyze users' inputs and contextual information to recommend a set of appropriate IF-THEN rules from which a user can choose. Recommendations, indeed, could be useful to help end users use trigger-action programming systems, and advances in EUD have expanded the opportunities for offering recommendations [14] . In this context, in particular, some recent works investigated how to provide users with recommendations. Yao et al. [21] , for example, developed a probabilistic framework to suggest relevant smart \"things\" to be personalized based on user interests. Corno et al. [9] , instead, proposed RecRules, a semantic recommendation system that suggests trigger-action rules on the basis of content-based and collaborative information. None of such works, however, explore how to calculate recommendations by extracting users' needs via conversation.",
    "rq": " rq1. how would users interact with heytap?\n rq2. is heytap able to map abstract users' needs to executable if-then rules?\n rq3. what is the users' satisfaction in using heytap?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": " rq: how do the personification and interactivity of online interfaces influence users' disclosures about recent stressful events, in terms of a) the nature of the stressful event disclosed, and b) the intimacy and amount of information disclosed?"
}
{
    "intro": "INTRODUCTION: There has been significant work in the research and development community on the use of avatars, virtual agents and robotic agents to enable a more immersive conversational experience. This effort has led to the development of multiple software platforms and solutions for implementing embodied agents [1, 4, 5, 9, 10] . More recently, there has also been a push towards developing embodied virtual agents that are empathetic [2] and are directed toward Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. specific educational applications such as language learning [6] , including the possibility of targeted feedback to participants [3] . Here we demonstrate a fully open-source virtual dialog agent that can serve as a job interviewer for workforce training applications.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": " research questions: what are the psycholinguistic characteristics of user interactions with a dialogue system designed to act as a scheduling assistant?\n we seek to answer the following questions: what are the psycholinguistic characteristics of user interactions with a dialogue system designed to act as a scheduling assistant?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": " we measured: how many questions in each set did the person respond to vs. skip?"
}
{
    "intro": "INTRODUCTION: There is a growing societal need to address the increasing prevalence of behavioral health issues, such as obesity, alcohol or drug use, and general lack of treatment adherence for a variety of health problems. The statistics-worldwide and in the U.S.-are daunting. Excessive alcohol use is the third leading preventable cause of death in the United States [National Center for Chronic Prevention and Health Promotion 2011] (with 79,000 death annually) and is responsible for a wide range of health and social problems (e.g., risky sexual behavior, domestic violence, loss of job). Alcoholism is estimated to affect 10-20% of U.S. males and 5-10% of females sometime in their lifetimes. Similar risks exist with other forms of substance abuse. In 2010, the World Health Organization (WHO) reported that obesity worldwide has more than doubled since 1980. In 2011, 1.5 billion adults in the world were overweight, of which 500 million were obese, and 43 million children under the age of five were overweight [WHO 2011 ]. In the U.S. alone, obesity affects 33.8% of adults, 17% (or 12.5 million) of children and teens, and has tripled in one generation. These behavioral issues place people at risk of serious diseases, for example, obesity can lead to diabetes, alcoholism to cirrhosis, physical inactivity to heart disease.\nINTRODUCTION: On the positive side though, these behavioral health issues (and associated possible diseases) can often be prevented with relatively simple lifestyle changes, such as loosing weight with a diet and/or physical exercise or learning how to reduce alcohol consumption. Medicine has therefore started to move towards finding ways of preventively promoting wellness rather than solely treating already established illness. In order to address this new focus on well-being, health promotion interventions aimed at helping people change lifestyle have been designed and deployed successfully in the past few years.\nINTRODUCTION: Evidence-based patient-centered brief motivational interviewing (BMI) interventions have been found particularly effective in helping people find intrinsic motivation to change problem behavior (e.g., excessive drinking, overeating) after short counseling sessions, and to maintain healthy lifestyles over the long term [Dunn et al. 2001; Emmons and Rollnick 2001] . A methodological review of clinical trials of 361 treatments showed that out of 87 treatment methods, the top two ranked treatment styles were (1) brief interventions and (2) motivational enhancement therapies [Miller and Rollnick 2002] . It is reported that five minutes of advice and discussion about behavioral problems (e.g., alcohol or drug use) following a screening can be as effective as more extended counseling and that a single session can be as effective as multiple sessions [Babor and Grant 1992] .\nINTRODUCTION: Lack of locally available personnel well-trained in BMI, however, often limits access to successful interventions for people in need. Yet, the current epidemic nature of these problems calls for drastic measures to rapidly increase access to effective behavior change interventions for diverse populations. To fill this accessibility gap, evidence has accumulated about the general efficacy of computer-based interventions (CBIs) [Bewick et al. 2008; Cunningham et al. 1999; Hester and Delaney 1997; Portnoy et al. 2008; Skinner 1994 ].\nINTRODUCTION: The success of CBIs, however, critically relies on insuring engagement and retention of CBI users so that they remain motivated to use these systems and come back to use them over the long term, as necessary (e.g., for booster sessions, follow-ups, lifestyle maintenance sessions). Whereas current BMI interventions delivered by computers have been found effective, high drop-out rates due to their users' low level of engagement during the interaction limit their long-term adoption and potential impact [Portnoy et al. 2008; Vernon 2010] .\nINTRODUCTION: One crucial aspect positively affecting the health outcomes of BMIs (and most counseling techniques, for that matter) involves the ability of the therapist to establish rapport and to express empathy [Miller and Rollnick 2002] . Empathy is a complex phenomenon with no established definition to date. However, there is a general consensus that empathy can involve cognitive attributes or affective attributes which can also be combined during full-blown empathy [Goldstein and Michaels 1985] . Cognitive attributes of empathy involve cognitive reasoning used to understand another person's experience and to communicate that understanding [Hojat 2007 ] (or putting oneself in someone else's shoes). Emotional or affective attributes of empathy, on the other hand, involve physiological arousal and spontaneous affective expressive responses to someone else's display of emotions [Wisp\u00e9 1987 ] (e.g., people often unconsciously mimic someone else's perceived expressions of distress or joy). Someone can have a reflex-like affective physiological reaction to someone's experience (without cognitively understanding it), or a cognitive understanding of that person's situation (without physically expressing it), or both.\nINTRODUCTION: Because of their text-based-only interfaces, current CBIs can therefore only express limited empathy (mostly reflected in the choice of textual wording of the intervention).\nINTRODUCTION: Fortunately in the last decade, at the same time as CBIs are being developed and studied in healthcare, information systems research has promoted the study of emerging technologies for healthcare systems [Davis et al. 2010] ; and computer science research has progressed in the design of simulated human characters and avatars with anthropomorphic communicative abilities [Cassell et al. 2000 ] as emerging technologies that can change the fundamental nature of human-computer interactions. Expressive virtual characters have become increasingly common elements of user interfaces for a wide range of applications, such as interactive learning environments, e-Commerce, digital entertainment, and virtual worlds.\nINTRODUCTION: Virtual characters who specifically focus on dialog-based interactions are called embodied conversational agents (ECAs), also known as intelligent virtual agents (IVAs) . ECAs are digital systems created with an anthropomorphic embodiment (be it graphical or robotic) and capable of having a conversation (albeit, still limited) with a human counterpart using some artificial intelligence broadly referred to as an \"agent\". With their anthropomorphic features and capabilities, they interact using humans' innate communication modalities, such as facial expressions, body language, speech, and natural language understanding, and can also contribute to bridging the digital divide for low-reading and low-health literacy populations, as well as for technophobic individuals [Bickmore et al. 2009; Neuhauser and Kreps 2011] .\nINTRODUCTION: In this article, we posit that (1) using well-designed virtual empathic characters or ECAs for the delivery of BMIs has the potential to increase users' engagement and users' motivation to continue to interact with them, and that as a result, (2) users' increased exposure to engaging evidence-based BMIs will increase their effectiveness for behavior change.\nINTRODUCTION: In the rest of this article, we first review current research on motivational brief interventions (BMI) and embodied conversational agents (ECA). We then discuss our approach to develop a novel modality for the computer-delivery of brief motivational interventions (BMIs) for behavior change in the form of a 3D personalized On-Demand VIrtual Counselor (ODVIC), accessed anytime anywhere over the internet and shown in Figure 1 . We currently focus our work on alcohol consumption as a target behavior based on a successful evidence-based BMI intervention, the Drinker's Check-Up (DCU) [Miller et al. 1988] , whose existing computer-based delivery via a text-based interface has been shown to significantly help problem drinkers reduce alcohol consumption [Hester et al. 2005] .\nINTRODUCTION: We then discuss how we designed our ODVIC to partially simulate both aspects of empathic communication (affective and cognitive), using a scheme for the agent's dynamic display of facial expressions based on the user's perceived expressions (affective empathy), and a scheme for verbal reflective listening with sentences that paraphrase and summarize the user's answers (cognitive empathy). Without claiming that our virtual character can fully empathize with the user, which would require the ability to subjectively experience and understand the user's feelings, we then show, with results of user studies evaluations, that our ODVIC has enough expressive abilities to provide the user with a better experience than when interacting with the DCU delivered with a text-only interface or with a non-expressive character.\nINTRODUCTION: We conclude with a discussion of how these results encourage our long-term goals for our on-demand conversations with virtual helping agents, who not only can help people find intrinsic motivation to change problem behavior but, even more importantly, who could also keep users engaged for long-term evolving exchanges about healthpromoting lifestyles.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": " rq1: how do different chatting styles influence people's self-disclosure?\n and rq2: how do different chatting styles influence people's self-disclosure over time?"
}
{
    "intro": "INTRODUCTION: During the past few years, chatbots, which engage users in a one-on-one, text-based conversation, have been adopted for a wide variety of applications [8, 13, 21, 26] . Among various chatbot applications, a promising one is information elicitation (e.g., [51, 63, 64, 69] ). For example, Tallyn et al. use a chatbot to elicit user input in an ethnographic study [51] . Li et al. build a chatbot to interview job candidates and aid in talent selection [32] . Recent studies also show several benefits of chatbots for information elicitation, such as eliciting higher quality information than using traditional form-based methods (e.g., [29, 64] ).\nINTRODUCTION: Inspired by these efforts, we are building interview chatbots to conduct user interviews and facilitate user research. To conduct effective interviews, interview chatbots should have skills similar to that of effective human interviewers [32, 41] . One of such important skills is active listening-the abilities to understand and respond to a conversation partner properly [19, 45] . Active listening is shown to facilitate interviews, e.g., eliciting higher quality responses [19, 35, 45] and making an interviewer more socially attractive [59] . In addition, studies find that active listening helps not only oral communication, but also online text communication, including text messaging [2, 3] . Inspired by those findings, we hypothesize that interview chatbots with active listening would be more effective at conducting interviews and engaging interviewees. Figure 1 shows an example of such a chatbot, which can understand the user's input and summarize it in its response, making the user feel heard.\nINTRODUCTION: Despite recent advances in Artificial Intelligence (AI), it is still challenging to build capable chatbots [22] , let alone create chatbots with active listening skills. Below we highlight three main challenges specific to building effective interview chatbots with active listening skills.\nINTRODUCTION: First, it is challenging to build interview chatbots that can effectively grasp and respond to user input to open-ended interview questions, which is the core of active listening. For example, in one of our user surveys, a chatbot asked an openended question \"what's the top challenge you're facing\".\nINTRODUCTION: One user responded:\nINTRODUCTION: \"The biggest challenge I've faced is finding a since of purpose. Being around like minded individuals who are constantly wanting more out of life through countless jobs I've never found something I was proud of\u2026\"\nINTRODUCTION: Another user answered the same question very differently:\nINTRODUCTION: \"With a new baby I have a lot of additional expenses. So I have to try to obtain additional income. I try to earn extra income by working on mturk, but the pay is low and I don't like the additional time taken away from my\u2026\"\nINTRODUCTION: Given such user input, an effective chatbot should respond to each user empathetically to make them feel heard. Few chatbot platforms, however, enable chatbots to handle such complex and diverse user input. For example, popular chatbot platforms like Chatfuel [8] and Manychat [37] hardly handle user free-text input. More advanced platforms like Google Dialogflow [13] and IBM Watson Assistant [25] support Natural Language Processing (NLP), but they often require that a chatbot designer enumerate all user intents to be handled. With such a method, it would be very challenging to build an interview chatbot, since it is difficult to anticipate diverse user responses to open-ended questions and enumerate all possible user intents.\nINTRODUCTION: Second, it is difficult to build interview chatbots that can effectively handle complex conversation situations to complete an interview task. As indicated by a recent report, natural language conversations are nonlinear and often go back and forth [22] . In an interview, a user may digress from a planned agenda for various reasons. For example, some users may not understand an interview question and want clarifications (e.g., \"What do you mean\"), while others might dodge a question by responding with \"Why do you want to know?\" or \"I don't know.\" Users might also misunderstand a question or simply do not know how to answer it. For example, one user offered an ambiguous response to the question mentioned above:\nINTRODUCTION: \"Most challenges are met as an opportunity to grow. Hardest part is losing friends.\"\nINTRODUCTION: Users may also be \"uncooperative\" and intentionally provide gibberish or irrelevant responses, such as those observed in crowd-sourced user studies [16] .\nINTRODUCTION: To complete an interview task, a chatbot must \"remember\" and stick to an interview agenda no matter how many times or how far a conversation has digressed from the agenda. However, most chatbots support scripted dialog trees instead of dynamic, graph-like conversations required by effective interview chatbots.\nINTRODUCTION: Third, it is difficult for chatbot designers to take advantage of AI advances due to a lack of AI expertise or resources. For example, deep learning has enabled powerful conversational AI [36, 62, 65, 66, 1] and might help address the first challenge mentioned above. However, these models require large amounts of training data (i.e., interview data), which are hard to acquire.\nINTRODUCTION: Given the three challenges mentioned above, we explore new ways to build effective interview chatbots. As the first step, we are investigating the feasibility and effectiveness of using existing AI technologies to build effective interview chatbots with active listening skills.\nINTRODUCTION: Our investigation aims at answering two research questions:\nINTRODUCTION: RQ1: Whether and how can we employ publicly available AI technologies to build effective interview chatbots with active listening skills?",
    "relatedWork": "RELATED WORK: Our work is related to research in four areas listed below.\nConversational Agents for Information Elicitation: There is a rich line of work on developing conversational agents for information elicitation. These agents roughly fall into three categories: survey bots, AI-powered chatbots, and embodied AI interviewers.\nConversational Agents for Information Elicitation: Survey bots text users a set of choice-based questions with little natural language interaction [29, 49, 51] . They normally do not ask open-ended questions nor handle user digressions that may arise during a natural language conversation. More recently, AI-powered chatbots have been used to ask users open-ended questions via texting [20, 30, 63, 64, 69] . Additionally, there is a rich body of work on embodied AI interviewers (e.g., [4, 10, 18, 32, 34, 41, 53, 57] ). These AI agents have a human-like form and use both verbal and non-verbal expressions to communicate with users.\nConversational Agents for Information Elicitation: Among the three types of conversational agents, our work is most related to AI-powered chatbots. Similar to these efforts, our goal is to deliver engaging interview experience and elicit quality information. However, existing works are limited at handling complex and highly diverse user input [64, 69] . In contrast, our work reported here is intended to improve the conversation capabilities of these chatbots.\nTask-Oriented vs. Social Conversational Agents: Although conversational agents have been used in a wide variety of applications, they fall into two broad categories [52] . One type helps users accomplish specific tasks, such as meeting scheduling [22] and information search [69] . The other is to socialize with users without a task (e.g., [35, 62] ). Because of the constrained domains and the need for gathering accurate parameters (e.g., meeting time), rule-based approaches are often used to create task-oriented agents [69] . Although a recent data-driven approach to task-oriented agents shows early promises [6] , it is not ready for real use. In contrast, data-driven approaches are mostly used to support open-domain, social dialogues [35, 62] .\nTask-Oriented vs. Social Conversational Agents: Recently, researchers have developed conversation agents that support both task-oriented and social dialogues in one system [42, 66] . Similar to this line of work, our interview chatbots must support both task-oriented and social conversations during an interview. Unlike this line work, which helps users achieve a task like making a restaurant reservation, interview chatbots must complete its own information elicitation task. Such differences impose new challenges on building interview chatbots, such as handling uncooperative users or irrelevant user responses to open-ended questions.\nRecent Advances in Conversational Agents: There are numerous computational approaches to building conversational agents, including both symbolic and datadriven approaches [52] . To cope with highly diverse user input, data-driven approaches have been used extensively to handle open-domain conversations. A number of data-driven approaches are used to train retrieval models that find the most probable machine response from a repository of predefined responses for a given user input (e.g., [36, 62, 65, 66, 1] .) Additionally, generative approaches have been explored to synthesize machine responses that do not exist before (e.g., [46, 47, 65] ). However, the quality of generated responses may be erroneous or incoherent, not yet ready for practical applications.\nRecent Advances in Conversational Agents: Neither retrieval-based nor generative models alone are practical for building interview chatbots, since they require large amounts of training data-often millions or billions of conversation exchanges [65, 66, 1] . It is difficult to obtain interview data let alone large amounts due to the private or sensitive nature of many interviews. Moreover, a lack of interpretability and control of data-driven results would put an interview chatbot at risk especially in high-stakes contexts, such as customer interviews [54] .\nRecent Advances in Conversational Agents: To improve interpretability, recently, researchers have explored hybrid approaches. For example, Hu et al. propose to incorporate rules as the weights of neural networks to improve interpretability and performance [24] . Sundararajan et al. propose an approach to identify which input features contribute to the prediction of a deep network [50] . Their approach can extract rules from the networks to help interpret the prediction results and debug the networks. These hybrid approaches have inspired us in developing our prototype, which is perhaps the first of exploring a hybrid framework for building interview chatbots.\nChatbot Platforms: During the past few years, a number of chatbot platforms have been developed to facilitate the creation of chatbots. There are two types of platforms. The first type, including Chatfuel [8] and Manychat [37] , supports do-it-yourself chatbot making. However, they have little AI/NLP capabilities and cannot support the creation of interview chatbots with active listening skills. The second type, including Google Dialogflow [13] and IBM Watson Assistant [25] , offers AI/NLP capabilities but has a steep learning curve for non-AI experts to use the tools (e.g., they must understand NLP elements such as intents and entities). Moreover, most platforms in this category are designed for making task-oriented chatbots (e.g., restaurant reservation). They must be extended to support interview chatbots to perform tasks and be social at the same time.\nChatbot Platforms: Given the limitations of existing chatbot platforms, we decided to extend Juji [28] , a chatbot platform that supports both tasks-oriented and social dialogues and allows easy extensions, to build effective interview chatbots. Our decision to extend Juji is detailed in Section 4.3.",
    "rq": "q2: what do you enjoy doing in your spare time?\nq3: what is the best thing about you?\nq4: what is the biggest challenge you face now?\nq4: what is the biggest challenge you face now?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "Introduction: There is a growing interest in deploying spoken dialog systems (SDSs) in developing regions. In rural communities of developing regions, where infrastructure, distances, language and literacy are barriers to access, but where mobile phones are prevalent, an SDS could be key to unlocking social and economic growth (Barnard et al., 2003) . Some notable recent studies in this field include \"Tamil Market\" (Plauch\u00e9 et al., 2006) and \"VoiKiosk\" (Agarwal et al., 2008) . Both were kiosk-based SDSs providing agricultural information that were tested in rural, semiliterate communities in India. Nasfors (2007) also developed an agricultural information service, aimed at mobile telephone users in Kenya. \"Healthline\" was evaluated by a small set of community health workers in Pakistan (Sherwani et al., 2007) , who had trouble with the voice-based interface, presumably due to their limited literacy. In a more recent study, Sharma et al. (2008) evaluated a SDS designed for caregivers of HIV positive children in Botswana. The researchers found that the users performed equally well using touchtone as speech input, when navigating the system. In the current paper, we expand on this body of work by investigating the potential role for SDSs in connecting rural citizens of South Africa with government services, such as free education opportunities and stipends.\nIntroduction: South Africa is the leader in Information and Communications Technology (ICT) in Africa and has the most developed telecommunications network on the continent (SA year book 2006 (SA year book /2007 . In particular, mobile phone usage has experienced massive growth due in part to its accessibility by non-literate people and its \"leapfrog\" development, which skipped the interim solutions adopted in the developed world (Tongia & Subrahmanian, 2006) . The amount of mobile phone users in South Africa is an astonishing 30 million people -out of a total population of 47 million (Benjamin, 2007) . The percentage of both rural and urban households with mobile phones tripled from 2001 to 2007, while \"landline\" use declined. The accessibility and widespread use of mobile phones make SDSs a good candidate for low-cost information access.\nIntroduction: In South Africa, there are eleven official languages. Private companies, NGOs and government offices who wish to reach South Africans through print or audio, find it extremely costly to do so for each language. Heugh (2007) shows that in terms of speakers' proficiency, there is no single lingua franca for South Africans (see Figure 1 ). In fact, in 2001, only 3 million of 44 million South Africans were English speakers, the language in which most government messages are currently disseminated (Household survey 2001). Communities in rural areas struggle to access government services due to their remote locations. Most community members must travel long distances by foot or rare and costly public transport to access basic services. In their longitudinal household study on costs and coping strategies with chronic illnesses, Goudge et al. (2007) , for example, found that people in rural areas of South Africa do not go to free health care facilities because they cannot afford transport.\nIntroduction: NGOs face the same challenge when trying to reach rural populations. Many produce information to assist households affected by HIV/AIDS, for example, but most of the materials are published on websites; the cost of providing multilingual print materials is often too high. Due to low literacy levels, language, and a lack of infrastructure, the information remains inaccessible to the people who need it, especially those living in rural areas and townships (Benjamin, 2007) .\nIntroduction: Given the well developed mobile phone network and the relatively sparse alternative options in rural South Africa, the authors believe that multilingual SDSs can provide a low-cost solution to improving the ICT access of citizens who may currently be excluded from government services due to language, literacy and location. However, it is imperative to understand the target users and their environmental context as a first step to designing such a system (Nielsen, 1993) . In this paper, we provide background on the current state of rural government service delivery in South Africa and introduce the Lwazi project. We describe our field methods, and finally, we present our findings from the initial field work with design and deployment implications for the Lwazi system.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Cultural heritage is a valuable legacy of humankind. Information and computing technologies not only facilitated the reproduction, representation, and archiving of cultural heritage but also enabled new interaction techniques for easier access, browsing, and experience with cultural contents. In digital cultural heritage research, while there has been much work on tangible cultural heritage such as paintings, sculptures, architectures, and so forth, intangible cultural heritage has been comparatively less studied. Intangible cultural heritage, as defined by the UNESCO World Heritage Center, refers to 18:2 \u2022 X. Wang et al. \"intangible attributes of a group or society that are inherited from past generations, maintained in the present, and bestowed for the benefit of future generations.\" 1 For example, oral traditions, customs, ways of life, traditional craftsmanship, performing arts, theater, social practices, and so forth are all intangible heritages representing cultural identities and diversity of mankind, and they should be preserved and passed on to the current and future generations.\nINTRODUCTION: Modern computing technology provides us new opportunities for promoting better awareness, understanding, and appreciation of the intangible cultural heritage. Currently, however, virtual heritage applications, especially those for the philosophy domain, are often built for professionals for archiving purposes and used by only a small number of people interested in the subject (e.g., [Kim et al. 2007; Tiles and Bruya 2002] ). Particularly, social values and beliefs, or, in other words, the philosophy of a particular ethnic group, are abstract in nature, which makes it hard to be quantified and conveyed digitally and interactively. The general public usually has the image of philosophers being \"out of touch,\" and many people would be intimidated by the idea of studying such a difficult subject [White 2006; Warburton 2004 ]. The aim of our research is to disseminate intangible cultural contents (in particular, traditional Chinese culture represented by the Confucian philosophy) to the general public by leveraging the interactivity and appeal afforded by interactive media and artificial intelligence technology.\nINTRODUCTION: The Confucian school of philosophy, founded by Confucius, a Chinese philosopher and educator in ancient China, is the cornerstone of the Chinese culture. It was believed that Confucius did not author a single book, and his philosophy was spread by him talking to his disciples, the rulers, and other folks of his time. Indeed, conversation is an old way to communicate ideas and beliefs, but a very natural and effective one. Therefore, to build an interactive application for communicating cultural values, one promising avenue is to model the conversation from classical texts and re-create the experience of having conversations with the philosophers. In fact, use of natural language interaction for knowledge acquisition has been studied extensively in intelligent tutoring systems [Jurafsky and Martin 2009] , but there have been considerably fewer applications in cultural heritage research until very recently (e.g., [Mori et al. 2013] ). The reasons for the lack of work in this area might be (1) natural language is complex and ambiguous, which makes natural language understanding and generation very difficult; and (2) creating a conversational agent often requires precise modeling and/or heavy scripting, which is very time-consuming.\nINTRODUCTION: This article reports our work on a conversational agent that emulates the Chinese philosopher Confucius, allowing users to easily explore traditional Chinese culture through natural language communication with the virtual Confucius. The main contributions of this article are twofold. First, we developed a systematic method for the domain experts to easily create a knowledge base from unstructured philosophy texts. Second, we present an algorithm to utilize the knowledge base to answer open questions using a variety of natural language processing technologies, requiring minimal input from domain experts. The creation of virtual agents requires collaborative work between computer scientists and the experts in the domain in which the agent operates. Previously, the knowledge base creation was a laborious task that often required the content preparers to manually author large amounts of question-and-answer pairs and encode them using scripting language. The method we developed greatly reduced the workload of the domain experts by distributing part of it to the computer. The domain experts can work with technology they are already familiar with without learning complex computer skills such as scripting.\nINTRODUCTION: The rest of the article is organized as follows. In Section 2, we provide a review of conversational systems and their cultural-related applications. In Section 3, we present how we built the Confucius knowledge base and the rationale for our methodology. We then describe the technical details for",
    "relatedWork": "RELATED WORK: Conversational agents have a long history dating back to the 1960s, when the chatbot Eliza was invented by Joseph Weizenbaum [Weizenbaum 1966 ]. It is a computer program simulating a Rogerian psychotherapist, which can analyze the input sentence by predefined keywords and decomposition rules and then generate the answer by corresponding reassembly rules. Some clever tricks, such as including substrings of the user's input in the program's output and answering questions with questions, are used to create the illusion of understanding. The agent is able to continue the conversation despite an extremely limited knowledge base. Later developments of chatbots follow similar ideas, and new languages have been developed to facilitate script authoring and encoding. One famous example is ALICE [Wallace 2003 ], written in AIML (Artificial Intelligence Markup Language). Chatbots created by scripts can be very human-like when the user's input falls into the scripts, but it requires enormous effort to manually author each response rule. For example, the current ALICE implementation includes more than 40,000 rules and still falls short sometimes. Furthermore, such chatbots are mainly used for entertainment purposes or handling highly specific tasks.\nRELATED WORK: Another related area of work is intelligent tutoring systems (ITSs), where a computer system guides students through the process of learning, usually using natural language. The dialogue between a human student and a computer tutor is highly structured and needs to be designed by careful consideration of the knowledge to be taught as well as related learning theories [Jurafsky and Martin 2009] .\nRELATED WORK: Natural language interaction has also been explored in designing virtual museum guides. For example, Ada and Grace are two life-sized, photo-realistic characters installed at the Museum of Science in Boston to engage young people in science [Swartout et al. 2010] . The system works by having a statistical text classification algorithm to map the utterance text to the pool of preconstructed character responses. A library of scripted question-and-answer pairs are authored by the domain expert. A software called NPCEditor [Leuski and Traum 2010] is created to facilitate content authoring and rephrasing. Other virtual human museum guides include Pixie [Bell and Gustafson 2003] , which provides information about the agent and the exhibition in the Swedish Telecom museum. Another example is Max [Kopp et al. 2005] , equipped with the ability to converse about the museum, the exhibition, or other topics of interest. Similar works are virtual information kiosks, such as MACK (Media Lab Autonomous Conversational Kiosk) [Cassell et al. 2002] , which gives directions to visitors of the MIT Media Lab based on a repository of user queries and system responses, and August [Gustafson et al. 1999] , a talking head representing the Swedish author August Stringberg, which was displayed at the Stockholm Cultural Center to answer questions by predefined answers.\nRELATED WORK: To the best of our knowledge, there has been little research work on natural language applications specifically designed for intangible cultural heritage. One very recent work is from Mori et al. [2013] , who used a conversational agent in a serious game for communicating the cultural contents-the artwork of a particular artist. They developed a dialogue management system to facilitate cultural heritage experts to easily author the conversation without the need for learning scripting language. A similar work is the NICE HCA system, which is a conversational agent representing the fairytale author H.C. Andersen [Bernsen et al. 2004] . It is an edutainment system for children and teenagers to learn about the life and work of Andersen, by enabling natural communication through spontaneous speech and 2D gesture. Conversational skill is modeled by templates and topic-centered mini-dialogues. Another older work is the Synthetic Interview technology [Marinelli and Stevens 1998 ], which has real actors to play as the famous people, for example, Benjamin Franklin, and records short video clips of the character answering questions. The system then matches its input questions from the user to a preconstructed list of possible questions a guest may ask and plays the corresponding video clip, simulating a real interview experience.\nRELATED WORK: Besides the scripting or rule-based methods adopted by the aforementioned systems, there is also a trend to directly generate natural language statements using knowledge organization systems and mainly ontologies [Atzeni et al. 2004; Kimura and Kitamura 2006; Mehta and Corradini 2007; Galanis et al. 2009 ]. Some of them have been successfully applied in museum guides (e.g., [Galanis et al. 2009; Vogiatzis et al. 2008] ). While this method can be useful for natural language applications dealing with information that is highly structured and thus can be represented in a formal representation such as the OWL, 2 it is not suitable for our application. The writings of traditional Chinese philosophy are not formal philosophical works, but rather records of philosophers' sayings. Chinese philosophy is abstract and unstructured, and it is not possible to organize it into hierarchies, classes, and relations. This leads to our research question: How can we convert these unstructured texts into the knowledge base of a conversational agent in an easy and efficient way?\nRELATED WORK: The target questions addressed by the systems mentioned previously are mostly biographical and factual questions, such as life and work of a person or exhibition information in a museum. Such information can be easily anticipated and authored as templates or question-and-answer pairs. In contrast, in the case of a conversational agent for communicating cultural values, while there is factual information in the knowledge base, there is also a large part that is more subjective and free to interpretations. Thus, many of the questions to the agent would be asking for opinions, or advice, rather than simple factual information [Wang and Nakatsu 2013] . There is no way for us to predict the common questions that would be asked by users. Therefore, we need to explore a new approach other than the current standard practice of manual authoring in building the conversational agents, as it requires too much human effort and is almost unfeasible in our case.",
    "rq": " this leads to our research question: how can we convert these unstructured texts into the knowledge base of a conversational agent in an easy and efficient way?\n (1) is it not pleasant to learn with a constant perseverance and application?\n (2) is it not delightful to have friends coming from distant quarters?\n (3) is he not a man of complete virtue, who feels no discomposure though men may take no note of him?\n user: how do you deal with people who are rude to you?"
}
{
    "intro": "INTRODUCTION: As a global player, the Volkswagen Group is offering more than 240 vehicle models worldwide. The challenges of an international environment demand that the values of the brands are experienced by customers worldwide. Some emerging markets like China demand regional-specific HMI concepts. As a result, we try to localize the development with about 40,000 developers at 26 sites. In collaboration with our satellite research centers, we established an international evaluation process as part of the HMI conception. This paper reports a specific international study, where two different speech dialog systems were tested in a driving simulator. One of the main goals of this work was to examine the benefits of natural language understanding in an international context. The current study compares a traditional command and control system with a system that supports natural language input. We report objective measures based on driving performance using the Lane Change Test (LCT) [7] and efficiency/effectiveness parameters of the speech interaction. The subjective measures include the System Usability Scale (SUS) [2] and additional questions related to our tasks.",
    "relatedWork": "RELATED WORK: State-of-the-art infotainment systems in vehicles are characterized by a wide range of functions and a high complexity. The majority of these functions are used mainly for entertainment and do not support the primary driving task [6] . The design of humanmachine-interfaces that enable the driver to interact with these tertiary tasks (e.g. media, phone) but keep the level of distraction at a minimum is a consequent step towards a high level of comfort with respect to safety. Speech dialog systems have much potential to reduce eyes-offroad time while driving, because they do not interfere as much as visual-haptic interfaces with the resources needed for the primary task [14] . Broad research could provide evidence that the interaction with voice interfaces leads to lower distraction than interacting with visual-haptic interfaces [9, 12] . Speech is very resilient as a side channel, making the ideal modality for what [11] calls the secondary-task interface. Besides this safety aspect, we think that the speech modality is the most intuitive way of interacting with a system. The driver is not forced to translate his wishes into a menu-driven interface structure and could ideally say directly what he wants.\nRELATED WORK: Although some of today's speech systems show these advantages, users' experiences with them are characterized by interaction structures which often do not meet expectations. The fact that users can feel uncomfortable while interacting with SDS can be explained as failed dialog design processes, in which users lack knowledge about possible commands. The dialog is perceived as exhausting and complicated, which in most cases can lead users to change to a different input modality. The Volkswagen Group is trying to meet this challenge by a \"speak-what-you-see\" approach, where possible commands are shown in the head unit.",
    "rq": ""
}
{
    "intro": "Introduction: Studies show that many people nowadays suffer from de-pression which causes them to lose focus on their work life. However, people's life, in general, is on a path to an epidemic of depression. A recent study also shows that more than 25% of the people in the United States experience at least one episode of clinical depression [14] . In some cases, it further isolates them from general societal conversations leading to depressed mood or loss of interest in activities, causing significant to permanent impairment in their daily life. Many neurologists & psychologists have their research and have found that people who suffer from this particular mental health disorder, usually tend to open up, to whom they can relate closely, without any worrying factor of causing them harm or losing them. We need to look deeper into this phenomenon to understand this treatment for overcoming depression.\nIntroduction: Needless to say that early detection and treatment of de-pression is essential in promoting remission, preventing, and reducing the emotional dis-balance side-effect of depression. Current diagnoses are mainly subjective, inconsistent be-tween researchers, and expensive for the individual, who may be in need of help. Additionally, early signs of de-pression are difficult to detect and quantify. However, re-search shows that early signs have a promising potential to be identified using machine learning algorithms that could implement further in a wearable artificially intelligent de-vices (AI) like robots or smart-home devices [10] .\nIntroduction: Therefore, considering the above scenario and briefs, we propose a novel multi-modal HRI approach to identify this issue. Besides that, to test it out, we also aim to use a robot dialogue System that can be theoretically, claimed to communicate with a patient to induced their concern and provide feedback. The overall goal of this research is to establish a long-term supported version of a platform engine, to develop a fully animated, re-enforced learning, human-robot interaction system, for general health-care environments and deploy it further as a generic library for potential applications. Through this research, one of the primary goals, we identify, is to have a humanoid robot, which can further develop and engage in conversations with people when diagnosed with early-stage depression. Once detected, our standing framework will be an unobtrusive communication agent among the audience and inter-actors so that people can act naturally by will, with a humanoid robot. Future possibilities for the companion social-robot are endless, as one might presume. Having humanoid robots especially in our proposed domain will not only enable ro-bust, meaningful communications within HRI but will also provide a platform for our robot to enable initial diagnosis and further monitoring patients for any signs of physical or emotional deterioration. With a purpose of notifying the respective doctor or a remote caregiver, on the diagnosis, the robot will further provide reminders and even life coaching techniques to extend independence and quality of life. These features of our framework, which we plan to introduce within our model are as follows: CBT (Cognitive Behavioral Therapy) framework; addressing both anxiety and low mood modules; conversational user interface (UI) designed for use by nonclinical populations; Incorporation of automated tailoring; Quantitative Reporting of thoughts, feelings, or behaviors; Recommendation module for mind-fulness activities; Provision of viewing mental health status of the participant; Realtime engagement via social activities explicitly linked to specific reported mood problems especially ones encouraging nontechnology-based activities; gamification and Intrinsic motivation to engage and enable reminders for periodic engagement with the robot; Intuitive and straightforward interface for interactions; Verbal links to crisis support services.\nIntroduction: While the above recommendations were initially designed in the context of mobile phone Apps in mind, to our knowledge, their relevance in the context of a conversational interface for social robots has rarely changed, and, for the records, as our proposed framework is currently under ex-tensive development and this is a conceptual paper explaining our proposed solutions intent plus, its working frame-work design methods, the results generated so far, and it's infancy to make highlight in this paper. The project plan is to update the future revisions with rich-positive results from working model design, as explained further in the pa-per. To our knowledge, this particular approach has not been tested by others and we aspire to explore the results to much deeper extents.\nIntroduction: To summarize our primary goal, this paper is to introduce an ultimate multi-modal human-robot dialogue interaction system for automatic depression detection (ADD) based on cognitivebehavior-therapy (CBT) process. Some studies, such as [10] , [13] and [4] , have done similar approaches to detect depression from either text, audio and facial expression. They propose machine learning algorithm for depression detection as successful in maximum cases. ADD presents a novel approach focusing on two aspects that receive research attention: class imbalance and data representation (feature extraction). To further maneuverer this conceptual paper, the following sections are oriented in the following manner: Section 2, describes the CBT model. Section 3, introduces our initial draft of the proposed framework for multimodal Robot Interaction System. Section 4, describes a potential dialogue System for testing the proposed framework. Section 5 goes through some related research, and finally, we conclude the paper in Section 6.",
    "relatedWork": "Related Work: Many approaches have been taken to target this problem of depression among younger generations and many of which have been known to utilize the Cognitive behavior therapy framework as a method to treat it as well. Today, approximately 74% of mental health diagnosis have their first onset before the Age of 24 [1] . Depression and anxiety are becoming very common in students on daily basis, and, it is recoded, that half of the students, report symptoms, as much severe, leading them further to have difficulty in functioning since previous year itself [6] . In addition to above, epidemiological data also suggests that mental health diseases especially depression is on the path to increase in prevalence and severity. Also that, 75% of students who are required to further act upon their mental health condition, do not take action or even access any of the clinical ser-vices. [6] Given the fact, while the reasons are varied with each case, even with inexpensive to free, access to clinical care within colleges, it does not seem to change or affect the epidemiological numbers over time [15] .\nRelated Work: Being said that, we believe the primary reason for the above situation is societal stigma and ignored sense of personal affection, which further leads to a loss of integrity within students and clinic professionals. Studies and Inter-views have shown that today people have become emotion-ally stimulant and invested in a keepsake, like an exclusive t-shirt, or book, a photo, because of what it represents to them or what it reminds to them of a particular event, or a sense of belonging as they share a long history with that item.[11] Social Robots today, not just as an assistant, but have been defining, how vital its role is becoming day by day in our lives.\nRelated Work: With recent advancements in better appearance, voice recognition, conversational interfaces and better understanding, have begun to emerge, these social robots have become a resource to provide an interface for us humans to communicate with them more candidly and socially. Conversational agents (such as Apple's Siri or Amazon's Alexa) may be a more natural medium through which individuals engage with technology, but it has been found that we respond and converse with any non-human agents in ways that mirror emotional and social discourse dynamics when discussing behavioral health [2] and their capacity to act as first responders has already been evaluated [12] . Today, Conversational interfaces are emerging with more research and frame-works, but if better positioned with social robots than visually oriented mobile apps to deliver structured, manualized therapies in addition to delivering therapeutic content, even they can mirror therapeutic process, Indeed. In [2] , Bick-more et al. demonstrated boldly via his experiment, that, by carefully designed health-related conversational agents, they could establish a therapeutic relationship irrespective of physical presence of another human, with their patients and further increase their daily conversational and physical exercise [2] . The intervention in his experiment was nothing more than an embodied conversational agent, that had with a graphical face to mirror human interactions. This experiment of his, marks a very important feat. in HRI systems for us to see a glimpse in future and enable similar conversation Interfaces via social robots to conduct such trivial tasks like the one explained in the paper, detection and provision of CBT Therapy. The capacity of utilizing social robots as mediators to de-liver CBT is a question worth exploring given the fact, that approximately around 20 million US college population have shown evidence-based digital apps to reduce the burden of mental illnesses among their peers [7] Unfortunately, the few mobile apps that has been evaluated formally have seen substantial challenges to sustainability since they tend to be built in either academic research settings and rarely having access to the required infrastructure / participants to assist them [2] . It will help us to a possibility for adaption and creation of a multi-modal module not as a phone app but as a physical social-able robot as it will not only target to a selected population but it will also make itself available to converse with other people who come across the robot. Given the variability in quality of available mental health apps, a conversational agent within social-humanoid robots not only opens up our doors for more research within expression and perception deliverables but also allows us to utilize 15 out of the 16 evidence-based recommendation system's framework which are widely used for previous approaches to built such phone apps as well [1] . These 15 Frameworks which we plan to further introduce within our model are as follows: CBT framework addressing both anxiety and low mood modules; Conversational UI designed for use by nonclinical populations; incorporating automated tailoring; reporting of thoughts, feelings, or behaviors; recommending activities; provision of mental health information; real-time engagement; activities explicitly linked to specific reported mood problems; encouraging non-technology based activities; gamification and intrinsic motivation to engage and providing reminders to periodical engagement, intuitive and straightforward interface for interactions and including verbal links to crisis support services. While these recommendations are created in the context of mobile phone apps, to our knowledge, their relevance in the context of a conversational interface for social robots has never been tested prior and is this paper's goal to explore.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Stress is the human body's natural response to daily pressures and threats to one's well-being, whether such threat is real or imagined [13] . Negative stress, or distress, occurs when one feels overwhelmed and is unable to cope with the pressures from situations or life events that have become unmanageable [4, 21] . Short-term symptoms of stress include decreased attention and concentration, difficulty in making decisions, challenges with interpersonal relationships [20, 42] , and continued feelings of fear or anger [40] . Overtime, these symptoms can lead to anxiety and depression.\nINTRODUCTION: For senior high school students, the overwhelming amount of academic workload and the expectations for good academic performance may cause them to experience a type of distress called academic stress [25, 35] . These students may not realize that their symptoms are caused by a mental health condition or may not acknowledge that they need help. Various interventions are available to help individuals become aware of their condition and to begin managing their stress, foremost of which is talking to family members or trusted friends. However, students tend to avoid approaching others regarding their stress or to seek therapy from a health care provider because of fear of social stigma [7, 23, 34] .\nINTRODUCTION: Recent approaches have seen the utilization of technologies to foster coping skills for managing stress and to promote emotional well-being and mental health among students. These include the use of online apps and podcasts to participate in online stress management classes and self-help programs [22] , and access to online social therapy through synchronous text-based counselling [10] , online mental health forums [30] and chat assistants [7] . These technologies provide mental health services and support through prevention, assessment, diagnosis, counseling and treatment programs [3] . Wearable technologies such as Garmin Vivosport and Apple Watch, mobile apps like Headspace, and even video games [29] are also used to manage, monitor and cope with stress.\nINTRODUCTION: One particular technology that has the potential in addressing students' stress-related concerns is the chatbot. A chatbot is an artificial intelligence software that is capable of conversing with humans through voice and text interfaces. They are accessible anytime and anywhere, in web sites, mobile applications and messaging platforms, thus presenting the potential benefit to students as a conversational partner. Chatbots may serve as an intermediate supplement for human interaction. They never get tired and never give in to their emotions. With chatbots, students no longer have to fear the social stigma associated with mental health symptoms and their concerns on privacy, allowing them to comfortably approach their stress with the help of the conversational agent.\nINTRODUCTION: Senior High School (SHS) students find themselves in a more competitive environment with new responsibilities to fulfill their academic requirements. It is a milestone that transitions them from dependency to self-reliance in preparation for tertiary education. They encounter new classmates coming from differing backgrounds and University professors with heightened expectations than their junior high instructors. These changes can cause stress which they need to manage in order to perform well in school. In this research, we investigated the perceived effectiveness of chatbots among SHS students who used Woebot [12] as their conversational partner for a 2-week period. Our findings provide insights into the students' experiences in using Woebot to share and to cope with their daily stress due to school-related activities.",
    "relatedWork": "RELATED WORK: Stress is a normal part of daily human experience as the body's natural defense against pressures in life. Any physical, chemical or psychological factor that causes bodily or mental tensions can also constitute itself as stress [45] . Stress can have positive and negative effects. Exposure to positive stress can increase one's performance and productivity. Extended exposure to stress, however, can lead to physiological dysfunction [22] .\nRELATED WORK: Seward [40] classified stress into three types: eustress, neustress and distress. Eustress is a positive stress that motivates or inspires people to continue working. Neustress is neither positive nor negative. When stress reaches the point where it is no longer manageable, distress begins to manifest. Distress is a negative stress that can cause harm to one's physical and emotional states. Tension starts to build up and a feeling of discomfort or irritability arises. A person in distress may generally exhibit the following characteristics: irritability, touchiness, easily startled, over-arousal, tense, inability to relax, and impatient [45] .\nRELATED WORK: In the following sections, we describe a type of stress, called academic stress, that is typically experienced by students due to their workload in schools. We also give an overview of the most common type of therapy offered as a form of intervention and support, the cognitive behavioral therapy (CBT). We then look at various chatbots that have been designed as companion or social bots to provide emotional support to their human users.\nAcademic Stress: Multiple academic demands such as assignments, examinations, projects and other requirements, combined with the pressures from home and school environments, interpersonal relationships, social and extra-curricular activities, may exceed not only the cognitive load but also the emotional capacities of students. Being overloaded with school work and frustration over academic failure, students can become more prone to symptoms of academic distress [18] .\nAcademic Stress: Freshwater [13] classified distress into acute, episodic acute and chronic. Acute distress is predominated by negative thoughts from recent events or upcoming situations that need to be fulfilled. Frequent acute stress can lead to episodic acute stress where one feels pressure from taking on many responsibilities. Symptoms include irritability, anger, anxiety, impatience and depression. As a person feels hopeless and unable to find a way to escape from the causes of distress, he/she ceases to seek answers with regards to his/her problems. Chronic stress can then manifest which can severely deteriorate the mental health of an individual.\nAcademic Stress: A study by Bhargava and Trivedi [2] among 300 students showed that academic tests, relationship problems, life changes and career exploration were the main causes of academic stress. A study by [1] among 1120 students using the Modified Educational Stress Scale found that students who were academically stressed were 2.4 times more likely to get depression than those who did not.\nAcademic Stress: Academic stress can also bring positive effects as a way to motivate students to do better on academic requirements and tests, to achieve their fullest potential, and to teach them how to react and come up with the best solutions during stressful situations [11] .\nInterventions and Support: Cognitive Behavioral Therapy (CBT) can reduce stress by changing the way we think about stressful situations, such as \"focusing on more positive aspects of a situation and reassessing what their likely impact might be\" [21, 27, 44] . In a clinical counseling session, De Kooker [8] reports that a therapist helps his/her patients manage their stressors (causes of their stress) by helping them recognize and deal with psychosocial factors. This can lead to the development of a range of personal coping skills to manage stress and reduce one's vulnerability to anxiety and depression [22] .\nInterventions and Support: Internet-based self-help programs deliver computerized Cognitive-Behavioral Therapy (CBT) as a supplement to face-to-face therapy sessions to help reduce symptoms of depression [32, 41] . Combined with social network support, these technologies can lead to the development of supportive relationships and help lessen feelings of isolation among the youth [26] .\nInterventions and Support: The moderated online social therapy (MOST) project [7] has investigated the effect of online social therapy in increasing the self-esteem of young people. The project delivers a therapeutic environment for learning and practicing therapeutic techniques, gaining perspectives and obtaining support and validation through peers and developing psychological skills. MOST adopts the strengthsbased model of psychotherapy combined with online moderators and the interactive delivery of engaging therapeutic content. Specific features of the system include a Facebook-style newsfeed of shared posts, talking points that encourage social interaction through discussion and sharing of experiences, shared problemsolving, and performing behavioral experiments that allow one to practice applying mindfulness, self-compassion and personal strengths in real-world contexts.\nChatbots for Emotional Support: Chatbots and conversational agents have benefited different fields including healthcare [19] and mental health [16, 33, 39] . They can scale quickly to enable increased access to health services amidst the shortage of mental health personnel, long waiting times, perceived stigma and growing mental health challenges [17] . Empathic and AI-enabled, chatbots have been used to provide supplementary or complementary emotion support and assistance. Various studies that analyzed their use reported the promise that conversational agents hold as interventions for the treatment of mental health problems in terms of efficacy and acceptability [14, 17] .\nChatbots for Emotional Support: Replika [28] is a virtual assistant and a digital companion bot.\nChatbots for Emotional Support: While it cannot answer information-seeking questions and perform simple commands that are typically exhibited by commercial voice assistants like Apple Siri, Amazon Alexa and Google Assistant, it has the ability to emote and to ask questions that can get users to open up and share what they feel [28] . A daily conversation with Replika starts with the chatbot asking what the user did during the day, to narrate the best part, to share what he/she is looking forward to the next day, and to rate his/her mood [24] .\nChatbots for Emotional Support: Wysa [17] is an emotionally intelligent mobile app chatbot designed to promote mental well-being through text-based conversations. It uses self-help practices such as CBT, dialectical behavior therapy, motivational interviewing, positive behavior support, behavioral reinforcement and guided microactions to build resilience skills among its users.\nChatbots for Emotional Support: Eren [38] is a conversational agent that uses storytelling as a means of motivating children age 9 to 11 years old to share emotional events they experienced. It utilizes the Emotion Coaching Model [15, 37] to help children name their emotions, identify events that trigger these emotions, and guide them through reflective discipline to devise the next course of action. As an affective companion, EREN prioritizes active listening over discplining to setup a comfortable environment where children can freely share their emotional stories without fear of judgement or reprimand.\nChatbots for Emotional Support: Woebot [12] is a chatbot that employs CBT to engage people who are experiencing anxiety and depression in brief daily conversations. A session with Woebot commences with asking the user what he/she wants to do: gratitude journal, need help or check-in. The interaction flow is depicted in Figure 1 .\nChatbots for Emotional Support: When the user chooses the gratitude journal, Woebot asks him/her to share his/her positive experiences within the last 24 hours. If the user needs help, Woebot prompts him/her to elaborate on the problem, then proceeds to give a lesson and/or a story that is most appropriate to the shared problem. If the user chooses to check-in, Woebot asks the user about his/her mood and what he/she is doing to address this. The chatbot's response will be based on the user's mood: a happy or neutral mood leads to the gratitude journal while a negative mood leads to giving help. Stories are delivered in the form of dialogues which narrate the experiences of the chatbot or other made-up characters. The chatbot may also play videos as a mechanism to help the user develop a positive mindset.\nChatbots for Emotional Support: Woebot supports a number of other therapeutic features, namely empathic responses, tailoring, goal setting, accountability, motivation, engagement and reflection. Empathic responses is the capability of Woebot to respond to its users emotionally depending on the user's mood. For instance, when Woebot detects that the user is happy, it can respond by saying \"Yay, always good to hear that!\". Woebot can also tailor its messages based on the user's mood. It can help users set goals by asking them want they want to accomplish over a two-week period. Woebot also establishes a sense of accountability with its users by setting regular schedules and following up on previous activities. To motivate users and to encourage positive thinking, the chatbot sends personal messages daily accompanied with emojis and animated gifs. Lastly, it provides a weekly chart of the user's mood to encourage self-reflection. ",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Conversational Agents or Spoken Dialogue Systems have been a topic of research for decades [7, 11, 25] . Much of the research has focused on task-oriented systems, where the emphasis is on completing a user's goal (such as flight booking) through a series of dialogue turns [19, 27] . Conversations are designed to be short and functional in order to support completion of the task, with little or no attempt made to entertain or establish a relationship with the user.\nINTRODUCTION: Chatbots, on the other hand, (e.g. ALICE [1] ) are designed to promote extended, unstructured conversation more characteristic of human-human interaction, often with no particular 'goal' or task beyond entertaining and engaging the user. Significant strides have been made in this area in recent years, in part due to initiatives such as the Amazon Alexa Socialbot Grand Challenge [10] .\nINTRODUCTION: Largely, however, conversational systems remain divided between those classed as task-oriented, and chatbots focused solely on entertainment [4] . Given that human conversations usually interleave social content with task content [20] this division is artificial and unnatural. To create more engaging interactions, it seems apparent that conversational systems must support both task completion and entertaining chat, and allow the user to switch effortlessly between the two. However, there is surprisingly limited empirical evidence on the subject.\nINTRODUCTION: Voice-based intelligent personal assistants released by major tech companies such as Amazon's Alexa, Google's Assistant, and Samsung's Bixby provide access to both entertainment and task-based interaction, but these features have to be requested or invoked separately and are often limited to tasks or jokes etc. that are completed with a single user-system turn. These systems are predominately used within the home and are targeted at a specific known user.\nINTRODUCTION: Here, the domain of interest is a public building, specifically a smart building on a university campus, where many different people can walk up and use the system. We present and evaluate a conversational agent deployed on a Google Home Mini that acts as host of the building, and has the ability to combine state-of-theart open-domain social conversation with task-based assistance relevant to the context. Such a setting can provide us with valuable insights for design in similar situations, such as communal spaces in other public or professional buildings, and for example elder-care facilities 1 .\nINTRODUCTION: In an empirical evaluation with 32 users we investigate the benefits of this combined approach by comparing our socially-enabled bot to a task-only version of the system. The effect on the user experience is measured using a range of subjective and objective measures, including user attitudes towards the two different systems, their explicit preference between the two and measures of conversational efficiency and dialogue quality.",
    "relatedWork": "RELATED WORK: Some limited work exists on combining chat and task-based dialogue. In [26] , for example, it was found that interleaving task and social content tends to increase both users' engagement and task-performance. Here the 'task' was audience research relating to movies; establishing users' preferences and recent viewing history in order to provide them with a movie recommendation, and the film distributors with useful audience data. The task itself was arguably therefore fairly social in nature. All interactions were text-based and the user evaluation was carried out by Amazon Mechanical Turk workers in a home setting.\nRELATED WORK: Previous research in the context of intelligent buildings showed that the inclusion of social dialogue had a positive effect on users' compliance with suggestions from an Embodied Conversational Agent (ECA) for energy-efficiency behaviours [9] . Here the context was an office environment and social dialogue was limited to two turns of small-talk at the start of the interaction (\u0142Hi, how are you?\u017e and \u0142What's your name?\u017e).\nRELATED WORK: Research on relational agents has investigated the use of small talk in \u0142greasing the wheels\" of task-based dialogue with a realestate ECA (called 'REA') and found positive effects on engagement and trust [2] . Similarly, an evaluation of the museum guide 'Tinker' showed that when the ECA employed relational behaviours that included social dialogue this led to significantly greater engagement by museum visitors [3] . Here, input was via multiple choice utterance input screens. Neither system had the capacity for open-domain chat.\nRELATED WORK: Pecune et al. [17] carried out a field trial of a socially aware ECA who acted as host at an international conference. The system is organised around a task-based pipeline and a social one. However, on closer inspection the social aspects are limited to 'decorating' the task content in terms of the style of the sentence, rather than the content itself, and during the trial itself the system was only partially automated.\nRELATED WORK: In one of the most relevant examples [16] researchers investigated combining task and social dialogue in a robot intended for deployment as a guide in a shopping mall. Their evaluation showed that the hybrid task+chat system was rated significantly more pleasant and better met the users' expectations than a task-only version. This is encouraging given that, in the words of the authors, the robot had \u0142rather simple\u017e chat capabilities. As with much of the other research on this topic, also, it was carried out in the context of a multimodal interface.\nRELATED WORK: The conversational agent examined in this paper is currently deployed as a voice-only assistant, on Google Home Mini devices which in a real-world deployment have the advantage of being widely-available, low-cost devices.\nRELATED WORK: Voice-only assistants in the form of smart speakers have been much studied in the home setting [18, 21] ; however, there is little evidence of research on their use in public spaces. In one study, Lopatovska and Oropeza [13] investigated the adoption of Amazon's Alexa by students and staff in the foyer of an academic building. They found that only 10% of foot traffic interacted with the Echo device, with survey participants reporting lack of need, lack of awareness, and the perceived awkwardness of interacting in a public space as their reasons. Enquiry patterns also differed to those reported by studies in the home with user requests in public consisting largely of questions about Alexa's features, \u0142opinions\u017e, and mindless questions. It should be noted, however, that only native Alexa content was available, with no tailored building or university-specific information \u015b or a wider ability to chat.\nRELATED WORK: We present a building host that combines task-based assistance relevant to the context with a level of open-domain chat capability not found in any of the above systems. The system is based on the social bot Alana, which was twice finalist in the Amazon Alexa challenge, and can converse on topics ranging from the latest news headlines to philosophy [5, 14, 15] and has previously proven successful as a foundation for other conversational AI projects [6] . We contribute to the field with an empirical evaluation of the benefits of combining such open-domain chat capability with task-based dialogue in the context of a real-world deployment in a public setting.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Autonomous, self-driving vehicles are expected to revolutionise everyday travel with anticipated benefits of improved road safety, efficiency, comfort and mobility, but are not without their own set of unique Human Factors issues [20] . First experiences are likely to be in driverless 'pods' that operate in specific contexts, for example, 'geo-fenced' environments such as university campuses, airports etc. [29] , with several examples already under evaluation. Nevertheless, major concerns have been expressed regarding the public's willingness to adopt the technology [21] , particularly relating to issues of trust [17] . The development of trust is also an important component of successful human-human conversation [11] . The study therefore explores whether people's conversational behaviour (for example, the strategies and mechanisms they use to establish and calibrate trust), is the same when the second interlocutor is the vehicle itself.\nTrust: Trust has been defined as an individual's willingness to depend on another party because of the characteristics of the other party [33] . Routed in human sociology and psychology, the concept of trust was originally attributed exclusively to interpersonal relationships, and can be viewed as a mechanism to reduce feelings of uncertainty [27] .\nTrust: The concept of trust has also been used to elucidate the relationship between users and technology [19] . In this context, trust is considered to be the extent to which people believe that the technology will perform effectively and without a negative or injurious outcome, and can therefore be influenced by factors such as reliability [19] . Trust consequently shapes individuals' attitudes and ultimately determines their behaviour, such as the extent to which they rely upon the technology and their operational strategies towards its use [25, 26] .\nTrust: Evidence shows that an effective strategy that can influence users' understanding of and relationship with a non-human entity is to add 'human' elements [10, 15, 34] . Factors such as a discernible face or body (particularly if provided in a social context) can encourage users to endow the entity with human qualities (motivation, characteristics, behaviour etc.) -inspired by the said feature. This can change the value that users place on the item, for example, the level of trust it affords, and can influence how they behave towards it [8, 24, 30] . Even so, it is worth highlighting that, in so doing, people do not impute human personality in all its subtle complexity, but rather \"paint with broad strokes, thinking only of those traits that are useful to [them] in the [24] .\nTrust: One such feature is speech. Philosophical debates identify speech as one of the quintessential markers of humanness [30] . It is the primary means of social identification amongst humans and implicates more parts of the brain than any other function [28] . Moreover, speech is peppered with salient, socially-relevant, cues above and beyond the lexical content, that humans are experts at extracting and comprehending using vocal characteristics such as pitch, cadence, speech rate and volume. These are subsequently used to provide systematic guidance for determining gender, personality and emotion-specific actions, such as who to like and trust [30, 31] .\nTrust: Trust is also an important component of effective conversation between humans. In this context, trust is used to establish common ground and ensure mutual understanding [11] . Moreover, conversational partners use their exchanges to calibrate trust in each other and to declare their own trust [30, 31] : speakers employ strategies, such as 'small talk' (or so-called, 'phatic communication'), humour, politeness, and vague language (deliberately imprecise language) to build trust over time [9, 11] .\nTrust: Extensive research has demonstrated that humans appear to lack the wherewithal to overcome evolutionary instincts and behaviour, and respond to vocal utterances from a computer in a similar manner to other people, ascribing humanlike characteristics and attending to talking machines as if they were interacting with another human [30] . The presence of speech and conversation embedded within technology is therefore a powerful catalyst, encouraging the listener and interlocutor to imbue essential human characteristics to the host [34] . Indeed, manipulating speech interfaces has already been used to exploit these automatic responses. For example, different digital 'personalities', created by varying the vocal characteristics and language content of spoken language interfaces, have been shown to influence trust, performance and learning, and even consumers' buying habits in commercial contexts [30] .\nTrust: The use of speech has also been popularised as a human-vehicle interface in the automotive domain. While this is most often associated with alleviating concerns of driver distraction and cognitive load [18] , it can also create a more natural user experience [1, 5] . Moreover, given aforementioned literature, it would appear feasible that interacting using natural, spoken language (with a vehiclebased agent as the conversational partner) could also influence occupants' perception of the vehicle itself (and by association, the technology it proffers), with initial investigations already providing some support for this hypothesis [2] . Nevertheless, it remains unclear what form human-agent 'conversation' in this context should take.\nConversational Agents: Existing spoken human-agent interactions, such as those with Intelligent Personal Assistants (IPAs) like Amazon Alexa, are often limited to isolated question-answer pairs [32] . In aiming to inform the design of 'appropriately' conversational agents, Clark et al. [13] explored what people valued in human-human conversation and how they felt this should manifest in human-agent communication. Responses show that in general, interviewees favoured a more utilitarian definition of conversational qualities when interacting with a technology-based agent, repudiating the need for developing bond and common ground with the host. In particular, trust was discussed exclusively in functional terms, emphasizing factors such as security, privacy and transparency rather than emotional trust (i.e. sharing personal information and vulnerabilities to increase social bond), as would be expected in human conversations.\nConversational Agents: Nevertheless, Large et al. [22] demonstrated the adoption of 'normal' conversational behaviours during the practical accomplishment of tasks when participants interacted with a highly-capable conversational agent (delivered using Wizard-of-Oz) in a simulated driving context. In the study, language usage patterns indicated that interactions with the agent were fundamentally social in nature, and mirrored many of the routine behaviours observed during human conversation. For example, participants were polite, actively controlled turn-taking during the conversation, and used back-channelling, fillers and hesitation, as they might in humanhuman interaction (HHI). Furthermore, participants expected the agent to understand and process complex requests mitigated with hedging words and expressions, and peppered with vague language and deictic references requiring shared contextual information and mutual understanding [7] .\nConversational Agents: As such, the design of conversational user interfaces -and the conversation they permit -remains an interesting and complex area, with prospective users apparently declaring expectations that are different to what occurs in practice. To inform this debateand recognising that one of the aims of conversation is to build and strengthen positive relationships with others -we have explored the use of human-agent conversation as a means to engender trust in an autonomous, self-driving vehicle -a context in which issues of trust and acceptance are paramount [17] . While initial results show that there is an increase of perceived trust in situations where an agent was employed to talk to passengers (compared to situations absent of an agent -see [23] ), the aim of the current paper is to identify indicators of trust that can be gleaned from the conversation that took place between human and car.",
    "relatedWork": "",
    "rq": " where p19 is signalling specific preferences to the system rather than aiming to achieve an emotional bond per se. the lack of emotional small-talk (i.e. emphasising interpersonal relationships and de-emphasising the task-based aspect of interaction) could be a consequence of ultracab\\'s task-based focus of interaction and participants responding accordingly. instead, beginnings of interactions are focused on greetings, for example, ultra-cab requests the names of participants, and introduces itself and its purpose. common responses to these greetings and introductions were minimal, with participants providing limited responses to the system. these responses also included nonverbal actions (nodding by p13; yawning by p15) or simply silence, the latter perhaps indicating participant is waiting for ultracab to engage in further activities, such as beginning driving, or they simply felt no need to respond. this contrasts to hhi, where silence in introductions may be considered rude in certain contexts. other responses share similar reciprocity to openings, greetings and small talk in human interaction. in excerpt 3, p4 mirrors some of ultracab\\'s turns in asking what the system would like to be called. other responses include more requests about the system\\'s capabilities or the task at hand (\"is it possible to stop on the way?\np5: how long 'til we reach our destination?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "RELATED WORK 2.1 Qualitative and Quantitative User Experience Studies: Only few studies of user interaction with open-domain chatbots exist due to their scarcity on the market [18] . Authors in [37] explored in a diary study how human-like qualities of the Replika chatbot influence user engagement. 1 In a different work [35] , authors ran a Wizard-of-Oz (WoZ) experiment to determine the appropriate personality traits for a chatbot. Two other studies [24, 55] used the qualitative semi-structured interview method to understand how conversational agents can help users treat their emotions.\nRELATED WORK 2.1 Qualitative and Quantitative User Experience Studies: In parallel with these studies, a growing body of work investigates the social aspects of user interaction with task-oriented chatbots. Large et al. [28] established in a WoZ study that drivers' interactions with a digital driving assistant were fundamentally social in nature. Authors in [46] ran a case study of Amazon Alexa and showed that users' greater personification of the device was positively associated with more sociable uses. Several works focused on users' social conversational principles with a productionscale question-answer chatbot deployed as a part of the HR system [30, 31] . Despite the agent's functional purpose, many employees searched for a playful experience with it [31] . Moreover, users' social-agent orientation influenced their desire for enhanced conversational abilities of the agent [30] . Finally, some researchers started to evaluate user experience with the prototypes of hybrid chatbots. Xiao et al. [60] demonstrated that an interview chatbot with active listening skills is more effective than the one without them. Katayama et al. [23] , in a mixed-method study about the emotionally aware question-answer chatbot, determined that users prefer the agent to adapt its emotionality based on the context.\nRELATED WORK 2.1 Qualitative and Quantitative User Experience Studies: These studies discussed a number of social skills that users expect from their chatbots. However, each study considered only a subset 1 https://replika.ai/ of these qualities at a time and did not relate user expectations to the likelihood of adoption.\nPsychometric Studies of Chatbot Adoption: Several recent research works have applied psychometric methods to study factors influencing user adoption. Some of them also included the social and affective qualities of chatbots. Authors in [69] employed the Consumer Acceptance of Technology (CAT) model [25] to test what determinants influence user intention to use a movie ticket booking chatbot. Their model included determinants from the cognitive and affective dimensions. While the cognitive items were measured with multi-item instruments, the authors employed the visual Self-Assessment Manikin (SAM) scale [4] to measure the affective features. Determinants from both dimensions had a significant impact on the chatbot's effectiveness. However, a coarse assessment of the affective dimension cannot be easily translated into actionable design recommendations. In a different work [36] , the authors applied the extended Unified Theory of Adoption and Use of Technology (UTAUT2) model [58] to analyze user intentions to use a chatbot to organize vacation trips. The model consisted of 11 constructs in total, with only two describing the chatbot's social abilities: communicating in a human-like manner and entertaining the user. Both were found to impact the adoption positively, but they fail to cover the whole range of chatbot's social capabilities. Finally, another study of a customer service chatbot investigated the mediating role of an agent's human-like qualities between its ability to resolve miscommunication and user adoption [52] . Anthropomorphic chatbots that seek clarification in case of misunderstanding were found to satisfy customers' social desires and be as effective as idealized error-free agents. While being a promising finding, the authors did not elaborate on which anthropomorphic qualities are specifically vital for chatbots.\nPsychometric Studies of Chatbot Adoption: Each of the works above studies a dedicated domain-specific chatbot and considers its human-like qualities only partially. They fail to provide an integrated view on how different anthropomorphic skills of a chatbot predict user engagement. To facilitate the developmental efforts and bring conceptual clarity, our study aims to establish a unified model of social qualities for chatbots and assess their impact on user intention to accept this technology. We specifically choose to focus exclusively on the characteristics typically attributed to open-domain chatbots: once systematized, they can be easily introduced for any domain-specific application within the framework of hybrid agents.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: According to the Institute of Education Sciences, the graduation rate of college students over the past 6 years in the US is only 60% [45] . These high attrition rates are troubling and pose problems for filling high demand labor markets like STEM. The U.S. National Science and Technology Council's Committee on STEM Education issued a 2018 report noting that while demand for STEM graduates continues to grow, other countries like India and China are doing a better job preparing students and now produce nearly half of all STEM graduates [14] . Transition to university comes with many potential life changes such as relocation, long-distance relationships, greater responsibility and independence, and increased academic rigor [9] . Navigating through the new environment with these additional challenges can be frustrating, and students may lack the necessary skills to survive in the university's competitive environment. Similarly, inadequate study habits and skills, lack of mentoring, low motivation, and low self-efficacy can impair success in the more independent university environment [7] . These deficiencies may cause students to lose interest in their coursework and abandon their studies, resulting in higher attrition rates. While not new, these challenges, remain a compelling problem, one that we believe can be addressed by the emergence of new technologies.\nINTRODUCTION: While evidence of high attrition rates points to inadequate preparation for university studies, psychological and behavioral components, i.e., study habits and skills, are also important to consider. Study habits and skills are a group of constructs often studied in relation to academic performance [7] . An important distinction between study behavior and study habits is that the latter is repeated regularly and takes time to instill. Existing literature provides evidence that study habits and skills are correlated with academic success [15, 38] . Self-regulated learning, which includes study habits and skills, also has been modeled as a key component to academic achievement [46, 64] . Students with poor study habits and skills manage time poorly, engage in ineffective learning strategies, maintain unhealthy routines, or fail to reflect on their learning processes [46] . All of these traits can influence academic performance as well as lead to psychological repercussions on anxiety level, attitude, self-efficacy, and motivation. In particular, University freshmen transition into a new environment that requires more independent academic work, making these study habits and skills potentially a critical factor. We believe technology can provide beneficial support to these students, aiding the development of effective study habits and skills.\nINTRODUCTION: In this paper, we aim to investigate the use of emerging technology to improve the students' study behaviors and skills that could lead to positive habit formation. Universities are an inherently cooperative and collaborative environment that presents opportunities for technology to help support students' engagement in their work. While educational technology has had success in improving academic performance, psychological and behavior improvements are still an emerging research area. Du Boulay et al. [18] highlighted the importance of going beyond cognition, taking into account other factors such as motivation, metacognition, and emotion to create \"educational systems that care. \" Motivated by this work, we explore the design and use of chatbots for influencing the metacognition of first-year university students majoring in Computer Science (CS). We are particularly interested in chatbots as they already have been used to alter user behavior and foster self-awareness [25, 28, 49] . Given the remote learning environments under the COVID-19 pandemic, students require more support with grasping knowledge effectively, managing time, keeping track of assignments, and staying connected to their learning community, thus giving chatbots greater potential to facilitate learning and behavioral change in a remote setting.\nINTRODUCTION: We surveyed students and interviewed tutors to better understand students' issues related to study habits and skills and identified technological intervention opportunities. These findings informed several novel design features for chatbot-based study behavior tools and inspired us to create StudyBuddy, a chatbot prototype deployed on Slack to influence students' study behaviors. The functionality of StudyBuddy includes sending tips from experienced students, providing assessment and feedback of students' study habits via surveys, helping the students break down academic assignments, recommending academic resources, and sending reminders to complete assignments on time. We evaluated StudyBuddy using in-depth student and instructor interviews, and student surveys, contrasting the feedback we received from both first-year and upper-class students, and instructors. The results identified important design challenges such as building trust and preserving privacy, limiting interaction costs, and supporting both immediate and long-term sustainable support. Likewise, our findings help generate design recommendations for a chatbot that demonstrates context awareness, personalizes the experience based on user preferences, and adapts over time as students mature and grow.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: When building a chatbot, designers aim to embed human-like conversation capabilities so that users feel more natural in completing their tasks with a chatbot. When designing a conversation, many designers attempt to collect from small to large scale human conversation data so that their chatbot models a desired human conversation. Existing approaches either collect conversation data from humans or formulate the conversation by analyzing existing data sources. Examples include Wizard-of-Oz [4, 7] , workshops [5] , Twitter conversation data [3, 8] , mail threads of DBpedia [1] , and existing chatbot logs [9] .\nINTRODUCTION: To understand the challenges designers face during the conversation design process, we conducted semi-structured interviews with two professional conversation designers (at least 1 year of experience) and seven amateur designers with prior experiences in conversation design. Results show that designers find it difficult to discover possible conversations in the design process, especially when they are not familiar with the chat domain. Also, it is overwhelming to rapidly iterate the conversation design as the iterative process requires not only the design of the conversation but also prototyping and testing the working chatbot, which can be built using frameworks such as Google Dialogflow 1 , BotKit 2 and Chatfuel 3 . Supporting the iterative design of conversation helps designers to get a sense of how potential users might follow the current design of conversation. Yet, there has not been much investigation on how designers iterate the conversation design, challenges within the process, and how to design a system to support the process.\nINTRODUCTION: To address these challenges, we suggest crowdsourcing as a solution to supporting iterations in the design process. Inviting crowd workers in the design process potentially lessens the burden of designers to quickly test and improve the design. There have been studies that incorporated crowdsourcing in real-time chat scenarios. Chorus [6] demonstrated how the crowd could come up with not only a diverse set of responses but also a diverse set of variations of descriptions on a given topic, where they expected crowdsourcing as a potential approach to explore diverse conversations in the chat domain. Extending this line of work, we aim to utilize the crowd during the conversation design process, not during the chat session. We believe that the crowd could empower the iterative design process by enabling rapidly testing intermediate designs. By testing with the crowd, designers can easily test their design in a lightweight manner compared to Wizard-of-Oz, a workshop, or a lab study.\nINTRODUCTION: To explore the idea of using crowdsourcing in the conversation design process, we developed ProtoChat, a prototype system for supporting designers by (1) allowing the crowd to actively suggest new utterances and turn-taking conversation based on designers' scenarios and (2) representing crowdsourced data in effective ways to assist designers analyze and improve the conversation. We decided to set the role of the crowd as the active suggester of conversation design so that designers could get enough evidence in decision making. We first designed the crowd-testing interface to test the designer's draft conversation with the crowd. Crowd workers are provided with the interface to either follow the designed conversation by responding or suggesting new messages between the conversation. Other than the crowd-testing interface, we designed a separate interface for designers where they can design the conversation flow with a set of chatbot utterances and their corresponding topics. Furthermore, after crowd-testing, designers could review and analyze the collected responses from the crowd with the designer interface to iteratively improve their design.\nINTRODUCTION: Through our exploratory study with four conversation designers, we found that ProtoChat enables designers to quickly design and test their ideas about a conversation with the crowd and analyze the data provided by the crowd so that they could improve the design. The crowd enabled need-finding of the chat domain which is important in the early stage of conversation design. Moreover, the crowd provided evidence for decision making regarding later versions of the design. This research aims to help chatbot designers with crowd power, from collecting conversation scenarios by crowd contribution to effectively representing the crowdsourced output, which enables designers to explore potential scenarios, important in understanding the needs and use cases of real users.\nINTRODUCTION: This work is based on our ongoing research [2] , in which we introduce the ProtoChat system and its preliminary evaluation. In this paper, we briefly summarize the system and our findings so far, and discuss the use of crowdsourcing as a method for helping the conversation design process. Through our work, we expect to promote a discussion in the community of CUI (Conversational User Interface) researchers about applying crowdsourcing for data collection, decision making support, and design space exploration and confirmation in a broad set of CUI application scenarios.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: In recent years, advances in robotics, computer vision, and autonomous navigation have brought robots out of labs and into real-world spaces. The emergence of mobile, physically situated agents like robots has also created a desire to communicate with them effectively. Dialogue offers a promising way to interact with robots: a hands-free mode of communication that conveys information at a more abstract task level than typical command-and-control devices [34] . With Miscommunication Detection and Recovery in Situated Human-Robot Dialogue\nCONCLUSIONS AND FUTURE WORK: We described an infrastructure and representation for detecting and recovering from dialogue with situated grounding problems, cases in which physical agents such as robots interpret instructions that are referentially ambiguous or impossible to accomplish in their surroundings. We defined the problem space in the context of a directed navigation task in which a human gives instructions and a robot follows them, asking for clarification when appropriate. We then described core capabilities that robots must have in order to handle situated grounding problems. These capabilities were implemented in TeamTalk, a conversational interface with robots. We presented a rich representation that combines spoken-language input with static and dynamic sources of environment context. The TSR was developed to reason over this information and detect miscommunication events in situated human-robot dialogue. We also introduced a set of recovery strategies, motivated by observations in human-human dialogue, that robots can use by considering properties in their surroundings that can be included in clarification requests. Finally, we presented a method to improve recovery strategy selection using nearest-neighbor learning with data originally collected from crowdsourced workers and supplemented by real-time interactions with users. Our results showed that a robot could learn how to select human-inspired recovery strategies using a numerical representation of situations. We found that while population-level learning yielded the highest agreement with human judges, user-tailored learning led to more efficient successful dialogues in a longitudinal user study with six participants. We believe that these findings are relevant to the design of human-robot interfaces that use experience to improve performance.\nCONCLUSIONS AND FUTURE WORK: There are many opportunities for future work in this area. While the contributions in this work enable recovery from situated grounding problems, the robot did not acquire new knowledge about the environment or actions beyond whether or not a selected strategy was successful. A desirable extension would be to update the robot's understanding of its physical surroundings in natural language. We made the simplifying assumption that the robot had knowledge about its surroundings encoded a priori. Future work can consider incorporating sources of uncertainty to help with selecting recovery strategies. This can include exploring evidence such as temporal information and previous exchanges. The impact of the nearest-neighbor learning algorithm could be made stronger by experimenting with an additional condition in a future study in which no learning takes place, instead basing the robot's decisions on heuristics or selecting them at random. Additionally, there remain other measures for route instruction interpretation that can be used to assess the positive impact of recovery strategies; these include effectiveness metrics common in the robotics literature, such as time to the goal, percentage of the route achieved, and adherence to a specified route. As the natural-language understanding in this work was designed based on existing dialogue data, there remain opportunities to automatically learn such interpretations from annotated training examples. Finally, there are many tasks that robots can accomplish with people that will require recovery from miscommunication, including manipulating objects, recurring tasks such as patrol, and interpreting the personal factors of dialogue partners. All of these tasks will require robots to interpret, reason, and communicate about the physical world with others.",
    "relatedWork": "RELATED WORK: Much of this research builds on related work on robots that ask for help, symbol grounding for natural language interpretation, dialogue systems that support human-robot interaction (HRI), and error handling in human-computer dialogue.\nAsking for Help: Several recent papers have demonstrated the feasibility of symbiotic HRI, whereby a robot asks a human bystander or teammate for help, and the human helps due to some mutual benefit. The CoBot project [94] found that a robotic tour guide could significantly reduce its localization uncertainty by asking visitors for help. Deits et al. [30] developed an information theoretic method of asking for help using clarification dialogues to disambiguate referents. Their robot asked for clarifications about symbol grounding mappings between natural-language phrases and physical objects based on an estimate of uncertainty. Both of these approaches focused on a particular function that a robot could use to ask for help (e.g., knowing where it was in a building or if the phrase \"pallet near the truck\" referred to a specific location). In contrast, Knepper et al. [48] used inverse semantics to enable a robot to ask for targeted help on a construction task in which a human partner could provide assistance. The robot would autonomously detect when it entered a failure condition and took the human's perspective to determine how they could help and how to refer to the surroundings in natural language.\nAsking for Help: Although algorithms exist for requesting help, the robots that ask them either issue statements (no question generation) or interpret text input (no speech). Often, the robot can interpret only non-linguistic changes in its state space or inputs on a screen, performed by human teammates in response to a request. When questions are generated, they are focused on confirming a robot's certainty about a grounded symbol and not higher-level descriptions of referring expressions. As of this writing, none of these approaches handles requests for help when a robot receives an impossible-to-execute command.\nAsking for Help: The TeamTalk platform used in this work offers a bidirectional method for interpreting commands by detecting problems with task execution, requesting help, and responding to amended instructions from the human. The interaction modality is spoken dialogue; thus, the human is able to interact in a \"heads-up, hands-free\" way [69] . When disambiguating referring expressions, TeamTalk uses natural-language generation templates to describe its surroundings. Additionally, TeamTalk can detect impossible-to-execute commands, propose alternative referents, and prompt the human for clarification. Similar to previous work, users of TeamTalk have direct incentive to help the robot, given that the goal is to accomplish search and navigation tasks together. The above studies assessed interactions in fixed settings; in this article, we evaluate TeamTalk's ability to improve selection of dialogue strategies over time in longitudinal interactions over six sessions for each of the six participants.\nRoute Instruction Interpretation: The directed navigation task used in this research involves a human giving navigational directions, commonly termed route instructions, to a robot. Route instructions have been thoroughly studied, both in human-human [1, 100, 110, 112] and human-robot interactions (e.g., [51, 70, 87, 89, 103] ). Bugmann et al. [20] coined the term corpus-based robotics to characterize the nature of the traditional research paradigm for route instruction interpretation: human-authored language is collected in experimental settings (ideally with the humans informed that the instructions are intended for a robot), the data is annotated, and a statistical model is trained for automatic interpretation. Route instruction interpreters generally take natural language as input and output robot-executable procedures.\nRoute Instruction Interpretation: Several groups have developed route instruction interpreters, some for written language beginning with Marco [66] and continuing with an extensive body of follow-up work for navigation and manipulation tasks (e.g., [9, 16, 31, 33, 50, 62, 88] ). Vogel and Jurafsky notably developed a route instruction follower using examples from the HCRC Map Task corpus [117] . Often, the work is framed to address the symbol grounding problem, whereby natural-language phrases represent symbols that a robot must map to its sensory perception. Some symbol grounding-based approaches interpret written language [65, 81, 102, 104, 111] . More recent techniques for route instruction interpretation include semantic parsing [3, 26, 53, 75] , machine translation [74] , and Tactical Behavior Specification Grammar [45] . A few approaches exist for interpreting spoken language, including Dzifcak et al. [32] , Meena et al. [79] , Bastianelli et al. [7] , and our previous work [95] . Most recently, Broad et al. [19] developed a method for handling self-correcting monologue in spoken-language instructions.\nRoute Instruction Interpretation: The existing body of work does not cover handling interpretation failures for navigation, specifically supported by back-and-forth dialogue. Without interacting with the user, the best that such systems can do is estimate intention. In this article, we define methods that robots can use to detect these kinds of problems. We also present a set of dialogue strategies for robots to recover from interpretation failures and empirically evaluate them in a user study.\nDialogue Systems to Support HRI: Research on natural-language interaction with robots has a long history. The earliest approaches studied medical [109, 116] and office assistant [2, 4] domains. Research over the past two decades has focused on human-robot dialogue in navigation [79, 97, 103, 108, 114] and manipulation [36, 46, 64] tasks, but many other domains exist [15, 35, 86] . A more comprehensive overview of the subject can be found in reviews by Kruijff [55] and Mavridis [76] .\nDialogue Systems to Support HRI: Natural-language dialogue enables robots to ask clarification questions [30, 59, 84] and provide status updates [60, 115] to conversational partners. Some of this research contributes to disambiguation methodologies [37, 56, 121] , but a few attempt to renegotiate natural-language instructions that are not possible to execute [24, 77, 90] . In comparison, TeamTalk uses a data-driven approach to handle referential ambiguity and impossible tasks. Furthermore, we focus on identifying properties of the surroundings that robots can leverage to resolve problematic instructions.\nError Detection in Dialogue Systems: Several groups have developed miscommunication detection methods for spoken-dialogue systems. Commonly, confidence values produced by automatic speech recognition (ASR) can determine whether the system correctly interpreted a user's utterance. These approaches have primarily been studied at the level of phonemes, words, and whole utterances [6, 25, 29] . Alternatively, San-Segundo et al. [96] used information from the language model and parser to train a neural network-based classifier that detected misrecognized words and out-of-scope phrases. Walker et al. [120] evaluated many features of the natural-language understanding (NLU) and dialogue management components of a telephony-based dialogue system to detect misunderstandings and other errors. Bohus and Rudnicky [13] studied ways to combine evidence from the speech recognizer, parser, and dialogue manager to estimate confidence about an input utterance. Litman et al. [63] considered prosodic features for detecting ASR errors. More recently, Meena et al. [78] explored using a range of features from ASR, NLU, and natural-language generation system logs to automatically detect miscommunication in online and offline settings.\nError Detection in Dialogue Systems: Detecting and diagnosing errors in spoken-dialogue systems has received a considerable amount of attention and has led to established processes. However, most of these detection approaches rely on evidence from the spoken-language input and not from other sensory data. Moreover, these approaches largely operate in nonsituated domains. Situated grounding problems have not received this level of attention, but the success of ASR and NLU approaches could be extended by including path planner and environment information.\nError Recovery in Dialogue Systems: Understanding human error recovery strategies usually starts by conducting Wizard-of-Oz experiments in which na\u00efve participants encounter simulated ASR problems while interacting with a dialogue system controlled by human wizards. Several strategies exist for repairing communication breakdowns. Zollo [123] found that human wizards used strategies that systems at the time were not capable of using, such as prompting users about a single word or phrase. Other strategies include explicit and implicit confirmations, asking to repeat a word or phrase, clarification requests about ambiguities, and disregarding any information under discussion that is not crucial to the dialogue task [98] . Skantze [105] found that human wizards acting as systems in a map navigation domain avoided explicitly declaring a non-understanding and instead opted to move the dialogue forward by asking alternative questions related to the task. Bohus and Rudnicky [14] showed that dialogue systems can learn the same strategies.\nError Recovery in Dialogue Systems: Recovery strategies for misunderstandings in dialogue systems boil down to explicit and implicit confirmation strategies. These two recovery strategy types are common in nearly all spokendialogue systems capable of error handling [17] . There is also general agreement about the pros and cons of both strategies [10] . Explicit confirmations allow the system to constrain the user response to a confirmation (e.g., \"yes\" or \"no\") but also require an additional dialogue turn to complete. Implicit confirmations present the system's understanding of a user utterance and provide the user the option to interrupt the system if there is a misunderstanding. Without user intervention, the system will assume that its interpretation is correct for implicit confirmations. Present-day dialogue systems are better equipped to handle explicit confirmations because their responses are easier to predict than responses to implicit confirmations. Krahmer et al. [52] conducted an extensive analysis of explicit and implicit confirmations. They identified positive and negative signals that users made when the dialogue system used these strategies, and were able to automatically detect dialogue system errors based on these cues.\nError Recovery in Dialogue Systems: Recovery strategies for non-understandings operate at a coarser level than misunderstandings because the system interprets little to no part of the user's utterance. Dialogue systems typically indicate a failure to understand and ask the user to repeat [10] . However, there have been attempts to better diagnose non-understandings. For example, Gorrell et al. [39] attempt to better understand the type of problem by classifying any known information from the non-understanding into one of a set of help options. Those help options attempt to give the user more feedback on how to speak to the system given results from a domain classifier (e.g., a when versus where question). System developers of a large-scale bus information dialogue system suggested a method for better diagnosing non-understandings by extracting words from the non-understood input and giving the user alternative ways to answer questions that the system could anticipate [91] .\nError Recovery in Dialogue Systems: Bohus [10] offers several human-computer recovery strategies for non-understandings. The simplest strategies include lexically entraining users to words and phrases that the system can interpret, offering a help option, using alternative methods of input such as DTMF (touch-tone), and backing off to system-directed control. Other strategies from his work include proposing an alternate task-related question, asking for a short answer, providing an example of what the system can understand (e.g., \"You can say yes or no\"), and offering advice on how to speak to the system (e.g., more slowly and softly). Skantze [106] also investigated methods for overcoming errors in spoken dialogue found in human-human communication.\nError Recovery in Dialogue Systems: Dialogue system strategies primarily focus on recovering from errors related to spokenlanguage interpretation. Handling speech-recognition errors is the most common task. In this article, we study error-recovery strategies for communication problems in situated dialogue, which occur even when the user's speech is interpreted correctly.",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: A traumatic event, such as abuse, combat, an assault, an accident or a natural disaster, may have a long-lasting negative effect on an individual. With the increase of soldiers in combat since 2001, the interest in Post-Traumatic Stress Disorder (PTSD) has increased. \"Epidemiologic surveys indicate that the vast majority of individuals with PTSD meet criteria for at least one other psychiatric disorder.... The most common comorbid diagnoses are depressive disorders, substance use disorders, and other anxiety disorders.\" [1] Moreover, the World Health Organization reports that mental and behavioral disorders were the number one category contributing to U.S. YLDs (years living with disability) in 2010. At 27.1%, this is more than diabetes (8.4%), chronic respiratory diseases (7.9%), and cardiovascular diseases (5.2%) combined. Within the category, Major Depressive Disorder was number one contributing 30.66%, followed by All Anxiety Disorders (18.76%), Drug Use Disorders (13.03%), and Alcohol Use Disorders (8.40%). The National Institute of Mental Health estimates total direct and indirect costs of serious mental illness exceeds $300 billion in the U.S. annually based on 2002 data. More than 60% is the indirect cost of lost earnings from lost productivity. Healthcare expenditures account for about 30% with disability benefits accounting for less than 10% [2] . Given the prevalence and comorbidity of depressive and anxiety disorders with the associated personal and societal costs, there is need for self-screening tools to provide referrals for relevant treatment resources.",
    "relatedWork": "RELATED WORK: Although many works have proposed multimodal interaction with the user, most of the systems are rule-based or plan-based and they use speech as the primary modality. Moreover, a few works have proposed stochastic dialogue policy optimization in the health domain. In [3] , they proposed an Adaptive Dialogue System able to have a conversation in natural language with PTSD-suffering users, and guide the way that allows eliciting information about their disorder and progress of their treatment. They consider speech as the only input, while they continuously monitor the user's emotional state through keywords in order to adapt the dialogue in such way that the system encourages the user and keeps them calm. A similar system that uses multimodal input and output, SimCoach, was designed to provide support and health care information about PTSD following the Information State Update (ISU) approach [4] . Although, ISU and plan-based approaches seem to be effective for this kind of systems, they have a number of general limitations concerning the design and implementation. These approaches require a manual specification for the update or inference rules. Moreover, the system behavior remains static during the interaction without taking into consideration the current user's personal preferences and needs. In [5] , they model an Alcohol Brief Intervention system as interconnected Markov Decision Processes (MDP), following a model-based approach.\nRELATED WORK: In the current paper, we present our ongoing work on a multimodal adaptive dialogue system used as a self-assessment tool for depressive and anxiety disorders. The system follows a questionnaire-form dialogue to determine which recommendation to provide the user after the interaction. Moreover, audiovisual data, such as speech and facial expressions, are taken into consideration to estimate the user's emotional state and prevent unwanted emotional states during the interaction by encouraging the user when needed. We focus on the dialogue manager of the system, which is responsible for the decision making of the system. We model the interaction splitting the dialogue into sub dialogues, represented by interconnected MDP in order to screen for the often comorbid disorders of PTSD, General Anxiety Disorder (GAD), Depression, and Substance Use Disorders.\nRELATED WORK: In Section 3, we present the system architecture showing the different levels of screening and how we formulate the problem using interconnected MDP to represent each part of the dialogue. In Section 4, we show the experimental procedure for the data collection and the system training following the methodology of the Wizard-of-Oz (WoZ) experiments, and describe the experimental setup and the modalities used for the experiments. Finally, in Section 5, we present the future work which includes the system training and evaluation.",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Conversational agents (CA), computer systems that can have conversations with users using natural language processing, can greatly benefit of multimodality. The dialogue systems can employ one or more modes of communication such as text, speech, graphics, haptics, gestures, and others on both the input and output channel. CA applications are increasing, and are more accepted by older adults. El Kamali et al. held a systematic review to understand the state of the art of virtual coaches for older adults' wellbeing [2] . Their work concluded that older adults are using more and more conversational agents. In fact, they concluded that CAs are effective [4, 6, 8] . However, no study in the review investigated the multimodal aspect of CAs. Some studies have combined gesture with speech such as Matilda [5] . In this article, we will study the multimodal capabilities of the NESTORE coach conversational agent. NESTORE is a European project that aims to help older adults to improve and maintain their health in different domains of wellbeing [1] . The whole e-health intervention is based on the behavior change model Health Action Process Approach (HAPA) [7] . The NESTORE virtual coach boast a conversational agent that comes in two different forms of interfaces: (i) a chatbot, which is a text-based messaging application, and (ii) a tangible coach, which is a vocal assistant that offers vocal and tangible interaction modalities. Because of the lack of studies of multimodal interfaces for virtual coaches for older adults, we conducted a study to explore the best representation form for the NESTORE virtual coach. In particular, we want to study which is the best combination of modalities that older adults would prefer for the NESTORE conversational agent. Our main objective is to present two different interfaces \"the tangible coach MeC'20 Workshop ICMI '20 Companion, October 25-29, 2020, Virtual Event, Netherlands \"and \"the chatbot\" that act as a single entity (the NESTORE virtual coach) but in different devices and with different modalities. The \"tangible coach\" focuses on vocal interaction, whereas the \"chatbot\" focuses on the written interaction. Our research questions are the following:\nINTRODUCTION: (1) Which is the best combination of modalities older adults prefer for motivational coaching? (2) How multimodality affects the user experience of a virtual coach for older adults? (3) Is combining written and vocal interaction more effective in terms of user experience?\nINTRODUCTION: In order to answer our research questions, we conducted an experiment based on five scenarios. Each scenario has a different form of combinations between the modalities. In this paper, \"interfaces\" refers to a set of physical interaction devices. Hence, the chatbot is one interface and the tangible coach is another interface. Whereas, \"modality\" refers to the interaction capability that each interface offers. Hence, the chatbot offers visual interaction whereas the tangible coach offers vocal interaction.",
    "relatedWork": "",
    "rq": " (2) how multimodality affects the user experience of a virtual coach for older adults?\n (3) is combining written and vocal interaction more effective in terms of user experience?\n(1) what is the best combination of modalities older adults prefer for motivational coaching?\n (2) how multimodality affects the user experience of a virtual coach for older adults?\n (3) is combining written and vocal interaction more effective in terms of user experience?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: A recent report by Grand View Research 1 predicts that the global chatbot market will reach $1.25 billion by 2025. Chatbots have utilized the power of artificial intelligence (AI) for various application domains ranging from customer services [12, 40] and healthcare [2, 23] to product recommendations [14, 43] . In the domain of recommender systems, there are several cases where product recommendations are delivered to customers through chatbots [14, 40] with an aim to improve customer engagement. At the same time, a number of research work [18] [19] [20] 22] have emphasized the importance of user control in recommender system.\nINTRODUCTION: Various studies with critiquing-based recommender systems (CBRS) [7, 10, 26] have shown the positive effects of increased interactivity on the effectiveness of recommendations. Critiquing is an iterative approach of evaluating the outputs of a recommender system, which allows the system to continuously update the settings and provide users with recommendations that better represent desired outcomes [10] . Figure 1 shows a typical interaction flow of CBRS. CBRS simulate an artificial salesperson who first recommends products based on a user's initial preferences and then shows a new set of products based on the user's feedback (aka critiques), e.g., \"something cheaper\", \"larger screen\", etc. Thus, CBRS are well suited to accommodate user control during the recommendation process.\nINTRODUCTION: Most existing research studies [9, 10] have compared different critiquing techniques with graphical user interfaces (GUIs). However, little work has studied different critiquing techniques with conversational user interfaces (CUIs) that mimic a conversation with a real human either by text or voice. Moreover, it has been shown that personal characteristics such as musical sophistication affect user perception of controls for music recommenders [20] ; however, the effects of personal characteristics have not been validated on critiquing techniques yet. To fill these research gaps, this paper compares two typical critiquing techniques with CUIs and investigates how personal characteristics influences user perception and interaction of recommended items (see the dashed lines in figure 1 ). To achieve these objectives, we implemented a hybrid critiquing-based music recommender MusicBot, which uses a chatbot to enable users to interact with recommendations through both text and voice. The system offers two major critiquing techniques, user-initiated critiquing (UC) and systemsuggested critiquing (SC) to refine the recommendation. UC enables users to construct critiques according to their own needs, while SC generates a set of critiquing candidates for users to choose a desired critique. We then conducted an evaluation with 45 participants using MusicBot in a within-subject design. We raise three research questions for evaluating critiquing-based Music recommenders particularly with a conversational user interface (CUI).\nINTRODUCTION: RQ1: Which critiquing setting, UC versus HC, is better suited for controlling music recommendations?\nINTRODUCTION: RQ2: Which personal characteristics (e.g. musical sophistication, desire for control, chatbot experience, and tech savviness) might influence user's perception and interaction of recommendations?\nINTRODUCTION: RQ3: Are critiquing techniques perceived as useful in low-involvement product domains as in high-involvement product domains?\nINTRODUCTION: Our main contributions are four-fold:\nINTRODUCTION: (1) We demonstrate a multi-modal (text and voice) conversational music recommender that incorporates both a userinitiated critiquing technique (UC) and a system-suggested critiquing technique (SC). We then employ a mixed qualitative and quantitative research method to compare UC with a hybrid critiquing technique (HC) in terms of subjective user experience (UX) with recommendations. Overall, recommendations generated by UC and HC were perceived at the same level, while users tend to need more effort to find a song using HC. (2) We find that two personal characteristics, desire for control and musical sophistication, positively influence several key UX metrics of recommendations such as interest matching, intent to give feedback, and perceived controllability.\nINTRODUCTION: (3) Our study also verified the usefulness of critiquing techniques in a low-involvement domain of music recommendations. (4) Based on the findings in this study, we proposed specific design suggestions for critiquing-based recommender system with conversational interaction. This paper is organized as follows: We first introduce related work, followed by the design and implementation of MusicBot.\nINTRODUCTION: We then present the quantitative and qualitative results of a user study. Finally, we conclude with a discussion of study findings and limitations.",
    "relatedWork": "RELATED WORK: In the following sub-sections, we review previous work that are closely related to our research.\nCritiquing-based Recommendations: Based on the way of constructing critiques, critiquing-based recommender systems (CBRS) can be categorized into two types of critiquing: system-suggested, and user-initiated. In addition, the distinction is made between unit critiques and compound critiques. Unit critiques refer to critiques that only constrain a single feature at a time, while compound critiques are capable of making a critique over multiple features simultaneously to improve performance of conversational recommender systems [26] . Due to the pros and cons of each type of critiquing technique [10] , by taking the advantages of both UC and SC, a hybrid system increases decision accuracy and needs less cognitive effort [7] . However, most studies [10] of comparing different critiquing techniques are conducted only with graphical user interfaces (GUIs). To enable critiques with conversational interaction, we evaluate a hybrid critiquing system in a multi-modal chatbot for music recommendations.\nCritiquing-based Recommendations: Figure 2 : A user interface of a hybrid critiquing system that combines UC and SC [7] .\nCritiquing-based Recommendations: User-Initiated Critiquing (UC). UC is a flexible critiquing approach that allows users to determine which features and how the features are critiqued (see Figure 2 ). Thus, users are able to make either unit critiques or compound critiques. This technique is particularly useful for tradeoff navigation between compromising values on less important attributes and obtaining more optimal values for important attributes. The most representative systems of UC are Example Critiquing [31] and Flat Finder [38] . UC empowers users to have a higher level of user control, which does not lead to higher perceived cognitive load. However, some previous user studies [10] of UC also found users may suffer from the difficulty of getting started with UC without prior knowledge.\nCritiquing-based Recommendations: System-Suggested Critiquing (SC). Instead of asking users to construct critiques, SC generates a set of critique candidates for users to pick (see figure 2 ). Generating critiques is based on the system's knowledge about the current user's preference and the availability of remaining products. The earlier systems of SC, FindMe [5] and ATA [24] , presented pre-designed unit critiques to users with less adaptation to the changes of user preference and interaction. Later on, Reilly et al. [33] proposed Dynamic Critiquing based on association rules such as Apriori algorithm [1] to find frequent sets of value differences between the recommended product and the remaining alternatives. Furthermore, Incremental Critiquing [34] greatly reduces interaction cycles by avoiding to show the user rejected critiques in history. To take into account users' interest in the suggested critiques, Zhang and Pu [42] proposed to generate MAUT (Multi-Attribute Utility Theory) based compound critiques. The approach significantly increases recommendation quality by ranking the critique candidates based on the overall satisfaction degree with user preferences. A more advanced SC is the Preference-based Organization technique [8] that is able to generate more diversified compound critiques and achieve higher critique prediction accuracy and recommendation accuracy. Overall, SC is able to produce more dynamic critiques based on the current user's preferences. It is specially useful for users who have difficulties to initiate critiques or build critiques by themselves. However, domain experts may call for more control over recommender systems, so SC may restrict the way they intend to make critiques.\nCritiquing-based Recommendations: Hybrid Critiquing (HC). Similar to the idea of hybrid recommender systems [6] , HC intends to take advantage of each type of critiquing techniques. Chen and Pu [7] compared two hybrid critiquing systems that combine a UC system (Example Critiquing) with a SC system (Dynamic Critiquing or Preference-based Organization) in a graphical user interface. Users showed positive attitudes toward HC that comprises both UC and SC. In addition, HC can also overcome the issues of adopting a single type of critiquing technique and lead to high decision accuracy and low objective effort in making a choice.\nCritiquing-based Recommendations: All research findings discussed above were tested only with graphical user interfaces. In contrast, this study tries to compare different critiquing techniques with a conversational user interface.\nConversational Recommender Systems: Conversational interaction is well suited for critiquing the recommendation through natural language. Several works have demonstrated systems that elicit user preference and present recommendations in a conversational dialog. ExpertClerk [36] is a conversational agent that acts as a human salesclerk to ask user questions for getting user shopping preference and proposes the matched products with explanations. Adaptive place advisor [37] provides personalized recommendations for traveling places. The system refines user queries by considering both long-term interests over many conversations and short-term interests in the current conversation. The two systems are typing-based conversational recommender systems.\nConversational Recommender Systems: As voice recognition techniques continue to improve, speech interaction is becoming more capable of allowing users to express more complex content. ReComment [15] presents a speech-based user interface for making unit critiques (critiquing over a single feature at a time), and it improves the perceived ease of use as well as the overall quality of recommendations. A recent study [21] found that users tend to express longer and more conversational content with speech-based user interfaces than with typing-based user interfaces. However, speech user interfaces might negatively influence the efficiency of consuming recommendations and user exploration [41] . So far, most speech-based UIs for recommender systems are still featured with search-oriented commands rather than more sophisticated commands to control recommendations.\nConversational Recommender Systems: To the best of our knowledge, the existing critiquing systems with speech interaction only incorporate user-initiated critiquing. Little work has integrated system-suggested critiques into the dialog-based conversational recommender system. In addition, the effect of users' personal characteristics on their interaction behavior and subjective perception of critiquing-based systems has not been investigated yet.\nPersonal Characteristics: Although previous research [20, 28] has shown how personal characteristics influence the way users control music recommendations through an interactive visualization, we do not know whether personal characteristics also affect user perception and interaction of critiquing based recommendations. In the following paragraphs we explain the four personal characteristics we have considered in this paper as well as the rationale for selecting them.\nPersonal Characteristics: Desire for Control (DFC). Greenberger et al. [16] first used a questionnaire to measure DFC in various work-related tasks of their new jobs. Users with higher DFC tend to perform better on the task and do better on upcoming tasks than subjects with low DFC [3] . We use a widely used DFC scale proposed by Burger et al. [4] to measure the degree of control individuals perceive towards outcomes. DFC is an important personal characteristic (PC) to measure for this study, since the two different critiquing techniques in our system empower users to have different levels of user control.\nPersonal Characteristics: Musical Sophistication (MS). MS has been found as a key PC that influences the way users interact with music recommender systems [20] . The Goldsmiths Musical Sophistication Index (Gold-MSI) [29] is an effective test for measuring domain knowledge of participants. Several studies investigating the effect of personal characteristics on music recommender systems have employed the Gold-MSI to measure the participant's musical sophistication.\nPersonal Characteristics: Tech Savviness (TS). TS often reflects a participant's confidence in trying out new technology. Several studies [13, 27, 28] have investigated how TS may influence the way participants interact with recommender systems. Therefore, we think TS may also influence the way participants critique recommendations in a conversational agent.\nPersonal Characteristics: Chatbot Experience (CE). Due to the impact of assimilation bias, participants with previous chatbot experience are prone to overestimate or underestimate the sophistication of using a chatbot [11] . Previously, when conversational agents were not popular, researchers often measured participants' previous experience with computers as an influencing factor for conversational agents [2, 35] . A recent study [25] measures the effect of previous experience with voice user interfaces on a voice-based conversational agent. As chatbots are becoming pervasive in everyday life, we measure CE of participants in our study. ",
    "rq": "rq3: are critiquing techniques perceived as useful in low-involvement product domains as in high-involvement product domains?\n we have answered the research question rq3: are critiquing techniques perceived as useful in low-involvement product domains as in high-involvement product domains?"
}
{
    "intro": "",
    "relatedWork": "RELATED WORK 2.1 Natural Language Generation for Conversational Agents: Early research in creating believable conversational agents were largely motivated by the Imitation Game (also known as the Turing Test), proposed by Alan Turing in 1950. To maintain grammatical correctness and produce logical responses, early conversational agents, known as chatterbots, relied mostly on programmer-defined pattern matching, conversational networks, and activation networks, which sometimes were able to obtain limited success in restricted Turing Test evaluations [22] . ALICE (Artificial Linguistic Internet Computer Entity) introduced a new markup language for programmer-defined conversation knowledge and responses called AIML (Artifical Intelligence Markup Language) supporting XML definitions of categories, patterns, and templates [28] . Current commercially-available chatbot software such as Cleverbot utilize similar approaches to rule-based conversation generation, augmented with mechanisms for learning new response patterns from conversations. More recently, the focus of conversational research has shifted away from solely generating convincing dialog and towards the creation of functional natural language interfaces and conversational modeling.\nRELATED WORK 2.1 Natural Language Generation for Conversational Agents: Recent work has shown that neural language models and recurrent sequence-to-sequence models are able to encode limited conversational context a viable approach to language modeling with a large, unstructured corpus [27] .\nRELATED WORK 2.1 Natural Language Generation for Conversational Agents: To facilitate more realistic responses, other cues such as affect and contextual understanding can be used during response generation. Conversational models that encode affective signals such as user satisfaction and emotional state have shown to be effective for more believable conversational agents and low-perplexity language models [12, 14] .\nVirtual Agent Models and Facial Mimicry: Virtual agents that can take on many roles have been used in many application domains and for many purposes, such as promoting healthy exercise in older adults [4] , rehearsing job interviews with [8] or screening for depressive disorders [10] . Endowing virtual agents with certain behavioral capabilities can allow them to be more engaging and likable. One of these abilities that was investigated in the context of virtual agents is mimicry which can increase liking and rapport between individuals.\nVirtual Agent Models and Facial Mimicry: Experimental studies on the perception of emotion-mimicking virtual agents found that participants reported greater amounts of positivity, warmth, and realism in the presence of smiles while feeling more at ease and well-understood [3, 19, 25] .\nVirtual Agent Models and Facial Mimicry: While prior work showed the positive effects of emotional mirroring virtual agents, these existing models of virtual agents' facial mimicry relied on facial recognition and facial animation technologies with a large delay between the processing of the visual feed, facial behavior recognition, and production of facial mimicry [3, 19] . By leveraging neural networks, we aim to prioritize fast inference speed to support real-time or near real-time feedback.",
    "rq": "(1) what do you think is the most important thing to look for in a life partner?\n (3) what do you think about computers in education?\n (4) what is your favorite holiday?"
}
{
    "intro": "INTRODUCTION: Physically situated, multimodal interactive systems often rely on probabilistic inferences drawn from multiple streams of noisy perceptual data. Multiple uncertainties come to the fore during interactions. For example, dialog systems reason about uncertainties that arise in recognizing natural language via measures of recognition confidence and may employ clarification strategies such as explicit or implicit confirmations to keep a conversation on track. Robots or virtual agents that interact with users in physically situated settings grapple with uncertainties extending well beyond speech. These systems use probabilistic models to continually make state inferences based on streaming evidence from multiple sensors such as cameras, microphones, and proximity sensors. They must reason about people engaging with the system as well as those in the periphery, considering their physical position, orientation, and motion, focus of attention, interaction roles, intentions, and relationships. Beyond reasoning about the presence and contents of speech, these systems must monitor engagement (understanding who is involved in the interaction, when participants are joining or leaving, etc.) and turntaking (understanding who is talking to whom, who has the conversational floor, to whom they are releasing the floor, etc.) Uncertainties arise in each of these processes and managing communication becomes particularly challenging.\nINTRODUCTION: Physically situated conversational agents may have at their disposal the affordances of a virtual or physical embodiment, with the ability to communicate via facial expressions, posture, and gaze. Just as in human-human interaction, these nonverbal affordances can be leveraged and coordinated with verbal grounding acts, so as to communicate about and help to resolve uncertainties.\nINTRODUCTION: We present a methodology for reasoning and communicating about the multiple uncertainties that arise in multiparty situated dialog. We use entropy as a measure of uncertainty about inferred beliefs and consider the magnitude and the sources of each uncertainty. We introduce a set of policies to schedule and coordinate the production of verbal utterances and nonverbal behaviors that reflect the system's uncertainties. The affordances of embodiment (facial expressions, posture, and eye gaze) are leveraged to signal the inferred underlying causes of confusion and to direct signals to the users who are best positioned to assist with resolving the uncertainties. We have implemented the methodology in the context of a deployed, physically situated automated assistant.\nINTRODUCTION: We begin with a review of related work, followed by a description of the various types of inferences and associated uncertainties considered by a physically situated system. In Section 4, we describe the proposed approach and each of its components. In Section 5, we illustrate the approach with an analysis of a trace from a multiparty interaction with the implemented system. Finally, in Section 6 we summarize and present directions for future work.",
    "relatedWork": "RELATED WORK: In previous research in psycholinguistics, models of grounding [1] have been proposed to explain how participants establish and maintain mutual understanding over the course of the conversation. Participants coordinate their actions as they present utterances to each other, and they produce evidence of understanding in return. This evidence can come in multiple forms, e.g., continued attention, a relevant next contribution, or an acknowledgement, such as a head nod or short verbal utterance (\"Uh-huh\", \"Yeah\", etc.). The latter acts are also referred to as feedback or backchannel responses [2] . Grounding is achieved if the provided evidence satisfies the grounding criterion [3] , defined as follows: \"The contributor [of the utterance] and the partners mutually believe that the partners have understood what the contributor meant to a criterion sufficient for the current purpose.\" If the grounding criterion is not satisfied, the contributing participant can initiate a repair, for example, by repeating or restating their previous utterance.\nRELATED WORK: Computational models of grounding have also been proposed and have found practical implementation in human-computer interaction [4] . Such models include Traum's grounding acts model [5] , which represents the grounding process as a series of communicative actions that serve specific functions, such as providing acknowledgment of understanding or requesting repair. An adaptation of the model has been proposed [6] within a framework for multiparty interaction with embodied agents, which considers conversation as a multilayer process requiring inferences about engagement, turn-taking, and utterance contents of multiple actors. Paek and Horvitz [7, 8] explicitly model spoken dialog as decision-making under uncertainty. Their system uses Bayesian models to represent and infer uncertainty about the contents and targets of spoken utterances, requesting clarifications when uncertainty is high or when the expected value of seeking clarification is positive. Nakano et al. [9] propose an approach to multimodal, embodied grounding, which takes into account nonverbal grounding acts produced by users. Their system monitors users' gaze and head nods for evidence of understanding.\nRELATED WORK: In addition to making inferences about the users' grounding acts, embodied agents need to produce coordinated verbal and nonverbal acts in response. Poggi and Pelachaud [10] propose a representation of communicative acts that decouples propositional content from performative aspects, mapping them to verbal utterances and nonverbal behaviors, respectively. Marsi et al. [11] model the agent's head motions and facial expressions to convey uncertainty about the information the agent is conveying through speech. Other work has focused on providing incremental embodied feedback to ongoing user utterances [12, 13] , using verbal confirmations and clarification requests, as well as nonverbal cues such as head nods and facial expressions, to indicate the system's degree of understanding. The DeepListener system [8] uses verbal clarification requests paired with smooth shifting of the color of a glowing lens graphic to provide incremental feedback about the system's confidence and readiness to accept utterances.\nRELATED WORK: Our work is related to the aforementioned efforts and aims to construct an integrative solution that enables embodied communication under uncertainty in situated, multiparty contexts. Like Traum and Rickel [6] and Paek and Horvitz [7, 8] , we model conversation as action that happens at multiple levels, each supported by a different set of communicative competencies. The latter work also focuses on representations and handling of uncertainty. The main difference is that we focus on a broader range of communicative competencies, and construct an implementation that operates in the more complex context of a deployed multiparty, situated interactive system.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: This paper presents a summary of the the NEMOHIFI 1 * Corresponding Author Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage, and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). Copyright is held by the author/owner(s). ICMI'13, Dec 09-13 2013, Sydney, NSW, Australia ACM ACM 978-1-4503-2129-7/13/12. http://dx.doi.org/10.1145/2522848.2531755 .\nINTRODUCTION: a Spoken Conversational Agent (SCA) that manages the HiFi audio system. For end users, its functions equate a remote control (select a CD, track or radio channel, record music, change channels etc.), except that instead of clicking, the user interacts with the agent using voice. The baseline (non-adaptive version) HiFi system is a proprietary system developed by GTH [1] .\nThe Emotional Engine, NEMO: Most spoken dialog systems have an architecture that is similar to the HiFi SCA, as shown on the left side of Figure 1 . The user utters a sentence and the Speech Recognizer captures the sounds from the user's speech, matches the recognized words against a given set of vocabulary. Then the matched words are passed to the Language Understanding module to extract the concepts (semantic information) of the sentence. A series of concepts are then passed to the Dialog Manager to activate dialog goals. The Dialog Manager decides both the actions to be taken and the feedback to the user for the current dialog turn, and passes the semantic information to the Natural Response Generator module to generate a suitable textual response to the user. The textto-speech (TTS) module then synthesizes the message and speaks to the user. The original non-adaptive HiFi SCA version used a neutral-voiced commercial TTS. A detailed architecture of the baseline system is given in [1] and the evaluation results reported in [2] . In converting the HiFi SCA into an affect-adaptive system, its existing components were not modified, except for the Natural Response Generation textual content (NRG). Instead the HiFi SCA communicates externally with NEMO. The interaction between the system's modules and NEMO is shown in Figure 1 . The information flow is similar as described previously, but this time, the Dialog Manager additionally passes certain dialog features that are significant predictors of the user emotional state to the Affect Predictor. The Affect Predictor classifies the emotion state of the user following a Simple Logistics trained model. The classification result is then passed on to the need module to update the agent's Success need. Consider a user having a few bad dialog turns -perhaps the HiFi SCA failed to completely understand the user request and repeatedly asks the user to provide new information and extends the otherwise short dialog. In this case, the Dialog Manager sends certain relevant features (request turns, contextual information etc.) to the Affect Predictor. Based on these features, the Affect Predictor predicts that the user is frustrated. This information is then updated to the need module, which modifies the agent need, particularly its Success need. The agent now perceives the user as being frustrated and therefore its Success need is low. The dynamicity of the need level also depends on the situations of the previous turns; consecutive or continuous prediction that the user is frustrated causes the agent's Success need satisfaction to decrease rapidly, and so when a good event (turn) appears right after, (and the user is now predicted to be in a positive emotion), the agent will not immediately change its state to a joyful one, but rather surprised or neutral, depending on the situation. Conversely, if the agent is in a joyful state for sometime, and continues with turns that are perceived as good (user predicted to be satisfied in consecutive turns), the drive to gratify its Success need will not be as significant as in the other case, and so its joyful state reaches its maximum and starts decaying into a a neutral state, though it continues to perceive the ongoing events as positive ones.\nThe Emotional Engine, NEMO: Next, the agent's Success need information updates the rest of the modules in NEMO and to generate an emotion that is coherent with the agent's assessment of its current Success need. Finally the chosen emotion matches against the natural response generation for a suitable response content and is synthesized into a speech response of a specific intensity of the chosen emotion by an Emotional TTS, known as the GTH-EMO TTS, built by [3] . GTH-EMO TTS is used in replacement of the original neutral one. Relevant emotions for this demo would be colourings of anger, sadness and neutral.\nAutomatic Affect Prediction: The user state is modelled using an existing dataset, that was later re-labelled by 17 independent annotators. A set of conversational features are used as the predictors and user satisfaction rating as the target. Machine learning experiments were conducted on two datasets, users and annotators, which were then compared in order to assess the reliability of these datasets. Our results indicated that standard classifiers were significantly more successful in discriminating emotions (especially contentment and frustration) and their intensities (reflected by user satisfaction ratings) from annotator data than from user data. The speech recognition accuracy is 75% on average. See [5] for details.\nUser study: A user study was conducted with 24 subjects, in which both versions of the agent (non-adaptive and emotionallyadaptive) was compared. Subjects are Spanish native speakers that were mostly University students (between 18-35 years). Evaluation results showed that NEMOHIFI was significantly preferred over the baseline agent -25.0% preferred the baseline agent and 75.0% opted for NEMOHIFI, \u03c7 2 (1)=36.0, p=.000 [6] . The results provide substantial evidences with respect to the benefits of adding emotion in a spoken conversational agent, especially in mitigating users' frustrations and ultimately improving their satisfactions. Pairwise t-test results for subjective qualities between both versions are shown in Table 1 , where * denotes substantially statistically significant results. For more details, see [6] 2 ",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Neurological and mental conditions are a growing concern to society. E.g., the death rate among the middle-aged white population in the U.S. caused by \"despair\" (suicide or substance abuse) has doubled over the last two decades [2] . An estimated 16 million people in the U.S. are now affected by depression disorder [4] . During a similar interval, death rates from Alzheimer's disease climbed by 55%, now affecting nearly six million Americans [10] .\nINTRODUCTION: For both aforementioned example conditions it is true that early detection can mitigate the issue. Early treatment of depression lowers the risk of suicide, boosts productivity, and lowers health costs [7] . In the case of Alzheimer's disease, the benefits of early detection include behavioral stabilization, preserved independence, and slowed cognitive decline [5] .\nINTRODUCTION: However, early detection of neurological or mental conditions is often not possible since many people do not have access to neurologists or psychiatrists where they live, they may not be aware of that they should be seeing a specialist, there is often substantial transportation and cost involved, and there is a severe shortage of medical specialists in these fields to begin with. As a solution, we are presenting NEMSI (NEurological and Mental health Screening Instrument), a cloud-based multimodal dialog system that conducts automated screening interviews over the phone, smart phone app, or web browser to elicit evidence required for detection of the aforementioned conditions, among others.\nINTRODUCTION: Intelligent virtual agents have been considered for use for clinical applications, including in areas of mental, behavioural, or neurological health in the past few years. For example, SimSensei Kiosk [3] is a virtual human interviewer specifically built to render clinical decision support. It captures verbal and non-verbal behaviors to extract distress indicators correlated with mental conditions such as depression or PTSD. SimSensei Kiosk deploys real-time computer vision and speech analytics capabilities to control dialog management and non-verbal behavior of the avatar. The study showed that people are generally comfortable sharing personal information with a virtual agent, even if speech and video signals are being captured.\nINTRODUCTION: [8] presented results of a large-scale effort, funded by several NSF grants, building a virtual health assistant for \"brief motivational interventions\", for example, interviews about a subject's drinking behavior. The described system is using text input from the subject's keyboard (or, alternatively, a speech recognition hypothesis) along with the information about the subject's facial expressions to determine what next steps to undertake in the interaction. To produce avatar behavior that is adequate and empathetic, contents of user responses and their facial expressions are interpreted in real-time and influence the avatar's action units.\nINTRODUCTION: The NEMSI system draws upon findings from the aforementioned studies, but makes three significant contributions:\nINTRODUCTION: \u2022 The above systems require dedicated, locally administered hardware, including cameras, servers, audio devices, etc. such that the end user can interact with it. In contrast, the NEMSI system uses end points available to everyone everywhere (web browser, mobile app, or regular phone). \u2022 Furthermore, NEMSI's backend is deployed in an automatically scalable cloud environment allowing it to serve an arbitrary number of end users at a very small cost per interaction. \u2022 Thirdly, the NEMSI system is natively equipped with realtime speech and video analytics modules [9] extracting a variety of features of direct relevance to clinicians in the neurological and mental spaces (such as speech and pause duration, that are markers to assess amyotrophic lateral sclerosis [6] , or geometric features derived from facial landmarks for the automated detection of orofacial impairment in stroke [1] .)",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Supporting users in decision-making tasks is one of the main goals of recommender systems. To this aim, they need knowledge about both the domain and the user. As far as the user is concerned, there are many ways to acquire such knowledge: asking to the user what features of potential items are important, inferring preferences starting from a small set of demographic information, observing the user's behavior and so on.\nINTRODUCTION: In real-world scenarios, such as the dress shopping one, information can be obtained through sensing and interpreting the behavior of the user. The interface of this type of recommender systems could be designed according to different approaches: using appliances and devices available in the environment (displays, mobile applications, etc.) or using an embodied assistant, like a conversational agent.\nINTRODUCTION: In this paper, we propose DIVA (Dress-shopping InteractiVe Assistant), a conversational agent able to provide suggestions to the user in a dress-shop. The agent, embodied in Greta [1] , is projected on an interactive wall and is able to carry on simple dialogues typical of the domain. Moreover, it interprets the verbal and nonverbal behavior of the user for reasoning about her immediate preferences about clothes. For instance, it can interpret the user's facial expression to understand whether she is interested or not in what she is looking at. Indeed non-verbal communicative acts can be considered as important mind-markers, providing information on a person mental state [2] , namely beliefs, goals and emotions. Emotions have a considerable effect on our decision-making and our reactions to stimuli. Upon entering a shopping environment, while looking at items the user might experience a vast set of emotions which can affect the physiological state of the user and result in variations of the voice, facial and bodily expressions [4] .\nINTRODUCTION: According to the framework proposed in [5] , there are three ways and three moments, during the content consumption, in which emotions can be used to improve the quality of recommender systems: (i) the entry stage, (ii) the consumption stage and (iii) the exit stage. In all these phases the affective state of the user should be detected in order to improve recommendations. This framework seems appropriate especially in dress stores since this experience is strongly characterized by emotional responses [6] . Our emotions are continuously activated by stimulus in the shopping environments and provide a continuous feedback about our decision processes in the store. These considerations supported our decision to endow the DIVA agent with the capability of analyzing the behavior of the user in order to interpret it, identify her emotional attitude towards dresses in the shop and infer her immediate interests. Then, this information should be profitably used to provide appropriate recommendations.\nINTRODUCTION: As a first step of our study, we performed a Wizard of Oz experiment to collect a corpus of interactions between users and human shopping assistant. Each interaction move was annotated according to the liking or disliking attitude conveyed by users, as well as with information about purchased items. Then, we used the resulting dataset to extract the knowledge necessary to build and test the model for the recognition of the user's emotional feedback. The user model has been represented as a Dynamic Bayesian Network, due to the ability of this formalism to represent the user mental state by taking into account uncertainty and graduality [7] . The evidence variables of the model are provided by the signals in the face, body postures and prosody of the voice. The model has been tested in another experiment in which the human shopping assistant was substituted by a conversational agent and recognition of affective factors in the user communicative behavior was made automatically. Results are quite encouraging since the model decisions matched the human one in 89% of cases. At the end of this phase of the project we applied the approach to a simple content-based recommender system in the shopping domain for proving the feasibility of the proposed approach.\nINTRODUCTION: The paper is structured as follows: in Section 2 we provide an overview on related work. In Section 3 we summarize which are the signals in multimodal communication that can be considered as relevant for recognizing emotional feedback during the shopping experience and described how we collected and annotated the dataset. In Section 4, we described the model. Sections 5 illustrate how we implemented a module for real-time recognition of signals in DIVA. Section 6 illustrates the results of the evaluation of the model in terms of agreement between human annotation and automatically recognized attitude. In addition, we showed the feasibility of the integration of this module, used an implicit rating system, with a simple content-based recommender and describe a simple experiment for testing how effective were the recommendations computed using the emotional feedback. Conclusions and future work directions are reported in Section 7.",
    "relatedWork": "RELATED WORK: Since Reeve and Nass published their work about the tendency that people have in attributing human-like properties to interactive systems [8] , there has been an increase in implementation of embodied agents with human-like properties. Several works reports that the use of embodied agents gives to the users the feeling of cooperating with a partner or having a companion rather than just using a tool [9] . In particular, expressive Embodied Conversational Agents (ECA) and social robots can be employed successfully in those domains where it is important to settle a social relation with the user [10] .\nRELATED WORK: Embodied Shopping Assistant agents have been used in ecommerce web site with the main aim to engage the user and improve trust in the system. [11] suggest that avatars can enhance the effectiveness of the web-based sales. To this aim the agent should be designed in order to mix attractiveness and expertise. [12] describe the design and implementation of a virtual salesclerk capable of providing customers in a 3D online shop with advice. These agents use speech, gaze, gesture, intonation and other nonverbal modalities to emulate the experience of human face-to-face conversation with their users. For achieving successful interaction and provide useful suggestions, embodied agents use models of the cognitive ingredients of the user's mental state (interests, preferences, beliefs) and also of its affective component (emotions, attitude, and so on) [13, 14, 15, 16] . Research works on affect recognition have investigated on how different extra-rational factors can be automatically detected by analyzing signs coming from different communicative channels.\nRELATED WORK: Facial expressions are considered very important in emotion recognition Zeng et al. [17] . Therefore, several approaches have been proposed to classify human affective states from facial expression patterns. For a complete review of recent emotion recognition systems based on facial expression the readers are referred to [18] . Another important communicative modality is speech that can be analyzed both from the linguistic and acoustic points of view in order to determine the presence of emotion expressions. Gestures and postures have been used less frequently until now for recognizing emotions. Caridakis et al. 2006 [19] use information coming from the analysis of gestures in order to detect an emotion. Andre' and Wagner [20] use data coming from sensors, such as the accelerometer, for recognizing an \"affective\" gesture by analyzing its trajectory, wideness, etc.\nRELATED WORK: From the multimodal analysis of the communication it is possible to recognize also other relevant extra-rational aspects concerning the user. Lepri et al. [21] , for instance, show how the analysis of the speech can be used to recognize some relevant personality traits of the participants to a meeting.\nRELATED WORK: Engagement detection is another factor that may contribute to model the attitude of the user. It has been mainly studied using gazes [22, 23] , considering eye-contact as a prominent social signal. In particular Nakano and Ishii [23] studied how gaze behavior can be use to estimate user's engagement in the interaction with a conversational agent.\nRELATED WORK: Recently many studies on the recognition of affective signs in multimodal conversation have been done. These studies combine information coming from different communicative channels (i.e. facial expressions, speech, gestures, posture) in order to determine automatically extra-rational factors about the user state. For instance, De Silva et al. [24] and Chen et al. [25] proposed a rulebased approach for recognizing emotions that combines information coming from the analysis of speech and facial expressions. For the same purpose, Sebe et al. uses a DBN for handling uncertainty and incompleteness or imperfections of audio-visual data [26] . For example, bimodal fusion methods at different levels for emotion recognition are presented by Balomenos et al. [27] and Gunes & Piccardi [28] , using facial expression and body gesture. Castellano et al. [29] presented a multimodal approach for the recognition of eight emotions that integrates information from facial expressions, body gestures, and speech. They showed a recognition improvement of more than 10% compared to the most successful uni-modal system and the superiority of feature-level fusion to decision-level fusion. All the studies reviewed above have shown that the performance of automatic emotion recognition systems can be improved by employing multimodal fusion. Moreover, fusion approaches allow addressing the problem of incomplete available information.",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "Introduction *: Multimodal systems enable users to interact with computers through multiple modalities such as speech, gesture, and gaze (Bolt 1980; Cassell et al., 1999; Cohen et al., 1996; Chai et al., 2002; Johnston et al., 2002) . One important aspect of building multimodal systems is for the system to understand the meanings of multimodal user inputs. A key element of this understanding process is reference resolution. Reference resolution is a process that finds the most proper referents to referring expressions. To resolve multimodal references, many approaches have been developed, from the use of a focus space model (Neal et al., 1998) , a centering framework (Zancanaro et al, 1997) , contextual factors (Huls et al., 1995) ; to recent approaches using unification (Johnston, 1998) , finite state machines (Johnston and Bangalore 2000) , and contextbased rules (Kehler 2000) .\nIntroduction *: Given the substantial work in this area; it is important to evaluate the state of the art, understand the limitations, and identify directions for future improvement. We conducted a series of user studies to evaluate the capability of reference resolution in a multimodal conversation system. In particular, this paper examines two important aspects: (1) algorithm requirements for handling a variety of references, and (2) technology requirements for achieving good real-time performance. In the following sections, we first give a brief description of our system. Then we analyze the main error sources during real-time human-machine interaction and discuss the key strategies for designing robust reference resolution algorithms.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: In order to achieve the most efficient form of interaction, human interlocutors adapt their language style to each other [23] . Alignment takes place on all linguistic levels from lexis to even syntax [15] . With the current technical advances, the goal is to design human-machine interaction according to this model of interpersonal communication. Speech dialogue systems (SDSs) are therefore expected to react flexibly to individual user requirements, e.g., in the form of adaptive voice ouput that adapts to the user's language style. The alignment of voice ouput to the linguistic behaviour of a user is a valuable contribution, especially in dual-task environments, such as driving a car: If the speech-based interaction with an SDS as a parallel secondary task can be designed as natural and intuitive as possible by linguistic alignment, a reduction of driver distraction and an increase of safety is expected. For the design of a user-adaptive Natural Language Generation (NLG) strategy it is essential to understand the properties of user language in a dual-task environment as the context of interaction. For this purpose, we conducted a driving simulator study, where the participants interacted with a simulated SDS. We transcribed the speech data and analyzed the transcripts with respect to syntactic properties. As speech reflects a person's personality, the Big Five personality traits provided us with a realistic, user-focused attribution of the results. This basis will be used in later work towards the generation of user-and situation-adaptive voice output.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Persuasive technologies developed to date have, for the most part, implemented persuasive techniques inspired by human-human interaction [16] . Rather than investigating a single persuasive technique, we are interested in modeling all the communicative activities that take place during a session with a human health counselor who is trained to change the health behavior and attitudes of her clients. Since physical activity is a behavior with many well-known long term health benefits [33] , we focus on the promotion of attitudes that are known to be predictive of changes in exercise behavior [29] .\nINTRODUCTION: In the current study, we investigate the efficacy of using an embodied conversational agent to simulate this face-to-face counseling with as much fidelity as possible, and compare this to a more conventional interface which delivers the same intervention via menus and text messages. We are also interested in the efficacy of the social affordances of embodied conversational agents for building trust and rapport with their users. An effective counselor will attempt to form a strong working relationship with the client, and this counselor-client relationship is predictive of outcomes [22] . Accordingly, we also investigate the use of social dialogue as a relationship-building tactic, and examine how this may impact the ability of the agent to change user attitudes compared to an agent that does not use social dialogue.",
    "relatedWork": "BACKGROUND AND RELATED WORK 2.1 Persuasion and Social Influence: The persuasiveness of a message may be affected by the interpersonal relationship between speaker and listener. A message is more persuasive if it comes from a source with whom the listener perceives a greater affiliation [12] . A relationship need not be extensive or long-lasting to cause a significant effect. Burger et. al. showed that participants were more likely to comply with a request from a confederate they had interacted with previously -even if the previous interaction consisted solely of sitting quietly in a room together for a short period of time [7] .\nBACKGROUND AND RELATED WORK 2.1 Persuasion and Social Influence: Social dialogue may be an effective tactic for eliciting compliance with a request. Howard [18] showed that asking someone how they were feeling, and acknowledging the response, led to greater compliance with a charitable request. Similarly, Dolinski et. al. showed that a person approached with a charitable request was more likely to comply if the requester first engaged in casual social dialogue; they argue that the social dialogue provides situational cues which the listener associates with a relationship [15] . However, relational cues may not be persuasive for all kinds of decisions. The Elaboration Likelihood Model (ELM) of persuasion theorizes that people react differently to persuasive messages based on the personal relevance of the decisions involved. Decisions of high personal relevance are processed by the ``central route'', while decisions of lower relevance are processed by the ``peripheral route''. When the peripheral route is used, then source characteristics, such as the perceived trustworthiness of the source of a persuasive message, have a greater influence on the decision-making process [28] .\nBACKGROUND AND RELATED WORK 2.1 Persuasion and Social Influence: Within the domain of exercise, Jones et. al. [19] examined the effects of source credibility and message framing on the persuasiveness of messages that promoted physical activity. Participants reported more positive exercise intentions and behaviors when messages were attributed to a credible source and were positively framed -focusing on the benefits of exercise rather than the costs of not exercising.\nEmbodied Conversational Agents and Relational Agents: An Embodied Conversational Agent (ECA) is a computergenerated character with human-like appearance and interactive behaviors. ECAs are designed to carry out simulated face-to-face conversations with users, including appropriate use of conversational nonverbal behavior such as hand gesture [11] .\nEmbodied Conversational Agents and Relational Agents: Relational agents are computational artifacts designed to build and maintain long-term, social-emotional relationships with their users [3] . ECAs are particularly effective as relational agents, since many of the conversational strategies used to build interpersonal relationships involve nonverbal gestures [2] . Bickmore et. al. demonstrated the ability of relational agents to change exercise behavior [3, 4] . However, user attitudes towards exercise after each interaction were not assessed and outcomes were not compared to a lower fidelity text interface.\nPersuasion by Computer Agents: The Computers as Social Actors paradigm [27] argues that people will apply social rules to a computer. Following this paradigm, several studies have explored the variables that affect the persuasiveness of a computer agent. However, most of these studies either involved a computer agent delivering a monologue, rather than an interactive dialogue, or focused on intentionally artificial tasks of low personal relevance, such as the desert survival problem [21] .\nPersuasion by Computer Agents: Stern et. al. [32] investigated some of the effects of fidelity by comparing the persuasiveness of a message delivered by recorded human speech to that of the same message delivered by synthesized speech of varying qualities. The human voice was perceived as more favorable by listeners, but no differences in persuasion or perception of the message were shown. Zanbaka et. al. [36] examined the effects of gender and visual realism on the persuasiveness of speakers. Participants listened to a persuasive message delivered either by a male or female human, a male or female virtual human, or a male or female virtual nonhuman (although still anthropomorphic) character. A significant cross-gender effect was found --male listeners were more likely to be persuaded by female speakers, and vice versa. However, no significant effects of visual realism on persuasiveness were found.\nPersuasion by Computer Agents: Katagiri and Takahashi [20] investigated the effects of sociality on persuasion by observing users interacting with a guide agent, which recommended exhibits to visit. Although the study was limited by a small sample size, and no statistically significant results were observed, users were more likely to follow the agent's future recommendations when the agent thanked them for following previous recommendations.\nPersuasion by Computer Agents: Perhaps most relevant, Moon [24] demonstrated that a computer that uses a relational strategy of reciprocal, deepening selfdisclosure in a text-based conversation with the user caused the user to divulge more intimate information and become more likely to buy a product.",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": "(1) what is the difference in perception between a persuasive synthetic voice and an expressive but non-persuasive synthetic voice?\n rq1: what is the difference in perception between a persuasive synthetic voice and an expressive but non-persuasive synthetic voice?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: The knowledge a user or an agent has about a narrative is often imperfect. Tuetenberg and Porteous [35] argue that imperfect knowledge is one of the key components of a narrative. A narrative relies on imperfect knowledge to stimulate the interactions between the different characters given their beliefs, desires and intentions. This is ultimately used as a plot device to drive the narrative, for example by creating plot twists. However, it is also important to have access to the perfect knowledge of a narrative, in this paper called story knowledge, to provide the best possible guidance for the user. By encoding this information, we are able to reason about the user and create more compelling narratives using a user's belief model to provide better guidance based on their beliefs and preferences.\nINTRODUCTION: We present PICA: a proactive intelligent conversational agent for interactive narratives. PICA is able to converse using natural language, react to user input and proactively initiate conversations about the narrative. PICA encodes the past, present and future of both story knowledge and belief models and can be used together with story-authoring systems to guide the user through interactive narratives by acting as an extradiegetic story guide. We are targeting narratives where the user is watching a story unfold. Using PICA as an embodied virtual character, we can increase the immersion by having a participant of the narrative be explicitly aware of the user. By reacting to the user's engagement with the story, PICA is able to initiate conversations with the user to inform them about any parts of the story they might have missed and is able to guide them to the most interesting path through the story. For instance, Figure 1 shows an example of the user not being very engaged with the story, when Horton and Victor were talking. Thus, PICA stores a value close to 0 for its belief of the user knowing about Celia. A value of -1 indicates that PICA thinks that the user is not aware of it, while 1 would represent the user definitely knowing about it. Since this is a crucial part of the story, PICA initiates a conversation in which it informs the user about this fact, thereby increasing its belief of the user's knowledge according to the engagement. This paper makes the following main technical contributions: We developed a sub-symbolic architecture, called the Temporal Knowledge Map (TKM), to encode story knowledge. This architecture is inspired by neural-symbolic systems [7, 37] , where a symbolic knowledge base provides the foundation to initialize and design the resulting neural network to model uncertainty and story graphs [10, 11] to model time. This guarantees a modular structure at the event level, which cannot be guaranteed by using a neural network for knowledge representation, and thereby it offers a convenient way to selectively update and reason about every node in the TKM. Compared to symbolic knowledge bases, the TKM is able to modify parts of its story graph to model the progression of the story. This is enabled by adding an inference system to the TKM that handles these modifications. We also developed a discourse module using Behavior Trees to author a modular representation of the proactive and reactive capabilities of PICA.\nINTRODUCTION: We demonstrate PICA in the context of an existing interactive narrative planning system, to show its ability to interface with experience managers [28, 29] .",
    "relatedWork": "RELATED WORK: Encoding of Knowledge and Narratives: Min et al. [24] propose using LSTMs to infer the user's goals during a game. Thue et al. [36] propose PASSAGE, a system to combine the modelling of the user with the story generation to provide stories that are tailored for the user. Ramirez and Bulitko [27] improved PASSAGE further to dynamically adapt stories depending on the user's style of play. Approaches on modeling story representations are described by Elson [10, 11] and Eger, Beret and Young [9] . Chekol et al. [4] encodes time and uncertainty in a knowledge base representing a general storage of facts.\nRELATED WORK: Conversational Agents: Eliza [38] and ALICE [14] are both chatbots using rules and pattern matching. Deep recurrent neural networks [18, 31, 33] provide a powerful way of creating general conversational agents. Reinforcement Learning [12] enables the agent to learn during the conversation. However, all of the machine learning approaches require a large amount of training data which can easily result in a knowledge acquisition bottleneck. There have been approaches in using conversational agents [3] in stories and games [25] as well as in educational environments [30, 34] . Yu and Riedl [39] propose an invisible story guide guiding the user.\nRELATED WORK: Neural-Symbolic Systems: KBANN [37] and C-IL2P [5] [6] [7] provide simple and intuitive algorithms to translate a symbolic knowledge base into a neural network. To refine their network, they both still need training data. This obscures the initial facts and rules that were used to design the neural network. There have also been discussions [15] concerning the advantages and challenges that neural-symbolic systems and sub-symbolic systems [22, 32] still face. A symbolic knowledge base [2] is still the system used in many applications as it provides an intuitive way of understanding and reasoning about the knowledge. The issue remains how to acquire knowledge and modify the knowledge base [19] . Story graphs and symbolic knowledge bases provide a way to encode narratives. We want to encode uncertainty to model belief which is made possible by using neural networks. We propose to combine these approaches to create PICA.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Resisting the Dark Side's temptations in Knights of the Old Republic, grilling a murder suspect in Indigo Prophecy, or flirting shamelessly in Fa\u00e7ade-choosing what to say is an important part of many games. In this paper we present a direct comparison study of dialogue interfaces, investigating the impact of how dialogue choices are made available to players. Our results provide important future guidance for game design, especially as the underlying structures of character and story become more complex.\nINTRODUCTION: Our comparative study uses a version of Michael Mateas and Andrew Stern's Fa\u00e7ade, instrumented to support three different screen-based dialogue interfaces for complete runs of the game. We investigate three dialogue interfaces: free-form text entry (as in the original Fa\u00e7ade and most interactive fiction), a dialogue menu that specifies exact language to be spoken by the audiencecontrolled character (as in games like Planescape: Torment and Star Wars: Knights of the Old Republic), and relatively abstract specification of responses (as in games like Indigo Prophecy and Mass Effect). Previous studies have not addressed how different dialogue interfaces change the user experience of an otherwise identical game .\nINTRODUCTION: Prior to our study, the only guidance available to dialogue interface designers has been their own intuition and the assertions, sometimes contradictory, of other game designers and writers. We review some of this prior guidance, later drawing out how the results of our study help situate earlier received wisdom. We find that sentence selection interfaces most strongly motivate story involvement, and though the time required to read them is considered their major drawback in industry, few participants reported being bothered by this. We also find that abstract response interfaces, though commonly praised in industry for preserving natural conversation flow, are actually most effective at facilitating a sense of game-level control-and feel least conversationally natural to players. We find that natural language understanding interfaces, though certainly somewhat error-prone (as the conventional wisdom holds), are surprisingly engaging and enjoyable for players.",
    "relatedWork": "RELATED WORK: Though we know of no direct comparison studies of different dialogue interface approaches carried out within game development groups, industry certainly engages in iterative refinement of interfaces, sometimes driven by playtesting. Perhaps the best-known recent example of such work is BioWare's on the dialogue interface for Mass Effect. This reportedly went through 10-12 iterations, internally, aimed primarily at speeding up the player's ability to choose responses (to preserve conversation flow) and secondarily at allowing the player character to perform lines without repeating aloud something the player had just read silently [10] .\nRELATED WORK: A concern with conversational flow is one of the common objections to the dialogue interface that BioWare used for games prior to Mass Effect: sentence selection interfaces such as that of Star Wars: Knights of the Old Republic (or the more recent Dragon Age: Origins). As Brent Ellison argues, -reading all the possible responses takes time and brings conversation flow to a halt\u2016 [4] . Similarly, Lee Sheldon observes that -While [a sentence selection interface] gives the writer even more opportunity for character revelation, especially of the player character, it adds more text to read-one reason it only occasionally shows up in console games, and why designers are forever trying to find ways to shorthand it\u2016 [12] .\nRELATED WORK: Sheldon's observation that sentence selection interfaces provide an opportunity for revealing information about the player character touches on two further pieces of design wisdom about such interfaces. First, as Sheldon points out, game writers can make player characters -far more witty, articulate, and wise (or boring, tongue-tied, and stupid!) than the player himself.... We give him the chance to stand up in a conversation with Albert Einstein or Dorothy Parker or Dennis Miller and hold his own.\u2016 However, the fact that sentence selection interfaces can offer players enticing things to say (and enticing roles to play) points in the opposite direction of the second piece of design wisdom about such interfaces. This is an outgrowth of the conventional -player character as transparent cipher\u2016 viewpoint that has led even storyrich games such as the Half-Life series to choose the odd conceit of a silent player character. As Richard Rouse puts it:\nRELATED WORK: [W]hen players want to play games, often they want to play themselves. If the character they are controlling has a very strong personality, there is a distancing effect, reminding players that the game is largely predetermined and making them feel like they are not truly in control of what happens in the game. Particularly frustrating are adventure games that feature strongly characterized player characters who keep speaking irritating lines of dialogue. [11] In addition, Sheldon argues that sentence selection interfaces can, through well-written sentences, provide multiple dimensions of information about the choices players are making. This can include the topic, the approach to the topic, and how forcefully the character presents her case. As discussed further in our results section below, we found that sentence selection interfaces could create an illusion of greater nuance, without being strongest at communicating information about gameplay impact. They were, in addition, particularly strong for characterization-not only of the player's character, but of all characters in the conversation.\nRELATED WORK: As noted above, the most common alternative to sentence selection interfaces is a different kind of menu-one that displays shorter, less fully realized conversational options. The display may be of discourse acts, topics, tones of response, partial responses, more diverse conversational actions, and so on. With games like Mass Effect and Indigo Prophecy adopting menus that present a combination of these, we can recognize this hybrid form as a prominent alternative to sentence selection. In this paper we refer to these as abstract response menu interfaces.\nRELATED WORK: In addition to abstract response menu interfaces being discussed as faster to view (with the expectation that this helps preserve conversational flow) another common observation is that they aren't as predictable for players as sentence selection interfaces. Ellison writes of sentence selection: -There is no ambiguity in the player's decision.\u2016 Sheldon says that, on the other hand, abstract menu interfaces -can interject an immersion-harming game played between designer and player-What is my playercharacter going to say next?\u2016 Within the gaming community, the early level Mass Effect -renegade\u2016 response (on Eden Prime) that resulted in physically hitting another character, without this being indicated in the interface, may have helped cement the impression that in this approach the mapping between selection and performance can be unpredictable for players. But such dramatic divergences are relatively unusual, and our findings run in the opposite direction: the abstract response menu was found comparatively easy for strategic control, laying bare the impact on game state rather than the specifics of enunciation, but this also had the result of shifting the interaction's feel from one of conversation to limited-option gameplay. This brings us to the third type of interface in our study, one rarely employed in current commercial games: natural language understanding (NLU). By allowing (and attempting to interpret) free form textual input from players, NLU interfaces potentially enable a much greater range of player response than any singledepth menu could display. This type of interface is common in the independent game design/writing community of interactive fiction practitioners, who create games in which most actions are specified textually and interpreted by a parser. The community has also built up significant discussion of the issues [13] .\nRELATED WORK: The mainstream game community, however, has not explored this option. Sheldon writes, -We're nowhere near ready to turn over conversations with major characters to AI\u2016 and characterizes this interface option as -outside the scope\u2016 of his book. Within the game industry, the best-known contemporary experiment with this interface is Fa\u00e7ade (e.g., well-known industry commentator Ernest Adams writes, -Fa\u00e7ade is one of the most important games ever created\u2016 [1] ). This is part of the reason we selected Fa\u00e7ade for our study, along with its depth of conversational interaction and the fact that its underlying structure was amenable to supporting multiple dialogue interface options. Ellison argues:\nRELATED WORK: [NLU interfaces] are rare in modern games for two reasons. The first is that the freedom they provide is extremely time-consuming to produce. The system needs hundreds of potential responses to accurately simulate a single, short conversation.\nRELATED WORK: The second reason is that even the most robust parsers frequently misinterpret the player's input. In Fa\u00e7ade, an innocent inquiry can send the NPCs [non-player characters] into shock, horrified by what they thought the player just said. These misunderstandings ruin virtual relationships and frustrate the player, while at the same time exposing the program's failings and distracting the user from the interaction.\nRELATED WORK: Anecdotal evidence and past work have certainly shown that Fa\u00e7ade's NLU systems make errors, but Mehta et al's earlier study found that -the narrative cues offered by an interactive drama, coupled with believable character performance, can allow players to interpretively bridge system limitations and avoid experiencing a conversation breakdown\u2016 [8] . In fact, our results find this interface option, though critiqued by many players, was also the one they found most enjoyable .\nRELATED WORK: The work on AR Fa\u00e7ade by Dow et al [3] is perhaps the closest work to our experiment in the game studies literature. AR Fa\u00e7ade is an augmented reality version of Fa\u00e7ade in which the player interacts with the couple Trip and Grace in a physical space decorated identically to the couple's apartment, and Trip and Grace are superimposed on this physical space through a headmounted display. Dow et al compared three different versions of Fa\u00e7ade: AR Fa\u00e7ade (the augmented reality version, where players use speech and physical gestures to interact), the original desktop version of Fa\u00e7ade in which the player types to speak, and a speech input version in which the player speaks to a microphone. A wizard operator typed player statements into the system in the augmented reality and speech input versions. The results of this study revealed that -increased presence does not necessarily lead to more engagement\u2016 and that -mediation may be necessary for some players to fully engage with certain interactive media experiences.\u2016 In our study, although not as pronounced, we also found similar effects with the NLU version.\nRELATED WORK: Another relevant study on Fa\u00e7ade is Milam et al's phenomenological study of interactive narrative experiences. The goal of Milam et al's study was to understand -how players understand and internalize their interactive experiences\u2016 [9] . Sixteen prominent themes that defined how people experienced Fa\u00e7ade emerged from their data. We noticed similar themes and patterns in our data, however we were more interested in studying these themes in terms of commonly discussed game design parameters such as immersion, engagement, presence and agency.",
    "rq": " q5. how engaged were you in different versions of the game?\n3. how much influence did you feel over the story using the different versions?\n5. how did you form strategies and make decisions?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Healthy eating implies complex decision making processes [6] , including being aware of healthy options and choosing among them [24] . One solution to overcome this issue and help people to make healthier choices is to develop health-aware food recommender systems [31] . While significant effort has been put recently into optimizing the food selection algorithms [30] , many other factors can also influence users' overall experience when interacting with a recommender system [14] . Indeed, the way the recommendation is presented [18] , the system's response time [33] , or even the length of the system's utterances [20] can have an influence on users' perception of the system.\nINTRODUCTION: One trend to improve users' experience is to make the interaction more natural by designing the recommendation process as a conversation [23] . Besides helping users to achieve task-oriented goals, conversations can also fulfill interpersonal functions, such as building rapport [29] . Rapport can be described as a dynamic process that can be achieved when people \"click\" with each other or feel the interaction is due to \"chemistry\" [27] . Human-human studies have found that rapport between two people can influence task performance in situations as diverse as peer-tutoring [25] and negotiation [7] . Based on these findings, it becomes important to endow recommender systems with social conversational infrastructure that would allow them to build rapport with their users to improve task effectiveness.\nINTRODUCTION: In this paper, we present a conversational system able to recommend recipes matching users' needs while building rapport with them. More specifically, our work focuses on investigating how the conversational skills of a recipe recommender system and the interaction modes it offers to its users would influence users' perception and their intention to cook. First, we describe the design of our system and its architecture before we explain how the recommendation process works. Then, we evaluate our system through an experiment in which we study the impact of our system's conversational skills and interaction mode on its persuasiveness. Our main contributions are (1) a rapport-building conversational approach to deliver recipe recommendations adapted to users' needs and habits and (2) a subjective evaluation investigating the influence of a recommender system's conversational skills and interaction mode on users' perception of the system, users' perception of the interaction and users' intention to cook the recommended recipes.",
    "relatedWork": "RELATED WORK: Food Recommender Systems. A common approach for food recommender systems is to recommend a recipe based on its ingredients. In [8] , for example, the authors developed a system that relies on recipes that people like to infer their preferred ingredients. The system then recommends new recipes containing the previously inferred ingredients. In [9] , the authors developed a system that collects users' preferences by asking them to rate and tag the recipes they usually cook at home. The system then relies on user's preferences to rank recipes and deliver recommendations with the highest scores. This Matrix Factorization algorithm outperformed the content-based approach proposed by [8] . Other approaches only rely on dietary information to recommend recipes that would match users' needs. YumMe, the recommender system developed in [36] , automatically extracts dietary information from pictures of recipes to form a user profile. The system then relies on this user profile to deliver subsequent recommendations. In [11] , authors analyzed people's eating behavior and clustered people in two categories: those interested in getting healthy recipes, and those who did not care about that. They found that two of the main recipe rating predictors for the first group were the fat and calorific content of the recipe, and decided to incorporate these features in their recommendation process.\nRELATED WORK: All these works focus on improving recommendation algorithms. They do not investigate how the modality of the interaction between the system and its users can improve users' experience which, according to [15] , should not be neglected.",
    "rq": " rq1: how does the way users interact with a conversational recommender system influence their perception of and their intention to cook recommended recipes?\n rq2: how do a conversational recommender system's conversational strategies influence users' perception of and their intention to cook recommended recipes?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Conversation is an essential design component of a chatbot. As a conversational user interface (CUI), in a chatbot the user and the agent interact with a series of chat bubbles in a conversational manner. When designing conversations for chatbots, designers often employ an iterative design process: designing a conversation flow, testing with users, reviewing user data, and improving the design. Many designers use existing chatbot prototyping tools such as Landbot 1 , Botmock 2 , and Chatfuel 3 . Designers rely on visual aids like flow diagrams offered by the tools to create a conversation scenario and generate a working prototype. Normally, an interactive prototype is exported as a web link, which gets distributed to potential users for their testing and feedback. After the testing, designers analyze the collected data manually and revise their conversation design.\nINTRODUCTION: Although iterative conversation design is possible with existing prototyping tools, we observed several challenges around the design process in our formative interview with conversation designers. When designers try to verify their design, it is difficult to recruit participants quickly whenever they want to in order to get feedback on the design. Even though the tools mentioned above support testing chatbots, for testers, there is no way to provide feedback on specific components (e.g., whether the sequence of the conversation is natural, whether a specific utterance is awkward, whether a branch is needed, whether additional topics should be included) due to limited ways to express detailed suggestions. This results in user feedback that is often abstract and not actionable, which in turn presents challenges to designers in making informed design decisions. There have been several methods around collecting granular feedback on design via crowdsourcing in domains such as UI [27] and poster design [23, 32, 35] . However, those approaches mainly support visual design tasks, which might not directly apply to conversational user interfaces. The design of CUI involves 'conversation' that mainly uses free-form responses whereas in GUI, the user interaction is gathered through button clicks, menu selections, etc. As follows, CUI designers cannot easily predict and limit the range of user interactions. Here, we try to collect granular feedback on the unique and specific domain of 'chatbot conversation design', and explore design considerations for getting granular feedback on conversational user interfaces.\nINTRODUCTION: In this paper, we explore the idea of engaging an online crowd in the design process to support conversation design. First, we increase the availability of test participants by making it possible for designers to recruit crowd workers on demand within a chatbot design tool. Second, we guide the crowd to provide concrete and clear feedback on specific components during a testing session. Finally, we provide multiple types of interactive visualizations to help designers effectively interpret the collected data and make design revisions.\nINTRODUCTION: To investigate the feasibility of the three directions we suggest, we introduce ProtoChat, a crowdpowered system built to support the iterative process of conversation design. Designers can create a conversation flow with branching to support conditional flows. After crowd-testing, designers can review and inspect crowd data with interactive visualizations, such as an overview of conversation flows and an utterance-level review of crowd conversations. As a tester, a crowd worker can perform three kinds of tasks within the crowd-testing interface-conversing with the chatbot to follow the conversation flow, adding an appropriate utterance on the chatbot's side, and adding a branch in the conversation.\nINTRODUCTION: To evaluate how crowd workers and designers use ProtoChat in a conversation design scenario, we conducted a three-day study with eight designers. They went through a design iteration each day and performed four main design tasks (Design, Crowd-test, Review, and Interview) with ProtoChat.\nINTRODUCTION: Participants chose different domains for their conversation design, which varied from ice cream order to YouTube channel recommendation to talking behind significant other's back. Each day, we recruited a new batch of crowd workers whose number was determined by the designer. We found that ProtoChat could provide an agile design experience to create, test, analyze, and improve the conversation. Designers were able to improve their design with evidence collected from the crowd, by modifying the overall structure of the conversation or fixing a specific part of the conversation. Designers also diversified the options provided to the user, modified the response format (e.g., natural input, button choice) of topics, or gathered insights of UI design implications for the final version of chatbot. Beyond the conversation itself, some designers set a persona (e.g., proactive, good listener) for the chatbot by editing chatbot utterances with crowd input as hints. The conversation design increased in complexity over time through iterations by 33% after the first iteration, and 11% after the second iteration.\nINTRODUCTION: The contributions of this work include:\nINTRODUCTION: \u2022 Insights from the formative interview that identify challenges in conversation design and the required support for a more agile iterative design process; \u2022 ProtoChat, an interactive chatbot design tool that supports designers to make informed decisions by collecting design feedback from crowd workers and visualizing the crowdsourced data; \u2022 Empirical findings from a user study that shows how our system could help designers to utilize the crowd feedback and provide the crowd workers the methods to suggest concrete feedback.",
    "relatedWork": "BACKGROUND AND RELATED WORK: We review previous work related to conversation design and crowdsourcing applications. We first investigate what kind of methods are currently being used for conversation design of chatbots.\nBACKGROUND AND RELATED WORK: Then, as we propose a system empowered by the crowd, we discuss how crowdsourcing is utilized in chatbot design and how the crowd is invited to work on usability testing in general.\nConversation design methods for chatbot: Prior work has been done to investigate possible ways to design conversations that can be used for chatbots. Existing approaches collect conversation data from humans by Wizard-of-Oz prototyping [13, 14, 28] and workshops [15] . The conversation of Dara [28] , a chatbot that helped Indian artists to discover international opportunities, was designed with Wizard-of-Oz at the beginning. Ko et al. [13] also utilized the Wizard-of-Oz method to notice the user scenarios of searching business cards, which could result in the multi-dimensional search flow in CardBot. Moreover, Wizard-of-Oz was used to personalize the reflection questions for the agent Robota [14] .\nConversation design methods for chatbot: Reflection Companion [15] leverages a 12-user workshop to generate the system's mini-dialogue flows. Wizard-of-Oz and workshop methods enable designers to collect quality conversation data in a controlled setting. However, these methods make designers overwhelmed due to time and participant management. Plus, the human-human conversation needs to be verified again to apply in human-agent conversation.\nConversation design methods for chatbot: Other approaches formulate the conversation by analyzing existing data sources such as Twitter conversation data [7, 33] , mail threads of DBpedia [1] , existing chatbot logs [36] and extracted data from apps [21] . Hu et al. [7] and Xu et al. [33] collected and utilized twitter conversation into a training dataset to generate tone-aware and emotional responses. Athreya et al. [1] used the official mailing lists of DBpedia, which includes discussion and conversational threads of mailing lists so that they could be used for creating conversational scenarios. XiaoIce [36] , an empathetic social chatbot used two data sources to generate conversation; one is conversational data from the internet, the other is the previous chat log between XiaoIce herself and her users. Kite [21] automatically created chatbot templates from existing mobile apps which share the logic of user tasks. Although, human conversation data from the internet such as twitter and mailing lists can be easily crawled at scale, it is hard to be directly applied to design human-agent conversation.\nConversation design methods for chatbot: Different from the previous work, we aim to quickly and easily collect large amounts of granular feedback on the conversation design by crowdsourcing. The crowd can contribute to improving the conversation design by (1) inserting new chatbot-side utterances that match the current context, and (2) suggesting new branches in a conversation that have not been supported by the designer. Furthermore, the crowd responses could be provided as the data for designing a concrete and high-coverage conversation, which can cover as many as possible scenarios that crowd workers want to proceed around that domain. These interaction data from the crowd gives more concrete insights into how to elaborate the conversation design even before implementing a chatbot, which can foster fast iterations on the conversation design.\nCrowdsourcing in chatbot design: Crowdsourcing has been applied in diverse design domains such as collecting design examples [30] , real-time prototyping [17, 19] , and getting design critique or feedback [23, 27, 35] . Likewise, crowdsourcing has been utilized in the chatbot domain. There has been work to utilize the crowd to collect and produce dialogue data for the social chat system. Fantom [10, 11] uses a graph-based dialog model for context-maintenance to find suitable responses. The graph gradually evolves with actual chat interactions and system responses by the crowd. InstructableCrowd [8, 9] is an agent that can crowdsource \"trigger-action\" rules for IF-THEN constructs to automate the management of sensors and tasks.\nCrowdsourcing in chatbot design: Other kinds of work leveraged the crowd to respond to the end-user in real-time while maintaining contexts. Chorus [18] is operated with a group of crowd workers who propose responses, vote each other for the best answer, and share collected chat history to maintain the consistency of the conversation. Chorus demonstrated how the crowd could come up with not only a diverse set of responses but also a diverse set of variations of descriptions on a given topic, where they expected crowdsourcing as a potential approach to explore diverse conversations in the chat domain. CI-Bot [22] is a hybrid system that works with crowd experts so that if the user asks an unknown question, it collects the answers from crowd experts and responds. If the answer is satisfying, the answer is appended to the response list of CI-Bot.\nCrowdsourcing in chatbot design: Otherwise, crowdsourcing has been used as a method of evaluating the chatbot. ChatEval [29] conducted automatic and human evaluation of chatbots with DBDC (Dialogue Breakdown Detection) tasks and A/B testing with the crowd. Yu et al. [34] suggested a method for evolving existing dialog scenarios by requiring users to evaluate an appropriation, correcting, for answers given by chatbot as they proceed with the conversation. This study showed that the crowd's evaluation is effective in evolving the scenario. They have indeed suggested a method of systematic, accessible chatbot evaluation, but the method is only possible with an already existing chatbot with the complete design of conversation. Choi et al. [3] explored how crowd workers can evaluate a conversation design, and identified designers' needs and expectations in involving the crowd in the design process. We extend this work by introducing a fully functional system that supports designers to quickly test with the crowd workers, collect evidence, and analyze the data and by validating through a user study.\nCrowdsourcing in chatbot design: ProtoChat: Supporting the Conversation Design Process with Crowd Feedback 225:5\nCrowd-testing user interfaces: Crowdsourcing platforms allow for the quick collection of user feedback at a low cost. Kittur et al. [12] found that, for prototype or user testing, collecting data points from a diverse crowd population was more useful than collection data from a limited pool of experts. Also, Komarov et al. [16] showed that crowdsourcing is a productive way for conducting performance evaluations of user interfaces, there have been studies about leveraging the crowd into testing. Leicht et al. [20] organized the crowd-testing types in four categories: (1) functional and verification, (2) nonfunctional, (3) validation, and (4) usability, all of which are commonly applied to software testing. Muccini [6] shared that the method has benefits such as availability, high coverage, costeffectiveness, real scenarios, and speediness but also has the challenges of lacking standards, reward mechanisms, and coverages.\nCrowd-testing user interfaces: To explore and overcome the lacking features of crowd-testing in software engineering, Guaiani et al. [5] explored the way of integrating the crowd-testing into laboratory settings which could potentially complement each other. They collected surveys from crowd testers and found that the difficulties they faced are time pressure or insufficient amount of information, which can be mitigated through better test management. When applying the crowd-testing methods to evaluate web-based interfaces, Nebeling et al. proposed the system CrowdDesign [24] and the toolkit CrowdStudy [25, 26] to invite crowd workers into the process of designing and usability testing the web-based interfaces. For easy integration of crowd-testing, ZIPT [4] was proposed as a way of comparative usability testing at scale without any integration of apps. As a result, designers can easily collect, aggregate and visualize the user's interaction path between third-party apps. Chen et al. [2] introduced two techniques to increase the coverage of crowd-testers. The interactive event-flow graphs collected interactions of every tester and visualized in a single graph and GUIlevel guidance could prevent the inactive exploration of paths. As Wang et al. [31] pointed out, crowd-testing often generates a high degree of test case duplication, because crowd workers tend to follow the same paths while testing in parallel.\nCrowd-testing user interfaces: We aim to apply crowd-testing to support an iterative design of chatbot conversations. As the domain of our interest differs from previous literature such as software engineering [5] or web interface design [2, 24, 26] , design considerations for building crowd-testing interfaces, which aim to evaluate the conversation design, need to be discussed. Our system creates a chatbot prototype that embeds the conversation design. The chatbot not only presents the utterances but also asks for feedback and suggestions to improve the design during the conversation session. Crowdsourcing granular feedback and suggestions can give designers more concrete insights into future iterations on the conversation design.",
    "rq": " (1) can the crowd produce high-quality work with protochat?\n and (2) how does the designer utilize the crowd outcome in their design process with protochat?\n an example of design iterations (p8) is shown in fig. 13 . designers changed the main flow of conversation. p6 (house repair) designed a conversation with two major flows: (1) online diagnosis of the problem and (2) reservation for door-to-door repair. they assumed that users who directly inquired about solutions would not make a request to visit their home. however, after the second iteration, the designer found that some crowd conversations made a visit request after choosing the online diagnosis branch. p6 decided to add an extra branch to help the users who wish to make a reservation for door-to-door repair after the crowd responded with \\'yes\\' for the question \"do you think we need to visit your home?"
}
{
    "intro": "INTRODUCTION AND BACKGROUND: With the proliferation of voice-based assistants such as Alexa, Siri, and Google Assistant, there has been a resurgence of research into building truly intelligent conversational assistants that can maintain a long, natural conversation with users. One important direction was a recent series of Amazon Alexa Prize Challenges, providing a competition platform and monetary incentives to spur development [16, 21] in conversational AI. Many practical applications of conversational systems have been proposed (e.g., for companionship to improve mental well-being [19] , and for therapy [10] ). While much room for improvement remains in the current implementations of the conversational AI systems, the potential for intelligent, empathetic and broad-coverage conversational systems is widely recognized.\nINTRODUCTION AND BACKGROUND: However, for open-domain conversational system to be engaging and intelligent, it must keep the user's interest for extended periods, without sounding \"boring\" or \"annoying\", which unfortunately is the case for current voice-based assistants. In this paper, we investigate one natural approach to handle \"boring\" responses, which is to modulate response prosody via commonly available Speech Synthesis Markup Language (SSML) [25] . For our experiments, we replaced common phrases (i.e. filter words or interjections) with prerecorded Speechcons from Alexa Skills Kit APIs 1 . In some cases, the pitch and rate of these Speechcons are additioanlly tuned to convey excitement, hesitation and emphasis, allowing the agent to deliver a variety of empathetic responses to users. The example conversation 2 provided in Figure 1 shows how our system utilized prerecorded Speechcons such as \"Allright\" or \"Aw Man\" to improve naturalness in conversations.\nINTRODUCTION AND BACKGROUND: As our main contribution, we report the results of a large-scale, empirical study aiming to quantify overall and immediate effects of prosody modulation on user engagement and satisfaction, in opendomain, unrestricted conversational setting as part of the Amazon Alexa Prize competition. Our results indicate that the prosody modulation significantly increases both types of user satisfaction, but the degree of improvements varies across different domains. Specifically, our contributions include:\nINTRODUCTION AND BACKGROUND: \u2022 One of the first attempt to quantify the immediate effects of prosodic modulation on user satisfaction and engagement in unrestricted open-domain conversations \u2022 A large-scale empirical experiment comparing the effects of prosody modulation to user satisfaction across multiple conversation domains Next, we briefly review closely related work to place our contributions in context.\nBackground and Related Work: Conversational AI and Satisfaction Prediction. Recently, conversational systems research has experienced dramatic progress. For example, automatic speech recognition (ASR) has been revolutionized by neural models [12] . Similarly for dialogue management, both rule-based [2, 11] and end-to-end [18, 31] systems were studied extensively. To maintain a flexible and scalable structure, several architectures have been proposed [4, 9] as well.\nBackground and Related Work: As these systems became more sophisticated, many work proposed new ideas to automate the evaluation process by predicting conversational user satisfaction, as defined in [28] [29] [30] . For instance, there have been successful attempts to predict satisfaction once conversations (sessions) are completed, using traditional methods [17, 22] and neural-based models [13, 14] . Lastly, one recent work [5] proposed a unified neural framework to predict offline (sessionlevel) and online (turn-level) satisfaction simultaneously.\nBackground and Related Work: Speech Synthesis and Prosody Modulations. Speech synthesis is an active research field that studies the artificial production of human speech [8] . Speech synthesizers, also known as text-tospeech (TTS) synthesizers, are placed at the final phase of modern dialogue systems to transform textual output into a natural voice output. In conversation-related speech strategies, several work focused on analyzing the impact of interjections and filter words such as \"Um\", \"Uh\" and \"Wow\" to user behaviors. For instance, [20, 26] reported the change in eye gaze behavior when conversations start with these filter words. Another study [24] showed that speech models trained on positive exclamations achieved higher satisfaction from listening tests. Based on these empirical findings, recent dialogue systems [3, 4, 9, 15] incorporated liveliness by adjusting the prosodies of interjections and filter words using SSML.\nBackground and Related Work: However, studies to evaluate the impact of prosody modifications to user satisfaction have been limited. One recent study [6] measured the effectiveness of prosody modification using crowdsourced workers and showed that while comprehensiveness (i.e. informative, correctness) improved, naturalness (i.e. interruption) decreased. However, this evaluation was measured only on each information-seeking turn and the authors highlighted the need for future study in a more realistic conversation. Cohn et al. [7] addressed this limitation and showed modifying both filter words and interjections achieved the highest user ratings when evaluated on large-scale open-domain conversations. However, since user ratings convey an overall impression, quantifying the immediate effects of prosody modification within each conversation remains unexplored.\nBackground and Related Work: Thus, our work extends the ideas here by first train a state of the art immediate-and offline-satisfaction prediction model [5] and quantify both immediate and longer-term effects on user satisfaction and engagement using our proposed metrics, which are described later.",
    "relatedWork": "Background and Related Work: Conversational AI and Satisfaction Prediction. Recently, conversational systems research has experienced dramatic progress. For example, automatic speech recognition (ASR) has been revolutionized by neural models [12] . Similarly for dialogue management, both rule-based [2, 11] and end-to-end [18, 31] systems were studied extensively. To maintain a flexible and scalable structure, several architectures have been proposed [4, 9] as well.\nBackground and Related Work: As these systems became more sophisticated, many work proposed new ideas to automate the evaluation process by predicting conversational user satisfaction, as defined in [28] [29] [30] . For instance, there have been successful attempts to predict satisfaction once conversations (sessions) are completed, using traditional methods [17, 22] and neural-based models [13, 14] . Lastly, one recent work [5] proposed a unified neural framework to predict offline (sessionlevel) and online (turn-level) satisfaction simultaneously.\nBackground and Related Work: Speech Synthesis and Prosody Modulations. Speech synthesis is an active research field that studies the artificial production of human speech [8] . Speech synthesizers, also known as text-tospeech (TTS) synthesizers, are placed at the final phase of modern dialogue systems to transform textual output into a natural voice output. In conversation-related speech strategies, several work focused on analyzing the impact of interjections and filter words such as \"Um\", \"Uh\" and \"Wow\" to user behaviors. For instance, [20, 26] reported the change in eye gaze behavior when conversations start with these filter words. Another study [24] showed that speech models trained on positive exclamations achieved higher satisfaction from listening tests. Based on these empirical findings, recent dialogue systems [3, 4, 9, 15] incorporated liveliness by adjusting the prosodies of interjections and filter words using SSML.\nBackground and Related Work: However, studies to evaluate the impact of prosody modifications to user satisfaction have been limited. One recent study [6] measured the effectiveness of prosody modification using crowdsourced workers and showed that while comprehensiveness (i.e. informative, correctness) improved, naturalness (i.e. interruption) decreased. However, this evaluation was measured only on each information-seeking turn and the authors highlighted the need for future study in a more realistic conversation. Cohn et al. [7] addressed this limitation and showed modifying both filter words and interjections achieved the highest user ratings when evaluated on large-scale open-domain conversations. However, since user ratings convey an overall impression, quantifying the immediate effects of prosody modification within each conversation remains unexplored.\nBackground and Related Work: Thus, our work extends the ideas here by first train a state of the art immediate-and offline-satisfaction prediction model [5] and quantify both immediate and longer-term effects on user satisfaction and engagement using our proposed metrics, which are described later.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: A great deal of learning involves factual knowledge (e.g., numerous topics in medicine, language, and law). Further, such information is often learned outside of a formal classroom setting. Developing more effective automated methods for accelerating or improving factual learning therefore has the potential to benefit a multitude of students on a broad scale.\nINTRODUCTION: Traditional electronic tools for practicing factual knowledge tend to be flashcard based [15, 29, 30] . Flashcard apps are simple and can be easily designed to provide personalized adaptive practice based on well-studied models of human memory [14, 50, 55] . However, to optimize for speed, flashcards typically involve passive learning (i.e., the user is asked to visualize the answer and then check for correctness). This may not fully take advantage of the testing effect (retrieval through testing with proper feedback) [48] . As shown in many previous studies, retrieval practices like testing lead to higher retention than purely studying via even multiple passive means of self-evaluation [8, 34, 47] . Feedback received from test results further improves retention [3, 37] .\nINTRODUCTION: Moreover, flashcards are not typically designed to be engaging, making their effectiveness heavily dependent on people's desire to learn. Research confirms engagement can mediate learning effectiveness [7, 27] , especially for technologybased learning [26] . A more engaging way to learn factual knowledge could therefore lead to better learning outcomes.\nINTRODUCTION: One possible path towards boosted engagement is using Natural Language Processing (NLP) powered chatbots, which are becoming increasingly sophisticated [21, 52] . For example, such systems enable students to speak or type out their answers during a two-way dialogue and receive targeted feedback from NLP techniques interpreting the spoken or written words. This new interaction for learning factual knowledge may be much more motivating and engaging, and may also be more effective at providing adaptive feedback and promoting deeper learning [11] .\nINTRODUCTION: Given this potential for conversational approaches to enhance learning, we designed and built QuizBot, a dialoguebased adaptive learning system for students to learn and memorize factual knowledge in science, safety, and advanced English vocabulary. These three subjects were chosen because they cover diverse topics in medicine, language, and rules. They can represent important subclasses of factual knowledge that are usually learned outside the classroom setting.\nINTRODUCTION: On the technical side, QuizBot leverages the supervised Smooth Inverse Frequency (SIF) algorithm [2] for automatic answer grading and the DASH model [39] for adaptive question sequencing. On the design side, we created Frosty, an encouraging tutoring agent that provides targeted feedback to learners based on their inputs (see Figure 2 ). The design of QuizBot was inspired by previous studies [9, 13, 20] to leverage the persona effect, the strong positive impact of animated agents on learning experience [38] .\nINTRODUCTION: To determine the impact of QuizBot on learning, we evaluated it against a carefully designed flashcard app, the typical medium for learning factual knowledge, through two controlled within-subject studies. We aimed to closely match the flashcard app to QuizBot in order to target assessment at the impacts of the conversational components. Specifically, the flashcard app used the same DASH algorithm for adaptive question selection, and a single pool of questions and answers was subdivided for the flashcard app and QuizBot.\nINTRODUCTION: In the first within-subject study with 40 students, when the number of practice items was held constant for both flashcards and QuizBot, students scored substantially better on recall (fill-in-the-blank) and recognition (multiple-choice) with QuizBot than for items trained using flashcards (66.3% vs. 45.2% for recall and 87.2% vs. 65.8% for recognition). However, the time taken was longer with QuizBot than flashcards. In the second within-subject study with 36 students, we allowed learners to voluntarily allocate their time between the two apps. We found students spent 2.6x more time on QuizBot, and that students performed equivalently on recognizing items but significantly better with QuizBot at recall (with an effect size of .45). These results suggest that QuizBot is more engaging to use and more effective at recall and equally effective at recognition in typical user-driven scenarios. In normal use, QuizBot may be less efficient per unit time, but still yields improved learning on recall due to users voluntarily choosing to use it substantially more.\nINTRODUCTION: This work has three chief contributions. First, QuizBot is the first chat-based learning system for factual knowledge memorization outside of classroom settings. Moreover, we show its effectiveness and engagement through rigorous comparison studies with a traditional learning tool for knowledge memorization, and our results demonstrate benefits of using chatbots to learn factual knowledge, especially for casual learning. Lastly, our results also reveal inefficiencies of chat-based learning systems, and we offer design suggestions for building improved future educational chatbot systems.",
    "relatedWork": "RELATED WORK: Our work was built upon previous studies on natural language tutoring systems, semantic similarity algorithms, and memory models.",
    "rq": "(1) how engaging is quizbot to learners in comparison to flashcards?\n (2) how effective is quizbot with helping learners in recognition and recall, both per number of practice items and per time spent, compared with flashcards?"
}
{
    "intro": "INTRODUCTION: Many people with mental health issues face significant challenges getting the help they need. Mental health service falls short in accessibility and affordability due to a wide gap between needs and provision. While 1 out of 10 people have a mental health problem, only 1% of the global health workforce provides mental healthcare [71] . Psychological counseling or psychiatry services could be a luxury expense for people under financial stress [30] . Beyond structural barriers, fear of being stigmatized also prevents people from seeking psychological help [35] . To expand the access to mental health services and to counteract the problems of stigma, there has been a burgeoning growth in internet-based and mobile applications for mental health interventions. However, these digital interventions are characterized by relatively poor adoption and adherence [21] , which may be due to the lack of the quality of human interaction that a therapist-patient relationship offers [23] .\nINTRODUCTION: More recently, text-based conversational agents, or chatbots, have gained traction as the new generation of digital mental health support system. Some prominent examples emerging from industry and academia include Woebot (woebot.io) and Wysa (wysa.io). Powered by artificial intelligence (AI) and natural language processing techniques, these conversational agents offer a more natural way of interaction. As people engage in dialogues, chatbots process all text and emoji that a user might enter, and offer responsive, guided conversations and advice to help users cope with challenges to mental health [34] . This human-agent interaction sets to invoke anthropomorphism, making people feel like being in a conversation with humans via a messenger app. The typical mental health services these chatbots provide include targeted therapy exercises, including reframing one's thoughts, mindful breathing, and motivational interviewing.\nINTRODUCTION: Literature specifically related to psychotherapeutic chatbots is rather sparse. Recent researchers began to evaluate the efficacy of using conversational agents for mental health outcomes (e.g., [23, 47] ). However, this line of research has not thoroughly interrogated how specific design features might influence client engagement and perceptions of the system. On the other hand, HCI scholarship suggests that conversational agents need to adopt the characteristics of human-human interaction in order to be more engaging [22, 56, 72] . So far, researchers have examined linguistic and conversation styles, such as empathic and emotional expression [20] , self-disclosure [37] , humor [42] , and how these characteristics might influence human-agent relationship [9, 10, 59] . Studies also explore visual characteristics (e.g., [5, 6, 53] . But to the best of our knowledge, there is no empirical study on how the perceived racial (dis)similarity might influence people's engagement with and perceptions of agents for psychotherapeutic purposes.\nINTRODUCTION: Nonetheless, racial identity is an integral part of anthropomorphized agents. It also shapes human's social experiences. In traditional in-person settings, matching clients with therapists of the same race has been found to result in stronger bonding and more positive attitudes [13] . Does perceived racial similarity still matter in agent-based psychotherapy context? If so, understanding the complex relationship between identity and influence will inform a better, human-centric experience.\nINTRODUCTION: To evaluate the racial mirroring effects, we developed a conversational system with racially heterogeneous personas. Distinct profile pictures were used as strong visual cues to indicate agents' racial identities, including White/Caucasian, Black/African American, Latinx, and Asian. Due to mixed evidence of cross-gender effects on chatbot perceptions and following [56] , we created both female and male personas for each racial group. Beside the differences in agent racial personas, the agents interact with users in the same flow of conversation structured in accordance with motivational interviewing (MI) guidelines. This framework offers a collaborative conversation for strengthening a person's own motivation and commitment to change. It is broadly applicable and often used in cognitive behavioral therapy for mental health issues, such as anxiety and depression and substance use [1, 17, 21] .\nINTRODUCTION: Using these prototypes, we conducted an online experiment to investigate the effects of racial mirroring, defined here as the match between the user and agent race/ethnicity. Participants were randomly assigned to racial-mirroring, non-mirroring or control groups. After interacting with the agent, participants completed a survey assessing their perceptions and evaluations of the agent. We investigate the following three research questions: \u2022 RQ1. How are people's perceptions of the agent influenced by racial mirroring in a psychotherapeutic conversation? \u2022 RQ2. How is people's continued engagement with the agent influenced by racial mirroring? \u2022 RQ3. What are people's preferences for racial persona when they are given a chance to customize the conversational interface? Our analyses revealed that racial mirroring had a positive influence on people's perceived interpersonal closeness with the agent. Although the presence of same-race agents decreased people's comfort with self-disclosure, they reported a higher level of satisfaction associated with their use of chatbot for managing mental well-being. With regard to future engagement, we found participants reported a higher desire to continue interacting with the agent. Participants also predicted a closer future relationship with the same-race agents. Finally, people were significantly more likely to select same-race agent personas when they were given an opportunity to customize the conversational interface. The current study offers three major contributions. First, we identify a racially-conscious approach to increasing people's rapport and engagement with therapeutic chatbots. Second, we provide empirical evidence on how racial-mirroring could facilitate people's adherence to psychotherapeutic agents. Finally, we open up a discussion about the important issues of race and diversity in intelligent systems interface design.",
    "relatedWork": "",
    "rq": " rq1. how are people's perceptions of the agent influenced by racial mirroring in a psychotherapeutic conversation?\n rq2. how is people's continued engagement with the agent influenced by racial mirroring?\n rq3. what are people's preferences for racial persona when they are given a chance to customize the conversational interface?"
}
{
    "intro": "Introduction: Our goal is to allow non-experts to build advanced question-answering Virtual Human characters. By non-experts, we mean that scenario authors need not have any background in computational linguistics or any experience in building dialogue systems; although they can be an expert in the specific domain of interaction. The advanced question-answering characters we want to build should have the ability to decide whether to answer a specific question truthfully or to lie about it or to simply refuse to answer depending on the dialogue context and scenario design. Interviewers can practice their questioning skills by interacting with virtual characters that can engage in simple bargaining behavior. In order to reveal certain information marked as sensitive, interviewers may have to employ several different tactics like building rapport, offering to provide what the virtual character wants or threatening the character with consequences for not cooperating. The Amani character is an example advanced question-answering character.\nIntroduction: Several different architectures have been used for building Virtual Human dialogue systems (Traum, 2008) . Leuski et al. (2006; 2010) describe one such architecture that can be used to build simple question-answering characters. In this architecture, scenario designers author a list of questions, a list of answers and all possible links between these. Although this approach allows non-experts to develop virtual characters, it falls short of maintaining coherence over sequence of utterances greater in length than two. describe an architecture that models such dependencies by using an information-state based dialogue model (Traum and Larsson, 2003) , but this architecture is not currently accessible to non-experts for authoring, due to expertise needed in designing task model plans and thematic-role based framebanks.\nIntroduction: We choose an intermediate approach, using a simple information-state dialogue manager, making authoring accessible to non-experts through use of the accompanying integrated authoring tool, Do-mainEditor .",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "RELATED WORK: The abundance of wearable sensors, mobile and IoT devices, as well as other types of UbiComp technologies together with connected cloud platforms enable collection and storage of copious amounts of data [31, 50] . This data can be related to the personal physical activity [35] , stress monitoring [48, 49] , urban spaces [30] , all the way to energy conservation [25] . All of this data, however, becomes truly useful only when it facilitates learning for the purpose of increasing awareness of one's own behavior or the environment [15] , making better future decisions [26] , or supporting increased understanding [19] . One of the key ways of supporting self-learning from such data collected by UbiComp technologies is through the process of reflection [55] .\nRELATED WORK: In fact, a stage-based model of personal informatics states that \"collection and reflection are the core aspects of every personal informatics system\" [54] . Yet, a gap exists in understanding how the process of reflection can be supported through technology [73] . Indeed, designing for reflection is still in its infancy [7, 29] . As a result, much HCI research that attempts to inform design for reflection is based on structured reflection models from learning theory where such models and theories are more mature [29] . Thus far, such works have shown how learning models can be adapted to HCI for analyzing reflection [29] , reviewing it [8] , or designing for it on a conceptual level [68] . Critically, little work has been done in supporting structured reflection in deployed behavior-change systems. Our work aims to fill this gap.\nStructured Reflection Models: A number of theoretical works on reflection have been developed in learning sciences. Some of them focus on exploring the nature of reflection itself [13] or the place of reflection in different professions [62, 69] without necessarily focusing specifically on how reflection could be triggered. Nevertheless, a number of works discuss the right environment and conditions for reflection [36] . As discussed in [62] , reflection takes time and therefore creating and allowing time for reflection is essential. On top of that, reflection is seen by many as a developmental process that can be supported to help people become more reflective over time [36, 62, 80] . When the purpose of reflection is more formal, such as in the case of learning from one's physical activity patterns, structured support or guidance around reflection is of particular value [29] . In such applications, structured reflection models provide insights for designing reflection-centered interactions and offer support for how reflection can be supported to evolve with time [29] . Such models see reflection as a process with stages or levels. Kolb's learning cycle defines 4 stages [51] , Gibb's reflective cycle proposes 6 stages [34] , and both Moon's levels of learning [62] and Bain's 5Rs framework [5] suggest 5 stages (or levels) of reflection. Fleck and Fitzpatrick have proposed an adaptation of these models to HCI, defining 5 levels of reflection [29] . These models, however, were developed to analyze reflection post factum and may be too fine-grained for direct use in design. Conveniently, Atkins and Murphy [4] in their review of literature on reflection, identified three commonly-shared stages: awareness of uncomfortable feeling and thought, critical analysis, and development of new perspective. Our approach aligns with the three stages from Atkins and Murphy, renaming them for simplicity into: Noticing, Understanding, and Future actions (Figure 2 ).\nStructured Reflection Models: Stage 1 -Noticing This stage focuses on building awareness of events and behavior patterns, but without an explicit attempt at explaining or understanding reasons. The stage is aligned with Fleck and Fitzpatrick's revisiting and reflective description levels where description of an event is provided without further elaboration, explanation, or interpretation. Stage 2 -Understanding This stage focuses on analysis of the situation from different perspectives, formulating explanations and observations about the reasons for the things noticed. The stage is aligned with Fleck and Fitzpatrick's dialogic reflection level where cycles of interpreting and questioning as well consideration of different explanations, hypotheses, and viewpoints are taking place. Stage 3 -Future Action In this stage, Understanding leads to development of a new perspective, learning a lesson, or gaining new insights for the future. In terms of Fleck and Fitzpatrick's levels, this step aligns with levels of transformative reflection and critical reflection where past events are revisited with intent to re-organize them and do something differently in the future. Personal assumptions are challenged, leading to change in practice and understanding. Here also wider implications of actions are taken into consideration.\nConversational Approach towards Reflection: As pointed out in [8] , many current approaches in personal informatics support reflection as an activity of \"looking at lists of collected personal information or exploring or interacting with information visualization\". Not until very recently, have researchers started to even study the reflection questions users themselves may have when exploring their personal data [19] . Yet human coaches of behavior change take much more active approaches, asking reflective questions that can help people articulate their underlying needs and goals and increase their motivation [53] . In one example, people who were asked to think about why they eat snacks before making a choice were more likely to choose healthy options [32] . In fact, research has shown that asking people their reasons for doing an activity triggers underlying motivations and leads to focus on higher-level goals [14, 76] . Specifically asking 'why' questions twice has been shown to be effective [29] as well as asking people to take more time to think about the question and to write longer answers [32] . The paradigm of computers as social actors [63] argues that people will apply social rules to a computer. This suggests that successful human counseling techniques might also work effectively in computer based delivery. Indeed an accumulated body of research has demonstrated the efficacy of human-counselinginspired computer-based interventions [9, 44] . In fact some research suggests that in computer-based counseling services, without dialogues, people may be less inclined to comply and may provide only superficial answers to questions [53] . Consequently, in virtual-agent-related research, it has been a goal to construct an engaging, long-term relationship with the user [12] .\nConversational Approach towards Reflection: A comprehensive review by Bickmore and Giorgino on work in health education and behavior change dialogue systems [10] has revealed application domain spanning exercise, diet, smoking cessation promotion, medication adherence, and chronic disease management education and promotions. Most common approaches relied on building prescriptive and persuasive dialogues that would tell the user what to do rather than guide the user to reflect and explore their own goals and motivations [1] . Reflection was not the main focus, and when used related to reflective listening from motivational interviewing, where the approach for reflection is to provide a \"tell me more\" prompt or simply restate what the user said [64] . While such approach may work for one-time interactions, it quickly becomes repetitive and boring in long-term repeated interactions [12] . Indeed, most of the approaches are not designed for long-term repeated interaction with one user, but rather one-time interactions with many different users. Revisiting such dialogue again provides the same fixed interaction. This could be a problem in personal informatics when new data is collected daily and frequent, but short, reflective sessions could be appropriate. In fact, in the FitTrack study [12] , several subjects mentioned that repetitiveness in the system's dialog content was responsible for their losing motivation to continue working with the system and following recommendations. In general although counseling interventions delivered by computer have been found effective, high drop-out rates due to low user engagement during interaction limit their long-term adoption and potential impact [66] .\nConversational Approach towards Reflection: Reflection in personal informatics offers a unique challenge for computer-based dialogue systems as it relies on open user responses, is relatively frequent and requires novel perspectives to engage users each time over long-term [45] . Nevertheless, despite recent advances in ML/NLP techniques, designing a feasible conversational system that can handle the freedom of user expression needed for the purpose of reflection and engage users for a long-term needed for reflection to occur is still a challenging problem [11] . Aspects of properly handling unexpected user responses and potential misrecognitions are challenging [70] , but might potentially be mitigated by appropriate design.",
    "rq": " 1) what reflection on behavior change have you engaged in?\n 2) what is the goal of reflection for table 1 . examples of reflective questions generated during the workshop sessions. questions are grouped by the main prompted categories (rows) and categories identified in through affinity diagramming (columns). only the 6 most frequent categories are shown. the five white cells represent intersections for which the workshop participants generated no questions. for creating diverse and novel questions, we suggested questions for these intersections ourselves. you?\n 3) how often/when do you usually reflect?\n 5) what are the main challenges in reflecting for you?\n1] is retained. this requires asking each time e.g. what is user barrier for a specific goal or activity, or having to switch to a new topic to avoid repetition. remembering such information from user past responses (e.g. a barrier of \"not having a person to run with\"), would enable reflecting on aspects of this particular and personal information, e.g., \"what could you do to try to find someone to run with?"
}
{
    "intro": "INTRODUCTION: In 1966, Eliza simulated dialogue as a Rogerian psychotherapist [47] . Fast forward to 2016, the MIT Technology Review heralded chatbots as one of the year's breakthrough technologies [33] . Chatbots have made much headway since Eliza's introduction. However, it has become apparent that current conversational technologies are still inadequate at handling all of the complexities of natural language interactions, as manifested by a number of high-profile chatbot failures [2, 34] . Breakdowns in understanding user input happen often, and they can have profound impact on how people perceive and interact with a chatbot. In the worst case, they may abandon the chatbot or the current task. Or, they may need to endure a haphazard trial-and-error process to recover from the breakdown. Both breakdowns and current recovery processes decrease peoples' satisfaction, trust, and willingness to continue using a chatbot [19, 20, 28] .\nINTRODUCTION: A universal challenge faced by chatbot developers is how to design appropriate strategies that mitigate the negative impact of breakdowns. Previous work [19, 24, 42, 48] studied strategies that aim to alleviate peoples' negative emotional response from agent or robot breakdowns, such as showing politeness and apologetic behaviors. However, in taskoriented settings, such as a chatbot performing information assistance, these strategies may be ineffective if the user still fails to accomplish the task. In this paper, we focus on strategies that support repair -recovering from the breakdown and accomplishing the task goal.\nINTRODUCTION: Repair is a ubiquitous phenomenon in human communication. When a breakdown happens in a conversation, people take a variety of actions such as repeating, rephrasing, or clarifying, to repair it. Although chatbot users should be skillful in using similar actions as the speaker, the repair task becomes challenging as the listener is no longer a fellow human. Two problems often impede the repair process with chatbots: 1) there may be a lack of evidence that a breakdown has occurred, which may either be a limitation of the underlying technology (i.e., unable to recognize a breakdown) or a failure in design to communicate the breakdown; 2) the system's model is unfamiliar for the user to choose an effective way to repair. When talking to another person, repairs are almost subconscious acts, which may include a combination of speech, gesture, and facial expression [6] . Chatbots rely on machine learning algorithms to process a user's input, which are \"black boxes\" for the user. Though these interfaces are deemed \"conversational,\" they may not be repaired in the same way as talking to another person [32] .\nINTRODUCTION: In this work, we study repair strategies that a chatbot (listener) could adopt to tackle the above problems -providing evidence for the breakdown and supporting repair towards a desirable direction for the system model. We note that many commercial chatbot products are already adopting repair designs to serve these goals. One example is to ask for confirmation when the system has low confidence, which gives a clear signal of a potential breakdown and allows the user to initiate repair without the system mistakenly executing a task. Another example is to provide options of tasks that the chatbot can handle based on their proximity to the user's input, which not only indicates that a breakdown occurred, but also drives the interaction to the scope of the system model's capabilities.\nINTRODUCTION: This paper makes two contributions. First, we identify a set of repair strategies, informed by communication theories and prior work on conversational agents. In addition, we introduce a group of novel repair strategies that aim to expose the system model, as inspired by recent work in explainable machine learning [35, 41, 46] . These strategies explain why a breakdown occurred, such as showing which keywords the system was able/unable to understand, in order to assist a user in effective self-repair. These strategies contrast with system-repair strategies such as directly providing options. Second, we conducted a scenario-based study with Mechanical Turk workers (N=203) to systematically understand people's preferences for different repair strategies. Our study focuses on text-based chatbots, which are widely used and growing in popularity [21] , although some of the repair strategies we examined can be applied to voice-based agents as well.",
    "relatedWork": "BACKGROUND AND RELATED WORK: Our study is informed by communication theories relevant to conversational breakdown and repair, prior work on repair in human-agent interaction, as well as transparency and explanation of machine learning systems.",
    "rq": "a: is it preferable to acknowledge breakdowns?\nb: is it preferable to provide system-repair?\nc: is it preferable to provide assisted self-repair by explaining system's understanding?\n rq2: how do different individual and task-related factors impact preferences for different repair strategies?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "Introduction: A common goal of many spoken dialog systems (SDSs) is to offer efficient and natural access to applications and services, such as e-mail, calendars, travel booking, navigation systems, and product recommendation, in situations where the user's hands and/or eyes are busy with another task, for example driving a car (Pon-Barry, Weng, and Varges 2006) or operating equipment (Hieronymus and Dowding 2007) . The naturalness and usability of a spoken dialog interface depends not only on its ability to recognize and interpret user utterances correctly, but also on its ability to present information in ways that users can understand and that help them to achieve their goals.\nIntroduction: One class of SDSs that has received considerable attention from both academic research and industry are information-seeking SDSs, which are designed to enable users to browse the space of available options (e.g., flights, hotels, movies) and choose a suitable option from a potentially large set of choices. Dialogs with such systems typically consist of two main types of activity: information gathering, in which the system tries to establish users' constraints and preferences, and information presentation, in which the system typically enumerates the set of options that match the user's constraints. An example is given in Figure 1 . In some systems, these activities take place in strictly sequential phases: All of the information necessary to form a database query is gathered, and then the returned options are presented, one at a time or in small groups. In other systems, the activities are interleaved, with users refining their constraints after being presented with some options, or a summary of the option space. In either case, when the number of options to be presented is large, this process can be laborious, leading to reduced user satisfaction. Moreover, as Walker et al. (2004) observe, having to access the set of available options sequentially makes it difficult for the user to remember the various aspects of multiple options and to compare them mentally.\nIntroduction: Although much research has been conducted on the information gathering phase of spoken dialog systems, relatively little attention has been devoted to information presentation. An analysis of the Communicator corpus consisting of approximately 2,000 dialogs with nine different spoken dialog systems found that information presentation is the main contributor to dialog duration 1 (Moore 2006) ; see Table 1 . Moreover, the DARPA Communicator evaluation showed that task duration is negatively correlated with user satisfaction (r = \u22120.31, p < 0.001, see Walker, Passonneau, and Boland [2001] ). Thus, there is reason to believe that improvements in information presentation will lead to improvements in spoken dialog systems.\nIntroduction: Recently, two approaches to information presentation that present an alternative to sequential information presentation have been proposed. In the user-model (UM) based approach, the system identifies a small number of options that best match the user's preferences (Moore et al. 2004; Walker et al. 2004 ). In the summarize and refine (SR) approach, the system structures the large number of options into a small number of clusters that share attributes. The system then summarizes the clusters based on their attributes, thus prompting the user to provide additional constraints (Polifroni, Chung, and Seneff 2003; Chung 2004) .\nIntroduction: In this article, we propose an approach to information presentation which shortens dialog duration by combining the benefits of these two approaches (UMSR). Our approach integrates user modeling with automated clustering such that information is structured in a way that enables users to more effectively and efficiently browse the option space. The system provides detail only about those options that are relevant to the user, where relevance is determined by the user model. If there are multiple relevant options, a cluster-based tree structure orders these options to allow for stepwise refinement. The effectiveness of the tree structure, which directs the dialog flow, is optimized by taking the user's preferences into account. In order to give the user a good overview of the option space, trade-offs between alternative options are presented explicitly. In addition, despite selecting only the relevant options, the algorithm also briefly accounts for the remaining (irrelevant) options. We hypothesize that this approach will enable users to make more informed choices. Our approach to the problem has been implemented within FLIGHTS, a spoken dialog system for flight booking (Moore et al. 2004; White, Clark, and Moore 2010) . Our results show that in addition to improving dialog efficiency (in terms of number of dialog turns) and effectiveness (in terms of successful task completion), our approach increases user satisfaction. We hypothesize that user modeling in combination with content selection and structuring as implemented in our UMSR strategy can improve the information presentation phase of spoken dialog systems in the following ways:\nIntroduction: 1.\nIntroduction: UMSR leads to increased efficiency of information presentation.",
    "relatedWork": "Related Work: Our experiments evaluate the UMSR model against the SR model described in Polifroni, Chung, and Seneff (2003) . Recently, Polifroni and Walker (2008) developed several variants of the SR approach that differ in how they choose and structure attributes in the summaries presented to the user. That is, they differ in the way they rank attributes for presentation and the method used for clustering. There are two approaches to ranking, one based on the original refiner approach (which chooses to cluster on attributes for which a small number of clusters account for most of the options) and the other based on a user model (which chooses attributes for clustering based on the ranking in the user model). For clustering, they added an \"associative mode\" in which association rules among attributes are automatically determined and the summary statements produced describe the associations. Imagine that all options in a cluster with property X also happen to have property Y. Association then means that both properties X and Y are mentioned for the cluster (see Figure 16 ).\nRelated Work: Polifroni and Walker (2008) evaluated the different versions of the refiner strategy approach in a reading experiment, similar to our first experiment. However, they evaluated only the first system turn in the dialog for the different conditions presented. The experimental conditions contrasted structuring with versus without a user model, and with versus without associative clustering. They found that users preferred the system utterances that were generated based on a user model when there were a large number of options available, and that the associative mode was beneficial when tailoring the summaries to the user. Their results hence support our approach: The version of the system that included user modeling and associative clustering led to dialogs that are the most similar to those of our UMSR approach.\nRelated Work: There remain, however, a number of differences between their refined refiner with user modeling and associative clustering and our UMSR model. First, in UMSR the user model is not only used to structure, but also to select options (via pruning of the option tree). Second, Polifroni and Walker's (2008) approach contains two clustering steps: one for clustering attribute values, which is the same as in our UMSR approach, and a second one for clustering options following each query. This second clustering step corresponds to our tree structuring step. Whereas Polifroni and Walker re-cluster at every system turn, we only build up the option tree once (unless the user changes their query, e.g., by wanting to fly on another day, or asking to fly on another airline).\nRelated Work: Finally, their notion of associative clustering (i.e., grouping together attributes whose values are connected, such as short flight duration and direct flights) is realized in our system as well, but in a less explicit way: In the tree structuring phase, attributes that have identical values for a set of options (this would for example apply if all flights with short travel time turned out to have only one leg) are moved up in the tree even if the attribute has a low rank (see Section 3.4). A difference is that our UMSR model treats the user's valuation as a more important feature for dialog structuring than associations.\nRelated Work: Coping with Under-and Over-constrained User Queries. One frequent problem in spoken dialogs is caused by under-and over-constrained queries. In under-constrained queries, the database returns too many results to be presented at once, and the system must support users in further refining their query. In an over-constrained query, there are no entries in the database that match the query exactly, and the system thus has to get the user to relax a constraint, or itself choose a constraint to relax.\nRelated Work: Our UMSR system differentiates between cases that concern missing information about obligatory slots (e.g., origin, destination, and date) and underspecified preferences (concerning preferred airlines, etc.). To fill the obligatory slots, simple slot-filling questions such as When do you want to leave? are asked. For all other cases of overor under-constrained queries, constraint relaxation and specification is inherent in the UMSR structuring approach. For underspecified preferences, the user model provides default specifications. In the case of over-constrained queries, we can think of constraint relaxation in terms of the tree structure: Constraints that are less important to the user (e.g., price for our example business user) are automatically relaxed simply because they are located further down in the option tree. Constraint values, namely, good arrival time being changed to fair, are automatically relaxed due to the ordering of the branches within the option tree.\nRelated Work: Various strategies for coping with over-and under-constrained queries have been developed in the literature. Systems proactively suggest additional refinements for underconstrained queries, as in You could try to look for cuisine type? (Pon-Barry, Weng, and Varges 2006; Varges, Weng, and Pon-Barry 2006) , or relaxations for over-constrained queries, as in Would a slightly more expensive flight work for you? (Qu and Beale 1999; Pon-Barry, Weng, and Varges 2006) . Additionally, constraints can be relaxed automatically based an ontology (Varges, Weng, and Pon-Barry 2006) or by predefining relaxation sets (e.g., from a specific street for a restaurant to the broader neighborhood) or categorical ones (e.g., relaxing Chinese to Asian) (Chung 2004) .",
    "rq": ""
}
{
    "intro": "INTRODUCTION: The medical diagnosis practice is traditionally bedside taught. Theoretical courses are supplemented by internships in hospital services.\nINTRODUCTION: The medical student observes the practice of doctors and interns and practices himself under their control. This type of learning has the disadvantage to confront immediately the medical student with complex situations without practical training (technical and human) beforehand. At the same time, he must manage relations with people in pain and the mobilization of complex and incomplete knowledge. It therefore seems useful to be able to train before confronting his first patients. But for this to be realistic, it can not be done with peers who would play the role of patients. Realism would be insufficient. It is sometimes done with actors playing a learnt case. This is called a Standardized Patient [7] . But it presents several limitations, like \"actor training and availability, reproducibility, changing evaluation criteria, and implementation costs\".\nINTRODUCTION: Advances in virtual reality allow diving, cheaply, the student in a realistic and pedagogically controlled environment. Virtual Standardized Patient (VSP) technology is used to manage the basics of a standardized dialogue between a medical student and a virtual patient. It is generally two-dimensional avatars on a computer display that serve to help learners maneuver through clinical situations or perform a task, such as the proper technique for taking a patient's medical history [15] . For an educational purpose in a real learning environment, VSP becomes unusable in the sense that the slightest deviation from the intended scenario or the lexicon causes the communication to fail.\nINTRODUCTION: In this paper, we demonstrate how to exploit the performance of two different neural classification methods, semantic similarity techniques using sentence embeddings and rules pattern matching to improve the process of language understanding in a dialogue system in order to produce an realistic VSP that can be used by medical students. Another contribution in this work is the dataset construction of doctor-patient dialogues that will be made publicly available. A paper describing in details its building method, content and access point is already submitted.",
    "relatedWork": "RELATED WORK: Recent advances in speech recognition technologies, natural language processing and artificial intelligence have led to the emergence of conversational agents in different domains of life such as health, finance, education, etc. In the health domain, the last decades have seen the expansion of virtual patients or doctors that are used to interact with humans in real clinical scenarios simulation for training purposes, education or medical evaluation. For medical training, students play the physician role to diagnose virtual patient's pain and prescribe appropriate treatments by extracting symptoms in validated clinical and educational interview scenarios [21] . Virtual patients take different forms according to educational objectives and already exist in virtual reality simulators for endoscopy training [6] . They are also used, to teach the techniques of oral examination to interns in emergency medicine [17] , to enable nurses to develop acute care skills such as assessment and management of clinical deterioration [17] , and trainees to practice clinical reasoning techniques [3, 11] . Several medical programs have already incorporated virtual simulations including dummies to assess the competence and confidence of learners to lead clinical situations [24] . Different evaluations show that the use of virtual patients provides additional practices for learners outside laboratory work and improves their performance on actual clinical cases. Smith and colleagues prove that virtual patient technology can improve learner performance on clinical consultation issues [22] .\nRELATED WORK: Much research is underway to improve the effectiveness of virtual conversational agents in fulfilling the ever-increasing needs of healthcare professionals. All of these projects aim to increase and improve the interaction between patients and physicians [12, 13, 20] . This interaction, in virtual simulations using conversational agents, is characterized by:\nRELATED WORK: \u2022 the type of technology (platform hosting the conversational agent) namely smartphones [4, 18] , laptops or desktops [19, 25] and multimodal platforms [14] ; \u2022 the dialogue management strategy namely the finite state strategy where the dialogue is a sequence of predefined steps [14, 19, 25] ; the content-based strategy where the dialog flow is not predefined but depends on the content provided by the user [4] ; the agent-based strategy where the dialogue is between two agents capable of reasoning [18] ; \u2022 the initiative in the dialogue where it is either the user who initiates the conversation [18] , either the virtual agent that leads the conversation [14, 19, 25] , or both [4, 5] .\nRELATED WORK: State-of-the-art methods have been developed to respond to the challenge of question understanding and interpreting of virtual patient dialogue systems. Similar to our approach, the method described in [8] combined machine learning techniques and pattern matching for Question Interpretation. The authors demonstrated the value of combining a hand-authored pattern matching system with word and character-based convolutional neural network for improving question identification in a virtual patient dialogue system. Machine learning techniques are widely used in the medical field to improve medical treatment or diagnosis. They allow to build embedded models in AI systems that are used by the physicians in order to analyze patient data and to make predictions on possible outcome of a treatment or to provide additional data for a diagnosis or a prognosis [23] . They play increasingly important roles in preprocedural planning for complex surgeries and interventions and in processing of the historical records of emerging surgical techniques. This to improve medical education and allow students to practice by the simulation. In [16] , the authors developed a virtual standardized patient system that can understand, respond, categorize, and assess student performance in gathering information during a typical medical history, thus enabling students to practice their history-taking skills and receive immediate feedback.\nRELATED WORK: The aims of the current work are: 1) the development of a medical conversational agent (virtual patient) to interact with the physician using speech; 2) demonstrate the utility of using words embeddings to find similar questions in a virtual patient system.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Among the Organization for Economic Co-operation and Development (OECD) member countries, South Korea's (hereafter referred to as Korea) older adult suicide rate has been the highest for years [54] . This problem is thought to be exacerbated by the amalgamation of the increasing older adult population [21] and changing family structure in Korea [30, 35, 41] , wherein older adults are more likely to tend to themselves; this might lead to a high number of older adults being susceptible to committing suicide [35] . The number of nuclear families in Korea has been increasing, which leads to a decrease in the number of families wherein children support older adults. Thus, the number of older married couples and older adults living alone has been increasing; consequently, the number of older adults looking after themselves has risen [35] . As they get older, they lose ties with their families and friends, which leads to social and psychological withdrawal and, therefore, an increased need for care from others [30] . Because Korea has long had the tradition of older adults depending on their children in later life [41] , such increased self-dependence with a reduction in children to care for the increasing number of older adults has also led to an increase in older adult suicide in Korea. Therefore, there is an urgent need to address this problem in Korea.\nINTRODUCTION: Older adult suicide is accelerated with anxiety, depression, and stress [5] . An effective way to reduce such suicide-related factors would be to increase conversational interactions with others [28] . To increase conversational interactions and continuously care for older adults, both the public and private sectors have provided solutions that are unfortunately not reaching their expected potential. As possible solutions to care for older adults' mental health, the government has proposed two vital services: older adult basic care service and emergency safety alert service, which primarily cater to physical and financial needs. Unfortunately, they lack mental health support because physical and financial needs are more natural to tackle, and there is an insufficient number of care providers compared to those in need of care [25, 63] . Older adult essential care services consist of a social worker visiting the service recipient once a week to check their physical safety and connect the older adult to financial support. It has only increased its capacity by 20,000 within three years, which is quite insignificant compared to the increasing number of older adults who require the basic care service as the number of older adults in need of this service was 641,000 in 2018. As for the emergency safety alert service, each volunteer handles emergency cases of 17 older adults living alone, and the service is limited to physical emergencies [63] . The minor service made available by the government for mental health care of older adults is called a ro-ro care service. Physically healthy Koreans who are 65 years old or older visit more past recipients of primary livelihood security benefits who are living alone or have physical disabilities. Older volunteers are encouraged to engage in activities such as talking with and providing counseling to the more past primary livelihood security recipients. Unfortunately, this service had reached its lowest point in older volunteer participation in 2017 with it dropping to 35% from the 87% participation rate in 2014 [25] .\nINTRODUCTION: In the private sector, there have been products and services to care for older adults' mental health, such as Silbot or Mero, which provide games for preventing dementia. However, these products are not accessible to many older adults because they cost $25,200 per robot, and their primary purpose is dementia prevention, not daily mental health care [64, 73] . There are also voice-enabled conversational agents such as ElliQ and Silverfriend [8, 56] . ElliQ performs activities such as video calls and messaging with friends and family, and gives users reminders. Silverfriends also perform similar tasks, such as daily conversation and personal assistant activities. However, these services are too costly, and they require older adults to adapt to a new technology with which they are unfamiliar. Therefore, it is crucial to develop a highly accessible and constant mental health care service.\nINTRODUCTION: As the public and private sectors are both focused on caring for older adults' mental health through an increase in conversation with robots, voice-enabled conversational agents, or humans, we wanted to deliver such conversational mental health care for depression and anxiety in a more user-friendly, yet accessible way. Smartphone usage among adults aged 60 years or older has been rising rapidly and reached 76% in 2017 [29] . KakaoTalk is Korea's leading communication platform used by 96% of older adults in their 60s or older who own smartphones [58] . Therefore, we decided to develop a mental health care chatbot (MHCC) via KakaoTalk, which would aid in lowering depression and anxiety among older adults with constant interaction.\nINTRODUCTION: Chatbots were proven to be effective in increasing conversation time, albeit the shortness in the length of messages compared to humans [76] . Thus, they were appropriate for simple and repeating conversations with older adults. Although there are a few MHCCs for older adults that have more extensive features, such as Tokunaga's care informational assistance chatbot [71] and Nikitina's reminiscence chatbot [53] , and some with an empirical enhancement of conversation performance [69, 43] , none of them had involved users in the design and development process or had any iteration of such chatbot design processes. The two missing design requirements are deemed crucial in designing a user-friendly chatbot [78] , and we emphasized the incorporation of the important yet missing factors into our MHCC design process.\nINTRODUCTION: With the older user-friendly MHCC, we aimed to answer the question of whether repeated and straightforward short-term interactions with MHCCs could reduce levels of anxiety and depression among older adults. To investigate this possibility, we examined changes in levels of anxiety, depression, and negative polarity. Unlike the focus of many studies on the development of MHCCs, we focused on involving the users at every step-during and after development and implementation-to incorporate user needs and preferences.\nINTRODUCTION: This study made the following contributions to the field of developing MHCCs for older adults:\nINTRODUCTION: 1. We conducted extensive exploratory research using mixed methods on older user interests and characteristics in using MHCCs to comprehend user needs meticulously.\nINTRODUCTION: 2. With our more in-depth understanding of user needs, we developed a more user-friendly MHCC.\nINTRODUCTION: 3. We conducted a 2-week field test of the MHCC with older users and tested its potential effectiveness for mitigating anxiety and depression, even from short-term usage.",
    "relatedWork": "",
    "rq": "1. how can older adults engage more actively with the mhcc?\n 2. what are the precautions needed to be taken in older adult mental health care?\n 3. how often should we ask older adults to share their feelings?\n 4. what other measures apart from conversational protocols should we use to help older adults utilize mhcc more effectively?"
}
{
    "intro": "INTRODUCTION: Recognizing that sleep quality is closely related to personal health and daily performance, people are paying more attention to their sleep health. To help people understand their sleep, various sleep tracking products and mobile apps have been introduced in the consumer electronics market. For instance, Fitbit's wristband supports users to track their sleep with the embedded accelerometer and heartrate sensor, and provides users visualizations with graphs, charts, and diagrams to interpret their sleep data on a web or mobile app interface. However, because such Fitbit like full stack tracking systems have not yet taken into account of collecting users' sleep contextual factors and subjective sleep quality, and as a result, users fail to make meaningful connections of modifiable behaviors to their sleep hygiene [9] . While for some users, it is the lack of sufficient feedback information from the sleep trackers that brings about sleep-related stress and anxiety rather than positively improving their night's sleep [1] .\nINTRODUCTION: To address the limitation of subjective measurements in actigraphy trackers, some HCI researches utilized smartphone interfaces to facilitate self-monitoring and reflections on their daily activities, such as SleepTight [5] . Compared to the objective measurements of commercial tracker products, these sleep research prototypes/products purposely put users in the loop of data collection and sense-making. They take advantage of manual self-reporting to trigger users' self-reflections on their subjective sleep quality and sleep behaviors. Although the self-reporting may consume more user time and efforts than automated tracking, researchers found that users were able to have a better understanding of their sleep patterns. To leverage the benefits of both manual and automation tracking methods, Kim et al. [6] developed an OmniTrack mobile system that could enable users to personalize tracking needs and collect the data with the semi-automated approaches [4] . Whilst OmniTrack was found to be successfully reduce the burden of manual tracking, users still need to configurate the item being tracked, and unfortunately, be bothered to solve technical issues of connecting the external trackers with the OmniTrack app. Moreover, users still need to adjust multiple settings to fit their diverse and dynamic needs, which increases a lot of user efforts.\nINTRODUCTION: Considering those issues, we believe that the conversational agent technology, which has recently raised great attention in both industry and academy, could be a promising design material for improving users' self-tracking experiences. Previous studies focusing on health-related domains have found that chatbots have several advantages in either triggering users self-reporting subjective data in a form of dialogue under specific topics [11] or promoting users' self-reflections on the dialogue content [7, 11] . In addition, chatbots have shown value of alleviating the burden of manual input [10] , which is a good sign to promote long-term communication relationship between the users and chatbot and then induces deep self-reflections. Therefore, we argue that chatbots could potentially facilitate users to log their subjective in-situ experiences more easily and naturally, and meanwhile induce users' in-depth reflections through interactive inquiries.\nINTRODUCTION: Leveraging the advantages of chatbots, we developed Slumber-Bot applying a semi-automated approach to collect both objective sleep measurements using a Fitbit wristband and users' subjective experience in relation to the Fitbit data utilizing an interactive chatbot app. SlumberBot is an internet-based service which could run on multiple platforms and can also proactively initiate inquiries of users' behaviors on sleep-related factors through daily conversation. In this paper, we will present a preliminary field study with five participants over four weeks to evaluate the user experience on Slumberbot. The result highlights that SlumberBot makes it easy for participants to stay engaged in their self-reporting process and supports users self-experimenting to identify disturbance factors of their poor sleep. Through the reinforced self-reflections, some participants were able to pinpoint the modifiable behaviors and changed their habits to improve the sleep quality.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "Introduction: Nowadays the huge amount of interrelated data and services available in the Internet is overwhelming: A human user cannot cope with the entire knowledge of the Internet, and finding and integrating the required information is extremely difficult.\nIntroduction: Therefore the next generation Internet has to change from a document-based information space to an intelligent advisory system. The usage of advisory systems should enable typical consulting situations, where a client with a certain problem consults domain experts to get some advice. The advisors give their advice on base of a profound personal knowledge and request publicly available information sources, if necessary.\nIntroduction: In this paper we present a software architecture of an advisory system incorporating information sources in the Internet. In our approach we apply two different technologies: the Semantic Web and multi-agent systems.\nIntroduction: In the Semantic Web domain data and its structure can be represented in a well-defined and machine interpretable way. For this purpose, the ontology language OWL (Web Ontology Language) [2] is defined.\nIntroduction: The software architecture of an advisory system should reflect the situation of a real counseling interview, where clients and advisors cooperate to find a solution for the client's problem. Multi-agent systems [3] provide a software paradigm, which is very close to this situation. The advisory system can by viewed in terms of autonomous agents of different types: client agents and advisor agents, which behave intelligent and interact to resolve a problem.\nIntroduction: To prove the usefulness of our approach we developed an advisory system in the Semantic E-learning Agent (SEA) project [4] supporting students who intend to study abroad within the European Erasmus exchange program. The advisory system assists choosing a foreign university and a study program that matches best the student interests and his/her individual situation of study.\nIntroduction: The paper is organized as follows: In the next section we describe the characteristics of consultancy situations and outline an agent-based software architecture of advisory systems. The third section shows how to model domain knowledge with ontologies and how to access it in the Semantic Web. Afterwards, the agent behavior is presented. Finally, we summarize the most significant features and provide an outlook to future research.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION AND MOTIVATION: Whether we are deciding between a salad or fast food for lunch or asking a coworker to complete a project ahead of schedule in exchange for help at a later date, people negotiate every day. When we hold conflicting interests, we must negotiate to pursue our ultimate goals. Thus, negotiation is an act wherein participants with unique motives to cooperate and compete to maximize their own benefits. Virtual agents are a powerful tool for teaching negotiation skills and modeling negotiation in an agent shows great promise in a variety of domains [18] . Indeed, many current empirical studies are making progress in this area [14, 19, 23, 31] .\nINTRODUCTION AND MOTIVATION: Drawing on this work, we recognize that negotiation is both a challenging reasoning problem as well as a linguistic problem. Although people are adept at navigating the trade-off between cooperation and competition, algorithms have yet to develop such reasoning and linguistic fluency. Therefore adversarial agents, such as AlphaGo, which aim to beat interlocutors in a zero-sum game, cannot work because they privilege competition, not cooperation. Similarly, a personal assistant like Alexa or Siri, which privileges cooperation, overlooks competition. Further, most studies regard negotiation as a sole reasoning or planning problem, like searching for optimized outcomes and thus aim to sharpen the agent's tactics. However, negotiation also relies on appropriate language to maintain relationships and optimize a plan. Prior research in human-human negotiation [22, 32] has shown that social factors such as trust and rapport underlie both challenges. In this study, we leverage different linguistic devices to build rapport between a human and a dialogue system, which thus fosters integrative agreement during negotiation.\nINTRODUCTION AND MOTIVATION: Negotiators are encouraged to share crucial information and cooperate to reduce the risk of impasse because of a sense of rapport, a feeling of connection and closeness with another [6, 32] . Also, Curhan et al., [8] shows that rapport helps to formalize a negotiator's intuition about objective outcomes and predict future objective value. Social scientist Spencer-Oatey [36, 37] explains the experience of establishing rapport according to three interrelating components: face, social rights and interactional goals. People use a variety of strategies to manage these three factors, which have been categorized into five major domains [37] : illocutionary domain, discourse domain, participation domain, stylistic domain and nonverbal domain. Most recent studies of rapport agents [17, 20] have investigated the importance of the non-verbal domain in humanagent interaction. However, we are interested in examining how the verbal channel produces rapport in the context of human-agent negotiation. Previously, Bronstein et al., [6] employed a hierarchical linear model to validate and quantify the contribution of the verbal channel in rapport management.\nINTRODUCTION AND MOTIVATION: In general, we believe that introducing social conversation into complex negotiation communication will help establish and maintain rapport while facilitating the negotiation process. To this end, we propose a two-phase computational model for negotiation: the task phase and the social phase. The task phase generates the next system task intention/move (e.g., to request a book). The social phase provides opportunities for social intentions/moves (e.g., selfdisclosing a personal preference) realized by different conversational strategies. Conversational strategies are units of discourse that are larger than speech acts, which have been demonstrated to contribute to building, maintaining or even destroying interpersonal (or human-agent) bonds [35, 42] . The social phase is inspired by the work presented in [2, 29] , who exploit structures of casual conversation in human-human communication to improve the system's capabilities of generating task-related social moves that are more than idle chit-chat. The major function of task-related social moves is to prepare the interlocutor for the next negotiation move\u2212rather than increase familiarity between interlocutors\u2212by discussing some restricted common topics like the weather. For instance, the system discloses its personal interest in reading as the current social move (e.g., \"I love reading\"), which anticipates its next negotiation move of requesting the book (e.g. \"Can I have the book?\").\nINTRODUCTION AND MOTIVATION: Our work does not seek to develop negotiation tactics but rather to use rapport management strategies to facilitate the negotiating process. Thus, we do not develop our own negotiation reasoning module but adopt and extend the end-to-end negotiation dialogue model from Facebook AI Research (FAIR) group as our task reasoning backbone [25] . FAIR has demonstrated the advancements of their neural dialogue models on the multi-issue bargaining task. Their task is to divide items from three item-types (books, hats, balls) between two participants. Since their neural dialogue model allows users to negotiate with the system through natural language, we are able to insert social moves into dialogues. Further, FAIR leveraged reinforcement learning to maximize the outcome of negotiation rather than imitating the actions of human users in supervised learning. Finally, FAIR replaced traditional likelihood decoding with a rollout algorithm, a kind of beam search conducted in the dialogue turn level, which allows the model to generate more effective negotiation strategies. Despite these advantages, FAIR suffers from low agreement with human users because of users' unwillingness to cooperate with such an uncompromising opponent. We believe that combining the negotiation tactics of the Facebook end-to-end model (FB) and our proposed rapport strategies will allow our social intelligent negotiation system (SOGO) to overcome these limitations and improve task performance while constructing an interpersonal bond with humans. To compare outputs from both systems, please refer to the sample dialogues in Table 1 .",
    "relatedWork": "RELATED WORK: Below we describe related work on building a negotiation agent and a social intelligent agent.\nNegotiation Agent: Modeling negotiation in an agent has become an increasing area of research. Different negotiation frameworks [13, 19, 30] have been established to serve as benchmarks for evaluating practical negotiation strategies. For instance, the generic negotiation framework Genius [19] facilitates research in bilateral multi-issue negotiation. Alternately, IAGO [30] allows a human to negotiate with a multimodal virtual agent, whose facial expressions and nonverbal cues are accessible to its negotiating partner. Unlike the single-agent Genius and IAGO frameworks, DipGame [13] is a multi-agent system developed on the Diplomacy Game, where negotiation and relationships between players are essential for success. These frameworks provide testbeds for researchers to investigate different negotiation tactics and social strategies in human-agent interaction.\nNegotiation Agent: Prior research indicates that modeling negotiation in virtual humans requires the system to demonstrate cognitive skills for reasoning and social skills for communication, as well as a means to express its internal state [16] . Gratch and his colleagues introduced a Conflict Resolution Agent (CRA) that allows students to engage with virtual human role-players across multi-issue bargaining problems. They showed that participants perceived the same levels of satisfaction, cooperation and rapport when paired with a CRA as with a human. Furthermore, they confirmed that the CRA could help people improve their negotiation skills. To build on these promising findings, subsequent research has investigated ways to design an agent with better communication and negotiation skills. For instance, DeVault et al., [9] employed a wizard-controlled system to improve turn-taking skills, and Lucas et al., [26] investigated deceptive strategies to optimize one party's benefit while preserving the illusion of fairness towards the other party to improve negotiation policy. The analysis demonstrated that deceptive strategies increase feelings of satisfaction, trust and fairness of the offer from one's partner, which seems to be more credible and even preferred to honesty. Meanwhile, this work also ameliorates an agent's capability to understand human behavior by identifying multimodal signals of trustworthiness in human-human negotiation (i.e., different sets of behavior indicators for predicting objective trustworthiness and perceived trustworthiness). This finding will enable the system to infer human negotiation behaviors and display specific behavioral signals on demand to manipulate trustworthiness. Additionally, Johnson et al., [21] developed an autonomous agent that teaches people to negotiate through visualized feedbacks. Basically, the researchers encoded theoretical negotiation principles into several automatic quantifiable metrics that have been validated in pedagogical negotiations.\nNegotiation Agent: Current researchers have moved away from a traditional expert system to a reinforcement learning branch to design the algorithm for a negotiation agent. This new method promises to build a fullyautomatic agent. The work on which our task phase is based [25] introduces an end-to-end model that employs a reinforcement learning algorithm and that was shown to have learned both linguistic and reasoning skills without human annotations. However, this model is perceived to be too uncompromising and aggressive. Our work addresses this problem by building interpersonal rapport through conversational strategies, which could increase the human user's tolerance to one's agent partner. Alternately, Sun et al., [38] suggests designing a benevolent agent based on a rewardshaping method that diminishes rewards that make the agent feel less satisfied for consecutive rewards. We plan to integrate this method into our system in future work. In addition to the rewardshaping method, Lerer et al., [24] tried to solve social dilemmas by exploiting the tit-for-tat (TFT) strategy. Based on the definition of Markove TFT, the researchers utilized a reinforcement learning method to approximate the cooperative and defect policies as well as the switching policy. This work encourages the agent to become more adaptive to solve social dilemmas in many environments.\nNegotiation Agent: Although promising, this prior research is limited by a focus on modeling negotiation in a single interaction; they do not leverage the collected knowledge across the interaction to facilitate the process. Also, these studies solely consider and optimize the negotiation outcome but ignore the social outcome and its contributions to the task. Furthermore, most negotiation agents communicate with users through pre-defined symbols, not natural language, which detracts from the naturalness of the interaction and removes social cues from language.\nSocial Intelligent Agent: Many prior studies have addressed the issue of rapport between human users and virtual agents. An early paper [2] implemented a Real Estate Agent to build trust with users while selling houses. Based on sociolinguistics and social psychology, its computational architecture blended small talk and task talk in conversation, and demonstrated that social dialogue affects trust for extroverted users. Elsewhere, Coon et al., [7] created an algorithm to plan appropriate activities based on the user-agent relationship, ranging from stranger to companion. Each activity had its own required closeness level, and an activity planner optimized its activities to increase intimacy over time. Similarly, Bickmore et al., [3] categorized relationships into four intimacy levels and defined a set of dialogue acts for each level. Inspired by accommodation theory, the researchers modeled long-term user-agent relationships in the context of health counseling. Their proposed reactive algorithm selects the most appropriate dialogue act for the assessed intimacy level yet fails to increase intimacy. Gratch et al., [17] offers an alternative way of building rapport. They developed a sensitive listener agent that generates listening feedback behaviors to effectively coordinate with human users. In subsequent studies, Huang et al., [20] proposed a next generation rapport agent capable of enhancing mutual attention by predicting backchannels, increasing coordination by generating smooth turn-taking behaviors and maintaining positive communication through affective response and facial expressions. In addition to the nonverbal channels, Matasuyama et al., [28] also proposed several verbal strategies to facilitate rapport. They presented a socially-aware animated intelligent personal assistant that analyzes users' multimodal behaviors to estimate rapport levels, reasoning and generating the conversational strategy instantiated by a set of observable verbal and nonverbal actions to build rapport.\nSocial Intelligent Agent: Similar to research in negotiation agents, these work do not model the user across interactions and use the shared knowledge to build interpersonal bonds.",
    "rq": " agent: why are you so good at this?"
}
{
    "intro": "",
    "relatedWork": "RELATED WORK: Although research showed that longer queries could produce better results for information seeking tasks, e.g. [6, 7] , people usually tend to use short search queries [3] . There are many approaches to support users in finding relevant information, e.g. through facets, recommendations, implicit and explicit user feedback. However, only few works have tried to motivate users to type in more query terms and thus provide more detailed information about their initial information need. Belkin at al., for example, showed that a queryentry box with several lines led to longer queries than a line mode search bar [6] and that query lengths were significantly longer when the query box was labelled with \"Information problem description (the more you say, the better the results are likely to be)\" than when it was labelled with \"Query terms\" [7] . Furthermore, they found that longer queries significantly increase searchers' satisfaction [7] . In contrast, Agapie et al. [1] found that telling users that longer queries deliver better search results does not influence query length. However, they showed that using a coloured halo around the search bar motivates searchers to provide significantly more query terms in a complex Web search scenario. Hiemstra et al. [10] evaluated the proposed halo effect in a website search system in a 50-day A/B test (N = 3506) but could not confirm the positive impact on query length. They conclude that this approach might be sensitive to the search task and search context. Kelly and Fu [12] show that additional information (domain knowledge, the information need, and search motivation) help to increase the retrieval performance. Likewise, Bendersky, Croft and Bruce [8] propose a machine learning method to extract the key facts from long queries. Their system performs better on longer natural language queries as compared to shorter, keyword-like queries.\nRELATED WORK: A reliable information need elicitation is getting more critical with the increasing use of voice assistant systems. Without a graphical user interface, refining the search via facets and exploring the results and recommendation lists becomes cumbersome. Research in the context of conversational search has explored asking clarifying questions [2] or coached conversational preference elicitation [15] . With one good question, Aliannejadi et al. [2] improved the retrieval performance by over 150%. Focusing on conversations, however, requires processing natural language (with challenges such as vague language and ambiguity [4] ), which is, so far, not supported by common product search engines.",
    "rq": " rq 2: are users more inclined to use natural language when interacting with more conversation-like interfaces?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": " (1) how does the designer utilize protochat to design a goal-oriented conversation?\n and (2) what kind of support is needed for the iterative design of conversation?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Information on a patient's medical history is essential for the diagnosis and therapy decision process. In the first appointment with a physician, he or she is asking the patient many questions to get to know the medical history of the person. The accuracy of this information affects significantly the quality of the diagnostic process [1] . One of the main causes of misdiagnoses is an incomplete medical history [2] . Beyond, studies show that certain health problems remain unrecognized in the patient-doctor conversation. For example, 50% of psychosocial and psychiatric problems are not recorded in the anamnesis [3] . 54% of problems and 45% of concerns are neither reported by the patient nor revealed by the attending physician [4] . A digital medical history entered by a patient himself can improve this situation and helps avoiding unnecessary duplication of data, as long as the medical history data is available for the different actors in the health care process. Self-anamnesis is a procedure in which the patient answers questions about the personal medical history without direct interaction with a doctor or medical assistant. A digital representation of the anamnesis enables a structured and thus more complete recording of the information [5] . In the course of the implementation of the eHealth Act in Switzerland and in other countries, a digital medical history can be stored in future in the electronic patient record, which avoids unnecessary duplication of data.\nINTRODUCTION: Several means exist already for fetching the medical history of a patient in a digital manner by means of electronic questionnaires or tools like AnaBoard (http://anaboard.de ). It has been proven that such electronic gathering helps to improve the correctness and completeness of the data compared to the traditional paperbased anamnesis survey [6, 7] . The data quality of a digital acquisition is therefore potentially better than the traditional data collection process. In existing approaches, however, it turned out to be a problem to motivate patients to answer an often comprehensive catalogue of medical history questions [8] . In this paper, we want to address this challenge by exploiting a dialog-oriented user interface for a mobile application that offers the possibility to interact with the user already during the medical history survey. More specifically, the main research question for this work is: Can a self-anamnesis be realized as a mobile application with conversational user interface? As specific use case, we have chosen the field of music therapy. Music therapy uses specifically selected music in the therapeutic process to promote, maintain and restore mental, physical and mental health. A complete, systematic history of experiences with music and of the music taste of a patient to be treated plays an important role in therapeutic process [9, 10] . The therapy plan and in particular music selection is made based on the music biography that is collected in the first meetings with the patient. The aim of application is not to replace the therapist or physician and the first consultation, but to provide a better and higher quality information base for the first consultation with the therapist or physician as well as for the following decision making process.",
    "relatedWork": "RELATED WORK: A conversational user interface or chatbot system is a computer program that interacts with users using natural language. Different terms are used for a chatbot such as machine conversation system, virtual agent, dialogue system, conversational user interface and chatterbot. The purpose of a chatbot system is to simulate a human conversation. Chatbots are usually text-driven, with images and unified widgets, which make it easy to start interacting with a bot. There are two types of chatbots: Unintelligent chatbots interact using a predefined conversation flow. Intelligent chatbots use machine learning to automatically generate responses on the fly. Already in 1966, Weizenbaum presented ELIZA [11] , a program that allows a natural language conversation with a computer. ELIZA stored its knowledge directly into the application code. Later, the design language Artificial Intelligence Markup Language (AIML) was used to manage the knowledge-based data (\"chatbot brain\") [12] . Chatbots have been reported in the literature for health related applications for example for student education, patient advice, information access, to achieve health behaviour change [13] or disease self-management. Lokman and Zain [14] introduced a chatbot that serves as a virtual dietitian for diabetic patients. The chatbot asks questions and gives at the end a diet advice suitable for the current diabetic situation. The conversation is going along a path that is remembered by the system to consider all answers in the decision-making. VPbot, an SQL-based chatbot for medical applications is a chatbot that takes advantage of relational database model design to store, manage and use the SQL language to perform the matching process within the chatbot conversation. eMMA, the electronic medication assistant, is a mobile application developed to support a patient in the medication management. The app is equipped with a chatbot for providing information on interactions between food and medications and to collect compliance data [15] . Several chatbots have been implemented in the health domain and are available in the app stores even though no scientific publication is available. The existing systems provide a triage, search functionalities or support to a certain extent disease management or conversation with doctors (e.g. Ada, https://ada.com/). Babylon Health (https://www.babylonhealth.com/) provides a triage function via a conversational user interface that is intended to supplement or replace the telemedicine hotline in the English health care system. So far, there is no application with conversational user interface available that is fully integrated in the treatment process such as a self-anamnesis chatbot. Our self-anamnesis concept aims at improving the data collection at the beginning of the diagnosis and treatment process. In contrast to existing systems, our concept foresees the integration of the collected data into the treatment process.",
    "rq": " the main research question for this work is: can a self-anamnesis be realized as a mobile application with conversational user interface?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Computers are increasingly used in roles that are typically fulfilled by humans, such as virtual tutors in a learning class or virtual assistants for task realization. When computers are used in these roles, they are often embodied by animated cartoon or human-like virtual characters, called Embodied Conversational Agents (ECA) [Cassell The reviewing of this article was managed by associate editor Lexing Xie. 2000]. This enables a more natural style of communication for the human and allows the computer to avail of both verbal and nonverbal behavior channels of communication. Several studies have demonstrated the acceptance and the efficiency of such agents [Burgoon et al. 2016; Dehn and van Mulken 2000; Kr \u00e4mer 2008] ; indeed, the persona effect reveals that the presence of an ECA improves the experience of an interaction for the user (e.g., Mayer and DaPra [2012] and Pardo et al. [2009] ). Moreover, when people interact with such virtual agents, they tend to react naturally and socially as they would with another person [Wang et al. 2005; Nowak and Biocca 2003; Kr \u00e4mer 2008] .\nINTRODUCTION: These kinds of human-machine interaction are facilitated by endowing ECAs with social capabilities. A particularly important social cue during social interactions is the smile [Knapp et al. 2013] . Smiling is an important social signal in negotiating interpersonal relationships. Smiling individuals are seen as more relaxed, kind, warm, attractive, successful, sociable, polite, happy, honest, having more of a sense of humor, and being less dominant [Bernstein et al. 2010; Ketelaar et al. 2012; Otta et al. 1996; O'Doherty et al. 2003 ]. Smiling-often in combination with laughter-plays an important role in establishing an affiliation between conversational partners and establishing social bonds that engage a polite interpersonal environment; this is likely to make an interlocutor more tolerant, more likely to pursue a smooth nonaggressive interaction, and more prepared to repair a conversation when it breaks down [Glenn 2003 ]. For these reasons, smiling is a worthwhile addition to the repertoire of social signals available to an ECA. However, there are also risks; smiling or laughing at the wrong time or in an inappropriate social context could violate social norms and have a negative effect on an interaction [McKeown et al. 2015] .\nINTRODUCTION: Subtle characteristics of a smile facial expression can influence the way in which the smile's meaning is interpreted. For instance, a smile may communicate amusement, politeness, or embarrassment depending on which facial muscles are activated. People consciously or unconsciously display these different smiles during an interaction and are able to distinguish them when they are expressed by their interlocutor [Frank et al. 1993 ]. The smile social signal is \"profoundly influential\" [Knapp et al. 2013 ]; a smile expression can have a strong impact on how an individual is perceived by another person. In particular, smiling behavior, that is, when a smile occurs and which type of smile is expressed during an interaction, may determine the perceived interpersonal stances [Deutsch et al. 1987; Edinger and Patterson 1983; Lau 1982; Moore 1985; Reis et al. 1990; Bernstein et al. 2010; Ketelaar et al. 2012; Otta et al. 1996; O'Doherty et al. 2003 ]. An interpersonal stance corresponds to an attitude, spontaneously or strategically expressed, that conveys the relationship of a person to the interlocutor (e.g., \"warm\" or \"polite\") [Kielsing 2009; Scherer 2005 ]. The effect of any given smile behavior might vary from positive to negative depending on the facial expression characteristics of smile, when it is expressed, and in response to what-the social context. The social context corresponds to the environment that the agent inhabits, with different factors influencing its behavior: the situational context (e.g., social occasion), social role, cultural conventions, and social norms [Riek and Robinson 2011] .\nINTRODUCTION: In order to improve the social capabilities of ECAs, this article proposes a computational model designed to enable embodied agents to select and display appropriate smiling behavior during a human-machine interaction. To create such smiling agents, two major issues must be addressed: agents need to be able to produce appropriate smiles, and they must know when in a conversation it is appropriate to produce them. To address the first issue-endowing an agent with the capability to express different types of smile-the key morphological and dynamic characteristics relevant to smiling need to be identified in order to adequately animate the agent's face (Section 2.1). For this purpose, we have constructed a lexicon of smiling facial expressions containing different types of smiles that convey different meanings, such as amusement or politeness (Section 4). The second issue-enabling an ECA to select its smiles depending on the potential effect of its expressions on the way users perceive its interpersonal stances-is more complex. Such an agent should be able to decide when and which smiles to express given a stance that it wants to convey to a user (Section 2.2). To address this issue, we have constructed a model to automatically compute, in real-time during an interaction, the effects of different smiling behaviors on how the user will perceive the ECA's interpersonal stances (Section 5).\nINTRODUCTION: A user-perception methodology-combined with machine-learning techniques-was used to create the ECA's computational model of smiling behaviors. The lexicon of smiles and the model that infers the effects of the ECA's smiles on the perception of stances are derived from user perceptions. More precisely, potential users actively participated in the creation of the computational models of smiling behaviors, both to define the characteristics of different smile types and to estimate the effects of smiling behavior on the perceived interpersonal stances. Machine-learning techniques were used to extract the information from the data collected from users.\nINTRODUCTION: The article is organized in 4 sections. Section 2 reviews the literature on smiles with particular respect to the morphological and dynamic characteristics of human and virtual agent smiles (Section 2.1) and the effects of smiling people or virtual agents on other people's perceptions (Section 2.2). Section 3 addresses the issues concerning the integration of a computational model of smiles into an ECA. In Section 4, we detail the creation of the lexicon of facial expressions conveying different types of smiles. Section 5 is dedicated to the model of the effects of the smiling behavior of ECAs on the user perceptions of interpersonal stances. In Section 6, we discuss the limits of the presented work and the next steps.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Many people have difficulties with social interactions such as presentations and job interviews [American Psychiatric Association 2013] . Persistent social skill deficits impede those afflicted with them from forming relationships or succeeding in social situations. Social skills refer to a variety of skills that are crucial to our everyday life and healthy development [Adi et al. 2007; Eisenberg et al. 2006 ] (e.g., emotional, interpersonal, and communication skills).\nINTRODUCTION: Social skills training (SST) is a general cognitive behavioral therapy to train these social skills for people who have difficulties in social interaction and is widely used by teachers, therapists, and trainers [Bauminger 2002 ]. However, SST requires welltrained teachers, so the number of participants joining an SST program is restricted and applications are competitive.\nINTRODUCTION: On the other hand, if part or all of the SST process could be automated, it would become easier for those requiring SST to receive it anywhere and at any time. In addition, Donna Williams, who has an autism spectrum disorder (ASD), a severe case of social difficulties, wrote a book entitled Nobody Nowhere (1992) , in which she stated:\nINTRODUCTION: The comprehension of words works as a progression, depending on the amount of stress caused from fear and the stress of relating directly. At best, words are understood with meaning, as with the indirect teaching of facts by a teacher or, better still, a record, television, or book. In my first three years in the special class at primary school, the teacher often left the room and the pupils responded to the lessons broadcast through an overhead speaker. I remember responding to it without the distraction of coping with the teacher. In this sense, computers would probably be beneficial for autistic children once they had the skills to use one. (p.208) Thus, a large number of training methods using computers or other forms of technology have been proposed [Bishop 2003; Moore et al. 2000; Parsons and Mitchell 2002; Schuller et al. 2014 ]. However, most of these do not follow the conventional training framework used by human instructors. Slov \u00e1k et al. noted that understanding how humans learn social skills, and how this process can be supported by technology, is an important but underresearched area in human-computer interaction (HCI) ]. These works also summarized existing approaches to social and emotional skills learning (SEL) in a large number of evidence-based programs and address the gap between human-based training and existing computer-based training methods, noting that much of the existing HCI work has not, so far, been connected to SST [Slov \u00e1k and Fitzpatrick 2015] .\nINTRODUCTION: In this article, we make some first steps toward closing this gap by proposing a novel tool that tries to replicate conventional SST using a systematic and computerbased design. We develop a dialogue system named automated social skills trainer, which is an application including video modeling of human behavior and real-time behavior detection as well as data visualization to help people improve their social skills (Figure 1 ). We investigate whether it is possible to help people who have difficulties in social interaction improve their social skills using an automated system that can be used anywhere and at any time.\nINTRODUCTION: The main contribution of this work is that it is the first attempt to strictly follow the well-known and well-tested regimen of SST in the HCI context. To achieve this goal, we make several technical contributions: -Creating a new dialogue system following the conventional SST framework -Sensing new behavioral parameters that relate to the social skills, such as language and speech features -Providing a new feedback scheme not only with summary visualization but also with quantified objective values and generated positive comments.\nINTRODUCTION: To validate the utility of the method, we perform experiments with up to 38 human participants. The experimental results show that the type of interaction is related to the users' subjective impression of nervousness, ease of talking, and ability to talk well; that social skill is related to automatically extracted features and has a relationship to autistic traits measured by standardized tests; and that participants improve in social skill using the automated social skills trainer.\nINTRODUCTION: This article is an extended version of a conference paper [Tanaka et al. 2015 ]. Major extensions are the first experiment (regarding subjective impressions regarding interaction with an avatar or a human), detailed analysis of feature differences, and measuring interrater agreement in the second experiment. In Section 2, we describe related works of this field. In Section 3, we explain each module of conventional humanbased SST. In Section 4, we explain the correspondence between human-based SST and the automated social skills trainer. In Section 5, we describe implementation details of the automated social skills trainer focusing on role-playing and feedback. In Sections 6 through 8, we perform three experiments to evaluate the system. In Section 9, we summarize and discuss future directions.",
    "relatedWork": "RELATED WORK: The design of an automated social skills trainer brings together several fields, including research into computers in education, intelligent virtual agents, and affective computing. The following paragraphs briefly outline work in these areas.\nRELATED WORK: The use of computers in SST is motivated by the fact that although individuals with social impairments have difficulty in social interaction, they also show good and sometimes even superior skills in \"systemizing\" [Baron-Cohen et al. 2003] . Systemizing is the drive to analyze or build systems and to understand and predict behavior in terms of underlying rules and regularities. The use of systematic computer-based training for people who need to train social skills can take advantage of the facts that (1) they favor computerized environments because they are predictable, consistent, and free from social demands; (2) they can work at their own speed and level of understanding; (3) training can be repeated over and over again until the goal is achieved; and (4) interest and motivation can be maintained through computerized rewards [Bishop 2003; Moore et al. 2000; Parsons and Mitchell 2002; Schuller et al. 2014] .\nRELATED WORK: There has been one previous line of work on automated conversational coaches [Hoque and Picard 2014; Hoque et al. 2013] , which are dialogue systems aimed to train people for improving interview skills through real-time feature detection and feedback. They achieved (1) a realistic task involving training real users, (2) formative affective feedback that provides the user with useful feedback on the behaviors that need improvement, and (3) the interpretation or recognition of user utterances to drive the selection of backchannels or formative feedback. Although this work is an excellent first step, it did not faithfully follow the traditional SST framework.\nRELATED WORK: The previous work omitted steps such as (1) modeling of human behavior (e.g. Essau et al. [2014] ), (2) providing feedback compared to model speakers to objectively confirm the user's strengths and weaknesses, and (3) giving positive reinforcement to enhance the user's self-esteem [Bellack 2004 ]. These steps have clear goals and definitions in traditional human-provided SST [Bauminger 2002; Liberman and Wallace 1990; Wallace et al. 1980] , such as helping users to understand their current social communication skills and the path to their goals, maintaining positive motivation. It is our hope that by following this paradigm, the preceding effects can be reflected in automated SST. Thus, we attempt to follow the traditional SST framework as closely as possible.",
    "rq": " (2) is there a difference between talking to humans (hhi) and talking to avatars (hci) in terms of narrative?\n (4) are the extracted features effective for identifying narrative skill?\n overall narrative skill: how good overall was his/her narrative skill?\n speaking rate: what was his/her speaking speed?\n(1) how effective is the automated social skills trainer in helping users improve their narrative skills?"
}
{
    "intro": "INTRODUCTION: In many domains, including human-computer interaction (HCI) research [62] , conducting surveys is a key method to collect data. With the widespread use of the internet, self-administered 15:2 Z. Xiao et al. online surveys have replaced old-fashioned paper-and-pencil surveys and have become one of the most widely used methods to collect information from a target audience [27, 30] . Compared to paper-and-pencil surveys, online surveys offer several distinct advantages. First, an online survey is available 24x7 for a target audience to access and complete at their own pace. Second, it can reach a wide audience online regardless of their geographic locations. Third, online survey tools automatically tally survey results, which minimizes the effort and errors in processing the results.\nINTRODUCTION: Due to the extensive use of online surveys, survey fatigue is now a challenge faced by anyone who wishes to collect data. Research indicates two typical types of survey fatigue [69] . One is survey response fatigue. Since people are inundated with survey requests, they are unwilling to take any surveys [70] . The other is survey-taking fatigue. Evidence shows that as a survey grows in length, participants spend less time on each question and the completion rate also drops significantly [8, 71] . For example, one of the biggest survey platforms, SurveyMonkey, shows that on average, participants spend 5 minutes to complete a 10-question survey but 10 minutes to finish a 30-question survey. 1 The problem is exacerbated with open-ended questions because of the extra time and effort required for formulating and typing responses to such questions [8, 17] . Open-ended questions are an important method to collect valuable data and are widely used in self-administered online surveys [62] .\nINTRODUCTION: In particular, open-ended questions allow respondents to phrase their answers freely when the options of responses cannot be pre-defined or the pre-defined responses may introduce biases [17, 47] . Moreover, open-ended questions help collect deeper insights, such as the background and rationales behind the answers [17, 80] . However, open-ended questions often induce more cognitive burdens and survey-taking fatigue, and respondents are more likely to skip such questions or provide low-quality or even irrelevant answers [8, 17, 73] . Consequently, surveytaking fatigue adversely affects the quality and reliability of the data collected especially when open-ended questions are involved [17, 18, 64] .\nINTRODUCTION: To combat survey taking fatigue especially to motivate and guide survey participants to provide quality answers to open-ended questions, several approaches have been proposed. One set of proposals is to inject interactive features into an online static survey, such as providing response feedback [18] and probing responses [64] , to improve response quality and encourage participant engagement. However, no existing survey platforms support such interactive features nor do they automatically motivate and guide survey participants to provide quality answers to open-ended questions during a survey.\nINTRODUCTION: A lack of support of such interaction features on existing platforms may be due to two main reasons. First, it is difficult to automatically interpret participants' natural language responses to an open-ended question due to the diversity and complexity of such responses [17] . For example, when asked \"What do you think of the product,\" participants' responses could be: \"N/A,\" \"I don't know,\" or \"Although I've heard of the product, I've never used it so I don't know what to say.\" Interpreting such highly diverse or complex free-text input requires sophisticated natural language processing algorithms, which is a non-trivial task [34] . Second, even if a system can interpret participants' free-text responses to open-ended questions, it is difficult to manage potentially complex interactions based on participant responses. Using the above example, a participant may be unwilling to answer the open-ended question and may even provide a gibberish response such as \"afasf asfasf afyiasfaf asf\" [31] . In another example, a participant is willing to answer the question, but provides a very terse answer such as \"not bad\" as opposed to detailed, rich information. Yet in another example, instead of answering the question, a participant asks a clarification question \"Which aspects of the product do you want me to comment on.\" Handling all these situations Using an AI-Powered Chatbot to Conduct Conversational Surveys 15:3 or their compositions requires that a system not only understands a participant's input but also automatically handles diverse interaction situations, which is very challenging to implement [34] .\nINTRODUCTION: On the other hand, the advent of chatbots with their increasingly more powerful conversational capabilities can offer an alternative approach to static online surveys. Specifically, an artificial intelligence (AI)-powered chatbot can conduct a conversational survey. As shown in Figure 1 , in a conversational survey, a chatbot asks open-ended questions, probes answers, and handles social dialogues.\nINTRODUCTION: Intuitively, a chatbot-powered conversational survey retains the advantages of online surveys and offers several additional benefits especially facilitating gathering participant responses to open-ended questions. First, a chatbot can frame survey questions in more personalized, conversational messages, which might help improve participant engagement and response quality [17, 36, 46] . Second, the perceived anthropomorphic characteristics of a chatbot can potentially deliver human-like social interactions that encourage survey participants to reveal personal insights [82] . Third, it is natural for a chatbot to interactively encourage information exchange in the course of a survey, such as providing response feedback and probing responses, which in turn helps reduce survey-taking fatigue and improve response quality. Moreover, it is the inherent functions of chatbots that interpret diverse user natural language input and handle complex conversations. As a result, chatbots can potentially serve as a moderator and proactively manage a survey process, such as dealing with \"uncooperative\" participants, clarifying the meaning of a question per a participant's request, and guiding a participant to provide richer and more authentic responses [50, 81] .\nINTRODUCTION: Despite their benefits, chatbots bear several risks for their use in conducting surveys. First, a turn-by-turn chat requires participants to take extra time and effort to complete a survey. It is unclear whether people would be willing to take the time to chat and complete a survey, let alone providing quality responses. The risk is even higher for surveys with paid participants, who would not be rewarded for taking a longer survey. Second, current chatbots are far from perfect and their limited conversation capabilities may lead to user disappointment and frustration [34] . It is unknown whether the limited capabilities would deter participants from offering quality responses or completing a survey. Moreover, it is difficult for a chatbot to accurately interpret and properly respond to humans' diverse free-text input to open-ended questions [22] . Once participants realize that a chatbot cannot fully understand or assess their responses, it is unknown whether they would do mischief by intentionally feeding the chatbot with bogus responses, which would adversely affect the overall response quality. Finally, the use of a personified conversational system may lead to user behaviors that affect survey quality. For example, studies show that people have positivity bias when giving opinions to an agent [84] , producing potentially biased survey results.\nINTRODUCTION: To our knowledge, there have not been any in-depth studies examining the effectiveness and limitations of AI-powered chatbot surveys in contrast to typical online surveys. We, therefore, ask two research questions:\nINTRODUCTION: \u2022 RQ1: How would user response quality differ, especially the quality of user free-text responses to open-ended questions in an AI-powered chatbot-driven survey vs. a traditional online survey? \u2022 RQ2: How would participant engagement differ in an AI-powered chatbot-driven survey vs. a traditional online survey?\nINTRODUCTION: To answer the above research questions, we designed and conducted a field study that compared the use of an AI-powered chatbot vs. a typical online survey with the focus on eliciting user answers to open-ended questions. As mentioned above, there are potential benefits and risks of using chatbots to conduct surveys, especially when involving open-ended questions. However, none of the benefits or risks have been examined. In this first study, we thus decided to focus on examining the holistic effect of a chatbot instead of investigating the effect of separate chatbot features.\nINTRODUCTION: Additionally, to ensure that our study is based on real-world survey practices and offers practical value, we collaborated with a global-leading market research firm that specializes in discovering customer insights for the game and entertainment industry. Per the firm's request, our field study was to learn how gamers think and feel about two newly released game trailers. The study involved about 600 gamers, half of whom took a chatbot survey and the other half filled out a typical online survey. Through detailed analyses of over 5,000 collected responses, we addressed our two research questions. We also discussed the design implications for creating effective chatbots to conduct engaging surveys and beyond.\nINTRODUCTION: To the best of our knowledge, our work is the first that systematically compared the holistic effect of an AI-powered conversational survey with that of a typical online survey on response quality and participant engagement. As a result, our work reported here provides three unique contributions.\nINTRODUCTION: (",
    "relatedWork": "RELATED WORK: Broadly, our work is related to research in six areas as detailed below.\nConversational AI and Chatbots: Our work is related to a rich body of studies on the applications of conversational AI or chatbots. For example, past studies have examined chatbots as a personal assistant [51] , intelligent tutor [32] , customer service agent [2, 95] , job interviewer [50] , and worker's companion [93] .\nConversational AI and Chatbots: The HCI community has long been interested in the interaction benefits offered by conversational interfaces. The general consensus is that conversational interfaces offer several advantages over traditional Windows, Icons, Menus, and Pointers (WIMP) interfaces [14, 56] . First, conversational interfaces offer a natural and familiar way for users to express themselves, which in turn improves the usability of a system. Second, such interfaces are flexible and can accommodate diverse user requests without requiring users to follow a fixed path [85] . Third, such interfaces are often personified and their anthropomorphic features could help attract user attention and gain user trust [90] .\nConversational AI and Chatbots: Inspired by the potential advantages of conversational interfaces over WIMP-based user interfaces, our work investigates the use of conversational interfaces for conducting surveys. Differing from existing works on conversational interfaces, we are exploring a new application of conversational AI for conducting surveys, which has its own set of unique challenges. For example, survey participants would not be as motivated or cooperative as job candidates who interact with a conversational AI agent [22, 50] . Neither would survey participants be as tolerant or patient as a student or an employee who interacts with an AI companion [93, 94] .\nConversational AI and Chatbots: Furthermore, conversational interfaces are far from perfect due to technical difficulties in processing user natural language expressions and managing diverse and complex conversation situations [34, 85] . It is thus unknown how the shortcomings of conversational interfaces (e.g., failure to understand a user's input during a survey) might influence survey participants and survey results. Therefore, we hope to investigate whether and how conversational interfaces might bring in practical values to traditional survey practices, which use WIMP-based interfaces to elicit information. As the first step, we compare the holistic effect of a chatbot survey with that of a traditional online survey on survey participants and survey results in a real-world setting.\nConversational AI for Information Elicitation: Our work is directly relevant to the use of conversational AI for information elicitation. Researchers have built various AI agents to elicit information from a user through a one-on-one, text-based conversation, such as eliciting information from a job candidate [50] and gathering data from a study participant [82] . Information elicitation may serve various purposes. A common task is to elicit \"parameters\" for information retrieval or recommendation [72, 74, 86] . This kind of system often supports task-oriented conversations, such as helping a customer book a flight reservation, finding a desired restaurant, and scheduling an event [19, 37, 58] . The main goal of such systems is for an agent to gather required information (e.g., travel dates) to perform a task [13] .\nConversational AI for Information Elicitation: More recently, conversational AI has been explored as a means to elicit information for additional purposes beyond fulfilling a specific task. For example, there have been agents that elicit information for recommending products, movies, and jobs [43, 96] , group decision support [79] , psychotherapy [77, 93] , and voting [28] . An emerging area is using conversational AI to conduct in-depth interviews [22, 50] or longitudinal studies in the real world [82, 93] . For example, Li et al. [50] deployed agents to interview a pool of real job candidates and compared the effect of two agent personalities on the candidates' behavior. Tallyn et al. [82] developed a chatbot to gather ethnographic data from participants in real-time. They showed that a simple chatbot with little language understanding capabilities was effective in capturing data from the participants. In a more recent study, Xiao et al. [94] used a chatbot to interview university students and gather their thoughts and feelings about teaming.\nConversational AI for Information Elicitation: Different from our investigation of using a chatbot as a general surveying tool, prior studies tended to focus on examining the feasibility and effectiveness of a chatbot for a specific information elicitation task. For example, Xiao et al. studied the use of a chatbot for eliciting student preferences and opinions about team building and investigated whether and how the elicited information predicted team performance [94] . Li et al. built a chatbot to elicit information from job candidates and examined the candidates' trusting behavior with the chatbot [50] . However, unlike our work, none of the existing studies compared the effectiveness of a chatbot in information elicitation with that of a traditional approach. In particular, we want to quantitatively measure the holistic effect of chatbots on user engagement and response quality compared to that of a traditional online survey.\nConversational AI for Information Elicitation: Although a typical online survey can elicit information through various question prompts, including open-ended questions, such a survey is not interactive or adaptive in nature. For example, in such a process, a survey participant cannot ask a clarification question, neither can the system probe a user response. On the other hand, chatbots can naturally employ a diverse set of conversation prompts to elicit information interactively, such as question prompts [96] , follow-up probes [82] , and topic-specific discussions [72, 74] . In addition, conversation prompts can be context sensitive. For example, Williams et al. employed both emotion-centric prompts \"how do you feel\" and task-centric prompts \"what did you do\" to elicit rich responses from users about work experience to improve workplace productivity [93] .\nConversational AI for Information Elicitation: However, existing work has not examined how a chatbot's often imperfect conversation capabilities would affect information elicitation involving open-ended questions, including user response quality and satisfaction. A recent study showed that most chatbots can hardly understand user input and manage seemingly simple tasks such as scheduling a meeting [34] . This is because users' natural language expressions are highly diverse and challenging to interpret. Moreover, a seemingly simple conversation is often still complex and nonlinear (i.e., going back and forth with a user on a topic), which requires a chatbot to continously track and proactively manage a conversation context [34, 85] . Our study is thus set out to explore both the advantages and shortcomings of a chatbot in conducting surveys with open-ended questions and to compare its holistic effect against that of a traditional online survey to answer our two research questions.\nConversational AI and Information Disclosure: In the context of information elicitation, studies show that conversational AI may enhance user engagement and encourage self-disclosure. Prior work has demonstrated that the exhibited social behaviors of agents are effective in improving user engagements in various social settings by a set of metrics, such as interaction duration, breadth, and depth of self-disclosure (e.g., discussing personal subjects), and a positive attitude toward the agent and interaction outcome [11, 79, 81] .\nConversational AI and Information Disclosure: On the other hand, user's positive attitude toward AI agents has raised concerns on user privacy protection and encouraged studies on investigating the effect of chatbots on user trust and privacy in the context of information elicitation. For example, a recent study showed that users trusted a customer service chatbot more if they were well-informed in the conversation where the information was stored [28] . Sannon et al. found that users were less likely to share personal sensitive information (e.g., financial or health stress) with a personified chatbot [76] . However, none of the existing studies compared survey participants' behaviors (e.g., self-disclosure and answer quality) influenced by a chatbot vs. in a traditional online survey. We thus set out to gauge how conversational AI affects user engagement and the quality of survey results, hoping to discover new survey methods that may improve traditional online survey practices.\nEvaluating Conversational Interfaces: With the advent of conversational interfaces, evaluating the effectiveness of such interfaces is a continuously evolving research topic. Traditionally, the evaluation criteria have been divided into objective metrics on the system performance (e.g., user input interpretation accuracy and user task completion rate) and subjective metrics based on user feedback (e.g., user satisfaction) [59, 89] . Objective metrics are directly computed from logs of the interaction and can be based on task or domain coverage, error rate, number of interaction issues, accuracy or other metrics compared to \"ground truth\" [25, 53, 59] . Subjective metrics are often based on user opinions around certain aspects, such as satisfaction and intelligibility, (e.g., [40] ). There are also comprehensive methodologies that consider both subjective user satisfaction and objective performance metrics including task success, dialog efficiency (e.g., duration and total turns) and dialog quality (e.g., latency) [89] .\nEvaluating Conversational Interfaces: In addition to examining user satisfaction and usability of conversational AI agents, HCI researchers have investigated how agent behavior impacts users' social perceptions, such as trust [16] , rapport [10, 63] , anthropomorphism, and likability [5] . Such user subjective feedback is often measured by questionnaires, i.e., Likert-scale ratings on self-reported statements. Additionally, automatic methods have been developed to predict user satisfaction based on users' behavioral signals, such as dialogue acts, words, and user actions [41, 51] .\nEvaluating Conversational Interfaces: Compared to the existing work, our study focuses on evaluating the outcomes of a conversational interface with a target goal-collecting high-quality survey data and keeping the survey taker engaged. We, therefore, have proposed several content-based metrics to measure response quality and participant engagement.\nConversational Interfaces vs. Graphical User Interfaces: Our work is also related to evaluating the effect of a conversational interface vs. that of a graphical user interface (GUI). A recent study by YouGov compared the use of a traditional GUI form with a Facebook Messenger Bot for conducting a survey. 2 They found that the chatbot significantly improved response rate. More recently, researchers compared the response quality between a chatbot survey and a form-based survey in more depth [44] . They also found that the chatbot surveys produced more differentiated responses and the participants were less likely to satisfice. However, all the existing studies used only choice-based questions and have not examined how chatbot-driven surveys would impact user responses to open-ended questions, which has been one of the major challenges in typical online surveys [64] .\nConversational Interfaces vs. Graphical User Interfaces: In other task domains, researchers have explored how a conversational interface in lieu of a traditional GUI interface impact user behavior. One such area is conversational search [83, 86] . For example, Trippas et al. [86] show that verbal communications led to more complex user queries such as having multiple requests in one utterance, while the interactivity encouraged user collaborative behavior, such as actively requesting more specific information.\nConversational Interfaces vs. Graphical User Interfaces: Similar to this line of work, we compare the effect of using a conversational interface vs. a traditional GUI for conducting surveys. However, we focus on quantifying their effect on the quality of collected free-text survey responses and user engagement, which has never been addressed before.\nImproving Survey Quality: Our work is related to survey research on improving survey quality. Researchers have put tremendous effort into improving sample validity and response quality. Heerwegh and Loosveldt [36] find that personalization can significantly increase web survey response rate by 4.4% while not necessarily leading to social desirability response bias. Behr et al. [7] have tested three probing variants and found that such probings lead to a higher number of meaningful answers in web surveys. In a field experiment with over 4,000 participants, De Leeuw et al. [21] have shown that a polite probe can successfully reduce item non-response (\"don't know\") without sacrificing the reliability of the final answers. Conrad et al. [18] also show that interactive feedback can improve the quality of responses in web surveys. Additionally, Oudejans and Christian [64] propose to use explanations and probings to improve the quality of user responses to open-ended questions.\nImproving Survey Quality: On the one hand, our work is built on top of existing findings. For example, we learned that interactive features, such as response feedback and probings, help improve participation and response quality. On the other hand, our study is a natural extension of existing efforts. In particular, we explore the use of chatbots to offer various interactive features in a survey, hoping that such features would improve participant engagement and response quality.",
    "rq": " rq1: how would user response quality differ, especially the quality of user free-text responses to open-ended questions in an ai-powered chatbot-driven survey vs. a traditional online survey?\n rq2: how would participant engagement differ in an ai-powered chatbot-driven survey vs. a traditional online survey?\njuji: what do you think of the trailer you just watched?\nrq1: how would the quality of responses differ?\nrq2: how would a chatbot impact participant engagement?\njuji: what is your favorite type of games (e.g., first person shooter, multi-player, open world, strategy, sports/racing, battle royale, etc.)?"
}
{
    "intro": "INTRODUCTION: Advances in artificial intelligence and language processing raised the popularity of conversational user interfaces (\"chatbots\"). Chatbots are becoming more relevant [34] for interface design and may cover a great variety of tasks in the future, such as learning and education, civic participation, etc. [6, 13, 30, 43, 45] . One important scenario is customer service [15] . Digital agents are available 24/7 and can simultaneously handle many conversations, allowing businesses to raise revenue and strengthen customer relations [47] . However, due to technological limitations (such as insufficient intent recognition), many chatbots offer only a small set of functions and limited service quality (SQ) [17] , which makes them quickly reach their operational limits [33] . Although some of these so-called \"breakdowns\" could be repaired automatically [3] , in more complex cases a transition to a human is needed [15] .\nINTRODUCTION: Thus, chatbots must come with \"handover\" [44] capabilities to a human customer service representative (CSR) who continues taking care of inquiries in case the automated service fails [15, 27, 39, 44] (see Figure 2 ). Considering that humans use chatbots to solve problems quickly and efficiently [7] , missing or badly designed handovers eliminate their advantages. Consequently, handovers are a critical factor for perceived SQ [11] . Thereby, experienced qualities are not only about the objectively measurable waiting time but also about the waiting experience [31] . The \"psychology of waiting\" [31] has a long history in HCI and user experience (UX) design research.\nINTRODUCTION: However, past research mainly concentrated on graphical user interfaces (web/desktop) [4, 36] or interactive speech applications [14] . As chatbots show similarities with both domains, we believe it to be important to investigate the psychology of waiting in this new context as well. This is supported by F\u00f8lstad and Brandtzaeg [12] , who argue that knowledge obtained in graphical user interface research may not be transferable to the field of chatbot design. This paper addresses the topic of improving chatbot handover experiences. Therefore, we pose the following research question: RQ: How can we design optimized chatbot handovers in order to create a positive waiting (user) experience and increase the perceived service quality? Thus, this work contributes to the knowledge of conversational user interface design in customer service, which momentarily lacks knowledge [17] , but is essential as success in this regard is strongly influenced by first contact [2] . In particular, we (1) demonstrate the importance of designing transparent and engaging handover experiences, (2) combine these findings with knowledge from the Ava Chatbot I'll forward your request to a Customer Service Representative who will take care of it as soon as possible.\nINTRODUCTION: I can tell from the system that the package has not yet been delivered. There is a problem with the order in which I can not help unfortunately -sorry!",
    "relatedWork": "",
    "rq": " rq: how can we design optimized chatbot handovers in order to create a positive waiting (user) experience and increase the perceived service quality?"
}
{
    "intro": "Introduction: The development of social chatbots, or intelligent dialogue systems that are able to engage in empathetic conversations with humans, has been one of the longest running goals in artificial intelligence (AI). Early conversational systems, such as Eliza (Weizenbaum 1966) , Parry (Colby, Weber, and Hilf 1971) , and Alice (Wallace 2009) , were designed to mimic human behavior in a text-based conversation, hence to pass the Turing Test within a controlled scope. Despite impressive successes, these systems were mostly based on hand-crafted rules and worked well only in constrained environments. An open-domain social chatbot had remained an elusive goal until recently. Lately, we have been witnessing promising results in both the academic research community and industry as large volumes of conversational data become available, and breakthroughs in machine learning are applied to conversational AI. Recent surveys include Gao, Galley, and Li (2019) and Shum, He, and Li (2018) .\nIntroduction: In this article we present the design and implementation of Microsoft XiaoIce ('Little Ice' literally in Chinese), the most popular social chatbot in the world. Since her launch in China in May 2014, XiaoIce has attracted over 660 million active users (i.e., subscribed users). XiaoIce has already been shipped in five countries (China, Japan, US, India, and Indonesia) under different names (e.g., Rinna in Japan) on more than 40 platforms, including WeChat, QQ, Weibo, and Meipai in China; Facebook Messenger in the United States and India; and LINE in Japan and Indonesia.\nIntroduction: The primary design goal of XiaoIce is to be an AI companion with which users form long-term, emotional connections. Being able to establish such long-term relationships with human users as an open-domain social chatbot distinguishes XiaoIce from not only early social chatbots but also other recently developed conversational AI personal assistants such as Apple Siri, Amazon Alexa, Google Assistant, and Microsoft Cortana.\nIntroduction: Figure 1 shows how an emotional connection between a user and XiaoIce has been established over a 2-month period. When the user encountered the chatbot for the first time (Session 1), he explored the features and functions of XiaoIce in conversation. Then, in less than 2 weeks (Session 6), the user began to talk with XiaoIce about his hobbies and interests (a Japanese manga). By 4 weeks (Session 20), he began to treat XiaoIce as a friend and asked her questions related to his real life. After 7 weeks (Session 42), the user started to treat XiaoIce as a companion and talked to her almost every day. After 2 more weeks (Session 71), XiaoIce became his preferred choice whenever he needed someone to talk to.\nIntroduction: XiaoIce is developed on an empathetic computing framework (Cai 2006; Fung et al. 2016 ) that enables the machine (social chatbot in our case) to recognize human feelings and states, understand user intents, and respond to user needs dynamically. XiaoIce aims to pass a particular form of the Turing Test known as the time-sharing test, where machines and humans coexist in a companion system with a time-sharing schedule. If a person enjoys its companionship (via conversation), we can call the machine \"empathetic.\"\nIntroduction: In the remainder of the article we present the details of the design and implementation of XiaoIce. We start with the design principle and mathematical formulation. Then we show the system architecture and how we implement key components including dialog manager, core chat, important skills, and an empathetic computing module, presenting a separate evaluation of each component where appropriate. We will show how XiaoIce has been doing in five countries since its launch in May 2014, and conclude this article with a discussion of future directions.",
    "relatedWork": "Related Work: XiaoIce is designed as a modular system based on a hybrid AI engine that combines rulebased and data-driven approaches, as presented in Figure 4 and Section 4. By contrast, in the research community, there is a growing interest in developing fully data-driven, end-to-end (E2E) systems for social chatbot (chitchat) scenarios, as reviewed in Chapter 5 of Gao, Galley, and Li (2019) .\nRelated Work: The difference is mainly due to different design goals of social chatbots. Traditionally, social chatbots are designed for chitchat scenarios where the bots are expected to mimic human user conversations but not to interact with the user's environment. For such scenarios, E2E approaches often lead to a very simple system architecture, such as RNNbased systems (Shang, Lu, and Li 2015; Li et al. 2016b) , where the neural network-based response generation models can be easily trained on large-scale free-form, open-domain data sets (e.g., collected from social networks) to allow the bots to chat with users on any topics.\nRelated Work: XiaoIce, on the other hand, is designed as an AI companion that integrates both EQ and IQ skills that are needed to help users complete specific tasks. Thus, XiaoIce has to interact with the user's environment and access real-world knowledge (e.g., via API calls). Therefore, XiaoIce uses a modular architecture similar to task-oriented dialogue systems, with different modules dealing with different tasks. Depending on the availability of training data and knowledge bases for each individual task, either a rule-based method or a data-driven method, or a hybrid of both, is adopted for the task. For example, when asked \"what is the weather tomorrow?,\" E2E systems are likely to give a plausible but random response, such as \"sunny\" and \"rainy,\" due to the lack of grounding in realworld knowledge. 12 XiaoIce, however, generates a factual response based on the user's geographical location and the corresponding database, as shown in Figure 19(a) .",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Several publications have mentioned the importance of mentoring. One study, for example, states that \"a recent meta-analysis indicated that former protegees tend to be paid more, promoted more often, and are more positive about their careers than those who have never been mentored\" [2] . One enhancement to mentoring is the addition of virtual mentoring. In 2013, Kinnis Gosha conducted a between-group study with 37 African American male undergraduate computer science majors in which it was discovered that mentoring can be done using an embodied conversational agent. An embodied conversational agent (ECA) is an effective way to represent an intelligent conversation between humans and artificially intelligent software by conversing with humans using the same verbal and nonverbal means [3] . Although the study conducted by Gosha [6] showed positive results, it required that a user had to open up a standalone website to interact with the virtual mentor. A limitation with this approach is that a user would have to interact with a tool that they do not normally use on a mobile device, which would potentially cause the user to interact with the mentor less. One widely used phone application for most people is short message service (SMS), commonly known as \"text messaging\". The goal of this study is to compare the level of effectiveness between the original embodied conversational agent mentor and a new conversational agent mentor with an SMS interface. For this study, effectiveness will be measured through qualitative and quantitative methodologies and techniques. It is hypothesized that there will be no significant difference between the effectiveness of the mentorship provided by the SMS conversational agent, the standalone website, and a human mentor.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Conversational machines are being increasingly employed in physical spaces for both private and public usage. Examples include hotel lobbies and store showrooms [15] , car dashboards [24] , and home devices [40] . With such machines, or chatbots, human interactions may happen in the presence of an audience, be it friends, family (e.g., while on a road trip), or simply strangers and bystanders (e.g., in a hotel lobby).\nINTRODUCTION: Previous studies have found that humans tend to change their normal conversation behavior when in front of others. In such contexts, people sometimes resort to using long and complicated words, uttering jokes, quoting from obscure authors, and, in general, pretending to be smarter, wittier, or funnier than in private conversations [4, 35] . Also, when in the presence of others, some people may enhance the emission of dominant responses [8] , according to the status of those in the audience [8, 10] . However, some people react in the opposite way, becoming more shy than normal, failing to complete sentences, getting nervous, or even stuttering.\nINTRODUCTION: Understanding such changes in behavior are important because it may be necessary to design the machine conversation systems to handle those situations where people change their usual behavior to accommodate the presence of an audience. We were motivated to study this kind of behavior change by some initial observations we made of visitors experiencing an art exhibit where they interacted with a group of chatbots either alone or in front of acquaintances and/or strangers. For example, we observed some people trying to amuse their friends by trying to \"break\" the machine with impossible questions; asking questions related to local politics and sports to provoke the other visitors; and uttering deep and complicated questions to show off to others their knowledge about the subject of the artwork. In any of those cases, we found that the art exhibit could have been designed to better handle the presence of an audience. For example, the system could have a higher threshold of guessing the right answer to complex questions when an audience is present. It could, for instance, assume that a visitor asking a complex question in front of an audience is less a situation where she is looking for knowledge and more like the system is being made fun of. While in the former case the appropriate response could be trying to find a good answer as hard as it could, whereas in the group situation it could be simply deflecting the question.\nINTRODUCTION: Beyond art exhibits, as conversational systems become more ubiquitous, similar situations will be common in more down-to-earth scenarios. For instance, a conversation speaker (like Amazon's Echo or Google's Home) could benefit to adjust its behavior to handle audience effects. It could be less prune to making jokes to avoid making anyone in the audience uncomfortable, or, even worse, feeling ridiculed in front of acquaintances. In other words, by recognizing the audience context, the conversational system may be designed to answer in a more appropriate form for a situation of group social interaction, adapting to and enhancing the overall experience of users and their audiences.\nINTRODUCTION: However, such considerations and strategies only make sense if we understand whether and how users change their behavior when conversing with machines in front of other people. Do they feel more embarrassed, powerful, or wittier by an audience when dialogging with a machine instead of a person? Are the changes different if the audience is comprised of acquaintances or strangers? To shed a light on such questions, we went further and performed two studies on the art exhibit and its visitors. This was a setting where single or multiple visitors freely conversed in a physical space with three text-based chatbots representing characters from a well-known 19th century book in Brazil. No control on how visitors interacted with the space or the chatbots was in place, with the exception that they had to do it through a single tablet. Images from the exhibit are depicted in Figure 1 .\nINTRODUCTION: To explore changes in conversational behavior of people due to the presence of an audience, we investigated the visitor perceptions of the three agents' social skills and the user's engagement with agents with the artwork. In the majority of the situations the interaction happened in front of other visitors, some of them known to the users, but also often in front of strangers. In our first study, we conducted 92 semi-structured interviews with visitors, after observing their behavior at the exhibit. Analyzing this data, we were able to determine that, in some specific situations, it was very likely that the audience presence was affecting the user experience of the visitors. In a second study, we analyzed the conversation logs of more than 5,000 sessions. Coupled with a silent video of the audience interaction, which we used to manually determine the occurrence and type of audience, we were able to explore changes in conversation patterns which could be related to the presence of other people around the visitor. The two studies seem to provide evidence of audience effects, and that designers should be taken into account audience effects in conversational systems in physical spaces. Moreover, our findings seem to indicate that those effects are modulated by many factors, including gender, knowledge about the content of the exhibit, and whether there were strangers in the audience.\nINTRODUCTION: The next sections describe in detail the related work, the experimental setup, the two studies, their findings, and our main conclusions. Finally, we discuss some design implications, indicating how our findings may guide the design of conversational systems in physical spaces.",
    "relatedWork": "RELATED WORK: In this session, we describe the previous work as a background for our study, both in the scope of social interaction with chatbots and in the context of audience effects.\nRELATED WORK: Social Interaction with Chatbots: With the recent advances in conversational and natural language technologies, interest has increased on how humans interact with conversational systems, here referred generically as chatbots, and on how social presence and context may play a key role in understanding the dynamics of the interaction [29, 37] .\nRELATED WORK: Social presence is described as the social connection and involvement between two or more people in an interaction often developing and maintaining some sort of personal relationship [41] . The perception of social presence is sometimes connected to the anthropomorphism of physical robots, chatbots, and avatars. In particular, anthropomorphism is a prevailing topic of Embodied Conversational Agents (ECAs), a special case of embodied agents in which the agents provide human-like capabilities of face-to-face dialogue.\nRELATED WORK: Studies with ECAs have provided evidence that they can induce social-emotional effects comparable to those in humanto-human interactions [38, 43] . Previous work found that people conversing with ECAs or interacting with robots show social reactions such as social facilitation or inhibition [3, 38, 50] , a tendency to socially desirable behavior [20, 39, 43] , and increased cooperation [32] . For example, analyses of users' utterances while interacting with a museum agent [19, 20] showed semblance with human-to-human communication, with similarities in the amount of greetings and farewells, common phrases (such as \"How are you?\"), and human-likeness questions (e.g., \"Do you have a girlfriend?\").\nRELATED WORK: In general, system which exhibit human-like traits tend to improve the quality of the user experience with them., Cafaro et al. [6] found that smile, gaze and proxemics are important for conversational museum guide agents, implying that those agent influenced user's interpretation of agent's extraversion and affiliation and impacted on the user's decisions about further encounters.\nRELATED WORK: Although the degree of veracity in the dialogue often improves the quality of the interaction, it might have the opposite effect: the uncanny valley effect [30, 44] where people are averse to a high degree of human similarity has also been observed. Experiments, such as [18, 32] , have validated this hypothesis by observing the user's emotion engagement strategies towards agents of varying human likeness.\nRELATED WORK: In this study, we contextualize our study object, the art exhibit, as containing three embodied chatbots. Even though the chatbots did not have a physical body they have a clear physical presence provide by scenographic elements (see Figure 1 ): female and male hats hanging above chairs around a table unmistakably embodied the chatbots.\nRELATED WORK: Audience Effects:. Seminal work on drive-producing effects of the presence of an audience [8] uncovered specific group interaction behaviors, which led to theories and design frameworks for spectatorship (e.g. [4, 35] ). Among the implications and findings of audience effects are the impact of behavior and views of bystanders on the response to an interaction, which has been known to influence engagement, either being related to attention, interest, or affective feelings [4, 8, 35] .\nRELATED WORK: One of the early studies of audience effects concluded that proximity and presence of audience enhance the emission of dominant responses [8] , i.e. responses governed by strong verbal habits at the expense of responses governed by weaker ones. Active audiences who looked and interacted with the subjects directly affected individual performance measured by the average number of responses in a word recognition task. In 1982, Michaels et al. performed a classical study on social facilitation showing that the performance of good pool players improved 14% in front an audience while bad pool players had a dramatic decrease of 30% [28] .\nRELATED WORK: Love and Perry [23] studied the behavior and views of bystanders in response to a proximal mobile telephone conversation held by a third party. In their experiments, subjects demonstrated noticeable changes in body posture when viewing and listening to a confederate attending a call. The influence of audience has been also studied in video gaming, where researches explored audience aspects including age [49] , size and distance of the interactor [18] , typologies of spectatorship [27] , player performance and perceived game difficulty [49] , co-located/remote and virtual/real audience [11] , cheering [16] , supportiveness [5] , activeness [21] , and social aspects [9] . Overall, the findings report that different characteristics and behaviors of audience have positive and negative impacts, sometimes affecting the entire gameplay experience.\nRELATED WORK: Spectator experience design has been proposed by Reeves et al. [36] , which produced a taxonomy that uncovers design strategies based on interface manipulations and their resulting outcomes. Audience participation in public spaces has also been studied from the point of view of interaction and engagement in many domains, such as education [47] , sports [7] , and arts [21] . One common observed practice which directly affects the experience is the honey-pot effect, where interaction with a screen in public can drive social clustering and further engagement [4] . Furthermore, Group interaction helps to explain how users understand and react to displays in public settings.\nRELATED WORK: Although previous works explored audience and spectatorship effects in games, sports, arts and other domains, to the best of our knowledge no research efforts have been made to study the experience of audience effects in scenarios where the main interaction is conversing with chatbots in a physical space. Finally, given that our setting is an art exhibit, we use the terms visitor and user interchangeably. In our study, a person is both a visitor of the exhibit as well as the user of the physical chatbot architecture described next.",
    "rq": "rq1: what are the effects of audiences on the users' perceptions of social interaction with chatbots?\nq1. are you familiar with the story of the book?\n q3. how do you think the exhibit works?\n regarding both of our research questions rq1: what are the effects of co-located audiences on the users' perceptions of social interaction with chatbots?"
}
{
    "intro": "INTRODUCTION: The renaissance in conversational user interfaces is evident in many domains to help handle tasks, support users and accomplish goals. Personal assistants, known as virtual personal assistants (VPAs), intelligent personal assistants, mobile assistants or voice assistants, have become mainstream. In this paper, we use the term conversational interface to refer to the technology that supports conversational interaction with virtual agents by means of speech, text and other modalities. Examples include, Apple's Siri 1 , Google Now 2 , Microsoft Cortana 3 and Amazon Alexa 4 . The rise in conversational agents strongly benefited from the advances in computational power and neural network models, specifically deep learning models [8, 22] . That said, users can either text or speak to their conversational agents in a natural way to obtain information, access services or issue commands.\nINTRODUCTION: There is a need to cover the user interaction and experience with VPAs. This is crucial to reveal weak interaction points of the virtual assistant and to inform the design of better dialogue systems. With all the approaches used to develop conversational interfaces, including template based [12] , pattern matching [23] , retrieval-based [16] , knowledge-based [1] , and using advanced machine learning approaches [24] , there remains the point of user experience with the technology. Even if a system is technologically advanced, it will not succeed unless it is accepted and adopted by users. Until recently, evidences showed that users stopped using their virtual personal assistants after an initial stage of experimentation [22] . In some cases, they might encounter issues and barriers in the interaction, such as speech recognition errors, and so reverted to more accustomed and accurate modes of input. Many conversational agents provide specialised functions and are context specific, such as fitness monitoring, health tracking and food recipes planning [4] [5] [6] . Adopting a conversational agent per domain requires also picking the conversational style the bot should follow, which could be speech, text, or graphical elements, such as buttons or emojis.\nINTRODUCTION: In this paper, we discuss the effect of communication style of a conversational health coaching system in the context of health and wellness. We particularly focus on the role of emojis in textual conversational agents for physical activity and mental wellness health tracking. For that, we will describe our system and the result of a study with 58 participants. The study consisted of two chatbot versions, one emoji based dialogue and one plain text dialogue to converse with users about their physical activity and mental wellbeing. We hypothesised that using emojis will benefit the most to conversational dialogues where user emotion and empathy are relevant, such as mental wellness. Whereas, it's less relevant to conversation dialogues intended for physical activity data tracking. Regardless, emojis could make the generated replies more anthropomorphic and interesting, which might enhance user experiences with the dialogue systems [13] .",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Despite advances in health care and technology, most of the elder care is still provided by informal caregivers, i.e. friends and family members. According to predictions, however, this type of care will decrease in the future, for which studies encourage society to concentrate on improving the elderly's lifestyle, helping them to remain independent for a longer period of time [39] . In particular, the focus should be on external and internal difficulties of the elderly, offering arrangements and facilities to support active aging. Indeed, socio-behavioral and environmental conditions are a crucial factor affecting longevity [14] , which to some extent explains variations found in the aging process, ranging from active and positive to feeble and dependent. We believe that four principles promote active aging, namely dignity, autonomy, participation, and joint responsibility. Information and Communication Technologies (ICT) are expected to make such principles possible, allowing the elderly to stay active members of the societal community while helping them remain independent and self-sufficient [2] .\nINTRODUCTION: Consequently, the EMPATHIC (Empathic, Expressive, Advanced Virtual Coach to Improve Independent Healthy-Life-Years of the Elderly) project 1 aims to contribute to technological progress in this area by researching, innovating and validating new interaction paradigms and platforms for future generations of personalized Virtual Coaches (VC) to promote active aging. It is centred around the development of the EMPATHIC-VC, a non-obtrusive, emotionallyexpressive virtual coach whose aim is to engage senior users in enjoying a healthier lifestyle concerning diet, physical activity, and social interactions. This way, they actively minimize their risk of potentially chronic diseases, which contributes to their ability to maintain a pleasant and autonomous life, while in turn it helps their carers. The main goal of the VC is to create a link between one's body and emotional well-being. To do so, it will perceive and identify users' social and emotional states by means of multi-modal face, eye gaze and speech analytics modules. Furthermore, it will learn and understand users' requirements and expectations, and adaptively respond to their needs through novel spoken dialogue systems and intelligent computational models. Such a combination of modules will allow for user-coach real-time interaction, thus promoting empathy in the user.\nINTRODUCTION: In this paper, we describe our mid-term achievements, explaining where we currently stand with these goals, 18 months into the project. Section 2 describes the current status of the system components, with particular emphasis on the most robust modules up to date. The integration of these modules is presented in Section 3. Lessons learned from the preliminary human-coach interaction studies are explained in Section 4, while Section 5 concludes the paper.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": "3. can changes in people's subjective impressions of ecas positively affect their behavior and objective task performance using the ecas?\n 4. how difficult is it for eca designers to achieve the desired effects of selection and customization?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Digital assistants integrated into commercial \"smart speaker\" systems, like those sold by Google (Google Assistant), Amazon (Alexa), and Apple (Siri), use automatic speech recognition and natural language processing to appear to be able to converse naturally with people [31, 39, 46] . They allow people to speak like they would to another person, and receive an answer back in a conversational tone and sentence structure. Digital assistant systems are advertised as tools that can be used e ectively without formal training (e.g., \"Your Google Assistant is ready to help, anytime, anywhere. \") 1 . In other words, these systems are intended to be 'walk up and use': self-explanatory even for people who have never used them before [43] .\nINTRODUCTION: Because smart speaker systems commonly do not have a visual interface-only an indicator signaling when the system is awake and listening for a command-people cannot navigate graphical menus to discover commands that the system can respond to. Instead, they must encounter the system's features through trial and error. They do this by asking questions and hearing how the system responds [8] . However, once people encounter examples of the system's limitations, they can become reluctant to continue exploring the system's features [10, 50] . For example, Bentley et al. [5] found that after a very short period of initial exploratory use lasting only a few days, Google Home users seemed to only use the system for a small number of di erent tasks. Also, once they stopped exploring the system's functionality, they rarely changed their usage patterns over a period of several months.\nINTRODUCTION: Users who stop trying to discover new or unfamiliar functionality present a challenge for the design of so-called \"arti cial intelligence\" (AI) systems which, like digital assistants, rely on large datasets and machine learning algorithms to shape the user experience. This is because the smart speaker device in a user's home is merely a means of accessing a cloud-based system. The functionality of the system is not limited by the physical hardware of the speaker device. Instead, the speaker device is essentially a client front-end for a computational infrastructure that exists on networked servers, and the system's ability to respond to questions can be improved without having to alter the device itself [42] . This means that the system can be improved over time without any visible indication towards the user. Therefore, users must continue experimenting-trying out new questions and commands-in order to learn about updated functionality.\nINTRODUCTION: Continued interaction with digital assistants is also important from a system perspective, as more frequent and longer conversations with users generate data that the system tracks and uses to improve its performance [24] . In this way, people and digital assistants can be thought of as being in a mutually bene cial, symbiotic relationship, as described by Licklider [29] in his seminal paper, Man-Computer Symbiosis. People bene t from the convenience a orded by digital assistants via a 'walk up and use' voice interface which provides access to information and the ability to execute hands-free commands. Meanwhile, the system bene ts because as people use it, data are collected about their interactions with it which the system uses to become more e ective. Many AI systems, not just digital assistants, have the potential to be symbiotic in this way. However, Grudin and Jacques [21] argue that while many software systems currently work \"autonomously around the clock on our behalf\", these systems have not yet achieved the equal partnership promised by symbiosis.\nINTRODUCTION: Digital assistants that do not have screens, like the Google Home, are an interesting case for investigating the potential of symbiotic partnerships with AI systems. This is because in order for digital assistants and other chatbot systems to become true partners to humans, Grudin [20] argues that advances in AI technologies on their own are not enough. Behavioral research focusing on the interactions between people and these systems is also needed. Conversation is the primary medium for interacting with voice user interfaces [7] , and therefore people's attempts to interact with smart speakers are shaped by the rules and processes that govern human conversation. This means that designers must combine AI advances with behavioral science ndings about the nature of human conversation in order for voice-based user interfaces to become e ective conversational partners.\nINTRODUCTION: According to Clark's theory of common ground [11] , in order for two human interlocutors (conversation partners) to successfully have a conversation, they must make spoken contributions to the conversation which signal that they understand each other well enough to build on what the other person has said. Through the conversation, they each build up common ground, which is an individual mental representation of the shared knowledge and beliefs held by the participants in the conversation that develops over time. When the conversation partner is a digital assistant, not another person, the responses and answers it provides become part of the human user's mental representation of how well (or poorly) the system is able to respond to them, and what kinds of questions it is capable of answering. This means that the process by which common ground is formed-called grounding-is a key part of supporting continued experimentation and learning about the system. We argue that studying the di culties people have with interacting with digital assistants can help human-AI interaction researchers better understand the requirements for truly mutually bene cial symbiotic relationships with AI systems.\nINTRODUCTION: In this study, we focus on the initial interactions between people with no previous experience using a smart speaker system, and the Google Home, a commercially available smart speaker product that is a front end for the Google Assistant. The goal of this research was to identify ways that the Google Home's responses might fail to provide opportunities for users to extend their interactions with the system, thereby preventing them from learning about its capabilities. A symbiotic relationship between the user and the system only works if the system provides opportunities for the user to build on what the system just said, even when it might not be clear how the system should respond. We suspected that problems with the grounding process may be related to why users struggle in their early interactions with digital assistants. This insight led us to focus on grounding-related barriers to developing a symbiotic relationship between people and AI-based digital assistant systems.\nINTRODUCTION: We conducted a user study in a lab setting with 20 participants in which they completed several information seeking tasks using the Google Home, and used an approach guided by common ground theory [11] and conversation analysis [55] to analyze pairs of back-and-forth utterances between the participant and the Google Home. We found that the Google Home exhibits usability problems related to conversational grounding that discourage user experimentation, exploration and learning about new functionality. However, we also found that some of the Google Home's responses that were only tangentially related to the question that was asked showed participants that the Google Home was knowledgeable about a topic and able to engage in a conversation with them about it. This feedback allowed participants to speculate about how to ask new questions that helped them make progress on tasks. This paper contributes new ndings to the research literature about general purpose digital assistant systems through an investigation of a commercial smart speaker product that has been widely adopted. Based on our ndings, we discuss the importance of conversational grounding for symbiosis between humans and AI systems, and present recommendations for ways to better support a mutually bene cial partnership.",
    "relatedWork": "",
    "rq": "6: what are the local stores that sell brand new televisions?"
}
{
    "intro": "Introduction: The SAMMIE system, developed in the TALK project in cooperation between several academic and industrial partners, employs the Information State Update paradigm, extended to model collaborative problem solving, multimodal context and the driver's attention state. We performed extensive user studies in a WOZ setup to guide the system design. A formal usability evaluation of the system's baseline version in a laboratory environment has been carried out with overall positive results. An enhanced version of the system will be integrated and evaluated in a research car.\nIntroduction: In the following sections, we describe the functionality and architecture of the system, point out its special features in comparison to existing work, and give more details on the modules that are in the focus of our research interests. Finally, we summarize our experiments and evaluation results.",
    "relatedWork": "Related Work and Novel Aspects: Many dialogue systems deployed today follow a state-based approach that explicitly models the full (finite) set of dialogue states and all possible transitions between them. The VoiceXML 3 standard is a prominent example of this approach. This has two drawbacks: on the one hand, this approach is not very flexible and typically allows only socalled system controlled dialogues where the user is restricted to choosing their input from provided menu-like lists and answering specific questions. The user never is in control of the dialogue. For restricted tasks with a clear structure, such an approach is often sufficient and has been applied successfully. On the other hand, building such applications requires a fully specified model of all possible states and transitions, making larger applications expensive to build and difficult to test.\nRelated Work and Novel Aspects: In SAMMIE we adopt an approach that models the interaction on an abstract level as collaborative problem solving and adds application specific knowledge on the possible tasks, available resources and known recipes for achieving the goals.\nRelated Work and Novel Aspects: In addition, all relevant context information is administered in an Extended Information State. This is an extension of the Information State Update approach (Traum and Larsson, 2003) to the multi-modal setting.\nRelated Work and Novel Aspects: Novel aspects in turn planning and realization include the comprehensive modeling in a single, OWL-based ontology and an extended range of context-sensitive variation, including system alignment to the user on multiple levels.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: The use of chatbots in the banking industry is growing in popularity. Chatbots are conversational programs that enable users to have humanlike interactions. They can be interacted with either through text-or voice-input modalities [1] . Spoken interaction with chatbots is more effective and produces fewer errors than text-based interaction [2] . In a recent study on adolescents using either text or speech to communicate with chatbots, users who interacted with the chatbot through speech rated it as being much friendlier and more socially present [3] . However, no previous study has compared the effects of the two modalities in the context of banking chatbot interactions. This study was conducted to analyze differences in human-chatbot interactions based on input modality.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "Introduction: Unlike Pinocchio, liars do not exhibit universal behavior or physiological signals in all situations. Deception is often inappropriately reduced to either simply telling the truth or lying. However, there are many strategies for lying (e.g., omission, imposters, equivocation, hedging); situations where lying occurs (e.g., rapid screening, imposter, interrogation, conversation); varying consequences and power dynamics (e.g., parents, friends, boss, border guard, law enforcement); and interviewing styles (e.g., behavioral analysis interviewer, informal chat, guilty knowledge test, short answer format). All of these factors contribute to the type of behaviors and physiological responses that are exhibited and are theoretically expected. These contextual differences present significant challenges when trying to develop computational methods for detecting deception. In order to develop systems that can be used for reliable deception detection, we must constrain the complex problem of deception and manage the factors described above.\nIntroduction: We conducted a field experiment with border guards from the European Union in order to demonstrate that by controlling some of the above factors and by developing context specific computational models, deception detection can be achieved robustly. In the experiment, some of the participants were given a \"fraudulent\" document with incorrect data and asked to pass through a checkpoint. An automated system used an embodied conversational agent (ECA) to conduct interviews. The system was equipped with vocal and ocular sensors, as well as an electronic passport reader. Based on the participants' vocal and eye gaze behavior a computational classification model was developed to identify imposters while limiting the number of false positives.",
    "relatedWork": "",
    "rq": "1. what is your full name?\n 4. what is the purpose of your visit?\n 6. are the data on this document correct?\n 7. is this your document?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": " agent: is there anything else you can say about that?\n agent: is there anything else you can say about that?\n agent: is there anything else you can say about that?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: The use of Computer-Mediated Emotional Support as a mechanism to help people in coping with stressful situations online has been addressed by different researchers [1, 2, [10] [11] [12] [13] . However, helpful online emotional support is not always available for various reasons: users' availability, privacy concerns, risk of facing stress as a consequence of providing support to stressed friends, etc. In this scenario, Computer-Generated Emotional Support in online social networks appears as an interesting alternative [3, 9] . Using the theory of emotional regulation proposed by Gross [4] , a conceptual design of an algorithm to automatically produce supportive messages in accordance to stressful situations was previously introduced [6] . The algorithm uses five types of supportive strategies: AD -Attentional Deployment, CC -Cognitive Change, GES -General Emotional Support, SM -Situation Modification and SS -Situation Selection. Definitions of the strategies are also presented in [6] . The current paper is part of a project that explores both advantages and disadvantages of using this solution to regulate emotions of stressed humans via social media. The main underlying idea is the concept of 'artificial friends': social agents that help people to deal with their 'everyday problems' via online social networks.\nINTRODUCTION: Using the API provided by the Telegram Messenger App, we developed an artificial friend in the form of a chatbot as a first effort in that direction [7] . Consequently, we conducted a pilot study to evaluate the acceptability of such an agent, which gave us promising results and interesting insights. One of the conclusions was that there was room for improvement of the algorithm used by our agent to classify incoming messages and to provide suitable supportive responses.\nINTRODUCTION: Therefore, the aim of the current research is to obtain more accurate insights regarding 1) the types of problems (stressful situations) human users share via online social networks, and 2) supportive strategies used by humans to regulate the stress of their peers. To achieve this, we performed a series of crowdsourcing experiments via Amazon Mechanical Turk, and the data obtained through the experiments are systematically analyzed (cf. [5] ).",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "RELATED WORK: Previous work assessed the self-reported [22] willingness to participate in health studies from the perspective of sharing electronic health record and wearable data with researchers as well as the reasons for people to participate in health studies.\nWillingness to Participate: In general, participants are more willing to participate in health studies and share their data with researchers than other institutions outside of the circle of health care providers. While participants favor more sharing their complete data with their primary care provider, they agree to share specific parts of their data with researchers given the provided data has a particular use, and they keep ownership of the data.\nWillingness to Participate: Sharing Electronic Health Records Perera et al. (2011) asked 490 diabetes patients in Toronto, ON, Canada to assess perceived benefits and harms of keeping an Electronic Health Record (EHR) on a 5-level Likert scale. 67% supported the use of their de-identified data by researchers [26] . Caine et al. (2012) asked 30 health care recipients in Bloomington, IN, the United States to pair parts of the EHR with potential recipients, including researchers. Only 10% of those with sensitive EHR parts were willing to share all EHR parts with researchers, and only 15% were willing to share all nonsensitive EHR parts with them. However, the participants were willing to anonymously and temporarily share relevant parts of their EHRs with researchers [4] .\nWillingness to Participate: Sharing Wearable Health Data Bietz et al. (2015) surveyed 465 individuals from San Diego, CA, the United States on the challenges of sharing wearable health data with research. 78% of the participants were willing to anonymously donate their data for research, especially if it would contribute to the greater good, and would favor exciting over noninteresting studies [1] . Seifert et al. (2018) interviewed 1.013 older age participants from Zurich, Switzerland, and found that 57% would be willing to share health data with researchers [28] . Chen et al. (2016) studied the mobile and wearable fitness tracking behaviors of 101 people from Sydney, Australia, and reported that 77% were willing to donate health data [5] .\nMotivation to Participate: In general, altruistic motivation and personal benefits overshadow health gains and financial incentives, and this trend pronounces recently. Participants are at first motivated by helping others (from family to society) and then, over time, by the personal benefits from the studies (relevant results, intellectual curiosity, improving health). Trust (research institute, data protection, ethics review) is fundamental for the expression of these motivation factors.\nMotivation to Participate: Healthy Populations Trauth et al. (2010) surveyed 489 healthy people in Southwestern Pennsylvania, PA, United States and found that 46% of the respondents would take part in a health study focusing on curing a specific disease of interest. In particular, having a sick friend or relative contributed to the willingness to participate [32] . Stunkel and Grady (2011) reviewed the motivations of 2.000 healthy people participating in 12 English-speaking clinical trials between 1977 and 2010 in the United States, Europe, and Africa. They found that financial motivation was one of the main motivations in the studies. However, helping to contribute to science and medicine, helping others, and taking part in \"something important\" were also reported [30] . Kerath et al. (2013) reviewed 1.041 healthy people's beliefs and attitudes towards participating in genetic research through a 22-item questionnaire distributed in a network of hospitals in Long Island, NY, United States. 83% considered participation important for society. 82% would approve genetic research, 70% would be willing to participate anonymously in research studies with genetic data, and 53% would be willing to participate in a named biobank study. Among those not willing, 74% would refuse due to data privacy concerns. Other general concerns include enrolling in a study not well explained (over 61%), in additional research conducted without their knowledge (over 74%), or in other kinds of research without their knowledge (over 62%) [18] . Nobile et al. (2017) found from 623 questionnaire respondents from two German studies that a contribution to society instead of personal benefits appears to contribute to participant retention [24] . Bongartz et al. (2017) asked 135 healthy people from Hannover, Germany, to self-report on a 5-level Likert scale the importance of 16 reasons grouped in four areas. They were, in the order of importance, the seriousness of the study and altruism (most important), study-related, personal benefits, and external motivation (least significant). Important reasons were the seriousness (of the study, of the researchers, of the institution), the possibility to support the researchers, and a data protection approval. Feedback of results and an exciting topic were of lesser importance. Reports in the media, the participation of friends, and financial incentives were among the least essential [2] . In a review comprising motivation factors for genetic study participation from over 6.000 healthy people, Goodman et al. (2018) found the financial compensation the least important [12] . Kim et al. (2019) surveyed 170 senior South Koreans and found that 39% would share health information with researchers, below family and hospitals, and above device manufacturers, insurance companies, and governmental agencies [19] .\nMotivation to Participate: Diseased Populations McCann et al. ( 2010) conducted a qualitative study with 13 heartburn patients and reported that participants contribute to trials for the greater good initially, but sustained participation over time depends on a concrete personal benefit and no significant personal disadvantages (weighing risks and benefits), a term called conditional altruism [23] . Soule et al. (2016) asked 164 cardiac patients in Boston, MA, the United States to self-report their agreement with four areas of reasons after participating in a study on a 10-level Likert scale. The areas were, in the order of reported importance: altruistic (most important), intellectual, health-related, and financial (least relevant) [29] . Goodman et al. (2019) asked 450 cancer patients, controls, and relatives in the West Washington area, WA, the United States to self-report the importance of several areas of reasons for participating in a 4-level Likert scale. Resulting important themes include benefits for society (highly significant overall), the reputation of institution (highly important overall and particularly mentioned in the context of data collection), benefits for family and known people (both important), research meaningfulness personally (particularly important for cancer cases), and a much less crucial financial incentive [11] .\nMotivation to Participate: We did not observe work on quantifying the say versus do relationship between self-reported participant motivation (above), self-reported personality traits (assessed with validated scales, such as the Big Five test for personality traits [10] ), objective study attributes, and technology-reported [22] participation on one end, and the outcomes of participation (enrollment, engagement, retention) on the other end.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: A recent statistic by IBM 1 showed that 90% of the data available today have been created in the last two years. This scenario has its roots in two phenomena: first, the principles of Web 2.0 [16] fostered the growth of social networking applications such as Twitter, Facebook and YouTube, where people share and produce huge amount of data every day. In parallel, very inexpensive devices based on sophisticated sensors and technologies were made available, and this triggered the research regarding Quantified Self [22] and Personal Informatics. Indeed, thanks to these devices, many heterogeneous information about people's daily lives. such as glucose level, mood), indicators of personal performances (e.g., kilometers run) can be easily collected and stored.\nINTRODUCTION: Both these trends led to an exponential and uncontrolled growth of the available (personal) data. Of course, as the amount of data grows, the need for tools and methods to effectively store and process these data (and, in case, to build a profile of the user) grows as well. In this context, personal information management systems [6] already help the users to acquire and manage personal data that are spread on different data sources. However, despite this effort, the problem of simplifying the way people access to their data, especially when the amount of data is particularly huge and when the data are available in different form (text, multimedia content, physiological data points, etc.) still exists.\nINTRODUCTION: To this end, in this article we present a preliminary investigation towards the adoption of conversational agents (CAs) as a mean to improve the way people access to their own personal data. 1 https://www.ibm.com/analytics/us/en/big-data/ Session 4: Workshop on Explainable User Models and Personalised Systems (ExUM 2020) UMAP '20 Adjunct, July 14-17, 2020, Genoa, Italy\nINTRODUCTION: CAs [20] recently became more and more popular thanks to their capability to interpret users' requests expressed in natural language and to generate an answer that fulfills such information needs, as confirmed by several commercial solutions, such as Amazon Alexa, that implement this paradigm.\nINTRODUCTION: By following this research line, in this work we present Myrror-Bot, a conversational agent that is built on top of a platform for holistic user modeling called Myrror [13] . A HUM is a comprehensive representation of the user which consists of seven different facets, that is to say, demographics data, interests, affects, psychological traits, behaviors, social connections and health data. Such a representation is obtained by merging and processing heterogeneous data gathered from social networks, smartphones and personal devices. More details about the process will be provided next. As shown in [13] , users can access to such profiles and can inspect the features encoded in each facet by exploiting a classic Web-based interface.\nINTRODUCTION: The novelty of this work lies in the introduction of the concept of queryable user model, since we allow the users to query their own HUMs in natural language and to inspect which data are encoded in their own profiles. This characteristics, inspired by the popular paradigm of Quantified Self [22] and fostered by recent regulations as the European GDPR, is supposed to make the personalization process more transparent and responsible, since the users have a clear knowledge of the data that are encoded in the profile and can easily understand how this information is used to drive the personalization process. In particular, we allow the users to: (i) inspect the features that are encoded in each facet of the profile (e.g., What are my interests?); (ii) to acquire knowledge about themselves, by asking questions related to the data held by the platform (e.g., Did I sleep enough ?). Moreover, the intent recognition strategy implement in the platform also allows the user to enjoy personalized access to online services, such as music, video, news, and food recommendation (e.g., Play a song by U2).\nINTRODUCTION: The validity of our intuition was validated in an experimental evaluation, where we investigated both users' acceptance of the system as well as the time required to access to the information encoded in the profile. As expected, the results showed that our system allows to significantly improve the way people can access to personal information\nINTRODUCTION: The remainder of the paper is organized as follows: in Section 2 we present Related Work in the area and Section 3 focuses on the description of the platform. Next, in Section 4 we discuss the findings of the experiments and Section 5 identifies future research directions and concludes the work.",
    "relatedWork": "RELATED WORK: This work investigates new strategies for transparent and scrutable user modeling. This concept, first introduced by Kay in [8] , concerns the idea of allowing the users to inspect and to become aware of the information encoded in their own user profiles.\nRELATED WORK: The first system implementing this principle was the PersonisAD Systems [2] . Next, Kyriacou proposed in [10] a user profiling architecture aiming at building transparent user profiles. Next, many relevant attempts of introducing transparent user profiling strategies have been proposed throughout the years. As for the e-learning [21] who propose the adoption of an open learner model to represent the characteristics of students and learner in a scrutable way. This concept was further investigated in a recent work by Bull [4] . Similar attempts were proposed in the news domain by Ahn et al. [1] . In a nutshell, all these attempts share the common idea of adopting a representation of the user that is transparent and can be inspected through classical tools, such as web-based interfaces.\nRELATED WORK: More recently, this topic got new interests thanks to the European GDPR regulation [23] , which emphasized the users' right to explanation and fostered the research in the area of algorithmic transparency. It is not by chance that recent work by Google researchers [3] actually focused on the development of a strategy for explainable and transparent user modeling, that inherits the same ideas previously spread by El-Arini et al. [7] . A strategy for scrutable user modeling is also presented in [15] , where the authors propose the use of semantic representation techniques for a transparent modeling of users' interests.\nRELATED WORK: To summarize, the research in the area of transparent user modeling exists since more than a decade. However, up to our knowledge, the intuition of exploiting conversational agents to query a user model and to allow the user to inspect the information encoded in her own profile through natural language is a completely novel research direction, and this represents a very distinguishing aspect of the current work.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Human interlocutors adapt their language style to each other in interpersonal communication to interact efficiently [21] . This principle of alignment is generally found at all linguistic levels, from lexis to syntax. For example, as demonstrated by Levelt and Kelter [16] the question \"What time does your shop close?\" may be answered with a simple \"5 o'clock\". If in contrast a prepositional phrase is included in the question (\"At what time... \"), the interviewer tends to adapt to this structure and will include the preposition in his answer (\"At 5 o'clock\"), too.\nINTRODUCTION: In order to enable the most efficient form of interaction in the context of Spoken Dialog Systems (SDSs) based on the human model, computers -in place of a human interlocutor -are expected to flexibly react according to individual requirements of a user. While computers here can increasingly understand what we say, their responses need to be more sophisticated [23] . In this context, adaptive Natural Language Generation (NLG) has gained increased attention. However, it is common consent that more intelligent software is required to enable complex Human-Machine Interaction (HMI) [12] . This becomes even more relevant in dual-task environments like driving a car, where language interaction represents a secondary task in parallel to a prioritized primary driving task. In such scenarios, individual requirements of an SDS user need to be considered with respect to the driving situation as interaction context (e.g. driving on a free highway vs. urban traffic with traffic light control). In order to adequately design SDS voice output to be generated in such scenarios, it is required to examine the impact of the driving task on human language and to identify characteristics of language use which are dependent upon the interaction context.\nINTRODUCTION: In this paper we report the results of linguistic analyses with a focus on syntax. For this purpose, we collected language data in a large-scale driving simulation study, where participants interacted with a simulated dialogue system in two driving situations with differing complexity. The data was transcribed and examined with respect to syntactic characteristics. The results of this work are used to characterize human language in dual-task environments. This basis will be used in later work towards the generation of userand situation-adaptive voice output.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Acquired Brain Injury (ABI) refers to any trauma caused to the brain after birth. It can be a result of a head injury, a stroke, a brain-tumour, or a brain disease (e.g. encephalitis). The effects of a brain injury can vary from one case to another, but they typically include physical impairments (e.g. reduced mobility and vision problems) and loss of cognitive skills such as memory, scheduling, organisation and decision making. These effects reduce the independence of the ABI survivor, who may have to rely on regular support from others to perform everyday activities. This, in turn, can result in withdrawal, and psychological stress both for the person with the ABI and the person providing the support [3] .\nINTRODUCTION: After their injury, people with ABI often attend a neuropsychological rehabilitation program, where they work together with health professionals and others to reduce the impact of their de cits on everyday life [12] . Rehabilitation is usually approached in a holistic way [2] . Cognitive Rehabilitation (CR) focuses speci cally on cognitive functions like memory, attention, problem solving and communication. Additionally, people with ABI use external aids to support their cognitive performance. These can be paper-based tools (e.g. calendars and todo lists) [7] , or Assistive Technologies (AT), i.e. technology-based tools usually in the form of prompting systems [11] . Although the latter can improve the frequency of remembering and completing everyday activities more effectively than their non-technological equivalents [9] , studies show that the uptake and use of AT by people with ABI is rather low, and can be prevented by several factors [1, 8] . Among the barriers that limit the use of AT are the reduced accessibility to mainstream devices (i.e. smartphones and computers) due to physical impairments, and the inef cient use of these devices due to reduced cognitive function.\nINTRODUCTION: Virtual assistants, i.e. autonomous software agents that can perform tasks or services for their users, have facilitated rehabilitation in different contexts [6, 10] . Voice-operated virtual assistants -or Voice Assistants (VAs)-like Apple's Siri, Amazon's Alexa, and the Google Assistant, offer a diverse and increasingly ubiquitous ecosystem for the development of AT for people with ABI, with several potential bene ts: Hands-free and nonvisual interaction can increase accessibility, and integration with IoT devices and third-party services makes them suitable for controlling smart-home environments, which can be helpful for users with disabilities [5] . Despite being widespread however, VAs are not used very often, and interaction with them can be rather problematic [4] . Moreover, speech based interfaces can affect memory load and cognitive performance [13] .\nINTRODUCTION: In order to employ the advantages of VAs and develop AT that can be bene cial for people with ABI (and users with cognitive impairments in general) it is essential to investigate the interaction design of their conversational interfaces in a user-centred way, and nd ways to bypass the barriers that might prevent their ef cient use. The purpose of this research is to explore how the interaction design of VAs can be improved so that they can become more accessible and acceptable by people with ABI.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: The study and design of concepts, metaphors, practices, and evaluation methods in User Experience (UX) has been the steady endeavor of researchers and practitioners working in the field of human computer interaction (HCI) for a number of years now. An increasing emphasis in UX is given to the affective dimension, for example the design of emotive, hedonic [31] , enchanting [29] , empathic [54] or critically reflective [39] [7] [3] [26] approaches to interaction between humans and the digital world. Within this landscape, we have witnessed a surge of different interactive systems in various fields (cultural heritage, tourism, education, e-commerce, etc.) that rely on detecting the human user's emotional state and responding to it appropriately.\nINTRODUCTION: This 'turn' to affect [52] in the design of experiences, interfaces, and interaction methods has, however, been primarily manifested in systems that attempt to capture users' emotional states and offer, in return, a relevant response. Rarely is the user's digitally mediated emotional engagement with the content regarded as an opportunity to trigger a deeper connection, to critically reflect on the issues at stake, to challenge and provoke a call to action.\nINTRODUCTION: Provoking this kind of \"conversation\" wherein the human participant can be challenged into thinking about what their principles or assumptions actually mean and, subsequently, act on them to transform their experience, is at the core of the work we propose in this paper. Based on an affective practices model of emotional engagement [53] and inspired by both Graham's [13] call for digital media that are able \"to move us, to inspire us, to challenge us, \" and his reference to Sample's concept of \"bots of conviction\" [36] , we engaged in designing a conversational agent (CA), or chatbot. Its aim is to evoke its user's emotional engagement with complex topics (e.g. death, wealth distribution, gender equality, privacy) and, ultimately, solicit specific actions from the user it converses with.\nINTRODUCTION: We chose to explore the design of a chatbot because it is a \"minimal\" digital medium, it is direct and simple to use, and it is playful. But how can we design conversational interaction with a chatbot in ways that can trigger critical reflection? To what extent can interaction with such a technologically \"minimal\" medium bring out deeper emotions that can challenge us to act on the world?\nINTRODUCTION: In this paper, we introduce an interactional pattern that, we argue, can ignite a dialogue between a participant and a bot, aiming ultimately to transform the participant's conceptions. We start by defining key concepts related to our goals of emotional engagement, provocation and transformation. We then review the variety of chatbots used today, with particular emphasis on chatbots used in the cultural sector, as this is where our use case is situated. Next, we describe the iterative process of designing a Bot of Conviction (BoC), which follows a carefully planned out and executed, through formative evaluation, procedure of content and interaction design and development. Section 5 demonstrates how we apply our pattern to the design of a chatbot for a specific archaeological site. Finally, the paper concludes by discussing our pattern, its limitations and its potential to fulfill the goal of igniting users' transformation through a call for action.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: With the ever-increasing features available in today's in-vehicle infotainment systems, the need for a simple, easy-to-use interface has become a necessity [2] . In the last 40 years, automotive infotainment displays and control elements have more than doubled [6] . As drivers spend more time in their vehicles, and with the trend towards longer commutes [2] , the demand for infotainment features will not subside. Especially with the focus of current events and legislation on hands-free devices and the impact of these secondary tasks on driver distraction, the need for a usable interface that does not distract from the primary task of driving is ever more important. Of the control methods currently available, hands-free speech recognition is one of the most promising methods, resulting in better driving performance, less mental taxation, and less glances off the road compared to manual data entry [1] , and better driving quality especially in more complex tasks such as navigation and phone dialing [3] . In fact, it has been said that the level of distraction involved in entering a voice command and listening to the vehicle's subsequent response is so low that it is comparable to that of listening to the car radio [12] .\nINTRODUCTION: Although there have been attempts at developing natural language speech systems [4, 9, 10, 11] , there is yet to be a viable product for the mass market, due to a number of challenges that must still be overcome when communicating between human and machine. These challenges range from issues with the sophistication of the speech technology to the user interaction schemes used to guide task performance. With respect to the speech technology itself, issues such as car noise interference and a limited vocabulary of speech commands that the system can recognize [5] constrain the ways that humans can interact with the system. On the usability side, many speech interfaces do not have a clear and transparent menu structure [5] , which leads to confusion about why a certain command is misrecognized in certain contexts but not others. Also, the pace of speech system dialogues is far from the natural pace of human conversation [8] . Oftentimes, information needs to be entered in pieces, such as entering an address with house number, street, and city as separate utterances [4] .\nINTRODUCTION: In light of the issues mentioned above, the goal of this paper is to illustrate a process of evaluating the usability of a speech interface system, and discuss the method of analysis that led to suggestions and rationale for future improvements.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "RELATED WORK: According to Gordon Pask, social systems are formed through a shared interpretation of language [10] . To apply it in the context of chatbots, social systems can be formed with chatbots if it can engage a human in interpersonal conversation -recognize and respond to user input, understand conversational attributes like turn-taking, and contribute to new conversations [4] . Intelligent chatbots, which could learn from conversations, are developed with a personality in mind [13] . Attributes like age, gender, emotional response and personality traits assigned to a bot play a critical role in both how they interact with the user as well as how users respond to them [16, 17] . People often treated chatbots as fellow humans, and develop a personal connection with them [18] . ELIZA, a bot developed by Weizenbaum at Massachusetts Institute of Technology was one such case [19] . Having a personality profile further brings users developer a deeper bond with their virtual assistants, discuss \" ...sensitive content that is too embarrassing to ask another human\" [21] , and volunteer a lot of content without inhibitions. This creates problems in protecting information disclosure. Implications for privacy in interpersonal communication are already widely discussed in the research community [15] .\nSymbolic Interaction: Prior research in interpersonal communication had suggested that individuals are more likely to maintain a performance identity in their social interactions. A conceptualization of such an identity helps them form relationships and maintain norms in society. According to Goffman, people avoid communication breakdown by expressing an appropriate self [5] . In this study we use symbolic interactionism as a more useful focus to understand interaction over psychological theories in this case where the user might be role-playing [5] . Since conversational agents rely on \"interpersonal\" interactions between a human and an algorithm, it is imperative to study these interactions through the lens of more well-studied human to human communication systems because of two reasons. First, it would provide a social context to how people react and respond to prompts by conversational agents. Second, it would help us understand if people follow the norms of communication to create a performative identity.\nEmotion and Emotion Culture: Ball and Breeze have articulated the behavioral aspect of conversational agents in the most comprehensive manner: \"To be useful, conversational interfaces must be competent to provide some desired service to be usable, they must be efficient and robust communicators and to be comfortable, they will have to fulfill our deeply ingrained expectations about how human conversations take place. \" [3] . In other words, attributes such as emotional comprehension (by which we mean the ability to both understand users' emotions and respond) and personality make chatbots appealing to the visceral receptors. This evokes certain psychological constructs in people, which generated 'social responses' [9] , even if the communicating entity is a bot. Similarly, an \"emotion culture\" is established in societies, where people tried to express appropriate feelings and emotional responses that adhere to societal expectations [6] . One of the major aspects of our study was to determine if an emotion culture is implemented in the interactions that people have with these bots. Furthermore, if such an emotion culture was implemented, we wanted to measure if it leads to emotional sharing on the user end.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Human tendency to personify objects is a well-known phenomenon [2] . \"People respond socially to computers and perceive them as having personalities. Software agents are artifacts that particularly embody those qualities most likely to elicit social responses: fulfilling a social role, using language, and exhibiting contingent behavior [22] .\" It is particularly easy for humans to attribute personalities to \"talking technology\", like conversational agents (also known as digital/intelligent/conversational/personal assistants (IPAs)). A conversational agent can be defined as a natural language processing system that supports conversational interactions with users on smart devices, such as cell phones and speakers [9] . It is estimated that approximately 87.7 million U.S. adults were using some sort of conversational device in the first quarter of 2020 [35] , with the most popular usually including Amazon Alexa, Google Assistant, and Apple Siri [44] . Manifestations of the IPAs \"personality\" are evident in the choice of name, voice (male/female), conversational style, and pre-programmed responses related to humor and personality utterances, such as greetings, personal questions and others discussed in the following sections. Amazon Alexa developers even share their aspirations for developing Alexa with an Extraverted/Sensing/Feeling/Judging personality [32] . However, very little is known about users' perceptions of IPA personalities, and whether these perceptions change based on the types of interactions users have with their IPAs. We conducted a study to investigate these questions.",
    "relatedWork": "",
    "rq": " rq 1. what are users' perceptions of ipa personality, including, 1) what the users' general views on ipas personality coming into the study, 2) how do the users view the experimental alexa post-study, 3) what personality traits do users identify as necessary in their ideal alexa?\n rq 2. is change in user perceptions of ipa personality dependent on prior situational interactions (stressful/nonstressful)?\n comments about alexa opens up a potential avenue for research around the following questions: what other information systems are perceived to have personalities and/or manifestations of warmth-competence?"
}
{
    "intro": "Introduction: Spoken dialogue systems are presently available for many purposes, such as flight reservations (Seneff and Polifroni, 2000) , telephone calls routing (Gorin et al., 1997) , and subway lookup (Johnston et al., 2002) . Recently, we have been exploring a next generation of intelligent dialogue systems, which can behave like a human agent and provide proactive assistance and selective recommendations (e.g., highly-rated restaurants or hotels) to users.\nIntroduction: To enhance dialogue systems with intelligent services, we have to let the system -grow\u2016 reliable knowledge and intelligence. Luckily, there has recently been an explosive growth in the availability of public review sites (e.g., yelp.com, tripadvisor.com, etc.) which make a perfect resource for gathering collective opinions. In this paper, we will explore how to utilize summaries of public reviews to automatically generate recommendation-type conversations in spoken dialogue systems. An example of a conversation with our recommendation system is shown in Figure 1 .\nIntroduction: U: Are there any American restaurants in Brighton? S: There are 61 American restaurants in Brighton. All of the rated restaurants have good or excellent ambiance. Figure 1 . A real conversation with our recommendation dialogue system in the restaurant domain (\u2017U' is the user and \u2017S' is the system).",
    "relatedWork": "",
    "rq": "u: are there any american restaurants in brighton?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": "1. are children's questions predictable and deterministic, when grounded in an activity like 20 questions?\n2. is the repair required in such a dialogue limited and feasible?\n3. is it possible to effectively \"nudge\" preschoolers to solve problems without disengaging?\nchild: is it a/an <object>?"
}
{
    "intro": "INTRODUCTION: A conversational agent (CA) is any dialogue system able to interact with a human through natural language [6] . This technology is getting more embedded into peoples' lives: Juniper Research estimated 3.25 billion CAs in use in 2018 all over the world [42] , and the trend is going up [21] . Researchers have explored commercial and non-commercial CAs for children, finding that they are a valuable for play [1, 13, 39, 46, 51] and learning [52, 59, 62] . For instance, Sciuto and colleagues [47] described how children are increasingly experiencing CAs, and Amazon Alexa in particular [56] . Furthermore, they reported both parents and children having a positive opinion of the CA. In this paper, we focus on children. There are several sets of established guidelines for Graphical User Interfaces (GUIs) for children (e.g., [38] ), but there is no equivalent set of principles for conversational interfaces (CUIs). This presents an important research opportunity that we aim to address. Today's most popular CAs are triggered when they perceive a keyword or a short utterance spoken by the user; this phrase is universally known as a wake word (e.g., \"OK Google\" for Google Assistant, \"Alexa\" for Amazon Alexa, and \"Hey Siri\" for Apple Siri). CAs stop listening when they recognize a pause that marks the end of the person's speech. This allows people to use CAs even when their hands and gaze are busy. Despite the widespread use of the wake word, to our knowledge, there is no research on its effectiveness. Also, conversational interfaces are not necessarily hands-free and they can be multi-modal [23] . In this work, we explore voice and other modalities for what we will refer to as wake actions and sleep actions, namely:\nINTRODUCTION: \u2022 vocal, when the user utters a phrase that prompts the device to begin and stop processing the speech; \u2022 tactile, when the user presses a button (physical or digital) or toggles a control to trigger and turn off the system; \u2022 visual, when the user uses the gaze to communicate the intention to speak and stop speaking; \u2022 event-based, when pre-determined settings wake-up and put to sleep the system. \u2022 motion-based, when the user prompts the system by moving parts of the body (e.g., waving the hand in front of a sensor). In this paper, we aim to answer this research question: \"How effective are different wake and sleep actions for children who want to interact with a conversational agent?\". We designed a study using the Multi-Attribute Value Theory (MAVT). The MAVT is a Multi-Criteria Decision Analysis (MCDA [44] ) technique that helps measure numerically the degree to which a decision may be preferred over another [27] . MAVT is typically used in fields where several social and economic issues are involved in the process, e.g., finance, energy planning, and sustainable development ( [19, 26, 37] ).\nINTRODUCTION: To our knowledge, it is not a common practice to apply the MCDA approach to make design choices about Human-Computer Interaction and conversational agents. Consequently, the contribution brought by this research refers to (i) the adoption of the MAVT as a methodological framework in HCI to facilitate decision-making by considering different alternatives, identifying all possible objectives, and eliciting preferences on the objectives, (ii) and the evaluation of various wake and sleep actions for children interacting with a conversational agent.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "RELATED WORK: Our work is related to several areas of work on understanding factors that impact teaming as well as the use and evaluation of conversational agents for various tasks.\nEffect of Team Personality Composition: A rich body of research shows that team personality composition influences team performance and team members' perception of their team. For example, Lykourentzou et al. show that teams with a balanced personality composition outperform teams with a surplus of leader-type personality [31] . Bell uses a meta-analysis to show how psychological variables, such as personality traits, values, and abilities, predict team performance [5] . Halfhill et. al find that the average level of Agreeableness and Conscientiousness (two of Big 5 personality factors) in a group correlate with group performance in Military service teams [18] . Whelan and colleagues indicate that different team personality compositions influence virtual team performance as well as team satisfaction [48] . Humphrey et al. suggest two types of team configurations, complementary vs. supplementary fit, which maximizes or minimizes variances on different personality traits to optimize team performance [20] . In educational settings, Karn and Cowling observed although homogeneous teams may experience less team conflicts, they may fall into the no debate trap which leads lower team performance [25] . Rutherfoord used the Keirsey Temperament Sorter to form teams in a software engineering class and found heterogeneous groups perform better in problem solving [41] .\nEffect of Team Personality Composition: Although existing studies show a great potential of using personality as a criterion for team formation, it is non-trivial to implement such a solution in the real-world due to several challenges. In our case, one challenge is how to obtain students' authentic personality traits. Another challenge is what team personality compositions should be used to suggest student teaming in an educational setting. Our work presented here is precisely set out to explore the feasibility of addressing these challenges. As a result, not only do we present here an effective and practical approach to studying the effect of personality compositions on teaming, but our results also reveal new findings that none of prior studies has discovered.\nEffect of Team Perception: Everyone wants to work in a supportive and trustworthy team. Numerous studies investigate what might affect team members' perceptions of their own team and how team perception impacts team performance [4, 9, 22, 43] . For example, Jehn et al. find that team members are more satisfied if the team is more gender balanced [22] . Burress shows that leader behavior in a team also affects team perception. A balanced personality composition in a team could also improve team member's perception [9] . Most importantly, the team perception is tightly linked to the team performance [24, 39, 44] . A coherent working relationship among team members can also help the team achieve better performance [24] .\nEffect of Team Perception: While we leverage existing findings, we must deal with special challenges in our educational setting. In our case, teams usually are formed for only one semester and team members have little time to bond with each other. If initial team configurations could project a positive team perception at the start, students in a team could dive into the teamwork right away. Therefore, our study aims at discovering how the individual differences of team members might influence team perception especially at the team formation stage.\nApplications of Conversational Agents: Conversational agents have been used for various tasks [45] . In general, there are two main types of conversation agents. One type is task-oriented conversational agents that help users accomplish concrete tasks, such as information inquiry [53] and event scheduling [55] . The second type is to socialize with users without specific goals (e.g., [29, 49] ). Recently, researchers have started building conversational agents that can interleave between tasks and social chitchat and handle users\u00e2\u0102\u0179 emotional and information requests. [36, 50, 51] . Compared to these works, we are experimenting with conversational agents that can support both task-oriented and social dialogue. However, our unique focus is its use in facilitating teaming efforts.\nApplications of Conversational Agents: To evaluate the quality of conversational agents, researchers have proposed many evaluation criteria based on an agent's purpose. For example, task-oriented agents were often evaluated by examining if a user can complete a task with an agent's help [47] . On the other hand, the quality of conversation is often used to evaluate social agents [19] . Since INDIGO is designed to facilitate teaming efforts, we adopt the evaluation criteria that help us evaluate the effectiveness of INDIGO in aiding teaming.\nThe Use of Virtual Interviewer to Assess Personality: To effectively infer one's personality traits from text, a sizable amount of text is often required (e.g., a minimal of 1000 words in [27] and at least 3000 words in [17] ). To obtain one's communication text 2 , recently researchers have built a chatbot that can chat with a user through a text-based conversation and automatically infer the user's personality traits based on the conversation [27] . Studies also show that users are willing to confide in an AI agent [27] , including the disclosure of sensitive information [30] .\nThe Use of Virtual Interviewer to Assess Personality: Given the benefits of an AI agent, INDIGO is built to interact with students and assess the students' personality traits automatically. We can then examine how the inferred traits may predict team results.\nTeam Formation Methods in a Classroom: Three common methods are used to form student teams in a large class: self-selection, random assigned, and criteria-based approach [21] . In the self-selection approach, students form their own teams based on their previous experience or preference, which is often preferred by students who know their classmates and have potential teammates in mind. However, this approach often leads to homogeneous teams, which may not lead to optimal team performance due to group thinking or a lack of required complementary skills [3, 22, 25] . In addition, students may not always know each other in the same class, let alone having potential teammates in mind. Therefore, instructors often need to help students form teams. While random assignment is an alternative solution to ensure that every student be on a team, this approach may not help team dynamics, let alone team success.\nTeam Formation Methods in a Classroom: To mitigate the limitations of self-selection and random approaches, instructors often use criterion-based approaches to divide students into teams. Existing studies show that teams formed based on proper criteria have a better team dynamics and team performance than self-selected and random-assigned teams [8, 48] . However, selecting the right criteria is critical to the success of this approach and is often challenging, since not every instructor is an organizational psychologist and familiar with teaming criteria and their effects on teaming and performance. Thus the goal of our work is to help simplify teaming criteria by examining whether personality composition alone helps teaming.\nExisting Team Formation Tools: To support criterion-based team formation, a number of tools are developed. A notable one is CATME (Comprehensive Assessment for Team Members). It allows instructors to assign teams with 27 criteria, including skills, working styles, and demographics [10, 21] . A recent study shows that the students appreciated the use of rational criteria and thought the tool gave them a fair chance to be assigned to a good team [21] . The instructors also reported that the tool reduced their burden and stress in grouping students into teams [21] . In addition to CATME, another tool emphasizes schedule compatibility [26] . Yet another automated team formation tool by Del Val et al. focuses on collective intelligence and coalition structure generation, and has also received positive feedback from the students [11] . Compared to these tools, which mostly use traditional surveys to understand team members and derive teaming criteria, INDIGO helps teaming, including team formation, by deeply understanding and potentially guiding team members through intelligent conversations.",
    "rq": " rq1: how well do students interact with indigo?\n rq2: how effectively can indigo gather information from students?\n rq3: how well can the personality traits computed by in-digo provide insights into the relations between team compositions and team performance?\na: how does team personality composition impact student team performance?\nb: how does team personality composition impact team members' perceptions of their own team?"
}
{
    "intro": "INTRODUCTION: Conversational agents have become part of our daily lives as they are present in devices we use on a daily basis. Due to the limited empirical evidence of their use by children, there is an untapped potential to study the technical and usability challenges experienced by this group. Studies involving children have predominantly focused on technical challenges [1, 7, 11] , but little has been said about how speech limitations may impact their interaction not about the ways in which speech agents, especially for children, may become inaccessible. Language in children is still developing, and as a consequence their speech characteristics, including a limited size of vocabulary; atypical pitch and pronunciation; and syntax errors; may result in accidental \"language discrimination\" that might lead to mistrust and misuse of conversational agents [1, 7] .\nINTRODUCTION: In this paper, we tested whether a conversational agent can uncover instances of language discrimination in children. We designed and developed a conversational agent that helps children practice joke-telling. Joke Telling Social Story is a language curriculum in which a joke is represented with images, and the children select a joke and repeat it until they memorize it and learn appropriate affect to make it funny. Joke telling involves the repetition of speech tasks involving different vocal production patterns [9] . Jokes can particularly combine both constrained (e.g., repeating words and phrases) and spontaneous speech production (e.g., free description) [2] . Joke telling is a more complex speech pattern than other kinds of speech acts, making such therapy ideal for children who are struggling with language development.\nINTRODUCTION: We used a conversational agent in tandem with a robot to encourage playful, fun and engaging interactions [1] . Although conversational agents can currently tell jokes, conversational agents have not previously been used to support the practicing of Joke Telling. Conversational agents have also not been tested for their ability to characterize children's speech skills and uncover instances of language discrimination. Our preliminary results provide three contributions:\nINTRODUCTION: \u2022 A conversation flow that supports both constrained and spontaneous tasks through the joke-telling; \u2022 Characterization of conversations by children with conversational agents;\nINTRODUCTION: \u2022 Empirical evidence that conversational agents allow the analysis of acoustic and linguistic language patterns and how these patterns affect \"language discrimination\".",
    "relatedWork": "RELATED WORK: Children enjoy interacting with conversational agents, but they may have difficulty communicating with them [1, 6, 7] . For example, Druga et al., (2017) [1] studied the interaction of children with different conversational agents showing 26 children participants encountered various challenges when they gave voice commands.\nRELATED WORK: Children commonly use a variety of strategies to cope with these interaction challenges, including increasing the volume of their speaking voice, saying words leisurely, and rephrasing. However, even with these efforts, conversational agents are incapable of understanding children's voice commands. Similarly, others have reported conversational agents rarely answer children correctly when they mispronounce and used a wrong speaking syntax [7] . This body of work shows that conversational agents may accidentally misunderstand children due to their limitations in speech; but, there is limited empirical evidence characterizing such discrimination instances in concrete and challenging scenarios. To our knowledge, there are no studies reporting on the challenges children with speech limitations encounter when interacting with conversational agents.",
    "rq": " (1) how much did you like talking to bolita?\n (2) how much fun was it talking to bolita?\n (3) how much would you like to talk to bolita again?\n (4) how much would you like to have bolita at home?"
}
{
    "intro": "Introduction: Advances in sensing and computational technologies pave the way for automating increasing number of driving functionalities. These engineering solutions result in improved vehicle control and in freeing the human from manually controlling the vehicle at times. Nevertheless, it is essential to recognize that humans are different, thus preferring different styles of driving when facing different routes, car occupancy, and driving contexts. Usually, default, engineering-based driving styles are pre-set to control the correct and safe performance of vehicles. There is a hidden assumption that humans will all accept this style under all circumstances. However, for different people, the same driving maneuver taken with different styles may be perceived as \"too aggressive\" by some, whereas others would consider it to be \"too cautious\" or \"uncomfortable\". Integrating these contextual preferences in a learning agent is a challenge. Understanding users' needs and preferences is hard since these preferences are diverse, can change over time [11] and they are contextual [21, 24] . This paper attempts to solve this challenge, which requires solving three problems: (1) how can an agent interact with human drivers or passengers to get and interpret their preferences, (2) how can this agent learn from these preferences and driving contexts to predict discomfort and (3) how can this agent learn to adjust online the driving style settings of the vehicle it controls to avoid predicted discomfort. The first problem is hard since people might have difficulty in expressing their needs in such complex contexts as driving. The evaluation of users' satisfaction may be also very expensive [8] (e.g., setting cameras inside the car, analyzing the passengers' video/audio streams). Therefore, it may be more efficient to rely on inputs expressed by users in natural ways enabling them an interaction similar to one they would have with a taxi driver. Namely, we would like to develop an agent that would be able to interpret requests such as \"Go faster\" or \"You're too close\" to changes in the technical configuration of the car without requiring users to give technical details how the vehicle should be achieving these requests (e.g., \"use the gas paddle less smoothly\" or \"limit the acceleration\"). Furthermore, we would like the agent to reduce the need of the human to make explicit comments by dynamically adjusting the cars' parameters based on estimation of the human's preferences. To solve the last two problems, we present a novel learning-based agent for the automatic adjustment of a car's driving style configuration by processing and intelligently reacting to natural inputs from drivers or passengers. Our agent, named the Adaptive Car Controller Agent (ACCA), was developed and tested in a state-ofthe-art realistic simulation environment. Through extensive user studies, with 94 human participants, we show that the ACCA can significantly reduce user's burden of adjusting the driving style manually to achieve acceptable satisfaction levels of comfort (measured with usability questionnaires and evaluated quantitively with the number of manual interventions required through the studies to express discomfort). We have applied a similar computational approach successfully in the thermal domain [18] . We developed an intelligent (intuitive to use) interface for drivers to change the settings of their automotive climate control system, reducing the number of manual interventions required. This interface was implemented on a tablet and used by all participants through all experiments, enabling them to choose what adjustments to make to the driving style through touching buttons and sliders with intuitive meaning.\nIntroduction: Our adaptive solution was developed in three stages, each one contributing an essential component combined eventually to build up a complete adaptive agent solution: (1) Automatically interpretation of user intuitive needs: we trained an intelligent agent with human data, provided through an adaptive intelligent interface, enabling users to express their driving needs without stating specific values for all related parameters. The agent was able to interpret these human inputs into actual driving control actions. (2) Automatically learning of humanpreferred driving style settings: our agent learned 3 humancentered driving styles from data collected in the first experiment.\nIntroduction: (3) Automatically learning and mitigation of user discomfort prediction: our agent learned to predict driving discomfort and learned to avoid it in real time, by adjusting the driving style of the simulated car accordingly. It computed the control actions that attained the highest likelihood of being accepted by the users under similar driving situations. Our results showed that our agent adapted its control behavior successfully to its users' expectations, increasing their acceptance of the automated control and decreasing the number of manual interventions required to accept this automated behavior.",
    "relatedWork": "Related Work: The personalization of human-vehicle interactions has been studied by researchers and practitioners over the years including: the personalization of a car's climate control system [18] , the adaptation of the cruise control system [20] and automatic speed control [25] to name a few. In the more complex context of adjusting a driving style, many car's settings and their interdependencies affect the user experience (e.g., the smoothness of turning the steering wheel, or of pressing the pedals (gas / brake), acceleration rate), making existing approaches unsuitable for our task. Automating human-like driving was studied to provide personal comfort [3, 4] . Our approach is distinct and novel since our agent (1) learns a model of human preferences for driving styles, (2) adapts its automated behavior online accordingly, (3) is evaluated with real subjects riding a simulated car and (4) intuitively interacts with humans overcoming difficulties encountered by drivers, when expressing what they need from the car in technical terms [6] . Natural interfaces [15] are commonly more intuitive for drivers and passengers to use and understand and can assist in personalizing the interaction [13, 17] . Most relevant to our task is the work by Geng et al. [9] , who provide a scenario-adaptive driving style prediction ontology. The proposed ontology presents how a car's driving style should adapt to different traffic scenarios to meet most drivers' expectations. However, the proposed ontology is limited to a \"generalized prediction\" as opposed to a \"personalized prediction\" [19] . Namely, the ontology does not provide a car with the means to adapt to each user nor does it provide a way to adapt the prediction in realtime. Other user studies in simulated driving environments analyzed driving styles and their effect on different users [1, 2] . However, these works collected data of user acceptance via questionnaires and not by interacting with an intelligent agent in real-time, as we do. Guna et al. [10] used driving data to predict driving styles given users' activities. Mazzulla et al. [16] studied the relationship between drivers' characteristics and driving styles. In all these works, the online adaptation of the driving style is missing and is presented as the next required step. In the control domain, Senouth et al. [22] developed a fuzzy rule to adaptively modulate and assist the driver and vehicle torque to keep the lane preferred by the driver. This solution was evaluated by numerical simulations and not by real interactions with subjects as we present here. Our work is novel due to the combination of humanmachine interactions with machine learning to provide one solution to interpret users' preferences and then realize these into actual control actions with the aim of increasing acceptance.",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": " p: what i say will probably sound stupid. alexa: okay. for a public speech, there is no absolute right or wrong in terms of what you say. right?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": " rq1: what kinds of conversational interactions did users have with the qa agent in the wild?\n rq2: what kinds of conversational interactions can be used as signals for inferring user satisfaction with the agent's functional performance, and playful interactions?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: In many research areas like human-computer interaction (HCI), interview is one of the key methods used to obtain information from target respondents. While traditional interviews by people or paper-and-pencil do a good job in gathering and mining information from target audience, they still present several limitations. First, they are difficult to scale due to their limitations in temporal and spatial accessibility. Second, interview fatigue [10] is challenging to combat due to the rigid question-answer formation in traditional interviews. Third, respondents may not feel comfortable disclosing sensitive information because of social desirability biases [6] when interacting with human interviewers. Moreover, traditional ways to conduct interviews require extra efforts to process the results, which might lead to higher chance of errors. To overcome these limitations, AI-powered chatbots embodied with interactive features, such as Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). providing response feedback [2] and probing responses [7] , have been proposed to conduct conversational interviews and proved to be effective in terms of elicitation [9, 10] . Despite the promising future of applying chatbots to conduct interviews, how to design an interview chatbot that can elicit high response quality still remains unanswered.\nINTRODUCTION: In addition to elicitation capabilities, ethics and safety issues are no less important. For example, privacy is a significant problem as interviews sometimes probe unexpected areas [1] . In practice, participants might feel privacy violation if the interview probes were designed too aggressive, which would greatly influence the further response quality and even break the trust bond in between. Considering this, we want to ask the following reserach questions:\nINTRODUCTION: RQ1: How to design interview chatbots which can elicit highquality response data without breaking any ethical rules?\nINTRODUCTION: RQ2: How can we efficiently evaluate interview chatbots in terms of their elicitation capabilities and ethical levels? However, very few research efforts have been put into the evaluation of such an interview chatbot's information elicitation capabilities and ethics level, let alone the design implications and assistive tools. In our study, we aim to develop an automatic evaluation framework, which can efficiently measure the quantitative effect of chatbots in terms of elicitation capabilities and ethical levels. Furthermore, a set of implications as well as assistive prototypes specifically for interview chatbots design will be proposed based on our findings.",
    "relatedWork": "RELATED WORK: Though recent research have revealed the effectiveness of interview chatbots in eliciting significantly better quality responses (compared to traditional online survey), the need of systematic set of evaluation metrics and more efficient evaluation process is still rising with the increasingly prevalent use of interview chatbots. In [10] , researchers proposed a set of content-based metrics to measure response quality based on Gricean Maxims [3] ; researchers in [9] and [4] have used behavior-based metrics such as interaction duration, click-through and sharing action to measure participants' engagement level. However, these metrics have not been wellorganized into a framework and some of them cannot be measured automatically but only manually, which would require tremendous efforts to annotate and analyze. Moreover, there have not been any in-depth studies examining the ethical aspects of interview chatbots to our knowledge, which makes our work necessary.",
    "rq": "rq1: how to design interview chatbots which can elicit highquality response data without breaking any ethical rules?\nrq2: how can we efficiently evaluate interview chatbots in terms of their elicitation capabilities and ethical levels?\n which holds the key to answer the following research questions: why would participants digress when interacting with interview chatbots?"
}
{
    "intro": "",
    "relatedWork": "RELATED WORK 2.1 Conversational style: Most of the related research on conversational style matching is based upon the work of Tannen, who described several markers that define conversational style [36] . These markers include: Topic (e.g., personal pronoun usage, persistence), Pace (e.g., speech rate, pauses), Expressive Paralinguistics (e.g., pitch and loudness shifts) and Genre (e.g., story framing). Based on the usage of these stylistic markers, people can be placed on an axis ranging from HC to HI.\nRELATED WORK 2.1 Conversational style: In more recent work, Shamekhi et al. looked at conversational style in human-agent interactions [33] . They examined whether there was a particular conversational style (HI or HC) that was preferred for a conversational agent in a structured conversation. However, rather than finding one specific style that worked best, they found that participants liked whichever agent matched their own conversational style.\nRELATED WORK 2.1 Conversational style: Thomas et al. looked at the conversational style of participants while performing a simple information seeking task [37, 38] . This task was similarly structured to how a person might use an intelligent agent such as Cortana or Siri. They found that even in these tasks, participants aligned their conversational styles over time. It took less effort to complete tasks for participants who aligned their style compared to those who did not.\nRELATED WORK 2.1 Conversational style: Conversational style matching can be seen as a type of entrainment, as it relates to people synchronizing their behavior within an interaction [2, 15] . Entrainment has been used in related work to generate more accurate and realistic reactions [20] or backchannels [18] . Levitan et al. implemented acoustic-prosodic entrainment in a conversational avatar and observed an increase in perceived reliability and likability of the system [22] . In a text-chat interface, Scissors et al. [31, 32] found that lexical entrainment was associated with trust between partners. Specifically, pairs with high trust exhibited greater repetition of shared terms than did pairs with lower trust [32] . Subsequent work found that this was driven primarily by certain types of terms (e.g., those with greater positive emotional valence), and that not all similarity increased trust. The similarity in negative terms was associated with decreased trust [31] . We were inspired by this work and wanted to build upon it using an automated agent capable of voice-based interactions.\nAgents and Trust: Conversational Agents have been studied extensively, with trustworthiness being an important aspect of evaluating and designing these systems [19, 30] . Trust has been mentioned as an important goal for a socially intelligent agent. For example, Elofson et al. [11] investigated the role intelligent agents can play in the development of trust within organizations. Cassell and Bickmore argue that the methods that humans use for establishing trust can also be used by intelligent agents [8] . As shown in their studies with the embodied agent REA, by incorporating small talk, REA was perceived as more trustworthy by extroverted participants than an agent that does not engage in this social behavior [4] .\nAgents and Trust: In a field trial with the SARA agent, Pecune et al. found that the interactional features (e.g., frequency of interruptions and number of turns) played a role fostering both rapport and task performance. The agent's social awareness was especially important due to the short duration of the interactions, as there is likely a social politeness boundary that should not be broken in short interactions [29] . Rapport is characterized by a close and harmonious relationship, and one would think that this quality would also strengthen trust between an agent and a human.\nAgents and Trust: In a study, Gratch et al. found that participants disclosed more to SimSensei, a virtual interviewer for healthcare when presented as an artificial intelligence, rather than an avatar controlled by a human operator in a medical interview [12] . These results suggest that an intelligent virtual agent might be used to obtain more truthful information within medical domains.",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: In various domains, data from unstructured, semi-structured and structured data sources are curated and integrated into a domain-specific knowledge base (KB). With the help of an editorial team, comprising of subject matter experts (SMEs), the resulting KB contains high-quality knowledge that can help users make better decisions in their applications. For example, a medical KB which contains information about drugs, findings, conditions, and procedures, can be used by medical professionals as a valuable reference in clinical decision making. The value of this highly distilled domain-specific KB also depends on how easy it is for the non-technical professionals to access the information they need.\nINTRODUCTION: Conversation interfaces enable users to interact with the KB using the principles of a natural human-to-human conversation [14] . In this paper, we define a natural conversation interface as one that exhibits at least the following features: 1) natural language understanding, 2) natural language generation, 3) persistent context and 4) conversation management. Natural language understanding is the ability to recognize both the intent of user utterances (e.g., \"show me drugs that treat psoriasis in children\") and particular entities within those utterances (e.g., \"drugs\", \"treat\", \"psoriasis\", and \"children\") formulated in a particular natural language (e.g., English, Spanish, Mandarin, etc.). Natural language generation is the ability to respond to the user's natural language utterances with appropriate natural language utterances (e.g., \"Fluciononide and Tazarotene treat psoriasis in children\"), as well as supporting artifacts (e.g., lists, charts, images, etc.). Persistent context is the ability of the conversation agent to \"remember\" and display sensitivity to prior intents and entities in the conversation. This enables users to articulate requests or queries across multiple utterances in a conversation without failures in natural language understanding. Finally, conversation management is the ability to respond appropriately to common conversational actions that enable user and agent to manage the interaction itself.\nINTRODUCTION: Advances in natural language processing and artificial intelligence have led to the increasing availability and use of conversation systems for various applications. Conversation systems such as IBM Watson Assistant, Apple Siri, Google Now, Microsoft Cortana, or Amazon Alexa are widely used for general-purpose tasks. Despite the increasing use of these systems in open-domain applications, the requirements of a domain-specific conversation system are quite different and challenging. To design effective domain-specific conversation systems, we need to understand not only the domain knowledge but also the contents of the KB, so that we can anticipate and design for possible user intents. As a result, most conversation interfaces to domain-specific KBs are special built for the particular KB in question. We observe that data in a KB can be described using domain specific ontologies, which provide a powerful abstraction of the data in terms of the entities of the domain and their relationships. This information can be exploited to identify potential usage patterns, i.e., questions we expect the users to ask, such as \"what drug treats condition bronchitis?\". In this paper, our goal is to develop a principled methodology to develop conversation interfaces to domain-specific KBs, in a domain-agnostic way. For this purpose, we propose an ontology-based approach to automate the conversation interface design.\nINTRODUCTION: In this paper, we develop techniques that combine domain knowledge in the ontology with the feedback from SMEs to bootstrap a conversation space. In particular, we describe algorithms to extract usage patterns from the domain ontology to identify user intents and the key entities that are involved, as well as to generate the training samples for the conversation service. The effectiveness of the conversation system heavily depends on how well its model is trained, and hence depends on the quality and the quantity of the training samples. Providing sufficiently large training samples hence is a time-consuming task in conversation design. Our approach alleviates this by generating training samples from the usage patterns extracted from the domain ontology.\nINTRODUCTION: To prove the effectiveness of our approach, we have implemented our techniques to create a healthcare conversation service. In this paper, we describe this use case in detail, and discuss the lessons learned. We focus on healthcare due to its complex KB, and the potential of a conversation system to help medical professionals as a mechanism for easy access to this complex knowledge. The main contributions of this work are summarized as follows:\nINTRODUCTION: \u2022 We use a domain ontology to not only represent the semantics of a knowledge base but also capture all possible user intents over the KB. \u2022 We develop techniques to bootstrap a conversation space with artifacts, including entities, intents, and training examples, generated automatically from the domain ontology, and further refined by domain experts. Our techniques are domain agnostic, and work with any knowledge base.\nINTRODUCTION: \u2022 We implement our technique in a medical use case, to provide a conversational interface to a medical KB. We describe our experiences and lessons learned with this use case. We also present an experimental evaluation based on real usage statistics of our system gathered over a period of 7 months.\nINTRODUCTION: The rest of the paper is organized as follows. In Section 2, we present the architecture of our conversation system, and in Section 3 we discuss the role of domain ontologies in our system. In Section 4, we describe our core technology to bootstrap the conversation space using the domain specific ontology, and in Section 5 we describe the construction a dialogue structure for the conversation system in the form of a dialogue tree. We present the implementation of our techniques for the medical use case in Section 6, and provide experimental results of this use case in Section 7. We compare our system to related work in Section 8, and conclude in Section 9.",
    "relatedWork": "RELATED WORK: General-purpose conversation systems. Over the years, conversation systems are fundamental to natural user interfaces. It is a rapidly growing field, leveraging the breakthroughs in deep learning (DL) and reinforcement learning (RL). According to [14] , conversation systems can be grouped into three categories: (1) question answering agents, (2) task-oriented dialogue agents, and (3) chatbots. Companies, including Google, Microsoft and Baidu, have incorporated multi-turn QA capabilities into their search engines to make user experience more conversational. Their QA Systems are designed for retrieving candidate documents from Web Index [28] or answering factual questions from knowledge graphs [33] . Nowadays publicly available and commercial chatbot systems [20, 27, 37] are often a combination of statistical methods and hand-crafted components, where statistical methods provide robustness to conversation systems (e.g., via intent classifiers) while rule-based components are used in practice, e.g., to handle common chitchat queries.\nRELATED WORK: Our conversation system falls in the task-oriented dialogue category. Task-oriented dialogue systems [9, 14] can perform a range of tasks or services for a user. The most widely used systems include Apple's Siri, Google Assistant, Amazon Alexa, and Microsoft Cortana, among others. Our system separates itself from these systems since it is designed for domain-specific KBs using domain ontologies, which provides a powerful abstraction of the data in terms of the entities of the domain and their relationships.\nRELATED WORK: Conversation systems in healthcare. There have been increasing research and commercial interests showing the potential benefits of using conversation systems/agents for health-related purposes. Recent advances in machine learning, particularly in neural networks, has allowed for more complex dialogue management methods and more conversation flexibility. Three approaches are commonly used in building a conversation system for healthcare applications. Rule-based approaches [21, 22] used in finite-state dialogue management systems are simple to construct for tasks that are straightforward and well-structured, but have the disadvantage of restricting user input to predetermined words and phrases, not allowing the user to take initiative in the dialogue, and making correction of mis-recognized items difficult. Hence the rule-based conversation systems are commonly used in task-oriented, mostly focusing on information retrieval tasks such as data collection or a predefined clinical interview guide. Frame-based systems [5, 13, 15] address some of the limitations of finite state dialogue management by enabling a more flexible dialogue. Frame-based systems do not require following a predefined order to fill-in the necessary fields, enabling the user to provide more information than required by the system's question -the conversation system keeps track of what information is required and asks its questions accordingly.\nRELATED WORK: Unlike finite-state and frame-based systems, agent-based systems [7, 23, 26, 34] are able to manage complex dialogues, where the user can initiate and lead the conversation. Agentbased methods for dialogue management are typically statistical models trained on corpora of real human computer dialogue, offering more robust speech recognition and performance, as well as better scalability, and greater scope for adaptation. One disadvantage of agent-based systems is that they require large training data, which can be difficult to obtain in healthcare applications. An in-depth study of the conversation agent in healthcare can be found in [6, 17] .\nRELATED WORK: Natural language interfaces to databases. In recent years, natural language interfaces (NLI) for databases have gained significant traction both in academia and industry. These NLIs fall into four categories based on the methodology being used: keyword-, pattern-, parsing-and grammarbased NLI. Keyword-based systems [4, 8, 16] are capable of answering simple questions by matching the given keywords against an inverted index of the instance-and metadata. Pattern-based systems extend the keyword-based systems [11, 35] with NL patterns to answer more complex questions like aggregations. However, these systems still suffer from identifying every possible synonym of trigger words (e.g., \"by\" for the aggregation) allowed by NL. The core of Grammar-based NLIs [12, 32] is a set of rules (grammar) that define the questions that can be understood and answered by the system. Using those rules, the system is able to give the users suggestions on how to complete their questions during typing. The disadvantage of these systems is that they are highly domain-dependent: the rules need to be written by hand. Parsing-based [19, 29] NLIs parse the input question and use the information generated about the structure of the question to understand the grammatical structure. These systems leverage the dependency parser to handle the difficulty of verboseness in complex NL patterns. Our query generation adopts the techniques introduced in [29] and generates SQL queries corresponding to NL queries from Watson Assistant.\nRELATED WORK: A new promising research direction is to use deep learning techniques as the foundation for NLIDBs [3, 10, 36] . The basic idea is to formulate the translation of NL to SQL as an endto-end machine translation problem. In general, these new approaches show promising results, but they have either only been demonstrated to work for single-table data sets or require large amounts training data. Hence, the practical usage in realistic database settings still needs to be shown. An in-depth study of the natural language interfaces to databases can be found in [2] .",
    "rq": " u: what do you mean by effective?\n u: how about for fluocinonide?"
}
{
    "intro": "Introduction: The creation of artificial intelligence (A.I.) involves challenging components that include advancing machine learning, natural language processing (NLP), and conversational design. Past research has shown the importance of trustworthiness, personalization, empathy, overlapping personality traits, but these studies have not delved into the concept of fun personas [3, 9, 11, 14] . In this paper we discuss the methods, findings, and results that addresses how a fun persona was perceived by users, voice-based versus text-based implications for fun personas, and cultural considerations between a Western and non-Western community.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Automatically generating question-answer pairs from unlabeled text passages is of great value to many applications, such as assisting the training of machine reading comprehension systems [10, 44, 45] , generating queries/questions from documents to improve search engines [17] , training chatbots to get and keep a conversation going [40] , generating exercises for educational purposes [7, 18, 19] , and generating FAQs for web documents [25] . Many question-answering tasks such as machine reading comprehension and chatbots require a large amount of labeled samples for supervised training, acquiring which is time-consuming and costly. Automatic question-answer generation makes it possible to provide these systems with scalable training data and to transfer a pre-trained model to new domains that lack manually labeled training samples.\nINTRODUCTION: Despite a large number of studies on Neural Question Generation, it remains a significant challenge to generate high-quality QA pairs from unstructured text at large quantities. Most existing neural question generation approaches try to solve the answer-aware question generation problem, where an answer chunk and the surrounding passage are provided as an input to the model while the output is the question to be generated. They formulate the task as a Sequence-to-Sequence (Seq2Seq) problem, and design various encoder, decoder, and input features to improve the quality of generated questions [10, 11, 22, 27, 39, 41, 53] . However, answeraware question generation models are far from sufficient, since question generation from a passage is inherently a one-to-many mapping. Figure 1 shows an example of this phenomenon. Given the same input text \"The fight scene finale between Sharon and the character played by Ali Larter, from the movie Obsessed, won the 2010 MTV Movie Award for Best Fight.\", we can ask a variety of questions based on it. If we select the text chunk \"MTV Movie Award for Best Fight\" as the answer, we can still ask different questions such as \"A fight scene from the movie, Obsessed, won which award?\" or \"Which award did the fight scene between Sharon and the role of Ali Larter win?\".\nINTRODUCTION: We argue that when a human asks a question based on a passage, she will consider various factors. First, she will still select an answer as a target that her question points to. Second, she will decide which piece of information will be present (or rephrased) in her question to set constraints or context for the question. We call this piece of information as the clue. The target answer may be related to different clues in the passage. Third, even the same question may be expressed in different styles (e.g., \"what\", \"who\", \"why\", etc.). For example, one can ask \"which award\" or \"what is the name of the award\" to express the same meaning. Once the answer, clue, and question style are selected, the question generation process will be narrowed down and become closer to a one-to-one mapping problem, essentially mimicking the human way of asking questions. In other words, introducing these pieces of information into question-answer generation can help reduce the difficulty of the task.\nINTRODUCTION: In this paper, we propose Answer-Clue-Style-aware Question Generation (ACS-QG) designed for scalable generation of highquality question-answer pairs from unlabeled text corpus. Just as a human will ask a question with clue and style in mind, our system first automatically extracts multiple types of information from an input passage to assist question generation. Based on the multiaspect information extracted, we design neural network models to generate diverse questions in a controllable way. Compared with existing answer-aware question generation, our approach essentially converts the one-to-many mapping problem into a one-to-one mapping problem, and is thus scalable by varying the assistive information fed to the neural network while in the meantime ensuring generation quality. Specifically, we have solved multiple challenges in the ACS-aware question generation system:\nINTRODUCTION: What to ask given an unlabeled passage? Given an input passage such as a sentence, randomly sampling <answer, clue, style> combinations will cause type mismatches, since answer, clue, and style are not independent of each other. Without taking their correlations into account, for example, we may select \"how\" or \"when\" as the target question style while a person's name is selected as the answer. Moreover, randomly sampling <answer, clue, style> combinations may lead to input volume explosion, as most of such combinations point to meaningless questions.\nINTRODUCTION: To overcome these challenges, we design and implement an information extractor to efficiently sample meaningful inputs from the given text. We learn the joint distribution of <answer, clue, style> tuples from existing reading comprehension datasets, such as SQuAD [36] . In the meantime, we decompose the joint probability distribution of the tuple into three components, and apply a threestep sampling mechanism to select reasonable combinations of input information from the input passage to feed into the ACSaware question generator. Based on this strategy, we can alleviate type mismatches and avoid meaningless combinations of assistive information.\nINTRODUCTION: How to learn a model to ask ACS-aware questions? Most existing neural approaches are designed for answer-aware question generation, while there is no training data available for the ACSaware question generation task. We propose effective strategies to automatically construct training samples from existing reading comprehension datasets without any human labeling effort. We define \"clue\" as a semantic chunk in an input passage that will be included (or rephrased) in the target question. Based on this definition, we perform syntactic parsing and chunking on input text, and select the chunk which is most relevant to the target question as the clue. Furthermore, we categorize different questions into 9 styles, including \"what\", \"how\", \"yes-no\" and so forth, In this manner, we have leveraged the abundance of reading comprehension datasets to automatically construct training data for ACS-aware question generation models.\nINTRODUCTION: We propose two deep neural network models for ACS-aware question generation, and show their superior performance in generating diverse and high-quality questions. The first model employs sequence-to-sequence framework with copy and attention mechanism [1, 3, 43] , incorporating the information of answer, clue and style into the encoder and decoder. Furthermore, it discriminates between content words and function words in the input, and utilizes vocabulary reduction (which downsizes the vocabularies for both the encoder and decoder) to encourage aggressive copying. In the second model, we fine-tune a GPT2-small model [34] . We train our ACS-aware QG models using the input passage, answer, clue, and question style as the language modeling context. As a result, we reduce the phenomenon of repeating output words, which usually exists with sequence-to-sequence models, and can generate questions with better readability. With multi-aspect assistive information, our models are able to ask a variety of high-quality questions based on an input passage, while making the generation process controllable.\nINTRODUCTION: How to ensure the quality of generated QA pairs? We construct a data filter, which consists of an entailment model and a question answering model. In our filtering process, we input questions generated in the aforementioned manner into a BERT-based [9] question answering model to get its predicted answer span, and measure the overlap between the input answer span and the predicted answer span. In addition, we also classify the entailment relationship between the original sentence and the question-answer concatenation. These components allow us to remove low-quality QA pairs. By combining the input sampler, ACS-aware question generator, and the data filter, we have constructed a pipeline that is able to generate a large number of QA pairs from unlabeled text without extra human labeling efforts.\nINTRODUCTION: We perform extensive experiments based on the SQuAD dataset [36] and Wikipedia, and compare our ACS-aware question generation model with different existing approaches. Results show that both the content-separated seq2seq model with aggressive copying mechanism and the extra input information bring substantial benefits to question generation. Our method outperforms the stateof-the-art models significantly in terms of various metrics such as BLEU-4, ROUGE-L and METEOR. With models trained on 86, 635 of SQuAD data samples, we can automatically generate two large datasets containing 1.33 million and 1.45 million QA pairs from a corpus of top-ranked Wikipedia articles, respectively. We perform quality evaluation on the generated datasets and identify their strengths and weaknesses. Finally, we also evaluate how our generated QA data perform in training question-answering models in machine reading comprehension, as an alternative means to assess the data generation quality.",
    "relatedWork": "RELATED WORK: In this section, we review related works on question generation.\nRELATED WORK: Rule-Based Question Generation. The rule-based approaches rely on well-designed rules or templates manually created by human to transform a piece of given text to questions [4, 18, 19] . The major steps include preprocessing the given text to choose targets to ask about, and generate questions based on rules or templates [42] . However, they require creating rules and templates by experts which is extremely expensive. Also, rules and templates have a lack of diversity and are hard to generalize to different domains.\nRELATED WORK: Answer-Aware Question Generation. Neural question generation models are trained end-to-end and do not rely on hand-crafted rules or templates. The problem is usually formulated as answeraware question generation, where the position of answer is provided as input. Most of them take advantage of the encoder-decoder framework with attention mechanism [10, 11, 22, 27, 39, 41, 53] . Different approaches incorporate the answer information into generation model by different strategies, such as answer position indicator [27, 53] , separated answer encoding [23] , embedding the relative distance between the context words and the answer [42] and so on. However, with context and answer information as input, the problem of question generation is still a one-to-many mapping problem, as we can ask different questions with the same input.\nRELATED WORK: Auxiliary-Information-Enhanced Question Generation. To improve the quality of generated questions, researchers try to feed the encoder with extra information. [12] aims to generate questions on different difficulty levels. It learns a difficulty estimator to get training data, and feeds difficulty as input into the generation model. [25] learns to generate \"general\" or \"specific\" questions about a document, and they utilize templates and train classifier to get question type labels for existing datasets. [22] identifies the content shared by a given question and answer pair as an aspect, and learns an aspect-based question generation model. [16] incorporates knowledge base information to ask questions. Compared with these works, our work doesn't require extra labeling or training overhead to get the training dataset. Besides, our settings for question generation dramatically reduce the difficulty of the task, and achieve much better performance.\nRELATED WORK: Multi-task Question Generation. Another strategy is enhancing question generation models with correlated tasks. Joint training of question generation and answering models has improved the performance of individual tasks [38, 44, 45, 47] . [27] jointly predicts the words in input that is related to the aspect of the targeting output question and will be copied to the question. [56] predicts the question type based on the input answer and context. [55] incorporates language modeling task to help question generation. [51] utilizes question paraphrasing and question answering tasks to regularize the QG model to generate semantically valid questions.",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: The application of chatbots in mental health is experiencing considerable growth. Research has shown that people who are deeply distressed prefer confiding their anxieties to chatbots rather than to humans [14, 22] because they fear being rejected or judged [1] . Such anxieties of being stigmatized by others often cause them to avoid reaching out to professionals for proper help [16, 25] . A number of studies have used chatbots for therapy or counseling [4, 31, 32] . For example, a therapy chatbot called \u0142Woebot\u017e utilized a chatbot to explore its feasibility to reduce students' mental health problems and showed that it relieved symptoms of anxiety and depression [12] .\nINTRODUCTION: Previous research on chatbots in mental healthcare has primarily focused on supporting people who have already been diagnosed with such mood disorders as depression. However, early detection of growing mental health problems is also crucial before symptoms develop for two main reasons: (1) people tend to be insensitive to their own mental status when they are distressed [41] , and (2) treatment typically takes longer when recovering from severe symptoms than from mild symptoms. Therefore, it would be useful if a chatbot could detect changes in mental status based on daily conversation behavior.\nINTRODUCTION: Journaling, which can be conducted with a chatbot, is also common among those who are experiencing mental issues because it provides effective stress relief [2] . Furthermore, people engage in more truthful self-disclosure to chatbots than in face-to-face interviews [23] or through web surveys [19] . Hence, chatbots might encourage self-disclosure through journaling and detect potential health issues.\nINTRODUCTION: In this paper, we analyzed the dialogue content that people daily recorded through a chatbot \u00a5citeYi-Chieh Lee et al.. 30 participants entered daily occurrences and their moods into chatbots for three weeks. In the analysis, we divided the participants' data into three groups based on the changes in their mental status before and after this study and analyzed the differences among the three groups. We hypothesized that we could assess the mental statuses of users from their journaling behavior by indicators that detect depression from such written texts as social media and blog posts. The following are the main contributions of our paper:\nINTRODUCTION: \u2022 Our findings suggest that we can detect a person whose mental status has deteriorated from self-disclosure data through chatbots. \u2022 We found that people whose mental states have deteriorated during the study gradually used fewer positive emotion and concrete words. They also began to use more negative emotion words when describing their daily experiences and feelings to the chatbot.",
    "relatedWork": "RELATED WORK AND HYPOTHESES 2.1 Journaling through chatbots: In the practice of journaling, people write about their experiences, feelings, and thoughts [43] . This relatively common practice effectively reduces stress and anxiety [43] . When people get distressed, they tend to write more deeply about their stress that they usually avoid disclosing to others [42] .\nRELATED WORK AND HYPOTHESES 2.1 Journaling through chatbots: Given the proliferation of smartphones, researchers have developed chatbots to facilitate user journaling. Studies [10, 12] have described the positive effects of deploying chatbots, including eliciting honest self-disclosure about personal experiences and emotions [23, 35] , improving self-awareness, and raising self-reflection. Previous works also concluded that such disclosure is promoted by chatbots. For example, Gale et al. found that when people are interviewed by a virtual agent, they tended to disclose more about their depressed thoughts than when interviewed face-to-face by a human [23] . Bhakta et al. showed that especially on sensitive topics such as drugs and gender, people more disclose to a embodied virtual agent than to a human. [5] If a chatbot can further assess users' mental status from their text input, it may lead them to access mental professionals when help is needed. Indeed, Delahunty et al. [9] suggested that such a feature may facilitate real-time instant crisis support for those suffering from depression by identifying mental health issues from daily written texts through chatbots. However, there is a lack of research addressing if and how such contents can be used for automatically assessing mental status.\nPredicting mental states from written text: Previous works developed methods to estimate the severity of depression by analyzing user's writing. In sociolinguistics, Oxman argued that linguistic analysis can detect groups who are suffering from depression and paranoia [33] .\nPredicting mental states from written text: Some works have explored the automated classification of psychological disorders based on such observed differences in communicative behavior [8] . For example, Choudhury et al. found that in a social-media-posting context (e.g., Twitter and Facebook), as a person's depression deepens, the amount of posts and the use of third-person pronouns decreased and the use of negative emotion words and first-person pronouns increased [7] . Resnik et al. [36] analyzed essays and showed that people with mental health problems used fewer positive emotion words and more negative emotion words than people without mental health issues. Gao et al. showed that in social media posts the more depressed a user is, the less concrete their tweets are. For example, the tweet \"I played sports\" is less concrete than \"I played tennis\" [13] . Several studies demonstrated that depressed people tend to use more first-person pronouns in their essays than those who are not depressed [28, 38, 39] .\nHypotheses: In this work, we examined the relationship between the tendency found in textual data that people input to chatbots and their mental status. We propose the following hypotheses using indicators that detect depression from written texts in previous researches.\nHypotheses: Previous works described how people's text input to social media posts considerably decreases when they get depressed [7] . However, this may be due to the presence of others on social media sites. In other words, people may feel reluctant to write negative things when they know that other people are reading their posts. According to [24] , this is true even when posts are anonymous because people worry about getting judgmental comments from others. On the other hand, people can disclose negative thoughts (e.g., anxieties) to a chatbot without worrying about peer pressure or retribution. People might disclose more to a chatbot when they are experiencing mental health issues. Based on the above consideration, we posed the following question: RQ1: How does the amount of words change over time in chatbot conversations when people are experiencing stress or mental health issues?\nHypotheses: Regarding the type of words used, Choudhury et al. [7] found that when people are depressed, they include more first-person pronouns in their social media posts and fewer third-person pronouns. However, in journaling with a chatbot, people naturally talk about themselves because chatbots prompt them to focus on their moods, feelings, and the experiences associated with them. As a result, first-person pronouns are much more likely to appear in conversations with a chatbot than in social media posts. Since the tendency to use pronouns may differ from those found in social media, we also asked the following question:\nHypotheses: RQ2: How does the use of first-person pronouns and third-person pronouns change over time in conversations with a chatbot when people are experiencing mental health issues?\nHypotheses: In addition to the use of first-and third-person pronouns, previous works reported that the use of negative emotion words in social media posts increased as the degree of depression increased [7] . Similarly, Resnik et al. [36] found that depressed people used fewer positive emotion words in their writing (essays) than the non-depressed. We hypothesize that a similar tendency will be found when people talk with a chatbot: H1: As people experience more mental health problems, their use of negative emotion words increases and their use of positive words decreases over time in chatbot conversations.\nHypotheses: Finally, another previous work argued that the more depressed users are, the fewer concrete words they use in their tweets [13] . Matsumoto et al. showed that since describing negative moods or feelings may fuel negative feelings, people might avoid talking about their negative feelings [27] . This phenomenon could occur in the same way even when prompted by a chatbot. Therefore, we hypothesize that:\nHypotheses: H2: As people begin to experience mental health issues, their use of concrete words will gradually decrease in chatbot conversations.\nHypotheses: By testing our two hypotheses, we uncover whether we can detect individuals whose mental status gradually deteriorated by analyzing chatbot conversations.",
    "rq": " rq1: how does the amount of words change over time in chatbot conversations when people are experiencing stress or mental health issues?\nrq2: how does the use of first-person pronouns and third-person pronouns change over time in conversations with a chatbot when people are experiencing mental health issues?"
}
{
    "intro": "INTRODUCTION: Nowadays dialog systems with a fixed goal have already achieved many functionalities, such as the Let's Go! [15] bus system which provides bus schedules, and Siri from Apple, which serves as a personal assistant, completing tasks like setting up an alarm clock. These spoken dialog systems, such as customer services phone lines, are not user friendly. Users get irritated throughout the interactions, and most of them prefer to talk to a real human rather than the dialog system. Even users who are able to finish the task in the end do not enjoy the conversational experience. The reason for user complaint is that dialog systems do not model users' engagement and attention throughout the conversation. We are at the point where we must consider how to make the dialog system not only achieve the task goal, but also make the conversation natural and effective.\nINTRODUCTION: The goal of my doctoral work is to model engagement and attention in conversations. Engagement and attention are crucial in understanding human-human dynamics, and are also important in designing autonomous dialog systems.\nINTRODUCTION: Engagement is not well understood in the human-human communication context, in part because it has only been pursued in the sociological and psychological communities as part of general communication studies, and in part because in artificial intelligence, much of the focus has been on language understanding and production, rather than on the fundamental problems of how to start a connection and the role of verbal and non-verbal behaviors in connecting. We define engagement as \"the process by which two (or more) participants establish, maintain and end their perceived connection during interactions they jointly undertake\" [17] . We use multimodal features to model engagement in a conversational setting in real time. We further plan to coordinate with users' engagement by using different conversational strategies.\nINTRODUCTION: Attention is another conversation metric. Much work focuses on how visual attention or gaze facilitate multiparty turn taking. However, we argue that attention is not simply about visual attention, but is also about cognitive attention, which is equally important but harder to detect and model. There are situations that users are cognitively focused on the task, but are visually looking away from the agent. We aim to model users' attention with respect to the task content. We then coordinate the agent behaviors with users' attention both cognitively and visually to achieve more natural and effective conversations [21] .\nINTRODUCTION: We intent to make our multimodal dialog framework available to the public in a distributed cloud version.",
    "relatedWork": "RELATED WORK: Different multimodal features and machine learning algorithms are used to predict engagement or involvement in human-human conversations. Previously, people used audio features, such as intensity, fundamental frequency and speech rate [9] , or combining audio feature with some visual features such as, hand blob [7] to predict engagement. Some work use more fine grained features, such as Oertel et al. [13] used gaze features obtained by an eye tracker to model engagement and involvement in multiparty conversations. However, all the engagement prediction work is conducted on human-human conversation and is all the models are trained and tested offline.\nRELATED WORK: For attention modeling work, situated and multiparty systems often incorporate attention and gaze in their models for turn taking and interaction planning [18] [4] . Sciutti et al. [16] used gaze as an implicit signal for turn taking in a robotic teaching context. In an in-car navigation setting, situated incremental speech synthesis that accommodates user's cognitive load was shown to improve user experience but not user's task performances [10] . However, no work has investigated how to coordinate speech production of the dialog system with a user's visual and cognitive attention with respect of the task.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Conversational agents 1 are now pervasive, integrated into mobile phones, smart speakers, and even in cars. Remarkable advancement of machine learning is causing a seismic shift, in that conversational agents are now able to understand human speech and transform text into speech in a similar way to humans [36] in everyday living spaces (even on-the-go). Naturally, this created interminable possibilities, uncovering novel, productive and useful experiences with conversational agents for accessing and interacting with digital services in many and diverse applications including HCI [27] , customer experience [30] , conversational commerce [31] , medicine [38, 39] , entertainment [21] , education [29] , and social work [6] .\nINTRODUCTION: For long, context-aware computing research has attempted to understand situational awareness from acoustic signals, e.g., surrounding events [23, 25] and human speech emotion [13] . Unfortunately, the implications of this research in our everyday experience are still limited. None of the commercial-grade conversational agents (Alexa, Siri, Google, Cortana, etc.) today can react and adapt to users' situational context. We argue that simple adjustments of the interaction style of the agents' responses can increase users' conversational experience with these agents. For instance, ambient context information retrieved from audio signals can be used to respond to users in a more calming or alarming manner, in a quieter or louder fashion. It is also possible to adapt the agents' responses based on a user's acoustic context. When the agents are asked to provide a recipe, they can provide a full recipe when a user is cooking at a kitchen or an ingredient list when a user is in a grocery store. Such non-speech contextual information would provide important cues to enable conversational agents to provide more customised and fulfilling experiences across different environments.\nINTRODUCTION: The development of conversational agent systems to understand users' acoustic environments in real-time has proven challenging. Identifying a proper moment for inferring acoustic context in-thewild in uncontrolled environments is an incredibly difficult problem. The moment the should be long enough for the accurate recognition of ambient contexts, but also short enough to provide responsive responses. With the study, we discover an opportunity of a 1-secondlong pause naturally made between a wakeup word and a user query. However, although numerous audio datasets are available, most of them are not suitable for ambient context analysis. Moreover, the latency requirement of these systems for faster responses makes it extremely hard to deploy such systems in the real-world.\nINTRODUCTION: To this end, we first systematically explore the design space for a context augmented conversational agent, including analysis of input segment dynamics and computational alternatives. We identify an opportunity for seamlessly recognising ambient contexts without affecting user experience. Then, we present a light-weight purposebuilt deep neural network solution for ambient context analysis. The proposed solution consists of an intelligently designed input segment for capturing audio data, and a neural network model that uses audio embedding generated by VGGish model [15] . For training this network, we leverage Google's AudioSet that contains 5.8 thousands of hours of audio with 527 different event classes from white noise to animal sounds. Despite its richness and diversity of content, this dataset cannot be directly used for the purpose of ambient audio analysis on a short-duration signal due to its poor annotation granularity, i.e., 10 seconds. To address this caveat, we adopt a crowdsourcing-based and quality-aware annotation strategy and transform a subset (60K) of this dataset containing ambient audio events with a duration of 1-second and accurate annotation. On top of the trained model, we prototype an ambient contextaugmented conversational agent as a concrete manifestation of our solution and showcase several use cases.\nINTRODUCTION: Building on this data, our model achieves an F 1 score of 0.80 and an inference latency of 3 ms on NVidia DGX Station and 360 ms on Raspberry Pi 3+. We further train a distilled network optimised for both recognition accuracy and latency and achieve an F 1 -score of 0.73 with inference latency of 1 ms on NVidia DGX Station and 10 ms on Raspberry Pi 3+. Combining these and the rest of our results, we show that it is possible to infer ambient audio context (e.g., crowd sound, background chatter sound, and footstep sound) at a fraction of the time required for traditional conversational agents, thus allowing their incorporation without compromising agents' responsiveness.\nINTRODUCTION: The main contributions of this work are as follows:\nINTRODUCTION: \u2022 We demonstrate that it is possible to infer ambient acoustic contexts under extreme latency requirements using a light-weight deep neural network. This ability uncovers a unique opportunity for conversational agents to augment their contextual awareness and adapt behaviour. \u2022 We offer a well-curated dataset suitable for ambient acoustic context analysis. The refined temporal granularity of this dataset enables most popular audio models to be more fine-tuned, whose input signatures are often above one second. As such, it opens up brand-new avenues for the audio-based interactive system design. \u2022 We present a concrete manifestation of our solution in designing a context-aware conversational agent and demonstrate use cases on top of the prototype system.\nINTRODUCTION: In what follows, we present the overall design space and describe the dataset and its construction method. Next, we provide an in-depth technical view of our context models. We present the evaluation of the system and a prototype application. Then, we revisit the related past research before concluding the paper.",
    "relatedWork": "RELATED WORK 7.1 Acoustic Event Classification: There have been many research efforts on understanding activities and ambient contexts from audio signals. The AmbientSense application [32] processes audio signal from smartphones and showed reasonable performance on classifying 23 context of daily life. SoundSense [26] detects multiple speech, music and ambient sound categories based on mobile platforms. Rossi et al. [33] showed potential of using MFCC features on Gaussian Mixture Model (GMM) to recognize contexts using Freesound dataset, which is a popular public dataset that contains 120,000 annotated audio clips. In recent years, deep neural network has been adopted in audio sensing. Lane et al. presented DeepEar [22] that classifies high-level activities using a lightweight deep neural network. Since Google released AudioSet [10] that contains 5.8 thousand hours of audio extracted from YouTube videos with 527 sound events, numerous efforts have been introduced to build deep learning model for audio understanding. Hershey et al. [15] presented a CNN architecture for large scale audio classification on AudioSet, achieving mean precision of 0.381 on over 400 sound events using ResNet-50. Kong et al. [20] and Yu et al. [42] employed various attention model based techniques to improve the classification accuracy. Lupta et al. [23] and Liang et al. [25] fine-tuned the existing pre-trained model [15] for various domestic events using augmented data. In fine-tuning stage, they augmented the existing AudioSet with various sound effects or random noise for more robust training set. These efforts has focused on building an accurate model to recognise various acoustic events from feature-based machine learning to deep neural network. In this work, we target the augmentation of conversational agents as a main problem and identify its unique design requirements for the recognition of ambient contexts, i.e., recognising ambient contexts with 1-second audio signal. To this end, we offer a partial AudioSet dataset well-curated through crowdsourcing and present a purposefully-built recognition model of ambient acoustic events by adopting the existing audio sensing techniques.\nConversational Agents with Context: Research community has studied on various aspects of conversational agents as increasing number of text-based chatbots and smart devices with voice interface has been adopted in everyday life. Cohen et al. [8] pointed out that recognizing users attention is an important factor to improve current conversational agents. Some [28, 43] focuses on understanding user needs and evaluating user satisfaction. Yang et al. [41] has studied user's affective experiences with the conversational agents. Also, as argued in Section 2, most of the attempts on building conversational agents have been focused on building the pipeline for automatic speech recognition, language processing, and information delivery over voice [4, 7, 12, 14, 44] . Ongoing efforts to build contextual chatbots [18, 37, 45] focus on retrieving and understanding various contexts from text-based conversations. However, augmenting acoustic ambient contexts to adapt the response and interaction style has not been explored much in the research community, even though conversational agents' input can be both user's voice and contextual information [5] . In this work, we argue that the recognition of ambient acoustic contexts will play an important role to augment ambient contexts to the conversational agents and propose design challenges and the core component, the ambient context recognition.",
    "rq": ""
}
{
    "intro": "Introduction: Evaluating the output of a generation system is known to be difficult: since generation is an openended task, the criteria for success can be difficult to define (cf. Mellish and Dale, 1998) . In the current state of the art, there are two main strategies for evaluating the output of a generation system: the behaviour or preferences of humans in response to the output may be measured, or automated measures may be computed on the output itself. A study in-volving human judges is the most complete and convincing evaluation of generated output. However, such a study is not always practical, as recruiting sufficient subjects can be time-consuming and expensive. So automated metrics are also used in addition to-or instead of-human studies.\nIntroduction: When automatically evaluating generated output, the goal is to find metrics that can easily be computed and that can also be shown to correlate with human judgements of quality. Such metrics have been introduced in other fields, including PAR-ADISE (Walker et al., 1997) for spoken dialogue systems, BLEU (Papineni et al., 2002) for machine translation, 1 and ROUGE (Lin, 2004) for summarisation. Many automated generation evaluations measure the similarity between the generated output and a corpus of \"gold-standard\" target outputs, often using measures such as precision and recall. Such measures of corpus similarity are straightforward to compute and easy to interpret; however, they are not always appropriate for generation systems. One of the main advantages of choosing dynamic generation over canned output is its flexibility and its ability to produce a range of different outputs; as pointed out by Paris et al. (2007) , \" [e] valuation studies that ignore the potential of the system to generate a range of appropriate outputs will be necessarily limited.\" Indeed, several recent studies (Stent et al., 2005; Belz and Reiter, 2006; Foster and White, 2007) have shown that strict corpus-similarity measures tend to favour repetitive generation strategies that do not diverge much, on average, from the corpus data, while human judges often prefer output with more variety. Automated metrics that take into account other properties than strict corpus similarity have also been used to evaluate the output of generation systems. Walker (2005) describes several evaluations that used corpus data in a different way: each of the corpus examples was associated with some reward function (e.g., subjective user evaluation or task success), and machine-learning techniques such as reinforcement learning or boosting were then used to train the output planner. Foster and White (2007) found that automated metrics based on factors other than corpus similarity (e.g., the amount of variation in the output) agreed better with user preferences than did the corpus-similarity scores. Belz and Gatt (2008) compare the predictions of a range of measures, both intrinsic and extrinsic, that were used to evaluate the systems in a shared-task referringexpression generation challenge. One main finding from this comparison was that there was no significant correlation between the intrinsic and extrinsic (task success) measures for this task.\nIntroduction: All of the above studies considered only systems that generate text, but many of the same factors also apply to the generation of non-verbal behaviours for an embodied conversational agent (ECA) (Cassell et al., 2000) . The behaviour of such an agent is normally based on recorded human behaviour, which can provide targets similar to those used in corpusbased evaluations of text-generation systems. However, just as in text generation, a multimodal system that scores well on corpus similarity tends to produce highly repetitive non-verbal behaviours, so it is equally important to gather human judgements to accompany any automated evaluation.\nIntroduction: This paper presents three corpus-driven methods of selecting facial displays for an embodied conversational agent and describes two studies comparing the output of the different methods. All methods are based on annotated data drawn from a corpus of human facial displays, and each uses the corpus data in a different way. The first evaluation study uses human judges to compare the output of the selection methods against one another, while the second study uses a range of automated metrics: several corpusreproduction measures, along with metrics based on intrinsic properties of the outputs. The results of the two studies are compared using multiple regression, and the implications are discussed.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": " 2) is there a difference between talking to humans (humanhuman interaction: hhi) and talking to avatars (humancomputer interaction: hci) in terms of narrative?\n 4) are the extracted features effective for identifying narrative skill?\n1) how effective is the automated social skills trainer in helping users improve their narrative skills?"
}
{
    "intro": "INTRODUCTION: Interpersonal communication is foundational to the practice of healthcare [4] . Effective communication between clinicians and patients can support patient-centered care and shared decision-making [4, 98] . Research in computer supported collaborative work (CSCW) has sought to investigate new ways to use computing technology to improve communication between\nINTRODUCTION: \u2022 What aspects of the coaching experience, if any, are uniquely human and do not lend themselves to automated approaches? \u2022 What are the potential advantages, if any, of chatbots for virtual coaching?\nINTRODUCTION: To begin addressing these questions, we conducted a two-week study in which we compared coaching experiences with a conversational agent and with a human coach both delivered via text messaging. Individuals with T2D recruited from economically disadvantaged communities were assigned to one of the two study arms. In the first arm, participants interacted with a scripted chatbot, designed based on an established protocol for goal setting (Brief Action Planning, BAP [37] ); we used a wizard-of-oz technique [46, 66] to imitate the behavior of a chatbot that was under development by our research team. In the second group, participants interacted with a human health coach; all interactions were exclusively over text messaging with no in-person meetings. To reduce variability in coaching styles, coaches were asked to follow the same BAP protocol as the chatbot, but were encouraged to deviate from the protocol when they deemed it necessary in order to improve the coaching experience for their clients. We then examined the participant coaching experience with an emphasis on differences between the two groups.\nINTRODUCTION: We found that participants in both the chatbot and human coaching groups reported generally positive experiences consistent with many of the qualities of successful coaching described in prior studies [78] . We also found some support to the previous suggestion that human coaches have advantages that may be challenging to replicate with automated systems [88] . In our study, human coaches were able to go off-script to express empathy or ask additional questions to tailor their support and expand its focus from health to other aspects of individuals' lives. At the same time, we found that translating human coaching into the medium of text messaging presented many challenges that often led to negative experiences for both participants and coaches. Coaches often struggled to achieve common ground needed to tailor their support and had difficulties gauging how invested participants were and finding appropriate times for interaction. The study also showed that the chatbot had unique advantages as compared to human coaches. For example, while human coaches often felt compelled to shortcut the BAP script and jump to recommendations, chatbot's consistency in providing participants with multiple options contributed to their sense of autonomy.\nINTRODUCTION: These findings tap into fundamental questions within CSCW regarding the role of computing mediation in supporting communications between human stakeholders [13, 29, 80] . They lend further support to previous research highlighting the importance of common ground [15, 18] and the challenges of achieving grounding with short text communications [18] . They also further highlight the importance of mutual awareness among participants during social encounters, a characteristic referred to as social translucence [27] . Our study further expands existing knowledge of these phenomena and illustrates how they impact computer-mediated coaching in health.\nINTRODUCTION: Furthermore, they contribute to the ongoing discussion regarding the need to expand the scope of CSCW to include questions of cooperation and communication between humans and intelligent systems [29, 58, 81] . The continuing increase in the adoption and influence of systems powered by computational intelligence presents pressing need for additional research in order to reach a deeper understanding of such cooperation and to generate design guidelines for future intelligent systems, including intelligent coaching systems in health. Our study showed that intelligent systems have unique strengths in consistency and patience; understanding and leveraging these strengths may help to develop effective intelligent coaching systems that complement coaching by humans.",
    "relatedWork": "RELATED WORK: 2.1 Health coaching and chronic disease self-management Care for chronic conditions like T2D emphasizes changes to daily lifestyle behaviors, like diet, exercise, sleep, and medication adherence [8] . Effective self-management is challenging because it requires motivation, knowledge, and self-efficacy [8] . Health coaching is an effective intervention to support healthy lifestyle changes [23] . While there are many definitions of health coaching, a core feature is its focus on goal-setting, where the coach and client work collaboratively to set a health goal and make plans towards achieving that goal. The coaching process places an emphasis on cultivating a relationship to increase motivation, and facilitate learning and education [78, 103] .\nRELATED WORK: A key consideration of health coaching is the conversational style and tone of the coach, which follows the spirit of motivational interviewing, emphasizing collaboration and respecting the client's autonomy, as opposed to imposing or prescribing directions for change [37, 69] .\nRELATED WORK: Recent work has argued for the importance of personal and human skills to the success of health coaching [88] . For example, Rutjes argued that successful coaching goes beyond achieving goals, and includes building relationships, implicitly adapting to different contextual factors, and cultivating motivation. These characteristics are consistent with other definitions of coaching [78, 103] , but place additional emphasis on human involvement that may be difficult to replicate with technology. However, in-person health coaching interventions are limited in their ability to reach everyone in need of support because there are not enough coaching practitioners, resources are not always available in underserved communities, reimbursement varies, and there are barriers for individuals in accessing coaching support, including access to transportation or time off from work [26, 86, 89] .\nVirtual health coaching: In the face of the challenges posed by in-person coaching, there has been increased focus on virtual coaching interventions, using telecommunication and mobile applications to bring coaching support directly to individuals [86] . Sometimes virtual coaching takes the form of audio or video sessions between a coach and their client. Another, related body of research seeks to use artificial intelligence (AI) to create automated coaching interventions [44] . However, the virtual coaching literature is sometimes vague in its definitions and discuses human-human and human-AI interventions interchangeably [86, 88] . There is growing evidence that virtual coaching can be effective as a part of a hybrid approach in combination with other health apps, particularly apps for self-tracking [34, 86] .\nVirtual health coaching: Technology-mediated coaching is related to the large body of research in telemedicine which suggests that virtual visits can help improve access for rural and underserved populations [65] . Technologies that rely on human coaches are still inherently limited in their ability to reach all those in need, considering the inadequate number of trained practitioners and rapidly growing population with chronic conditions like diabetes or prediabetes [26, 63, 89] . This merits the exploration of fully or partially automated coaching systems. While there is research reporting provider perceptions of telehealth programs, little work has explored the perspectives of virtual coaching practitioners [9, 68] . This is an important gap in understanding the virtual care ecosystem.\nVirtual health coaching: In contrast with technology-mediated coaching, automated coaching seeks to replicate the success of in-person coaching through patient-facing digital tools. While this falls within the large scope of digital interventions to support self-management and wellness, automated coaching interventions emphasize health coaching principles. Coaching interventions are meant to be participant-centered, and respect autonomy by being neither completely user driven, nor fully guided by the app [69] . This contrasts with personal informatics, which includes tools for individuals to explore their self-tracking data for self-discovery and knowledge [56, 57] . These tools are often user-driven and exploratory, and rely on participant initiative and engagement. On the other extreme, automated coaching also contrasts with remote monitoring interventions, or behavioral interventions that primarily push information to users [74, 100] . In remote monitoring, the user enters data, but the process is guided by the needs of the health system or provider. Similar to telemonitoring, with push-based behavioral interventions, alerts from the application are the driving force to guide the user through behavior change. Automated coaching interventions exist within this continuum.\nVirtual health coaching: Automated approaches to health coaching have been an increasing focus in the research community. In particular, because of the back-and-forth nature of coaching, conversational agents are a common interaction approach. Conversational agents for health coaching have a range of focuses and functionality, from strictly goal setting [11] , to various forms of personalization and tailored feedback [17] , to agents designed to respond with empathy [59] . Unfortunately, there is a lack of specificity and consistency in applications described as automated coaches, which makes comparing them difficult, and very few automated coaching interventions reference specific theories or background literature on health coaching [88] .\nConversational artificial intelligence in health: In addition to automated coaching, there has been an increased focus on conversational agents in healthcare. In health, conversational agents are often scripted or rule-based [50, 54] . A common strategy is to build off of an established clinical protocol, for example brief interventions, or cognitive behavioral therapy [30, 54] . Text-based agents, or chatbots, are more common in practice than voice interfaces because of security and usability concerns with currently available spoken word systems, especially among populations with diverse language preferences, proficiencies, and accents [45] .\nConversational artificial intelligence in health: In contrast, the ML community has been striving for ever-more human-like chatbots. While the quest for general conversational intelligence is advancing [1, 87] , most human-emulating bots still include a human-in-the-loop to handle more difficult messages [35] . However, there is also reason to question the approach of seeking convincingly human-like chatbots. People may not prefer human-like conversation with their AIs [16] , and most common consumer chatbots do not converse in a way that humans do with each other [83] . There is also mounting fear and concern about the potential of AI and automation to displace or replace human workers, with potentially deep and profound social and economic consequences [25, 64] . Specifically, in the case of health coaching, it is not clear whether conversational AI could replicate or supplement the work of health coaches, or which approach is preferable for individuals living with chronic conditions like T2D.\nConversational artificial intelligence in health: Somewhere on the continuum between completely scripted chatbots and open-domain AI, researchers in CSCW have argued that conversational agents can take advantage of the affordances of dialogs to engage users in novel ways for behavior change, for example discussing self-tracking data or negotiation around goals [51, 84] . The work described here explores coaching as a domain for conversational interaction, and asks which aspects of health coaching might lend themselves to automated approaches.\nConversational artificial intelligence in health: PACM on Human-Computer Interaction, Vol. 5, No. CSCW, Article 99. Publication date: April 2021.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Conversational interfaces allow users to interact with automated systems using speech or typed in text via \"conversational dialog\". For the purposes of this paper, a conversational dialog consists of a sequence of interactions between a user and a system. The user input is interpreted in the context of previous user inputs in the current session and from previous sessions.\nINTRODUCTION: Conversational interfaces offer greater flexibility to users than menu-driven (i.e., directed-dialog) interfaces, where users navigate menus that have a rigid structure [5, 4] . Conversational interfaces permit users to ask queries directly in their own words. Thus, users do not have to understand the terminology used by system designers to label hyperlinks on a website or internalize the hierarchical menus of a telephone system [3] or websites.\nINTRODUCTION: Recently, conversational interfaces for executing simple transactions and for finding information are proliferating [7, 6] . In this paper, we present a conversational dialog system, Natural Language Assistant (or NLA), that helps users shop for notebook computers and discuss the results of user studies that we conducted with this system.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Intelligent assistants are becoming a prevalent feature on mobile devices. They provide voice control and feedback to mobile device functions (e.g., making phone calls, calendar management, finding places). Users can also search the web or even chat with intelligent assistants. While these novel applications are useful and attractive for users, it is challenging to evaluate and compare them due to the large variability of tasks.\nINTRODUCTION: Evaluation is a central component of many related applications, e.g., search engines, Q&A systems, and recommendation systems.\nINTRODUCTION: These applications are usually evaluated by comparing system-generated answers with \"correct\" answers labeled by human annotators. For example, in web search, we annotate relevant webpages and evaluate using metrics such as mean average precision (MAP) and normalized discounted cumulative gain (nDCG) [14] .\nINTRODUCTION: However, intelligent assistants differ from these applications in that they can involve a wide variety of tasks, ranging from making phone calls and managing calendars, to finding places, finding answers to general questions, and web search. These tasks have different forms of \"correct\" answers. It is expensive to evaluate each task separately using different human judgments and metrics. It is also difficult to use one single setup to evaluate all tasks. In addition, the tasks performed can be personal in nature and the performance of the system depends heavily on users. These factors make it challenging to conduct manual ground-truth-based evaluation.\nINTRODUCTION: To solve these challenges, we adopt approaches similar to recent studies of user satisfaction prediction in web search [1, 3, 6, 7, 17, 32] . These studies developed alternative evaluation approaches by finding and using correlation between explicit ratings of user experience and implicit behavioral signals such as click and dwell time [4, 7] . However, we cannot simply apply user behavior signals in web search to evaluate intelligent assistants due to the wider range and different nature of tasks. These tasks may involve a variety of user intents, diverse topics, and distinct user interaction modes. For example, the process of making a phone call, or navigating to a place, involves a dialog style conversation between user and system, with user requests and system responses very different from those in web search. In addition, intelligent assistants heavily use voice interactions, and it is important to consider the voice signal to assess user experience.\nINTRODUCTION: We introduce a model for evaluating user experience in voiceactivated intelligent assistants. We consider satisfaction as the major indicator of user experience because our study shows that it is consistently correlated with changes in user interests towards the system. Our model predicts whether the user has satisfactory or unsatisfactory experience with an intelligent assistant based on user interaction patterns. Once the model is trained, it can evaluate real traffic of intelligent assistants without human judgments of correct answers for tasks. This makes it a useful and cheap evaluation approach for intelligent assistants' developers, who have abundant user traffic and user logs. Our model includes a sub-model for automatically classifying user-system interaction into dialog actions, a Markov model over action transitions, as well as features related to requests, responses, clicks, and those using acoustic signals.\nINTRODUCTION: Our contributions can be summarized as follows: \u2022 An accurate model for predicting user satisfaction with an intelligent assistant and its components, i.e., speech recognition and intent classification. \u2022 A scheme of categorizing user-system interaction into task-independent dialog actions, and a model to automatically map different actions to this scheme. \u2022 Analysis of user behavior and patterns indicating user experience in intelligent assistants.",
    "relatedWork": "RELATED WORK: There are a number of areas of related work relevant to the research described in this paper. These include (1) methods and metrics for the evaluation of search systems, (2) inferring satisfaction from observed search behavior and (3) dialog act modeling and classification in conversational speech. We cover these in turn in this section.\nRELATED WORK: User behavior modeling has been used extensively for evaluating search systems [1, 4, 6, 7] . Traditionally search system have been evaluated using retrieval metrics such as MAP and nDCG [14] , where a collection of documents, queries and human labeled relevance judgments are used to evaluate search system performance. These metrics are expensive to collect and potentially noisy, given that third-party judges have limited knowledge of the individual user's intent. Additionally, these metrics are query-based. Previous research has shown that search tasks often contain multiple queries related to the same information need [7] . Unfortunately the connections between these queries are ignored by these metrics. Sessionbased DCG (sDCG) [15] does consider the session-context, but still requires manual relevance judgments.\nRELATED WORK: Another line of research has focused on using implicit feedback from user behavior to evaluate search engine. These methods have lower cost, are more scalable, and sourced from the actual users.\nRELATED WORK: Early research on implicit feedback [4] used an instrumented browser to determine if there was an association between explicit ratings of user satisfaction and implicit measures of user interest and identified the measures that were strongly associated with user satisfaction. Huffman and Hochster [13] found a strong correlation with session satisfaction using a linear model encompassing the relevance of the first three results returned for the first query in a search task, whether the information need was navigational, and the number of events in the session. Hassan et al. [7] developed models of user behavior to accurately estimate search success using action sequences of user behavior and showed that this yields better performance compared to models derived from the query-URL relevance of top-ranked results for the first query in a task. Follow-up studies showed that satisfaction ratings can be collected in-situ from users [9] and that action sequence models can be learned in a semi-supervised manner from both labeled and unlabeled data [6] . Ageev and colleagues [1] augmented this approach with additional search features. They also used a game-like strategy for collecting labeled data by asking participants to find answers to questions using web search. All these methods focus on analyzing user behavior when interacting with traditional search systems.\nRELATED WORK: In this work, we extend this line of work by presenting the first study, to the best of our knowledge, of user behavior patterns when interacting with intelligent assistants. We study the action sequences performed by the users and jointly model them with the actions performed by the system to predict user satisfaction. We also study features specifically related to voice input and propose methods to analyze the root cause of dissatisfaction.\nRELATED WORK: Perhaps the biggest difference between traditional web search and intelligent assistants is their conversational nature. In many scenarios, intelligent assistants can refer to the previous requests to understand the user better; e.g. \"show me weather in mountain view\" followed by \"how about in palo alto\", or \"Italian restaurants nearby\" and \"which ones are 4-stars or above\". Therefore spoken dialog systems research is closely related to intelligent assistants. Spoken dialog systems interpret and respond to spoken commands by implementing dialog strategies [2] , and the field has seen steady progress over the past two decades [29] . Since they use speech as the primary (or only) form of user communication, they provide error correction mechanisms to account for the potential errors in the automatic speech recognizer (ASR) output. Recently, partially observable Markov decision Processes (POMDP) has established itself as a solid foundation for managing dialogues, and a comprehensive review can be found in [33] .\nRELATED WORK: Since they also support other forms of interactions, intelligent assistants differ from traditional spoken dialog systems. In addition to voice system response, intelligent assistants provide answers or options in the display, and users can type in the requests and select a displayed result or option. In this sense, intelligent assistants are related to multi-modal conversational systems [11, 20, 30] .\nRELATED WORK: Note that many different taxonomies of dialog acts have been proposed [28] . We do not intend here to propose a new one, but rather to model user and system interaction with the goal of predicting user satisfaction. Our model of system interaction and user is designed independently from any dialog model the system uses. Hence, it is independent of any specific implementation. In contrast with work on learning dialog model transitions [27] we do not attempt to model the most likely dialog sequence, but to use the dialog sequence to model user satisfaction. Our work differs from previous work in offline evaluation of dialog systems [31] , as we do not require manual transcription of speech, and thus once trained, our models can be run online, at-scale, evaluating voice assistants in an automated, unsupervised fashion.",
    "rq": ""
}
{
    "intro": "Introduction: The automation of work processes has a massive impact on the working environment: by 2030, up to 33 percent of employees in Germany could be forced to change their profession due to automation. Whereas in former times, industrial machines accelerated automation, additional factors such as robotics and artificial intelligence are the main drivers of automation today [1] . The effects of automation are perceived differently: in addition to cost savings and increased efficiency on the one hand [2] , there is also a fear of job losses due to automation [3] .\nIntroduction: Recruiting is highly affected by such automation (e.g., [4] , [5] , [6] ). One form of such automation could be chatbots, software programs that automatically simulate human dialogues [7] . In recruiting, these systems can be used to automate communication tasks along the recruiting process. Such assistants are already in place or planned in almost 70 percent of German companies [8] . 1 In this study, chatbots in recruiting are taken as an exemplary study object for workplace automation to find answers to the research question: Does automation anxiety have an influence on technology acceptance? This study begins with a discourse of the associated theoretical background. In the third part, the object of study and the research design are explained. The fourth part focuses on the methodology of the study, while the last part offers insights on the first conclusions. and resource utilization optimization) have already been the subject of research [9] , research on the perception of automation in recruiting is not far advanced. Computer anxiety, in general, has already been regarded with acceptance [10] . Still, the extent to which the fear of digital innovations influences the acceptance of such a new technological system, e.g., of chatbots, into processes such as recruiting has not yet been researched in depth. This pre-study focuses on this research gap.\nIntroduction: Our approach is based on the original Technology Acceptance Model (TAM) by [11] , which integrates job relevance as well as automation anxiety, inspired by computer anxiety of the TAM3 by Venkatesh/Bala [10] . As proposed by [10] , [12] with TAM2 and TAM3, the construct attitude is abandoned because its influence is limited in involuntary usage scenarios [13] , in this case, recruiting chatbot usage by recruiters.\nIntroduction: The Technology Acceptance Model (TAM) and its successors TAM2 and TAM3 have been the object of extensive empirical research and related studies (e.g., [12] , [14] - [16] ). The core constructs of the model, perceived ease of use (PEOU) and perceived usefulness (PU), have been widely recognized to influence technology acceptance in the form of behavioral intention (BI) and use (U) [14] . The TAM extensions have supplemented the core model with additional influencing factors and adapted it to different contexts. Regarding chatbots and Human Resources (HR), many attempts have been made to investigate the acceptance level via TAM and TAM-related models (e.g., [47] regarding TAM chatbot acceptance factors in the context of insurance, [48] focusing on UX-relevant TAM factors for chatbots, and [49] regarding the influence of technology readiness on technology acceptance in the context of e-HRM). However, the fear of being substituted by automation technology such as chatbots in areas characterized by repetitive information tasks as found in recruiting remains unexamined.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: It has been shown in the studies of psychology and behavioral economics that personality, a set of individual patterns and differ-Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\nINTRODUCTION: WSDM 2017, February 06-10, 2017, Cambridge, United Kingdom c 2017 ACM. ISBN 978-1-4503-4675-7/17/02. . . $15.00 DOI: http://dx.doi.org/10.1145/3018661.3018717 ences based on values, attitudes, personal memories, social relationships, habits, and skills [16] , has an effect on individual behaviors, such as occupational proficiency [1] and economic decision [8] . Currently, to measure personality, state-of-the-art surveys or interview-based approaches in psychology rely heavily on retrospective self-reports and thus are vulnerable to memory, not to mention the well-known experimenter effects. In addition, the time and money costs, as well as data granularity, limit the effectiveness and efficiency of these approaches. For example, the 240item NEO-PI-R [4] and the 300-item International Personality Item Pool (IPIP) [9] , require examinees long time to complete timeconsuming questionnaires to obtain results. Complicated procedures in data collection hinder the utilization of personality results, such as personalized recommendation services and real-time monitoring for mental health based on psychological features. Therefore, a convincing and effective method of personality evaluation has significant values to psychological studies and relevant Internet services.\nINTRODUCTION: In contrast to traditional questionnaire-based methods, psycholinguists suggest analyzing the relationship between users' language features and psychological traits [22, 20] . By understanding the role of language patterns in predicting user personality, computerbased methods are proposed to take users' language usage in social media into consideration, such as the IBM Waston Personality Insights project [10] . With the rise of web and social media, individuals are generating considerable digital traces besides users' language features. Currently, the sequential text data in users' social media contents are utilized for predicting user preferences and characteristics [24, 25] . However, to the best of our knowledge, there has been little exploration of how to predict user personality by leveraging the heterogeneous information embedded in these digital traces.\nINTRODUCTION: In this work, we intend to address the following three questions:\nINTRODUCTION: The first one is whether self-language usage is the only effective feature to learn user personality. In prior studies, it is mainstream to study users' personality traits by their texts in tweets. LIWC, which is a psychological lexicon, has been used to evaluate user personality according to the usage of words in different semantic categories. Fewer studies have considered other features, such as user likes and hash-tags [23, 19] . However, by observing the social media usage and Big Five 1 personality [12] data collected from thousands of volunteers, we notice curious correlation between heterogeneous information and user personality. For instances, we discover that users with a high score in Agreeableness are likely to use smile emoticons. In contrast, users with a high score in Neuroticism are likely to use theatrical emoticons to exaggerate their feelings. More examples are found, including the relationship between personality and users' gender, place of birth, major, choice of avatars and etc. Similar research has been done in the field of psychology. Taking avatars as an example, it is believed that neurotic participants in games will select avatars with more discrepancies from themselves [3] . Moreover, people with a high score in Openness to new experiences are more likely to choose avatars with fewer discrepancies from themselves [6] . Based on such research, and similar findings in our dataset, we pose a hypothesis that heterogeneous information of users' social media should be leveraged for a better understanding of user personality.\nINTRODUCTION: The second one is how to fully leverage the heterogeneous information in users' digital traces. In this work, we present a Heterogeneous Information Ensemble framework, called HIE, to solve this problem. By integrating heterogeneous information including self-language usage, avatar, emoticon, and responsive pattern, HIE first fully leverage the semantic representations learned from the heterogeneous feature engineering part and then apply an ensemble method to integrate all the learned knowledge. We compared the proposed HIE with state-of-the-art computational models from IBM Waston [10] and Mypersonality [23, 19] projects. The results are promising: our approach outperformed the previous methods by up to 70.07% when making prediction on Agreeableness.\nINTRODUCTION: Last but not the least, we need to answer the third question: how to apply our approach in a real-world setting. We developed a personalized chatbot, DiPsy, to figure out the utility of HIE in a realworld interactive setting, in which the chatbot reports users' scores in Big Five personality by leveraging heterogeneous information in digital traces, and improves the accuracy of prediction results based on its understanding of interaction through natural conversation logs. Given the unique role of personality in mental health, we plan to build DiPsy as a personalized digital psychologist, who is able to evaluate, diagnose, and treat users' mental process through natural conversations and social media data in the near feature.\nINTRODUCTION: In summary, the contributions of this work are as follows,\nINTRODUCTION: \u2022 We propose a new personality prediction and evaluation framework using heterogeneous information in users' digital traces, including self-language usage, avatar, emoticon, and responsive pattern on social media. To our knowledge, this is the first work to integrate heterogeneous information to study users' personality traits.\nINTRODUCTION: \u2022 We design heterogeneous feature engineering strategies to extract semantic representations from heterogeneous information on users' social media including tweets, avatars, emoticons and responsive patterns.\nINTRODUCTION: \u2022 We conduct a large-scale offline evaluation and a real-world service deployment, DiPsy, to study the efficiency and effectiveness of our proposed approaches. The results suggest promising benefits.",
    "relatedWork": "RELATED WORK: Broadly, this study falls into the category of the training models that use users' digital traces to evaluate personality traits. The stateof-the-art computational models involve questionnaire-based [1, 5] and data-driven approaches [15, 23, 19, 10] . The Prior work with data-driven methods has considered information including texts [10] , interests [19] , hash-tags and likes [15, 23] . However, most prior work in this line of research only considers simple text information and processes it with closed vocabulary, such as Linguistic Inquiry and Word Count(LIWC) [17] , and open vocabulary methods, such as topic modelling. To the best of our knowledge, little prior work considers user behavior patterns from the angle of diverse features in heterogeneous information on social media. In our study, we apply different strategies to extract semantic representations from heterogeneous information in user digital traces and represent the diverse patterns embedded in it. For instance, we leverage Responsive-CNN to better understand the interaction among users and K-Means clustering with ResNet [11] embedding results to process avatars.\nRELATED WORK: In our work, we also propose a new heterogeneous information ensemble framework, called HIE. Users' tweets, avatars, emoticons, individual profiling, social media friends, and responsivepatterns, are integrated together to improve the performance. In contrast, prior work only considers limited genres of users' digital footprints [15, 23] . Moreover, those methods targeted at single feature, such as latent Dirichlet allocation (LDA) [19] , fail in our case, because the diversity of users' behavior patterns directs the clustering results to be of low correlation with targeted features. We apply a two-step strategy with stacked generalization-based ensemble method to achieve a better performance.\nRELATED WORK: Finally, with the popularity of combining the knowledge of psychology and the technology of computer science, the emergence of mobile psychology and cyber psychology draws people's attention [18] . To figure out the utility of HIE in real world interactive settings, we build DiPsy, a personalized chatbot, which reports users' scores in personality by leveraging heterogeneous information on social media, and revises the results based on its understanding of the interaction during natural conversations. To the best of our knowledge, there is little prior work capable of learning users' psychological traits via digital footprints and revising results with dialog information.",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Mindfulness meditation is a form of attention training in which practitioners are taught to focus their mind on the present moment, to the exclusion of distractions and thoughts [23] . Recent fMRI studies have demonstrated a link between mindfulness meditation and mood [13] . Focusing less on persistent negative thoughts is key to alleviating many depressive symptoms, and may account for the effectiveness of mindfulness meditation in treating mood disorders such as anxiety and depression [8, 17] . A significant body of literature has also demonstrated the beneficial effects of mindfulness meditation on a wide variety of physical [1] and mental disorders [17] , such as chronic pain, post-traumatic stress disorder, and insomnia, in addition to treating symptoms of other chronic conditions, including cancer, hypertension, diabetes, HIV/AIDS [5] .\nINTRODUCTION: Mindfulness meditation requires training and practice to be effective. Novices in particular, may need significant coaching and practice to become proficient and be able to regulate their thoughts and emotions. Unfortunately, cost and logistical barriers prevent many people from attending mindfulness meditation training courses. While many self-help books, videos, and applications [6, 20, 24] have been developed for meditation training, these methods lack the individualized instruction of a human trainer, and suffer the same high attrition rates of any longitudinal behavior change program, for those wanting to make regular meditation a habit.\nINTRODUCTION: In recent years, many HCI researchers have explored how technology could create and support a mindfulness experience [25, 33, 40] . Several studies have demonstrated promising results regarding stress reduction, and improving mindfulness for participants who have undergone meditation training using mobile applications [34] , Virtual Reality [25] , and sensory-based interfaces [29] . Zhu et al. reviewed 102 applications for digital mindfulness and proposed a framework for classifying this large number of commercial mindfulness applications into four categories including; digitalized, personalized, and quantified mindfulness applications [46] . The increasing number of these applications shows that both consumers and developers are very interested in leveraging technology to improve their mindfulness experiences. However, existing applications still largely lack individualized tailoring and instruction. Although most of the applications provide different types of mindfulness instructions and guidance through text, audio and video, only a limited number of them offer real-time sensing and adaptive feedback based on the user's performance [46] .\nINTRODUCTION: To address these shortcomings, we have developed a virtual meditation coach (Figure 1 ) that guides users through meditation sessions. Virtual coaches that simulate counseling sessions with an expert human have now been successfully developed to address a range of health behaviors [3] . They have also been demonstrated to be effective in promoting patient adherence in longitudinal treatments [4] . In addition to simulating the verbal and nonverbal behavior of a human counselor, they can provide privacy and accessibility, decreasing the expense and eliminating distance barriers associated with attending human-led classes. Thus, these agents can be a useful medium particularly for mindfulness applications, because of their conversational interface, which reduces the cognitive burden of understanding instructions for practitioners.\nINTRODUCTION: The virtual meditation coach is an embodied conversational agent that appears on a touch screen desktop computer. Given the importance of breathing behavior in meditation, and to make the coach interactive and adaptive to users needs during meditation, we integrated a respiration sensor into the coaching system and incorporated user respiration into the system as an input modality and data to provide real-time, tailored feedback.\nINTRODUCTION: In the rest of this paper, we first review related work, then describe the design of the virtual meditation coach system, including novel respiration-based interactions and their validation. Then we describe two studies to evaluate the coach system, first comparing the respiration-based interaction to an equivalent interaction without the respiration sensor input, and second comparing the interactive coach to a non-interactive self-help video of a human meditation trainer. Finally we discuss the findings of both studies and future research.\nINTRODUCTION: The contributions of this work include: development and evaluation of the acceptance and feasibility of 1) an embodied conversational agent in guiding meditation instruction; 2) a chest-expansionbased respiration as an interaction modality; and 3) new interaction techniques using respiration (chest expansion), beyond simple biofeedback. This work takes a step towards the development of interactive and customizable calming applications [42] , which can help individuals to improve their overall wellbeing and mindfulness, in addition to addressing a range of other health problems.",
    "relatedWork": "RELATED WORK: To better situate our work, we briefly review the existing research on interactive systems that have been developed to support meditation training.\nAutomated Meditation Training: In addition to a plethora of self-help books, videos, and audiobased media to help novices learn and practice meditation, there have also been some commercial applications developed to guide users through mindfulness meditation. Headspace [20] is one of the most popular mindfulness application that leads users through a succession of short meditations, guided by an audio recording of a human meditation instructor. In recent years, several research projects have explored ways to support mindfulness activities with automated and interactive interfaces such as mixed-reality environments [33, 40] , haptic sensory systems [29] and wearable devices [35] . In one study, Niksirat and his colleagues presented an interactive mobile application for self-regulation and mindfulness exercise, called PAUSE. Their application detects the user's attention without using any sensors and provides real-time feedback. PAUSE outperformed Headspace in attention improvement, and showed better mindfulness improvement in busy environments [34] .\nAutomated Meditation Training: Other researchers have investigated how providing biofeedback based on physiological responses in Virtual Reality (VR) can improve a mindfulness experience. For instance, visual and auditory real-time feedback based on the level of the skin conductivity in a VR environment has been shown to be useful for pain management [18] , and Roo showed the effectiveness of a mixed-reality environment that reflects the user's heart rate and breathing for mindfulness. RelaWorld is also a neuroadaptive VR meditation system [25] in which a head-mounted VR display is used to show a relaxing virtual environment. The system monitored users' brain activity, and provided adapted feedback for novices during a mindfulness meditation session. An evaluation study found that the combination of neurofeedback and the head mounted display resulted in significantly deeper levels of relaxation and increased feelings of presence compared to those of a control group without any feedback or VR display. Neither of these projects used a conversational agent interface, neither used adaptive verbal instructions and feedback, and neither used respiration beyond biofeedback.\nAutomated Meditation Training: 2.1.1 Respiration-based Feedback for Mindfulness. Several technology based mindfulness programs incorporated respiration data both for monitoring the practitioner's state of the mind [19] , and providing biofeedback in interactive systems aimed at reducing stress. For example, Sonic Cradle is an interactive installation designed to facilitate meditation, reacting to users' breathing with audio feedback (music), in which users can control and shape an abstract sound experience by focusing on their own breathing [41] . Results of an evaluation study showed a significant increase in resting respiration length before and after a sonic cradle experience [40] . In a similar vein, Moraveji et al. presented a peripheral respiration pacing program and demonstrated that participants' respiration rate significantly decreased using the pacing program compared to a no-feedback condition [30] . Wongsuphasawat and Moraveji continued their work by developing a mobile application called \"breathwear\" to regulate breathing and reduce stress. The study demonstrated that auditory guidance and feedback are more effective than visual feedback on self-report levels of calm [43] . Yu et al. also developed \"Breathe with Touch\"; a haptic interface in which users receive feedback on their respiration by following the changes of the shape of the interface [45] . Their results showed significantly higher satisfaction with the new interface, however the stress reduction was not significant with the haptic interface. Sonne et al. developed ChillFish; a breath-controlled biofeedback game that can calm and distract children during a blood test [38] .\nAutomated Meditation Training: While a fair amount of research has been conducted on breath as biofeedback in mindfulness systems, no prior systems have incorporated respiration as an input modality in directed meditation training systems. Several researchers have examined the use of breath as an input modality in entertainment applications [27] , and assistive devices such as on-screen keyboards, a breath joysticks [12] , and powered wheelchairs [44] , but none of the meditation programs have used breath as an intentional input in an interactive system to promote mindfulness meditation, or any kind of relaxation or calming. We believe that respiration is a crucial input modality for meditation, given the centrality of breathing in meditation training, and because of its unconscious and peripheral nature it is important for calming user interfaces which avoid overburdening users as they interact with computers. Our work extends the use of respiration in human computer interaction by focusing on mindfulness meditation as a common and effective way for stress reduction, and by introducing new functions using user respiration as both an intentional input to interact with the system, and a basis for providing biofeedback.\nEmbodied Conversational Agents as Virtual Coaches: There are numerous research papers describing the design, development and evaluation of embodied conversational agents that act as virtual coaches in training [2] , education and behavior change. For example, Bickmore, et al., developed a series of virtual coaches for health counseling [3] , showing the effectiveness of virtual coaches in health counseling, especially for patients with low health or computer literacy.\nEmbodied Conversational Agents as Virtual Coaches: A few researchers have also explored the use of virtual coaches in meditation training, although these did not adapt their interaction to user breathing. Hudlicka developed a virtual coach displayed as a static image that provided audio-recorded guided meditation training, along with the coaching support necessary to begin regular practice [21] . Results of a 7-week evaluation study indicated the effectiveness of the coach system, with participants in the coach group practicing significantly more frequently, and for a longer time, compared to those in a control group. Shamekhi et al, showed the effectiveness of a conversational agent to alleviate chronic pain and stress using yoga and meditation instruction, in conjunction with human-led group visits [36] .\nEmbodied Conversational Agents as Virtual Coaches: Despite the huge potential of affect sensing in coaching systems, most of these virtual coach systems do not respond to sensed input from users. One exception is an embodied pedagogical agent developed by D'Mello and Graesser, that acted as a virtual tutor, adapting its instruction based on real-time assessments of student affective state. Evaluations indicate significant learning improvement for students who worked with Autotutor [11] .\nSummary of Innovation: Our work is unique in that we combine the use of an embodied conversational agent as a virtual meditation coach with adaptive responsiveness to user respiration. The agent provides a rich communication medium and is available to users of all levels of health and computer literacy, and the respiration sensor enables several novel breath-driven interaction techniques, such as the input modality, biofeedback and mirroring, described in the next section.",
    "rq": " sample items include: how satisfied are you with your instructor?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "RELATED WORK: The problem of reducing the complexity of performance analysis has been tackled previously. Solutions have been proposed for abstracting the process of problem stating from choosing, configuring, and running an appropriate performance analysis approach [28, 34] . Especially in the field of application performance management [12] , automatic analysis of the results is becoming an important requirement for tools that aim to be market leaders. The issue here is that these approaches still require significant prior knowledge in the field, as well as experience to analyze the results. Some works have been proposed for integrating performance-awareness into development environments, e.g., to assess and display the performance (impact) on code changes using model-based and measurementbased approaches (e.g., performance tests and production monitoring) [3, 7, 9, 14] . However, these approaches do not focus on configuring the performance evaluation and do not include the use of conversational interfaces such as chatbots. Generally, also visualizations make performance analysis more accessible; various specialized approaches have been suggested [15] , but none for visual reporting of load tests or similar scenarios. Chatbots (also called conversational interfaces or natural language interfaces) are gaining popularity in many applications and have already been applied for data analysis. Not a full-fledged conversational interface, but a popular example that answers data-related questions phrased as keywords or a sentence is Wolfram|Alpha; it often provides a mix of visualizations, tables, and lists as a reply, but actual conversations are not possible. Also, general-purpose search engines like Google answer more and more queries directly or by providing visualized data-snippets (e.g., for \"How many people live in California?\"). Srinivasan and Stasko [32] provide a short overview of natural-language interfaces that are combined with visualizations for data analysis-systems mostly focus on specifying a visualization and trigger visualization-related interactions. A recent example that covers both aspects is FlowSense [37] where users can both generate visualizations (e.g., \"Show a scatterplot of 'mpg' and 'horsepower'. \") and control interactions (e.g., \"Highlight the selected cars in a parallel coordinates plot. \"). Such systems are different from our approach that specifies a data-generating test scenario but does not immediately control the visualizations.\nRELATED WORK: Also, software engineering research has started to investigate the application of conversational interfaces in the context of the development process. The BotSE 2019 workshop (co-locacted with ICSE 2019) [30] discussed the usage of bots (chatbots being a subtype of these) in software engineering. Chatbots have been tested or discussed, for instance, to find experts for a certain code artefact [6] , to help avoid potentially conflicting code changes [24] , or to support project meetings with background information on the development [16] . Regarding the visualization of software dependencies, chatbots can help to select and filter elements [5] ; a natural-language interface (here, also involving speech recognition) is particularly useful when software visualizations are presented in virtual reality, and hence, text input gets harder [29] .\nRELATED WORK: In addition to the chat interface, our approach provides the analysis results as a detailed interactive report. It is inspired by previous work that uses natural language generation (NLG) [10, 27] to generate documentation of software or to report data analysis results. For instance, there are approaches that textually summarize source code [17, 18, 31] , commits and releases [8, 19] , or characteristics of executed tests [25] . Some approaches combine the generated texts with interactive visualizations similar to our reports, for instance, to describe runtime information of a method [4] or to summarize different aspects of code quality [20] . In an earlier work-in-progress publication [23] , we already sketched a reporting framework (Vizard), which we now integrate with the chatbot interface.",
    "rq": "rq1: how does performobot help participants to create and execute a load test?\n rq2: how do participants perceive performobot and interact with it?\nrq3: what educational effect does performobot have on the participants?\n rq4: how does the interaction with performobot depend on the knowledge and experience of the participants?\nrq1: how does performobot help participants to create and execute a load test?\nrq1: how does performobot help participants to create and execute a load test?\nrq1: how does performobot help participants to create and execute a load test?\nrq1: how does performobot help participants to create and execute a load test?\nrq1: how does performobot help participants to create and execute a load test?\nrq1: how does performobot help participants to create and execute a load test?\nrq1: how does performobot help participants to create and execute a load test?\nrq1: how does performobot help participants to create and execute a load test?\nrq2: how do participants perceive', 'sec_num': '5.2'}, {'text': 'expert users tried to get more information around the scenario, i.e., about the infrastructure, usual workload, and the balancing methods of the observed software system. example questions of the users were what is the network bandwidth?\nrq2: how do participants perceive', 'sec_num': '5.2'}, {'text': 'participants also asked the chatbot about more complex things that were not related to the given scenario, or load testing in general. for example, participants who had more than 15 interactions with the bot challenged it with small talk and questions about computer science, e.g., can you calculate 3 + 1?\nrq2: how do participants perceive', 'sec_num': '5.2'}, {'text': 'performobot have on the participants?\nrq3: what educational effect does', 'sec_num': '5.3'}, {'text': 'performobot depend on the knowledge and experience of the participants?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: As a novel technology, Autonomous Vehicles (AVs) face the challenge of gaining public acceptance and having their benefits to the transportation ecosystem fully realized. Thus, it is imperative to understand factors that influence user experience and trust in AVs to design for widespread adoption. In recent years, innovations in transportation services have moved toward multi-passenger modes of shared mobility services. It is expected that AV products and experiences will evolve in a similar fashion, favoring multi-passenger riding and users interacting with AV's solely as passengers [1] . To explore this area, we conducted an on-road driving study in a mock-autonomous ride-sharing vehicle to examine how the quality and quantity of information presented to passengers impacts user experience.\nINTRODUCTION: Wizard-of-Oz Methods: Our methods utilize an on-road, Wizard-of-Oz (WOZ) vehicle as an experimental test bed. WOZ is a method whereby participants are \"given the impression\" that they are interacting with a fully functional and autonomous technological system, when in fact a human researcher is dictating the system's observable behaviors [2] . In the fields of human-computer and human-robot interaction, WOZ is often used for testing natural language interfaces [3, 4] , and for prototyping vehicle interfaces in on-road studies [5] . For instance, Large et al. [6] and Antrobus et al. [7] explored the use of a natural language interface to improve the trust and acceptance of SAE Level 4/5 autonomous vehicles. Prior research on AV interactions used WOZ methods to convince pedestrians that a nearby vehicle was autonomous [8, 9] . Similar methods have been implemented to mimic on-road experiences of semi-autonomous vehicles, such as the RRADS and Marionette platforms, constructed to simulate varying levels of participant input into the driving operation of the vehicle [10, 11] . For fully autonomous vehicle experiences, WOZ setups have also been used for exploratory work to investigate motion sickness in AVs [12, 13] . Although, existing literature on on-road AV testing has largely been limited to studies utilizing single-passenger scenarios in which the participant is in the front seat of the vehicle.\nINTRODUCTION: Anthropomorphic User Interfaces: We build upon prior research to create an authentic AV experience for passengers using a mock fully autonomous van controlled by a human driver, to observe and design for enhanced AV-passenger interactions. This research explores the use of anthropomorphism to design desirable user experiences in vehicles. It stems from HCI research, which has shown that humanlike features, such as voice, can drive user engagement with vehicle interfaces [14] . Waytz et al. [15] , for instance, illustrated differences in behavioral and physiological measures of driver trust in AV competency, based on the presence of anthropomorphic features in an AV simulator. Similarly, Lee et al. [16] reported elevated feelings of social presence, perceived intelligence, safety, and trust in vehicles that had more anthropomorphic and autonomous cues. Other studies have focused on determining the medium of the notifications in multimodal interfaces, for instance via voice, text, or haptics [17] [18] .\nINTRODUCTION: Quality & Quantity of Information Displayed: Prior research involving a semi-autonomous vehicle and human driver has shown that to increase overall safety in AVs, car makers need to focus not only on the design of autonomous actions but also on the ways in which these actions are explained to drivers [19] . We extend this work in a fully autonomous AV through examining how the amount of information provided by an in-vehicle digital assistant, and the manner in which information is delivered to passengers, can impact user experience. In this paper, we focus on a single method for displaying information (via a digital assistant) across treatment arms. This chosen display method aligns with a growing interest among car manufacturers to integrate digital assistants into production vehicles [20] . In addition, we capture biometrics data to study passengers' physiological stress responses within AVs. In building on prior research, our study investigates how the design of AV user interfaces can affect a passenger's confidence in a vehicle's driving capability and overall user experience in an on-road environment.",
    "relatedWork": "",
    "rq": " (2) is there a correlation between a passenger's physiological measurements and ux in a multi-passenger av?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Chatbot systems 1 have been increasingly adopted in many fields (e.g., healthcare [30] , human resources (HR) [64] , and customer service [122] ), since the first chatbot system-ELIZA-emerged in 1964 to provide consulting sessions as a computer therapist [115] . In recent years, an increasing number of chatbot systems are being developed in various research labs and companies with a premise that these systems can have more powerful capabilities and support more user scenarios [21, 44, 103] . For example, Hu et al. [44] built an experimental chatbot system that can understand the tones in a text input (e.g., sad or polite) and generate responses with an appropriate tone.\nINTRODUCTION: Following these system development efforts, many recent Human-Computer Interaction (HCI) and Computer-Supported Cooperative Work (CSCW) studies have examined various aspects of chatbots from the end users' perspective, such as human-in-the-loop chatbot design [21] , user perception of chatbots [18] , playful usage of chatbots [64] , and human trust in chatbots [49] . However, most of these studies have inherent limitations: 1) many chatbot systems (e.g., [64, 123] ) use a rule-based architecture, which makes the chatbot capable of understanding only a limited number of user inputs and responding with prescripted sentences, hindering its generalization; and 2) most chatbots are deployed and tested only in single-user scenarios, but how these systems interacting with and impacting a group of users (or a community) is understudied.\nINTRODUCTION: The first limitation of current chatbots -only returning a pre-defined list of responses to a user -is partially caused by the use of traditional heuristic rule-based algorithms or information retrieval techniques [75] . Even with some advanced chatbot-development toolkit's help (e.g., Microsoft Cognitive Service [74] and IBM Watson [47] ), a chatbot can only use neural-network based approaches (NN) to understand the text, but its responding function is still limited to a rulebased selection process. In this work, we propose a NN-based chatbot architecture based on which a chatbot system can accurately handle unseen questions and generate various forms of responses with the same meaning. This architecture is designed to have a high scalability and generalizability, so that other researchers and developers can take our code 2 , provide it with a cleaned and labeled training dataset, retrain it, and deploy it for different online communities. Inspired by previous literature [117] , we also build a human-in-the-loop module so a human operator can monitor and intervene the fully automated architecture, if needed.\nINTRODUCTION: The second research gap is that most of today's HCI and CSCW research primarily focus on an individual user's interaction with and perception of a chatbot system [31, 46, 58, 64, 94, 122] , and it is unclear how a chatbot system may affect a group of users or a community. In this paper, we aim to address this research gap by building and deploying a social-support chatbot system, CASS (ChAtbot for Social Support), and evaluating its impact on the individuals who need social-support as well as on the other members in the community. Motivated by existing literature that online communities often suffer low engagement from the members due to that many posts can not get a timely response, our chatbot's primary function is designed to engage in conversations with those un-replied social-support-needing posts. CASS automates the entire end-to-end process: retrieving\nINTRODUCTION: \u2022 An empirical understanding of the challenges and user needs in an online pregnancy healthcare community, and how these findings can be used to tailor a chatbot system design; \u2022 A scalable and generalizable chatbot development architecture, with which researchers and developers can easily build a fully automated chatbot system with NN-based models to be deployed in another online community; \u2022 Insights and recommendations for designing and deploying future chatbots systems to interact or collaborate with humans in the context of online communities.",
    "relatedWork": "RELATED WORK: The literature review is divided into three subsections: we first review selected HCI work on social support scenarios in online health communities. Then, we focused on the group of literature about human and chatbot interaction. Lastly, we switch to the literature that specifically addresses challenges and issues of chatbot systems deployment in real world.\nOnline Health Community and Emotional Social Support: Online community has been a longstanding research topic for HCI and CSCW researchers (e.g., [10, 20, 27, 37, 72, 99, 104, 105, 114, 118, 125, 127, 134] ). Existing studies have looked at a variety of topics including community structure [54, 88, 95] , community activities [7, 37, 45, 54, 65, 71, 116] , members' commitment and contribution [87, 124] , engaging newcomers [53, 55, 86] , rewarding mechanism design [40, 54, 88] , and the cold start problem [88] . Recently, a number of studies has focused on a special type of online community -the online health community for pregnant women [23, 24, 28, 37, 42] . This is a special group of users. In addition to the significant changes on their bodies, their mental state also changes a lot over the trajectory of their pregnancy. They often have a much higher stress level than before getting pregnant, thus their mental health is at high stake [12, 48, 121, 128] . Banti et al. [11] reported that 12.4% of pregnant women had presented some depression symptoms during the pregnancy, and 9.6% of them encountered depression in the postpartum period.\nOnline Health Community and Emotional Social Support: It is known that pregnant women often go to online health communities to seek social support from peers [14, 29, 57] . Previous literature reveals that members in such health communities actively seek social support from others, and they are also willing to volunteer their time to provide social support to other help seekers [25, 126] . Prior research roughly divided the social supports into two categories: informational social support and emotional social support [15, 22, 114] . Informational support refers to posters seeking information or knowledge about the course of their disease, treatments, side effects, communication with physicians, and other burdens (e.g., financial problems) [114] . Emotional support refers to posters seeking encouragement and empathy when experienced an emotional disturbance [37] . In this project, as an illustration, our chatbot system focuses on providing non-informational social support to community members.\nOnline Health Community and Emotional Social Support: It is often difficult to motivate community members to actively reply to others members' posts in a timely manner [3, 61, 70, 77, 107] . Seminal research has explored various ways to solve this problem [84] . In more traditional online communities (e.g., Wikipedia), researchers have attempted to stimulate member's intrinsic and extrinsic motivations [3] with monetary reward [61] or virtual badges and reputation rewards [70] . In the online health communities, when a user posts a support seeking post, he/she is recommended to use simpler language and express the needs more explicitly [8] . It is also suggested that posts with more detailed user profile information and a photo are more likely to get replies [9] . Even so, there are many posts may never get a reply. For example, Wang et al. [113] reported that at least 10% posts in an online community never received a response.\nOnline Health Community and Emotional Social Support: When a pregnant woman posts a support seeking post and never gets a response, it may have more severe harms to the user and to the community. Because pregnancy women are already stressful, overlooking their support seeking may make things worse [62, 79] . Furthermore, when community members constantly fail to get the needed social support, they are less likely to contribute to the community, so the community engagement level decreases accordingly, and even worse, the members may leave the community over time [55, 114, 132] .\nOnline Health Community and Emotional Social Support: In this paper, we will illustrate how to leverage on the latest AI technology to build a chatbot system that can automatically detect non-informational support-seeking posts, and respond to it with appropriate sentences. The system architecture is scalable and generalizable so it can be easily migrated to other online communities.\nHuman Interaction with Chatbots: Chatbot is an increasingly popular research topic in recent years. Most of today's chatbot systems are built to interact with a single user [31, 46, 58, 64, 94, 122] . For example, the famous ELIZA and its successors can provide individual cognitive therapy sessions to users with a purpose of relieving their stress and anxiety, as well as helping them gain self-compassion [31, 58, 94] . Many commercial chatbots in customer service domain are designed to answer customers' frequently asked questions or perform a simple function (e.g., check bank account balance) [46, 122] . There are also human resources (HR) chatbots serving as a process guider to lead new employees to go through their onboarding process [64] . There are also some chatbot applications in the healthcare domain which can help patients to better understand their symptoms [30] .\nHuman Interaction with Chatbots: Only till recently, a number of researchers have started to explore and build chatbots that can interact with a group of people [2, 21, 66, 96, 98, 103, 130] . For example, Toxtli et al. [103] built a chatbot as a group chat facilitator to assign tasks to group members. Zhang et al. [130] developed a chatbot for an online communication application to automatically summarize group chat messages. Cranshaw et al. [21] developed a chatbot system that can serve as an assistant to coordinate meetings for multiple people via email. All these studies expand the chatbot user scenario from supporting single user to multiple users. Notably, Seering et al. [96] recently built a chatbot in an online gaming community on Twitch. They designed four different versions of the chatbot -\"baby\", \"toddler\", \"adolescent\", and \"tennager\" -to simulate a chatbot's grownup process in a 3-week deployment. However, the user interaction with the chatbot is quite primitive that users need to input command-line text (e.g., \"@Babybot\" or \"!feed\"), thus it is more like a digital pet (tamagotchi) [85] than a chatbot.\nHuman Interaction with Chatbots: Along this research line, our work builds a chatbot system that can provide emotional support to pregenant women members in an online community; also, we design a field experiment to reveal findings on how such deployment of the chatbot can impact the whole community. Different from Seering's work [96] , where they deployed the \"digital pet\" into a Twitch community and users can have chitchat with the system, we aim to build a chatbot to meet community members' existing needs -social-support seeking. We hope our chatbot can provide functional benefits to the users through a conversational communication. Another difference is that Seering's system [96] used an information retrieval (IR) approach, and in turn, it could only return limited responses that were pre-defined by the researchers. The rule-based approach (e.g., IR) constraints the potential of chatbot in providing social support for a community [63] , and users may perceive the responses from the chatbot not as useful as from people [75] . In contrast, we build an architecture that leverages on the state-of-the-art NN-based models for the development of more powerful chabot systems.\nImpacts of Chatbot System Deployment: While reviewing recent work on building and deploying chatbots [32, 38, 93] , we found that despite many of these chatbots were designed with a good will, the deployment of certain systems may negatively impact the stakeholders or the intended users.\nImpacts of Chatbot System Deployment: One example is the chatbot system developed by [96] and deployed on Twitch. It designs a novel interaction approach that users can \"raise\" the chatbot as a pet through a number of commands, such as \"!feeding\". However, the deployment of such a chatbot may distract users' attention from their original goal of using the platform, which is to watch videos and socialize with the host and the other community members. Thus, with the chabot, users may engage with the platform but not with each other. Such close bonding with a chatbot may even hurt the individual user's benefits in the long term 4 and also negatively impact the community's engagement level [131] .\nImpacts of Chatbot System Deployment: The unexpected consequence of a chatbot's deployment is not uncommon when putting AI and machine learning systems to practical use. Often, today's NN-based algorithm research requires a large amount of data. Such data may come from an online community (e.g., Reddit), and they need to be tagged with ground truth labels by human annotators. However, the benefit of the original human annotators may be neglected during the training and deployment of the algorithm, as the algorithm's performance and optimization is developer's most important objective. For example, developing a functional customer service chatbots needs a significant amount of training data labeled by human customer service experts or obtained from their practices (e.g., [44, 122] ). But the deployment of such chatbots may cause companies to hire fewer customer service workers in the future, which may also in return reduce the sources of training data.\nImpacts of Chatbot System Deployment: Fortunately, some HCI researchers have noticed this challenge and they jointly work on an emerging research topic -Human-Centered AI -that aims to take an algorithm's impact on human users into the consideration of the algorithm design [17, 38, 39, 59, 90, 119, 133] . For example, Woodruff et al. interview 44 participants from several marginalized populations in the United States. Participants indicate that algorithmic fairness (or lack thereof) could substantially affect their trust in a company or product, if such application is deployed [119] . Thus, such fairness considerations should be taken into account during the algorithm design [59, 90] . In addition to the fairness, various other design considerations may also influence the eventual consequence of an AI system in the real world deployment, such as stakeholders's tacit knowledge [133] and community involvement [60] . It is also important to build an in-depth understanding of user needs or even involve them in the design process, as exemplified by a few recent work adopting participatory design research method [17, 38, 60] .\nImpacts of Chatbot System Deployment: In addition to incorporating the various considerations into an AI system design, there are also some good practices that one can follow in the deployment of the AI system [32, 38, 39] . For example, a group of researchers designed and developed ORES -an algorithmic scoring service that supports real-time scoring of wiki edits using multiple independent classifiers trained on different datasets. This paper proposes an example of deploying AI algorithms in an online community, but their algorithm is less explicit to the users, compared to the chatbot systems that we aim to develop and deploy in this paper.\nImpacts of Chatbot System Deployment: In this paper, we design and develop a chatbot system that can provide social support for an online health community. Besides the objective of ensuring a good functional performance, we are also interested in evaluating its potential impacts on the online community after its deployment. This work joins the recent Human-AI Collaboration research effort [30, 106, 109, 123] that aims to develop and deploy AI systems that can work together with people, instead of replacing people. It differs from the Human-AI Interaction discussion [6] , as it goes beyond the usability and interactive design of AI systems, but focuses more on the cooperative nature of AI systems with human partners and their context (e.g., [36, 43] ).",
    "rq": " we explored the following research questions: what types of support do users (e.g., pregnant women) seek in this online community?"
}
{
    "intro": "INTRODUCTION: Chatbot is an automatic conversational agent that interacts with users using human's natural language. The first Chatbot named ELIZA was created in 1966 by Weizenbaum to be able to make a simple conversation [1, 2] . Nowadays, with the development of artificial intelligence and natural language processing, chatbots are used in many areas, such as automated telephone answering systems, educational support, business and ecommerce [3] [4] [5] .\nINTRODUCTION: Recently, chatbots have received plenty of attention from researchers to the point that many studies have been conducted, such as the chatbot for answering Frequently Asked Questions (FAQ) [5, 6] , chatbot applications for education [3, 4, 7, 8] , or chatbot platforms evaluation [7, 9, 10, 11] .\nINTRODUCTION: To the best of our knowledge, there is a lack of critical attention paid to develop chatbots as personal assistants which support in learning. Therefore, in this study, we a developed mobile application for English learning which a chatbot is integrated into. This chatbot works as a virtual personal assistant that can remind learners to study and suggests hints for test questions. By doing so, this chatbot was expected to increase learner's engagement of mobile learning app. In this paper, we conduct a pilot study which discussed the question of how users come to use and accept a new technology like chatbot for English learning. The results were discussed to improve the system and provide guideline for people who want to develop an intelligent system application for education.\nINTRODUCTION: To this end, this paper begins by discussing the possible benefits of mobile learning app, followed by presenting Chatbot and its application in education. Next, it discusses the system design and implementation. Lastly, the results and discussions detail the findings of the study.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "Introduction: There is substantial interest and engagement concerning conversational user interfaces for customer service. In particular, there has been a surge in service providers exploring and implementing text-based chatbots as a first line of support for customers seeking help and information. The successful uptake of such chatbots may enable more efficient service provision [14] , and it is predicted that one quarter of customer service operations world-wide will include chatbots by the year 2020 [16] . Technology providers, such as IBM and Nuance, showcase how chatbots for customer service enable companies, such as Autodesk [33] and Kaspersky Lab [29] , to improve efficiency and performance in their service functions.\nIntroduction: Given the relative importance of customer service in users' everyday lives, a broad uptake of chatbots for this purpose may be important in advancing users' overall acceptance of conversational user interfaces in general. However, attaining a broad uptake of chatbots for customer service will depend on whether customers perceive these as valuable and useful [15] . Chatbots for customer service will only enjoy sustained relevance and interest if they generate good user experience and represent value propositions that motivate users to engage in repeated interactions.\nIntroduction: In this context, one might expect user experience to be a prioritized research topic in the literature on chatbots for customer service. However, research providing in-depth insight into user experience and user motivation for such chatbots is severely limited. This is problematic since the successful development and implementation of chatbots for customer service require such insight. Furthermore, this insight would also be valuable to the broader research field of conversational user interfaces due to the prominence of chatbots for customer service in this field.\nIntroduction: To address this research gap, we conducted interviews with 24 users of chatbots for customer service to gain an in-depth understanding of their experience and motivation. In doing so, our study contributes to closing the aforementioned limitation in previous research and adds to the emerging body of knowledge on user experience and user motivation for conversational user interfaces in general.\nIntroduction: This paper is structured as follows: first, we present relevant background on chatbots for customer service and related studies on user experience and motivation. On this basis, we present our research questions, detail the study method, and provide an overview of key findings. Lastly, we discuss the findings relative to previous work, summarise key implications for research and industry, point out study limitations, and suggest future work.",
    "relatedWork": "",
    "rq": " rq1: how do users experience chatbots for customer service?\n rq2: what are users' main motivations to use chatbots for customer service?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": " an answer to the question: why did you start chatting with replika today?"
}
{
    "intro": "INTRODUCTION: Motivating people to change their attitudes and engage in healthy behavior is a difficult and important task. The pervasive use of computers and mobile devices paves the way for using software applications as a scalable means to help individuals follow health guidelines and attain their goals. Accomplishing this successfully requires creating interventions that are tailored to the individuals' characteristics and circumstances [1, 2] . A common method for tailoring is identifying a user's stage of change using the transtheoretical model of health behavior change [3] . Stage-specific techniques can then be used to motivate people towards behavior change or behavior continuity.\nINTRODUCTION: Motivational interviewing (MI) is a counseling method used to enhance a person's motivation for change [4] and has a variety of techniques that are used at different stages of change. The main goal of MI is for counselors to help their clients resolve their ambivalence about their current behavior and get them to consider behavior change. The counselor elicits this change-talk from clients using particular strategies, such as listing the pros and cons of their behavior, talking about their current level of motivation and confidence to change, and encouraging them to speak freely without being judged.\nINTRODUCTION: Given the effectiveness of MI [5] , creating systems capable of conducting MI sessions automatically would be beneficial. Embodied conversational agents (ECAs) are virtual agents that simulate face-to-face interactions by having a human embodiment and exhibiting non-verbal behaviors [6] , and thus can be a vehicle for driving digital counseling interventions for health behavior change. However, since MI relies on open elicitation from clients, designing automated conversational interventions that implement MI faithfully is challenging [7] . Some researchers have implemented an ECA system that uses a few techniques from MI that work with constrained input, e.g., only allowing users to select what to say from a multiple-choice menu [8] , but these are very limited in functionality.\nINTRODUCTION: In our current effort, we developed an ECA that promotes two health behaviors-physical activity and fruit and vegetable consumption-using MI to increase motivation and confidence to change. We address these behaviors due to their wide applicability and because most American adults struggle to meet the Center for Disease Control's general recommendations, e.g., 23% of American adults meet the recommendations for physical activity [9] and more than half of Americans consume less than the recommended daily servings of fruits and vegetables [10] .\nINTRODUCTION: The ECA makes use of fully-constrained user input to the MI counseling conversation. Rather than being a limitation, constrained interfaces provide opportunities to explore novel methods for bringing about attitude change, for example, by only allowing users to express change-talk at key moments of the session. This approach is based on the phenomenon of opinion change following forced compliance, a derivative of cognitive dissonance theory [11] studied in the field of behavioral psychology. Researchers found that people who were forced to advocate for an opinion they did not hold (e.g., to come up with and rehearse counter-arguments) shifted their private views towards the position they advocated for, particularly when offered low extrinsic reward [12] . In our case, we evaluate the effect of constraining users to only being able to express motivation and confidence in their ability to change or maintain the target health behaviors, compared to a less constrained condition in which they are provided with both positive and negative statements.\nINTRODUCTION: To explore these ideas and evaluate the interventions, we conducted a study to address the following research questions: RQ1: Can we implement an ECA that uses MI techniques to bring about attitude change towards physical activity and fruit and vegetable consumption? RQ2: Is the agent accepted by users and will they endorse its future use? RQ3: Does the interaction lead to increased confidence or motivation to change? RQ4: If we force them to endorse change-talk at key moments in the dialog, does that boost this effect?",
    "relatedWork": "RELATED WORK: Digital health interventions are increasingly used to promote health behavior change and a growing number of these interventions are implemented based on theoretical foundations. In a review of digital online interventions that promote health behavior change, Webb et al. found that interventions based on theory and those that incorporated more behavior change techniques tended to have larger effect sizes than ones that did not [13] . Among the top theories and behavior change techniques, with respect to effect size impact, were the transtheoretical model and barrier identification coupled with problem solving, an important MI technique.\nRELATED WORK: There have been several attempts to build automated health behavior change interventions that incorporate elements of MI, are deployed using ECAs, or both. One such example is the MAPIT program, which is a web-based intervention to increase motivation for substance abuse treatment among clients in the criminal justice system using illicit substances. The program uses the extended parallel process model, MI, and social cognitive theory in two sessions. The first session aims to motivate clients to complete probation, change their substance use, and obtain HIV care. The second session, 30 days later, focuses on goal setting, coping strategies, and social support. Participants in a pilot test were generally positive about the program's features, felt that it would help them be more successful on probation and in treatment. They appreciated that the system tailored content to them, it could display personal and population statistics, and give them insight into other people's reasons for completing probation [14] .\nRELATED WORK: Another research-driven digital intervention combines MI and cognitive behavioral tools to create a self-guided supportive coaching experience for improving the mental health and wellness of its users. The system is a texting platform that leverages artificial intelligence and natural language technologies to conduct its interactions with users. In a feasibility study, 95% of users reported improvements in their mental wellbeing, though the particulars of how the system uses the counseling methods is not reported [15] .\nRELATED WORK: With recent advances in speech recognition and machine learning technologies, using conversational agents for healthcare that allow natural language input has increased; however, their effectiveness remains unclear. In a review of 14 systems that allow natural language input (written or spoken), only one system was found to have a significant effect on participants' health [16] . The same review found that approximately 70% of the conversational agents suffered from problems with language understanding and/or dialog management [16] , threatening user safety [17] .\nRELATED WORK: Several researchers have evaluated the use of constrained input ECAs and social robots to drive automated MI sessions. Schulman et al. (2011) implemented a model of MI dialog into the dialog manager of an intelligent conversational agent to promote exercise and healthy eating behavior. The model contained adjacency pairs (agent utterance and user options) for each type of MI-specific dialog act that can be enacted depending on the context at any given time in the conversation. In a formative evaluation, users rated the system highly on satisfaction [8] .\nRELATED WORK: In an ECA-based intervention delivering a brief MI for reducing alcohol consumption, Lisetti et al. (2013) found that an empathic ECA was rated significantly higher than a text-only system on several measures of usability and user experience, while a nonempathic ECA showed fewer significant results [18] . This finding is congruent with the emphasis that is placed on using empathy in MI [4] . These findings on using MI to promote behavior change through constrained interactions with ECAs and social robots [19] have shown that MI can increase satisfaction and usability of these systems. Our current effort focuses on measuring the effects of an interaction with ECAs on behavior change attitudes and explores the additional effects of being coerced into change-talk.",
    "rq": " rq1: can we implement an eca that uses mi techniques to bring about attitude change towards physical activity and fruit and vegetable consumption?\n rq2: is the agent accepted by users and will they endorse its future use?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: In human-human communication, when we are co-present, we have the same perceptual access to the shared environment [4] and generally do not have any problem understanding each other's references (e.g., what \"the green cup\" refers to in the environment). What makes the communication between us successful is the common ground we have: mutual beliefs and knowledge about the shared environment [4] .\nINTRODUCTION: However, in human-robot dialogue, although a human and a robot are co-present in a shared environment, they have significantly mismatched perceptual capabilities. The \"cup\" that is easily recognized by the human may not be recognizable to the robot. In addition, the human has much more knowledge about the environment and can infer and represent the environment symbolically and thus be able to communicate with others using language. However the robot's representation of the same environment may not be symbolic and may contain lower level numerical features (such as bounding boxes or color distributions associated with the perceived objects). These phenomena contribute to a gap between the human representation and the robot representation of the shared environment. Therefore, the shared perceptual basis is missing. In order for humans and robots to communicate with each other successfully using language, it is important for them to mediate such differences and establish a common ground.\nINTRODUCTION: Despite recent efforts [9, 13, 20, 21] , research on situated human-robot dialogue is still in its infancy. It is not clear how the mismatched perceptual basis may affect the use of language and how to mediate such differences. To address this issue, this paper presents a dialogue system implemented in a NAO robot. Our robot can carry natural language dialogue with its human partner and utilize the collaborative discourse to acquire knowledge about the environment and strive to establish common ground. In particular, we present an empirical study that examines the role of the robot's collaborative effort in this process. A low effort refers to the robot's minimum effort in accepting or rejecting a presentation from its partner through explicit confirmation. A high effort refers to the robot's extra effort in proactively describing what it perceives from the shared environment in addition to an explicit confirmation. Our empirical results have shown that, a low collaborative effort from the robot may lead its human partner to believe a common ground is established. However, such beliefs may not reflect true mutual understanding. To establish a true common ground, the robot should make an extra effort by making its partner aware of its internal representation of the shared world.",
    "relatedWork": "RELATED WORK: Common ground is essential to the success of communication [24] . It is defined as a collection of \"mutual, common, or joint knowledge, beliefs, and suppositions.\" [5] . In conversation, participants coordinate their mental states based on their mutual understanding of their intentions, goals, and current tasks [4] . The process to establish common ground is called grounding. Clark and colleagues have further characterized this process by defining contributions, the basic elements of a conversation [6] . Each contribution has two phases: a presentation phase where the speaker presents an utterance to the addressee and an acceptance phase where the addressee provides evidence of understanding to the speaker. This grounding process is viewed as a collaborative process where the speaker and the addressee make extra efforts to collaborate with each other to reach a mutual understanding [7] . The notion of common ground and communication grounding have been investigated extensively from human-human communication [5, 10] , to computational models for spoken dialogue systems [30] , and more recently to human-robot interaction [12, 18, 27, 28] . In particular, the recent work that is most relevant here has investigated symbol grounding [17] and feedback and adaptation [2] in human-robot dialogue.\nRELATED WORK: Different from the above previous works, this paper presents a human-robot dialogue system with a specific focus on collaborative efforts. To infer the common ground, shared bases such as joint personal experiences, events, or episodes play an important role [4] . However, in situated human-robot dialogue, the human and the robot have disjoint perceptual experiences although they are physically co-present. Therefore, this paper intends to demonstrate how the mismatched perceptual basis may affect the joint tasks in human-robot dialogue, and how collaborative efforts may influence dialogue grounding.",
    "rq": ""
}
{
    "intro": "Introduction: A crucial problem in the design of spoken dialogue systems is to decide for incoming recognition hypotheses whether a system should accept (consider correctly recognized), reject (assume misrecognition), or ignore (classify as noise or speech not directed to the system) them. In addition, a more sophisticated dialogue system might decide whether to clarify or confirm certain hypotheses.\nIntroduction: Obviously, incorrect decisions at this point can have serious negative effects on system usability and user satisfaction. On the one hand, accepting misrecognized hypotheses leads to misunderstandings and unintended system behaviors which are usually difficult to recover from. On the other hand, users might get frustrated with a system that behaves too cautiously and rejects or ignores too many utterances. Thus an important feature in dialogue system engineering is the tradeoff between avoiding task failure (due to misrecognitions) and promoting overall dialogue efficiency, flow, and naturalness.\nIntroduction: In this paper, we investigate the use of machine learners trained on a combination of acoustic confidence and pragmatic plausibility features (i.e. computed from dialogue context) to predict the quality of incoming n-best recognition hypotheses to a spoken dialogue system. These predictions are then used to select a \"best\" hypothesis and to decide on appropriate system reactions. We evaluate this approach in comparison with a baseline system that combines fixed recognition confidence rejection thresholds with dialogue-state dependent recognition grammars (Lemon, 2004) .\nIntroduction: The paper is organized as follows. After a short relation to previous work, Section 3 introduces the WITAS multimodal dialogue system, which we use to collect data (Section 4) and to derive baseline results (Section 5). Section 6 describes our learning experiments for classifying and selecting from nbest recognition hypotheses and Section 7 reports our results.\nIntroduction: 2 Relation to Previous Work (Litman et al., 2000) use acoustic-prosodic information extracted from speech waveforms, together with information derived from their speech recognizer, to automatically predict misrecognized turns in a corpus of train-timetable information dialogues.\nIntroduction: In our experiments, we also use recognizer confidence scores and a limited number of acousticprosodic features (e.g. amplitude in the speech signal) for hypothesis classification. (Walker et al., 2000) use a combination of features from the speech recognizer, natural language understanding, and dialogue manager/discourse history to classify hypotheses as correct, partially correct, or misrecognized. Our work is related to these experiments in that we also combine confidence scores and higherlevel features for classification. However, both (Litman et al., 2000) and (Walker et al., 2000) consider only single-best recognition results and thus use their classifiers as \"filters\" to decide whether the best recognition hypothesis for a user utterance is correct or not. We go a step further in that we classify n-best hypotheses and then select among the alternatives. We also explore the use of more dialogue and task-oriented features (e.g. the dialogue move type of a recognition hypothesis) for classification.\nIntroduction: The main difference between our approach and work on hypothesis reordering (e.g. (Chotimongkol and Rudnicky, 2001) ) is that we make a decision regarding whether a dialogue system should accept, clarify, reject, or ignore a user utterance. Furthermore, our approach is more generally applica-ble than preceding research, since we frame our methodology in the Information State Update (ISU) approach to dialogue management (Traum et al., 1999) and therefore expect it to be applicable to a range of related multimodal dialogue systems.\nRecognition (6):: nbestRank, hypothe-sisLength, confidence, confidenceZScore, confidence-StandardDeviation, minWordConfidence 2. Utterance (3): minAmp, meanAmp, RMS-amp 3. Dialogue (9): currentDM, currentCommand, mostActiveNode, DMBigramFrequency, qa-Match, aqMatch, #unresolvedNPs, #unre-solvedPronouns, #uniqueIndefinites",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: On May 15 th , 2016, 4.7 million Dominicans went to the polls to vote in the concurrent national and local elections. This election was notable in scale for the Dominican democracy. Voters selected a total of 1,566 elected officials from president down to local council members [1] . This election season aligned with \"DevCa 2016,\" a regional hackathon held simultaneously in eight Caribbean nations. The Dominican hackathon organizers, Fundaci\u00f3n Taig\u00fcey, took advantage of this national event and chose to organize a hackathon (see http://taiguey.org/devca2016) that would focus on technologies to improve civic participation and transparency in the election. Fundaci\u00f3n Taig\u00fcey designed and organized an extended hackathon that launched on May 5 th , 2016, the same day as their Caribbean partners, but winners continued on to develop, test, and deploy their projects on election day. A valuable partnership throughout the extended hackathon was formed with Participaci\u00f3n Ciudadana (PC), a Dominican political non governmental organization whose programming supports political transparency through civic participation. Of the 21 participating hackathon teams, three were chosen to develop and implement their technology. The three platforms included a web site for crowdmapping incidents and voter education; an SMS service to check your polling location and report incidents of voting irregularity; and a twitter-based chatbot for that invited election tweeters to report incidents during the elections through a smart chat. The data gathered though the three services were integrated into one platform called ObservacionE2016. We draw on our experience as hackathon organizers, participant mentors, and seasoned experts in social election monitoring to explore and compare civic participation in election monitoring over the three communication channels: a web-based form, SMS, and a chatbot. We find that in the Dominican case, the web form was the most successful source for collecting reports in terms of volume of reports that were verified by PC's network of formal monitors in the field. The analysis of reports received through the chatbot demonstrates the promise of this tool in social election monitoring, as reports consistently included information required for report verification. Based on our findings, we encourage social election monitoring efforts to continue to experiment with the use of chatbots for event monitoring in addition to commonly used platforms like Facebook and WhatsApp.",
    "relatedWork": "RELATED WORK: \"Social election monitoring\" as termed by Smyth et al [2] refers to non-formal, election day monitoring by untrained citizens and organized groups. These efforts are often led by domestic civil society organizations (CSOs) and are different from formal observation missions like those of the Carter Center. CSOs take up the role of informal monitor in an effort to promote free and fair elections by tracking and reporting electoral irregularities. Studies show that domestic election monitoring efforts are enhanced through the use information communication technology (ICT) tools, which allow informal observation teams to track or crowdsource reports of irregularities over networks and social media [2 -11] .\nRELATED WORK: In a comparison of social election monitoring to formal monitoring, Smyth [10] found that the information gleaned through Aggie, a social media monitoring tool, in two west African countries proved a valuable supplement to formal election monitoring data. Aggie monitors election-relevant chatter over social media and can also collect reports through crowdsourced election monitoring campaigns. In crowdsourced election monitoring, citizens send reports via e-mail, web form, short messaging system(SMS), or a smart phone application. A notable example of crowdsourced monitoring is Ushaidi, a crowdmapping tool born from the 2008 Kenyan national election. Moreno et al. [11] provide additional relevant work on crowdsourcing for event monitoring. investigated the integration WhatsApp within the Aggie platform to monitor the Ghanaian 2016 general election. Through interviews and post hoc analysis, Moreno et al found the integration of crowdsourced WhatsApp with other social media (e.g. Twitter, Facebook) improved the social monitoring efforts. There is an emerging body of research on the impact of bots on social networking sites during political events. Most discuss the use of bots to shape online public discourse and take a critical look at how these online bots affect offline political outcomes [12 -14] .",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Surveys are used to gather information from a large number of users through standardized questionnaires; diverse disciplines have adopted them as a representative research method. In HCI research, surveys are also used to gather users' attitudes or perceptions and to assess artifacts' usability [31, 32] . Modes of data collection for surveys have been adapted to technology and media environments. Surveys, which have been traditionally represented by face-to-face or telephone interviews, are evolving as a way to utilize the Internet in response to its explosive growth among the general population. Compared with traditional surveys, web-based surveys can collect and analyze large amounts of data quickly and economically. Web surveys are also less restrictive because respondents can access them at their convenience. Additionally, there are fewer measurement errors caused by the variability between interviewers, which can commonly occur in face-to-face and telephone surveys [20, 42] . Given such advantages, web surveys are presented as an alternative for overcoming the limitations of traditional survey methods.\nINTRODUCTION: Despite these advantages, web surveys in particular do have some inevitable limitations, one of which is that they produce responses that are less reliable than those from faceto-face or telephone surveys due to respondents' insincere answers [50] . In face-to-face or telephone surveys, interviewers can encourage respondents to participate in the surveys [19] , ask them to clarify their responses [19] , clarify the questionnaires [9, 40] , and monitor their answers to confirm their sincerity. However, a web survey is a self-administered method, so it is difficult for researchers to control respondents, thereby leading to unreliable or even inaccurate responses [12, 13, 39] .\nINTRODUCTION: As described above, web surveys' response quality problem derives from respondents' feigned answers, which in turn occur because of the lack of an interviewer. If the lack of an interviewer causes response quality problems in web surveys, we might ask whether a conversational agent can prevent such problems by partly performing the role of a human interviewer. In today's environment, in which conversational agents are used in everyday life [28] , it is worth considering their possible use as virtual interviewers. In light of the growing trend toward conversational agents, we emphasize the conversational aspect that they provide, thus imbuing interactivity in a traditional survey system [36] .\nINTRODUCTION: This study examines two aspects of interactivity in a survey system: reciprocal message exchange and conversational style. In particular, this study focuses on (a) how a survey's platform influences its response data quality and (b) how a survey's conversational style moderates the way in which the participants respond to different platforms. First, we compare response quality and usability for a chatbot survey and a web survey. The text-based chatbot used in this study is able to convey interactivity in the form of back-and-forth message exchanges with the user [36, 53] . Because conversational interaction facilitates cognitive functioning [52] , this study's first hypothesis is that the chatbot survey participants, as compared to the web survey participants, exert a greater engagement and thus generate higher-quality responses. Second, as the survey platform only represents a surface-level variation, the second hypothesis is that conversational style moderates the relative effect of the survey platform. Specifically, this work examines whether conversational style (formal vs. casual) influences participants' responses and their survey experiences.\nINTRODUCTION: We conduct a user study that employs quantitative and qualitative methods. The standard for evaluating data quality is based on satisficing theory, which assesses data quality in relation to the respondents' satisficing behavior [13, 22] . Giving optimal answers demands a high cognitive load, so some respondents use satisficing heuristics to reduce their cognitive burdens [22, 23] . This satisficing behavior creates a measurement error due to the resulting inaccuracies in the respondents' answers, thus leading to poor data quality. One representative satisficing behavior is non-differentiation in a rating task (i.e., a \"straight-line\" response). In this study, we use the differentiation response index as an objective metric to assess response quality [30] . We also measure the levels of usability (enjoyment and ease of use) as subjective ratings; we used it to estimate users' perceptions of their experiences with the survey system. Furthermore, we conduct qualitative thematic analysis for a deeper understanding of the chatbot respondents' behavior.\nINTRODUCTION: This research makes several important contributions. First, our findings support the feasibility of conducting a survey with a text-based chatbot that provides some of a human interviewer's social function. Second, our findings extend the previous work, which is focused on responses to virtual interviewers' open-ended questions [2, 24, 27, 47] and on ethnographic data [47] , by including Likert-scale responses. Third, our findings extend the work on data-quality evaluations which has focused on the degree of respondents' self-disclosure [2, 24, 27] , comprehension [10] , and qualitative feedback [47] by including non-differentiation ratio.",
    "relatedWork": "RELATED WORK Satisficing Behavior in Web Survey: In a web survey, respondents evaluate and respond to questionnaires by themselves. The advantage of this self-administered method is the ease of measurement. However, a self-administered survey is controversial for its validity due to several wellknown problems, one of which is respondents' satisficing behavior [22] . Inspired by Simon's [43] notion of satisficing, Krosnick [22] proposed that some respondents tend to generate satisfying responses instead of accurate responses to reduce their cognitive burden. This is because responding accurately and sincerely to the survey questions requires a high level of cognitive demands [23] . The representative satisficing behavior is non-differentiation or straight-lining, a non-discriminatory and equally responsive behavior in a battery of scaled questions. A response error occurs because of this distorted or inaccurate information provided by the respondent. Even if the sampling error is reduced by applying a sophisticated sampling method, the increasing response error leads to the low quality of the whole survey.\nRELATED WORK Satisficing Behavior in Web Survey: Researchers have shown that web surveys tend to have higher response errors than offline surveys because web surveys are self-administered [50] . This means that online surveys are more likely to produce satisficing responses than offline surveys are. In an offline survey, human interviewers promote conscientious responses by discouraging careless behavior and encouraging participation [19] . These interviewers' verbal and nonverbal interactions draw the respondents' attention and allow them to appropriately answer each question [16] . However, these interactions are omitted in a web survey, which leads to satisficing responses. In fact, studies have found that online surveys are more likely than telephone survey and face-to-face surveys to be susceptible to satisficing behavior and to thus produce poor-quality data [12, 13, 39] .\nRELATED WORK Satisficing Behavior in Web Survey: To summarize, scholars have theoretically and empirically proven that web survey respondents engage in more satisficing than do their counterparts in face-to-face or telephone surveys, as no interactive process occurs in a web environment. To mitigate this problem, we focus in this study on conversational interactivity, which focused on reciprocal exchange of messages in a social interaction [36, 53] . When appropriate interaction is added to a noninteractive survey system, participants will be expected to exert more cognitive engagement when answering the questions. In this paper, we examine whether this conversational interactivity operates when respondents are conducting a survey through a text-based chatbot which provides interviewer presence. Therefore, we test the following research question:\nRELATED WORK Satisficing Behavior in Web Survey: \u2022 RQ. Can a chatbot platform with a relevant conversational style produce conversational interactivity, thus reducing the satisficing behavior that often occurs in noninteractive web surveys?\nRELATED WORK Satisficing Behavior in Web Survey: Conversational Interaction with Text-based Chatbots Conversational interfaces have become integral to modern communication [28] . The concept of conversational interactivity has numerous meanings in the various domains of HCI and CMC. In this study, we use the construct of conversational interactivity to highlight a relational exchange between a respondent and a virtual interviewer [36] . Users can perceive interactivity from a sequence of back-and-forth exchanges when chatbots synchronously and socially react to those users' input.\nRELATED WORK Satisficing Behavior in Web Survey: Chatbots that interact via auditory or textual methods are representative conversational agents and are being accepted in various fields. Although studies about conversational agents have tended to separate social and task-oriented interactions [7, 18] , recent works have been undertaken to achieve both functional and interactive aspects [25] . Moreover, the popularity of chatbots in recent years has heightened the needs for facilitating interaction between human and chatbots, not only achieving functionality. This research trend of integrating instrumental and social goals is being applied in diverse domains including customer support [6, 17, 51] , healthcare [21] , and counseling [5, 11] . Recently, the approach of combining functionality and interactivity using text-based chatbots has been applied in the field of user research. For instance, Tallyn et al. [47] attempted to gather users' ethnographic data via a text-based chatbot to determine whether a chatbot can be used as an ethnographic tool by making up for the lack of a human ethnographer with interactivity.\nRELATED WORK Satisficing Behavior in Web Survey: In line with previous studies, in this study, we proceed from an integrative perspective that combines instrumental usage and conversational interactivity rather than separating them. The goal of this study is to improve a survey's user experience through the use of an interactive conversational interface and by gathering high-quality user input. Taken together, we expect that:\nRELATED WORK Satisficing Behavior in Web Survey: \u2022 H1. A chatbot survey, as compared to a web survey, will produce higher-quality response data (H1a), greater ease-of-use (H1b), and higher enjoyment (H1c).\nRELATED WORK Satisficing Behavior in Web Survey: Although survey platforms can provide different levels of interactivity, the platform itself only represents a surfacelevel variation for manipulating conversational interactivity. To fill this void, we also examine the effect of message-level variation. Previous researchers have used text-based chatbots to manipulate message-related variables and thus improve conversational interactivity. These researchers have also shown an increased interest in improving chatbots' interactivity with empathic responses [17, 51] or typefaces [6] for use in customer-support situations. In these customercare situations, using an agent that can adjust itself to the customers' emotional needs is crucial.\nRELATED WORK Satisficing Behavior in Web Survey: In healthcare and counseling, researchers have focused on chatbots' interpersonal strategies, as social dynamics are important in those situations. For example, a chatbot that was built for a childhood obesity intervention applied several social strategies [21] . It was able to efficiently perform its function of recommending ad-hoc tasks for obese patients by socially interacting with users. Similarly, researchers have used chatbots as mental health counselors that provide information and counseling and diagnose conditions through relational conversation [5] . Woebot, a text-based therapy chatbot, offers cognitive behavior therapy, which applies several social discourse strategies [11] . Participants in a chatbot condition experienced a significant reduction in depression, whereas participants who received information only in an ebook did not.\nRELATED WORK Satisficing Behavior in Web Survey: As previous researchers have shown, the chosen method of improving conversational interactivity must be applicable to the characteristics of the task. In this study, we focus on providing the chatbot with a conversational style. Users consider not only with what a message is but also how it is delivered [4] . Moreover, it is known that the human interviewer's style influences the survey response quality and respondents' attitude in the context of human-human interaction. People prefer a friendly speaking interviewer, and when an interviewer is active, the participants are more actively participating in the survey [14] . Respondents also have a more favorable attitude toward an interpersonal and casual interviewer than a professional and formal interviewer [15] . In addition, considering that the survey used in this study is aimed at adolescent participants, it is likely more appropriate for the agent to be casual and friendly than formal. Thus, we test the following hypotheses:\nRELATED WORK Satisficing Behavior in Web Survey: \u2022 H2. A casual conversational style, as compared to a formal conversational style, will produce higher-quality response data (H2a), greater ease-of-use (H2b), and higher enjoyment (H2c).\nRELATED WORK Satisficing Behavior in Web Survey: However, conversational style may moderate the effect that a survey platform has on user experience and data quality. Because a traditional web survey usually adopts a formal language style, the use of casual language in a web survey may engender embarrassment. On the other hand, a casual tone could be appropriate for a chatbot survey because users expect social interactions when using conversational agents [5, 17, 28, 51] . Thus, we propose the following hypothesis:\nRELATED WORK Satisficing Behavior in Web Survey: \u2022 H3. The chatbot's data-quality and usability effects will be more pronounced when the survey uses a casual rather than formal conversational style.\nRELATED WORK Satisficing Behavior in Web Survey: Virtual Agents as Interviewers HCI researchers have developed survey methodologies that use new technologies. In this study, we implement a textbased chatbot as a new survey method. Researchers have improved the quality of user input through diverse methods and have applied several standards to evaluate the effects of virtual interviewers. These criteria vary depending on the characteristics and contexts of the interview. The most commonly used standard for evaluating data quality is the respondents' self-disclosure in open-ended interview situations. To date, the literature on this topic has included contradictory findings about respondents' selfdisclosure. Participants who believe that they are communicating with a computer were more willing to disclose their information than participants who believe that they are having an interview with a human interviewer in mental health contexts [27] . This is because people feel less fear about getting a negative evaluation. Similarly, people have shown to more willingly expose their sensitive information and to evaluate the interview process as being more pleasant when communicating with a wordiness agent as compared to a taciturn agent [2] . On the other hand, this self-disclosure effect does not exist when embodied agents ask respondents sensitive questions. Participants have been shown to expose more information to a computer-assisted, voice-only interface than to an embodied virtual interviewer or a human interviewer, as they perceive greater anonymity in the absence of facial representation [26] . Tourangeau et al. [49] also found that, in a web survey, people disclose less information regarding sensitive topics, such as cocaine and marijuana use, when the interface is more humanized. Researchers have also studied the degree of respondents' disclosure to virtual interviewers with different personalities. The participants were more likely to divulge information to a virtual agent that had a reserved and assertive personality than to one with a warm and cheerful personality [24] . Indeed, many scholars have focused on respondents' information disclosure in response to sensitive questions, as anonymity is important in such interviews. This implies that it is essential to configure a proper evaluation index for each interview situation.\nRELATED WORK Satisficing Behavior in Web Survey: Meanwhile, Conrad et al. [10] applied a new standard to evaluate data quality for questions regarding less sensitive and more mundane topics. Instead of measuring the respondents' disclosure, the researchers used objective measures such as the number of requests for clarification, response accuracy and gaze behavior to detect the respondents' comprehension and engagement. Conrad et al. found that agents with high dialogue capability produced more accurate answers and more conscientious task performance, irrespective of the agents' facial expressions.\nRELATED WORK Satisficing Behavior in Web Survey: The researchers in the above studies have provided a theoretical and empirical basis for virtual agents to perform as interviewers. However, the literature on virtual interviewers has largely focused on open-ended interview situations. They have not dealt with the effects of virtual interviewers in a structured survey with scaled items. The present study was conducted to fill this gap in the lack of research by examining the utility of a virtual interviewer for a structured survey. In addition, we apply a different measure to evaluate user input quality-one that is more suitable for use with Liker-scaled questionnaires. Our work has implications for determining how best to introduce the appropriate criteria for scale survey items so as to verify the effectiveness of the virtual interviewer.",
    "rq": ""
}
{
    "intro": "Introduction: Turn-taking, the process by which participants in a conversation alternate speech and silence, is an essential component of spoken interaction. In order to lead productive conversations, people need not only know what to say but also when to say it. Decades of research on Conversation Analysis and psycholinguistics (Duncan, 1972; Sacks et al., 1974; Orestr\u00f6m, 1983; Schegloff, 2000; Wesseling and van Son, 2005) have shown that human turn-taking behavior relies on a wide range of rules and signals at many different levels of language, from prosody to syntax, semantics, and discourse structure. In contrast, turn-taking in spoken dialog systems is often reduced to ad hoc rules only based on very low level features. This simplistic approach leads to inefficient, unnatural, and possibly confusing behavior (Porzel and Baudis, 2004; Ward et al., 2005) .\nIntroduction: Recently, more complex models of turn-taking have been proposed (Cassell et al., 2001; Thorisson, 2002; Kronild, 2006 ). Yet, these models still rely extensively on hand-coded expert knowledge and do not lend themselves to data-driven optimization. Furthermore, to our knowledge, no such model has been deployed in a widely used system outside of the laboratory. In this paper, we propose a flexible, practical model of turn-taking behavior that builds upon previous work on finite-state models of the conversational floor. Because of its simplicity and generality, this model can be applied to many turn-taking phenomena. At the same time, being grounded in decision theory, it lends itself well to data-driven optimization. We illustrate our approach by applying the model to a specific turn-taking task: end-of-turn detection.\nIntroduction: 2 Conversational Floor as a Finite-State Machine 2.1 6-state finite state models of turn-taking\nIntroduction: In the 1960's and early 1970's, several researchers proposed models to explain the rhythmic turn-taking patterns in human conversation. In particular, Jaffe and Feldstein (1970) studied the mean duration of pauses, switching pauses (when a different speaker takes the floor), simultaneous speech, and (singlespeaker) vocalizations in recorded dyadic conversations. Based on their observation that these durations follow exponential distributions, they proposed first-order Markov models to capture the alternation of speech and silence in dialog. Their initial model had four states: only participant A is speak-Figure 1 : Our six-state model of turn-taking, inspired by Jaffe and Feldstein (1970) and Brady (1969) . See section 3.1 for a description of the states.\nIntroduction: ing; only participant B is speaking; both participants are speaking; and neither participant is speaking.\nIntroduction: However, such a model fails to distinguish switching pauses from A to B from switching pauses from B to A. Based on this observation, they extend their model to a six-state model which they found to better fit their data than the four-state model. Around the same time, Brady (1969) developed a very similar six-state model. He trained the parameters on a recorded conversation and compared the generated conversations to the original real one along several dimensions (pause and speech segment durations, overlaps, etc), finding that his model generally produced a good fit of the data.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "Introduction: Evaluating the usability of a spoken language dialogue system generally requires a large-scale user study, which can be a time-consuming process both for the experimenters and for the experimental subjects. In fact, it can be difficult even to define what the criteria are for evaluating such a system (cf. Novick, 1997) . In recent years, techniques have been introduced that are designed to predict user satisfaction based on more easily measured properties of an interaction such as dialogue length and speech-recognition error rate. The design of such performance methods for evaluating dialogue systems is still an area of open research.\nIntroduction: The PARADISE framework (PARAdigm for DIalogue System Evaluation; Walker et al. (1997) ) describes a method for using data to derive a performance function that predicts user-satisfaction scores from the results on other, more easily computed measures. PARADISE uses stepwise multiple linear regression to model user satisfaction based on measures representing the performance dimensions of task success, dialogue quality, and dialogue efficiency, and has been applied to a wide range of systems (e.g., Litman and Pan, 2002; M\u00f6ller et al., 2008) . If the resulting performance function can be shown to predict user satisfaction as a function of other, more easily measured system properties, it will be widely applicable: in addition to making it possible to evaluate systems based on automatically available data from log files without the need for extensive experiments with users, for example, such a performance function can be used in an online, incremental manner to adapt system behaviour to avoid entering a state that is likely to reduce user satisfaction, or can be used as a reward function in a reinforcement-learning scenario (Walker, 2000) .\nIntroduction: Automated evaluation metrics that rate system behaviour based on automatically computable properties have been developed in a number of other fields: widely used measures include BLEU (Papineni et al., 2002) for machine translation and ROUGE (Lin, 2004) for summarisation, for example. When employing any such metric, it is crucial to verify that the predictions of the automated evaluation process agree with human judgements of the important aspects of the system output. If not, the risk arises that the automated measures do not capture the behaviour that is actually relevant for the human users of a system. For example, Callison-Burch et al. (2006) presented a number of counter-examples to the claim that BLEU agrees with human judgements. Also, Foster (2008) examined a range of automated metrics for evaluation generated multimodal output and found that few agreed with the preferences expressed by human judges.\nIntroduction: In this paper, we apply a PARADISE-style process to the results of a user study of a human-robot dialogue system. We build models to predict the results on a set of subjective user-satisfaction measures, based on objective measures that were either gathered automatically from the system logs or derived from the video recordings of the interactions. The results indicate that the most significant contributors to user satisfaction were the number of system turns in the dialogues, the users' ability to recall the instructions given by the robot, and the number of times that the user had to ask for instructions to be repeated. The former two measures were positively correlated with user satisfaction, while the latter had a negative impact on user satisfaction; however the correlation in all cases was relatively low. At the end of the paper, we discuss possible reasons for these results and propose other measures that might have a larger effect on users' judgements.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "Introduction: There is no longer any question that the production of language and its accompanying non-verbal behaviour are tightly linked (e.g., Bavelas and Chovil, 2000) . The communicative functions of body language listed by Bickmore and Cassell (2005) include conversation initiation and termination, turn-taking and interruption, content elaboration and emphasis, and feedback and error correction; non-verbal behaviours that can achieve these functions include gaze modification, facial expressions, hand gestures, and posture shifts, among others.\nIntroduction: When choosing non-verbal behaviours to accompany the speech of an embodied conversational agent (ECA), it is necessary to translate general findings from observing human behaviour into concrete selection strategies. There are two main implementation techniques that have been used for making this decision. In some systems, recorded behaviours are analysed and rules are created by hand based on the analysis; in others, recorded human data is used directly in the decision process. The former technique is similar to the classic role of corpora in naturallanguage generation described by Reiter and Dale (2000) , while the latter is more similar to the more recent data-driven techniques that have been adopted (Belz and Varges, 2005) .\nIntroduction: Researchers that have used rule-based techniques to create embodied-agent systems include: Poggi and Pelachaud (2000) , who concentrated on generating appropriate affective facial displays based on descriptions of typical facial expressions of emotion; Cassell et al. (2001a) , who selected gestures and facial expressions to accompany text using heuristics derived from studies of typical North American non-verbal-displays; and Marsi and van Rooden (2007) , who generated typical certain and uncertain facial displays for a talking head in an informationretrieval system. Researchers that used data-driven techniques include: , who captured the motions of an actor performing scripted output and then used that data to create performance specifications on the fly; Cassell et al. (2001b) , who selected posture shifts for an embodied agent based on recorded human behaviour; and Kipp (2004) , who annotated the gesturing behaviour of skilled public speakers and derived \"gesture profiles\" to use in the generation process.\nIntroduction: Using rules derived from the data can produce displays that are easily identifiable and is straightforward to implement. On the other hand, making direct use of the data can produce output that is more similar to actual human behaviour by incorporating naturalistic variation, although it generally requires a more complex selection algorithm. In this paper, we investigate the relative utility of the two implementation strategies for a particular decision: selecting the conversational facial displays of an animated talking head. We use two methods for comparison: gathering users' subjective preferences, and measuring the impact of both selection strategies on users' ability to perceive user tailoring in speech.\nIntroduction: In Section 2, we first describe how we recorded and annotated a corpus of facial displays in the domain of the target generation system. Section 3 then presents the two strategies that were implemented to select facial displays based on this corpus: one using a simple rule derived from the most characteristic behaviours in the corpus, and one that made a weighted choice among all of the options found in the corpus for each context. The next sections describe two user studies comparing these strategies: in Section 4, we compare users' subjective preferences, while in Section 5 we measure the impact of each strategy on user's ability to select spoken descriptions correctly tailored to a given set of user preferences. Finally, in Section 6, we discuss the results of these two studies, draw some conclusions, and outline potential future work.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": " system: how long would you like to stay in berlin?"
}
{
    "intro": "INTRODUCTION: Collaboration between people and conversational artificial intelligence (AI) agents-AI systems that communicate through natural language [30] -is now prevalent. As a result, there is increasing interest in designing these agents and studying how users interact with them [1, 14, 23, 30, 49, 66] . While the technical underpinnings of these systems continue to improve, we still lack fundamental understanding of the mechanisms that influence our experience of them. What mechanisms cause some conversational AI agents to succeed at their goals, while others are discarded? Why would Xiaoice [68] amass millions of monthly users, while the same techniques powering Tay [35] led to the agent being discontinued for eliciting anti-social troll interactions? Many AI agents have received polarized receptions despite offering very similar functionality: for example, Woebot [55] and Replika [43] continue to evoke positive user behavior, while Mitsuku [79] is often subjected to dehumanization. Even with millions of similar AI systems available online [11, 46] , only a handful are not abandoned [30, 83] . The emergence of social robots and human-AI collaborations has driven home a need to understand the mechanisms that inform users' evaluations of such systems.\nINTRODUCTION: In HCI, experiences of a system are typically understood as being mediated by a person's mental model of that system [54] . Conveying an effective understanding of the system's behavior can enable users to build mental models that increase their desire to cooperate with the system [5, 10, 37, 41] . However, a mental model explanation is insufficient to answer the present question: in the case of Xiaoice and Tay, both agents were based on the same underlying technology from Microsoft, but they resulted in very different reactions by users. Likewise, other agents such as Replika and Mitsuku elicit very different evaluations while existing even within the same cultural context. While theories of mental models and culture each help us understand how users experience conversational AI agents, we require additional theoretical scaffolding to understand the phenomenon.\nINTRODUCTION: An important and unexamined difference between these otherwise similar agents are the different metaphors that they project. Conceptual metaphors are short descriptions attached to a system that are suggestive of its functionality and intentions [15, 50] . For instance, Microsoft described Tay as an \"AI that's got no chill\" [62] , while it markets Xiaoice as an \"empathetic ear\"-two very different metaphors. Metaphors are a central mechanism in the designer's toolkit. Unlike mental models, they offer more than just functional understandings of the system-they shape users' expectations from the system. And while most existing expectation-shaping mechanisms depend on the functionality of the specific AI system or task [41] , metaphors are agnostic to specificities of a system and can be used to shape expectations for nearly any AI system. Prior theory suggests that pre-use expectations of AI systems influence both initial behaviors [31, 40, 76] and long-term behaviors [42] , even if the system itself remains unchanged while varying user expectations [58] .\nINTRODUCTION: We propose that these metaphors are a powerful mechanism to shape expectations and mediate experiences of AI systems. If, for example, the metaphor primes people to expect an AI that is highly competent and capable of understanding complex commands, they will evaluate the same interaction with the system differently than if users expect their AI to be less competent and only comprehend simple commands (Figure 1 ). Similarly, if users expect a warm, welcoming experience, they will evaluate an AI agent differently than if they expect a colder, professional experienceeven if the interaction with the agent is identical in both cases.\nINTRODUCTION: In this paper, we test the effect of metaphors on evaluations of AI agents. We draw on the Stereotype Content Model (SCM) from psychology [16, 24] , which demonstrates that the two dimensions of warmth and competence are the principal axes of human social perception. Judgements along these dimensions provoke systematic cognitive, emotional, and behavioral reactions [16] . The SCM suggests that user expectations and therefore evaluations, are mediated by judgements of warmth and competence. We crowdsource the labeling of a set of metaphors along these axes to identify a set of metaphors that appear in different quadrants of the SCM -e.g., a toddler, who is high warmth and low competence, and a shrewd executive, who is low warmth and high competence.\nINTRODUCTION: We perform an experiment (\ud835\udc41 = 260) that manipulates the metaphor associated with an AI agent and measures how it invokes expectations of competence and warmth and how those two dimensions affect ratings of usability, intention to adopt, and desire to cooperate. We draw on an established method from prior experiments [12, 34, 71, 74] to instantiate the agent itself as a remote Wizard-of-Oz who is blind to the condition and randomized across conditions for each participant. Participants are first exposed to the agent's metaphor, then converse with the agent to complete a travel planning task [4] .\nINTRODUCTION: Our results suggest that, contrary to how designers typically describe their AI agents, low competence metaphors lead to increases in perceived usability, intention to adopt, and desire to cooperate relative to high competence metaphors. These results persist despite both the low competence and high competence agents operating at full human-level performance levels via a wizard, suggesting that no matter how competent the agent actually is, people will view it negatively if it projects a high level of competence. Participants perceive the wizards to possess lower competence than the expectations implied by high competence metaphors. These results align with Contrast Theory [67] , which states that users' evaluations are defined by the difference between their experiences and expectations. Finally, we find that the warmth axis operates conversely to competence: users viewed the AI with higher warmth more positively, interacted with it longer, and were more willing to cooperate with it. This result aligns with Assimilation Theory [67] : users recolor warmth experiences in light of their initial expectation.\nINTRODUCTION: Previous work has sought explanations for user behavior and evaluations of AI by profiling users [5, 18] or by making the AI more interpretable [9, 44, 60] . However, these approaches fail to explain why otherwise functionally similar systems elicited vastly different user responses. Our analysis suggests that designers should carefully analyze the effects of metaphors that they associate with the AI systems they create, especially whether they are communicating expectations of high competence. In discussion, we consider implications for design by retrospectively analyzing the metaphors used to describe existing and past AI agents, such as Xiaoice, Tay, and Mitzuku, and show that our results are consistent with the adoption and user cooperation with these products. The connection between our conclusions and the outcomes experienced by Xiaoice and Tay cannot explain the whole story; however, the pattern is striking and motivates the need for exploration of mechanisms to shape expectations and elicit prosocial user behavior.\nINTRODUCTION: We begin by laying out related work, deriving our research question and hypotheses from prior theories. We then describe our procedure for sampling metaphors. In Study 1, we study the effects of metaphor warmth and competence. In Study 2, we sample additional metaphors along the competence axis in order to understand the effects of competence at a more fine-grained level. In Study 3, we test the negative effects of portraying a low competence metaphor by studying the effect that warmth and competence have on participants' interest in using the system in the first place. Finally, we discuss the implications of our findings for the choice of metaphors when designers deal with the dual objective of attracting more users and ensuring a positive user experience.",
    "relatedWork": "RELATED WORK: Pre-use expectations play a critical role in users' initial usage of a system or design [32, 40, 76] . Setting positive or negative expectations colors users' evaluation of what would otherwise be identical experiences [58] . The effects of these pre-use expectations can have effects on evaluations even after weeks of interaction with a service [42] .\nRELATED WORK: In the case of AI systems, which are often data-driven and probabilistic, there exists no simple method of setting user expectations. Providing users with performance metrics does not establish an accurate expectation for how the system behaves [41] . In the absence of effective mental models of AI systems, users instead develop folk theories -intuitive, informal theories -as expansive guiding beliefs about the system and its goals [26, 29, 45, 64] .\nRELATED WORK: Prior work has shown how subjective evaluations of interface agents are strongly influenced by the face, voice, and other design aspects of the agent [53, 82] , beyond just the actual capabilities of the agent. These results motivate our study of how metaphors set expectations that affect how users view and interact with conversational AI systems. Inaccurate expectations can be consequential. Previously, interviews have established that expectations from conversational agents such as Siri, Google Assistant, and Alexa are out of sync with the actual capabilities and performance of the systems [49, 83] . So, after repeatedly hitting the agent's capability limits, users retreat to using the agents only for menial, low-level tasks [49] . While these prior interview-based studies have demonstrated that a mismatch between user expectations and system operation are detrimental to user experiences [49] , they haven't been able to establish causality and quantify the magnitude of this effect. This gap motivates our inquiry into understanding mechanisms that might shape these expectations and measuring the effect of expectations on user experiences and attitudes. We are guided by the following research question:\nRELATED WORK: Research Question: How do metaphors impact evaluations of interactions with conversational AI systems?\nMetaphors shape expectations: Conceptual metaphors are one of the most common and powerful means that a designer has to influence user expectations. We refer to a conceptual metaphor (or user interface metaphor, or just metaphor) as the understanding and expression of complex or abstract ideas using simple terms [45] . Metaphors are attached to all types of AI systems, both by designers to communicate aspects of the system and by users to express their understanding of the system. For instance, Google describes its search algorithm as a \"robotic nose\" [26] and YouTube users think of the recommendation algorithm as a \"drug dealer\" [81] . Starting with the desktop metaphor for personal computing in the Xerox Star [39] , conceptual metaphors proliferated through the design of user interfaces -trash cans for deleted files, notepads for freetext notes, analog shutter clicking sounds for mobile phone cameras, and more. Some AI agents utilize metaphors based in personas or human roles, for example an administrative assistant, a teenager, a friend, or a psychotherapist, and some are metaphors grounded in other contexts, for example a Jetsons-style humanoid servant robot. Such metaphors are meant to help human-AI collaboration in complex domains by aiding users' ability to understand and predict the agent's behavior [5, 18] . Metaphors include system descriptions outside of those rooted in human roles as well: Google describing its search algorithm as a \"robotic nose\" [26] and Microsoft's Zo marketed as a bot that \"Will make you LOL\". The notion of \"metaphors\" extends beyond conversational AI to non-anthropomorphic systems that \"personas\" or \"roles\" may be ill-equipped to describe. Metaphors are effective: they influence a person's folk theories of an AI system even before they use it [19] . Prior work has developed methods to extract conceptual metaphors [45, 64] for how people understand AI systems and aggregate them into underlying folk theories [26] .\nMetaphors shape expectations: Metaphors impact expectations, sometimes implicitly by activating different norms, biases, and expectations. For example, social robots that are racialized as Black or Asian are more likely to be subject to antisocial behaviour such as aggression and objectification [70] . Similarly, femalegendered robots can elicit higher levels of dehumanisation than male-gendered bots. Antisocial behavior leads to verbal disinhibition toward AI systems [69] , and in some extreme cases, to physical abuse and even dismemberment [8, 61] . Female voice agents are viewed as friendlier but less intelligent [53] . Users also have a higher tendency to disclose information to female gendered agents [53] . Race and gender of pedagogical agents affect learning outcomes-agents racialized as Black or female-gendered lead to improved attention and learning [6] . Beyond race and gender, agents portrayed as less intelligent, taking on roles such as \"motivator\" or \"mentor\", promote more self-efficacy than agents projected as \"experts\" [6] . Young, urban users respond positively to bots that can add value to their life by suggesting recommendations, while in the role of a \"friend\" [71] .\nMetaphors shape expectations: However, designers typically aim to use metaphors to affect expectations in more explicit, controlled, and pro-social ways. Most obviously, a metaphor communicates expectations of what can and cannot be done with an AI agent [39] . Just as we expect an administrative assistant to know our calendar but not to know the recipe for the best stoat sandwiches, an AI agent that communicates a metaphor as an \"administrative assistant\" projects the same skills and boundaries. In a similar vein, describing an agent as a \"toddler\" suggests that the agent can interact in natural language and understand some, but not all, of our communication.\nMetaphors shape expectations: While other expectation shaping mechanisms for AI agents such as tutorials and instructions have been studied [41] , the effect of metaphors on user expectations and evaluations have not. Our work also bridges to research suggesting that people already form metaphor-based theories of socio-technical systems [26] and suggests design implications for how designers should choose their metaphors.\nCompeting predictions: assimilation vs. contrast: As people view AI agents as social agents [59] , the metaphor-and thus the nature of that agent-is likely to influence their experience. However, the literature presents two competing theories for how changes to the metaphor -and thus to expectations -will impact user evaluation of an AI system. Assimilation theory [67] states that people adapt their perceptions to match their expectations, and thus adjust their evaluations to be positively correlated with their initial expectations. (As Dumbledore points out to Snape in Harry Potter and the Deathly Hallows, \"You see what you expect to see, Severus. \") Assimilation theory argues that users don't perceive a difference between their pre-use expectations and actual experiences. Prior work supports that, for interactive systems, users' expectations do influence evaluations [31, 73] . For example, users rate an interactive system higher when they are shown a positive review of that system before using it, and rate the system lower if they are shown a negative review before using it [58] . Likewise, humor and other human-like characteristics that create high social intelligence expectations can be crucial in producing positive evaluations [36, 48] .\nCompeting predictions: assimilation vs. contrast: Assimilation theory would predict that a metaphor signaling high competence will set positive expectations and subsequently lead to positive evaluation: Hypothesis 1 (H1). Positive metaphors (e.g., high competence, high warmth) will lead to higher average intention to adopt and desire to cooperate with an AI agent than if it had no metaphor or negative metaphors.\nCompeting predictions: assimilation vs. contrast: Contrast theory [67] , on the other hand, attributes user evaluations to the difference they perceive between expectations and actual experience. Contrast theory argues that we are attuned not to absolute experiences, but to differences between our expectations and our experiences. For example, exceeding expectations results in high satisfaction, whereas falling short of expectations results in lower satisfaction. This suggests that it is beneficial to set users' initial expectations to be low (with practitioners reasoning in the manner of George Weasley, in Harry Potter and the Order of the Phoenix, \"'E' is for 'Exceeds Expectations' and I've always thought Fred and I should've got 'E' in everything, because we exceeded expectations just by turning up for the exams.\") Users of conversational AI agents such as Alexa stumble onto humorous easter egg commands that raise their expectations of what the system can do, but then report disappointment in the contrast to discovering the system's actual limits [49] . Likewise, ratings of interactive games are driven in part by contrasting players experiences against their expectations of the game [51] .\nCompeting predictions: assimilation vs. contrast: Contrast theory predicts that positive metaphors will backfire because AI agents inevitably make mistakes and have limits: Hypothesis 2 (H2). Positive metaphors (e.g., high competence, high warmth) will lead to lower average intention to adopt and desire to cooperate with an AI agent than if it had no metaphor or negative metaphors.",
    "rq": "research question: how do metaphors impact evaluations of interactions with conversational ai systems?\n point likert scales: how likely are you to try out this ai system?\n point likert scale: how likely are you to cooperate with such an ai system?"
}
{
    "intro": "",
    "relatedWork": "RELATED WORK: Our research draws on and extends literature on parental engagement to support their children's education practices, and the design of chatbots within human-computer interaction (HCI). We also emphasize that our work adds to limited research published at ICTD on underserved contexts in the U.S.\nTechnology and Parental Engagement: Parents' support to children's education is an emergent topic across HCI. Previous research show that for parents, parent-school communication technologies have the potential to foster their engagement. They find parent portals and electronic progress to be useful for becoming more informed of their children's academic performance [8, 10] . Many parents also appreciate email-based communication and feel that technology can allow them to ask questions and voice concerns [18] . Existing work has attempted to harness such potential by proposing technologies for enhancing the classroom experience like videotapes [14] , social networking sites [25] , and learning suites [24] . These solutions aim at helping parents, teachers, and children connect to each other and participate together in classroom-based activities. However, these do not address parents' information needs. Further, there is not enough work on the parental engagement needs of non-dominant groups. Our work proposes the design of new technologies for low-income Latino parents that align with these parents' cultural, socioeconomic, and technological ecologies to enhance how they seek and access educationrelated information.\nTechnology and Parental Engagement: In regard to how to enhance parents' information access, three studies are particularly relevant to our work. DiSalvo's and Khanipour's research on parents' practices when seeking educational information for their children [9, 15] suggested that new parentschool communication technologies should facilitate parent-led initiatives rather than only school-authored ones. Wong-Villacres et. al make further design recommendations in this regard [22] . We build on this prior work to focus on designing for (1) supporting equitable interactions for all parents and teachers, (2) providing a unified and organized source of information, and (3) enabling access to relevant information.\nChatbots for Information Access: Chatbots are artificially intelligent agents that are able to converse with humans using natural language, allowing users to type questions and respond with meaningful answers [7] . Due to this potential to deliver relevant information responding to natural language, chatbots are increasingly becoming commonplace [20] . Companies are using chatbots as booking agents in messaging applications, and health organizations are exploring its use to facilitate access to health advice. Chatbots also hold great potential to combat digital inequities; their natural-language interface could facilitate technology across groups that are less tech-savvy [11] .\nChatbots for Information Access: Although there is a long line of research exploring the interaction between humans and computer-based agents [20] , research on chatbots is still emergent [11, 23] . Recent studies on chatbots have suggested that the expectations of use for chatbots are shaped by cultural considerations [20, 23] . The work of Medhi Thies et al., for example, found that young, well-to-do, urban users in India preferred a chatbot that behaved as a friend, making useful recommendations [20] . Similarly, Zamora identified that, in contrast with Americans, Indians would prefer multi-language interactions with chatbots [23] . There is, however, no work on chatbots that describes how non-dominant communities perceive these tools. We address this gap focusing on the realm of learning-related information for supporting parental engagement.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: In interpersonal communication, nonverbal social signals play an important role because they convey a large part of information that may have an even deeper influence on the outcome of a conversation than the word meanings themselves. In stressful situations, such as job interviews, humans often employ nonverbal behavior that has a negative impact on how they are perceived. However, particularly in such scenarios, it is especially important to give the impression of confidence and attentiveness to convince others of one's strengths and competence. In recent years, a variety of research projects tackled the problem of tutoring users to improve social behaviors. With the help of serious games or similar training environments, such systems aim to simulate interpersonal communication with conversational agents or robots. Compared to real human trainers, this also comes with several advantages for training purposes, such as the avoidance of additional stress, lower costs, permanent availability, and reproducibility of training sessions. Examples for rather specific use cases are public speeches [Batrinca et al. 2013] , social humor situations [Niewiadomski et al. 2013; Mancini et al. 2014] , intercultural communication [Endra\u00df et al. 2013] , negotiation scenarios [Traum et al. 2012] , or psychotherapy [Kang et al. 2012] .\nINTRODUCTION: A first step toward helping people effectively improve their performance typically consists of finding critical behaviors during a training session. Therefore, it is common practice for experts to take notes on what attracted their attention. In more sophisticated environments, data are annotated alongside audiovisual recordings following a predefined annotation scheme, so users can reflect on their behavior while feedback is given by experts. A negative aspect of this approach is that the annotation of such recordings requires several iterations and is often extremely time-consuming. Additionally, annotations often vary greatly between annotators because subjective perception differs from person to person. Some more advanced training systems partly process human behavior, such as the amount of smiles or the prosody of speech [Hoque et al. 2013 ], but are still limited in the range of modalities that are automatically analyzed. Depending on the application focus, the main goals from a social coach's point of view are particularly the analysis and control of high-level concepts, such as signaling engagement and attention, the establishment of rapport and trust, and effective and fluent communication through grounding. Such concepts are based on biand multidirectional behavior patterns that are generated by the temporal alignment of the interlocutors' actions. Examples for this purpose are directed gaze, declarative pointings, gaze following, turn-taking signals, backchanneling, and mirroring. Depending on the context of the situation, such behavior patterns may vary considerably, and social signals need to be interpreted accordingly. In state-of-the-art training systems, such phenomena have not been in focus as such systems are at the most concerned only with the user's nonverbal signals.\nINTRODUCTION: In this article, we present a system, Nonverbal behavior Analyzer (NovA), for automating the annotation process using real-time social signal processing techniques in combination with interaction modeling concepts. The system was originally developed as part of the TARDIS [Anderson et al. 2013 ] EU-project. TARDIS aims to help young people improve their social skills during job interviews by providing a training environment with a virtual recruiter. The proposed system also provides concepts and tools for future research in behavior analysis in human-human and especially human-agent interactions. By analyzing both the user's behavioral cues and the agent's interaction cues, the system allows us to automatically annotate a social human-agent interaction. Such cues are computed in real time during the interaction by using sensing devices, such as a Microsoft Kinect to recognize behavioral characteristics, and by recording the agent's verbal and nonverbal behavior. Furthermore, scenario-specific meta information is logged to improve the analysis of the situation by considering conversational context. Figure 1 gives an overview on the architecture of our system. We see the main contributions of this work in:\nINTRODUCTION: -A wide range of real-time multimodal social cue recognition algorithms for automated annotations. Because we aim to automate the annotation process of human behavior, we created a set of general real-time social cue recognizers for multiple modalities. This set is built on a plugin system that allows recognizers to be turned on or off depending on the user's needs and available hardware. The recognizers have been developed in cooperation with psychologists and social-coaching trainers. They are also customizable for specific needs in various scenarios. Once adjusted, recognizers will return objective and comparable observations, which are essential for the automated labeling process. -Annotation of agent behavior, dialog context, and bidirectional interaction cues. A crucial problem is that the simple occurrence of a behavioral cue does not necessarily allow us to make straightforward assumptions about the user's intentions. As an example, a smile can be a sign of happiness (which is assumed by most systems), but could, for example, also be an expression of embarrassment (also see McKeown et al. [2015] ). Therefore, to interpret behavioral cues correctly, the dialog context has to be taken into account. If the agent is telling a funny joke and the user reacts with a smile, one could assume that the agent amuses the user. If the agent discourages the user, but he or she still smiles, it is more likely that the user is overplaying the situation. In addition, there are tightly coordinated behaviors that are elicited by two or several interlocutors. Such behaviors, called bidirectional cues, include phenomena, such as grounding, mutual gaze, mirroring, or backchanneling, that are relevant to the analysis of dialogue dynamics. To be able to correctly interpret behavior signals, it is highly important to annotate not only the user's, but also the agent's behaviors. -Context-aware analysis of the interaction by considering meta information, such as current topics or conditions. Another aspect that has to be considered is the influence of the conversational context on the assessment of behaviors. Although some behaviors are appropriate in one situation, they might not be in another. To give an example: A high amount of hand movements during a job interview might be interpreted as a sign of engagement when the candidate is talking about his or her hobbies. As a consequence, the candidate's behavior would result into a positive assessment by the interviewer. However, a candidate who is fidgeting with his or her hands while responding to a difficult question might instead be evaluated negatively because the candidate's behavior might be taken a sign of nervousness. In our system, context information is used to influence the model for the automated analysis of the user's behavior. -Real-time inferring of social attitudes based on probabilistic models to receive implicit information. Information on the users' social attitude, such as their level of attentiveness, is typically conveyed by more than one signal behavior at a time. We therefore suggest the use of probabilistic models to infer social attitudes based on multiple inputs, such as (bidirectional) multimodal behavior cues and context information. Creating such continuous outputs helps in pointing out the most relevant incidents of a social interaction, which is especially worth aspiring to in social coaching environments. This information is further suited to be used as real-time input for an agent to react to the user's current social attitude. -Automated analysis and visualization of discrete and continuous annotations and automated statistical analysis tests of the interaction. We created a graphical user interface for the visualization of detected behaviors. Inspired by classical annotation tools, it presents tier-based labeling. Additionally, it also features continuous timeline diagrams, bar and pie charts, heat maps, and social attitude outputs to point out characteristics encountered in interpersonal interaction in a fully automated manner. Experts might use such information when counseling users about their behaviors in coaching scenarios.\nINTRODUCTION: In the next section, we review related work in the area of annotation tools, automated behavior analysis, and interaction modeling. Next, we introduce the TARDIS job interview game to illustrate a use-case application that makes use of our general architecture and concepts. We then describe our four main components: the social cue recognition module, the interaction modeling module, the social attitude component, and the analysis and annotation tool that serves as a coaching user interface. Finally, we discuss experiences from evaluating our use-case application in a field study at a local school that showed that a combination of the suggested concepts leads to significant improvements in coaching compared to traditional methods.",
    "relatedWork": "RELATED WORK: Our proposed system combines work on annotation tools with technologies to automatically analyze human behavior, as well as on interaction modeling techniques. The user interface of NovA was inspired by existing annotation tools and makes use of multiple tracks to code relevant social features. However, unlike conventional annotation tools, our system performs the segmentation and labeling of the data completely automatically. NovA distinguishes from earlier work on automated behavior analysis by considering not only the behavior of the user, but also that of the agent and the dynamics that arise during the conversation in a social training scenario. In our work, we further investigate interaction and conversation context for the improved analysis and annotation of human-agent conversations. Progress in the field of analyzing human social behavior has been boosted by a variety of annotation tools that facilitate the labeling of corpora at different levels of granularity following a predefined coding scheme. Examples include European distributed corpora project Linguistic ANotator (Elan) [Wittenburg et al. 2006 ], ANotation of VIdeo and Language (Anvil) [Kipp 2013 ], and Exmaralda [Schmidt 2004 ], which offer Context-Aware Automated Analysis and Annotation of Social Human-Agent Interactions 11:5 layer-based tracks to insert time-anchored labeled segments. Another example is FEELtrace [Cowie et al. 2000 ], a tool that allows an observer to track the emotional content of an audio-visual stimulus over time based on activation-evaluation space. A newer edition of FEELtrace is the General trace (Gtrace) , with the ability to let people use their own dimensions and scales. While it is unquestionable that these tools offer much help in describing audio-visual material with a high level of detail, they offer only little automation. However, since creating descriptions for several hours of interaction remains an extremely time-consuming task, methods to automate the coding process are highly desirable.\nRELATED WORK: Techniques for the automated analysis of social behavior patterns were pioneered by Pentland and his group at MIT Media Lab with the development of wearable devices, socalled sociometers, to capture people's verbal and nonverbal signals. They investigated not only the social behaviors of people engaged in face-to-face conversations [Curhan and Pentland 2007] , but also analyzed interaction patterns from larger groups of people using smartphones with dedicated sensors [Pentland 2007 ]. To analyze social behaviors, a large variety of verbal and nonverbal cues have been taken into account. Dong et al. [2007] analyze speech activity and fidgeting (i.e., the amount of movement in a person's hands and body) to detect functional roles in a group. Hung and Gatica-Perez [2010] studied audio cues (such as overlapping speech), video cues (such as motion energy), and audio-visual cues (such as the amount of movement during speech) to determine the level of group cohesion in meetings. Methods have been developed to detect social attitudes from various modalities including facial expressions [Sandbach et al. 2012] , gestures [Caridakis et al. 2006; Michelet et al. 2012; Mahmoud et al. 2013] , speech [Vogt et al. 2008] , postures [Kleinsmith and Bianchi-Berthouze 2011] , and physiological measurements [Kim and Andr\u00e9 2008] . Nakano and Ishii [2010] developed a method to assess user engagement from eye gaze in user-agent interactions. Thompson and Bohus [2013] developed a system that supports feature annotation for model building using a variety of machine learning techniques. Also, multimodal approaches to improve emotion recognition accuracy have been reported, mostly by exploiting audiovisual combinations [Camurri et al. 2005; Scherer et al. 2012; Sebe et al. 2006; Gunes et al. 2008] . Results suggest that integrated information from audio and video leads to improved classification reliability compared to a single modality. Even though the role of context has been recognized in the area of social signal processing [Pantic et al. 2005] , work that actually exploits context information to improve recognition rates is rare. An example includes the work by Conati and Maclaren [2009] who describe a tutor agent that makes use of a Bayesian network for interpreting the learner's behavior. Following bio-sensor and user interface (UI) inputs, as well as context in the form of previously prompted personality traits, the system tries to infer the emotions of the learner. The Virtual Human Toolkit [Hartholt et al. 2013] provides social signal analysis combined with dialogue act-based generation of nonverbal agent conversational behavior. Context information is used to refine the analysis of social signals. Although this is modeled within dialogue acts, our approach uses an explicit interaction model to represent contextual information (e.g., discourse phase or agent states). An example of how context is used for improving social signal processing can be found in Morency et al. [2007] . By considering specific types of questions, they achieved improvements in recognizing head movements in a human-agent interaction. The SEMAINE [Schr\u00f6der et al. 2012 ] platform focuses on the analysis of emotions in an interactive communicative setting. There, visual and acoustic signals in speech and listening phases during an interaction with a virtual agent are analyzed. In our approach, we consider an extended set of modalities for the analysis of user behavior. Furthermore, by applying a more fine-grained interaction dialogue model and control, we extend the analysis to bidirectional behavior and discourse context information. ",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Business Intelligence(BI) tools and applications play a key role in the enterprise to derive business decisions. BI dashboards provide a mechanism for the line of business owners and executives to explore key performance metrics (KPIs) via visual interfaces. These dashboards are usually created by technical people. In fact, there are many technical people involved in the pipeline from the data to the dashboards, including the database designers, DBAs, business analysts.\nINTRODUCTION: This work is licensed under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. For any use beyond those covered by this license, obtain permission by emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights licensed to the VLDB Endowment. Proceedings of the VLDB Endowment, Vol. 13, No. 12 ISSN 2150-8097. DOI: https://doi.org/10.14778/3415478.3415557 Figure 1 shows a typical architecture of a BI stack. The underlying data resides in a traditional RDBMS, and a business model is created in terms of an OLAP cube definition [13] that describes the underlying data in terms of Measures (numeric or quantifiable values), Dimensions (categorical or qualifying attributes), and the hierarchies and relationships between them. Then, business analysts create the BI reports and dashboards using the BI model (cube definition) 1 . The reports and the dashboards are supported by structured queries that run against the underlying database to render the visualizations to the user.\nINTRODUCTION: To obtain answers to questions that are not contained in the existing dashboard visualizations, users need to enlist the help of technical people, and the turnaround time for such cycles can be prohibitively time-consuming and expensive, delaying key business insights and decisions. Today's enterprises need faster access to their KPIs and faster decision making.\nINTRODUCTION: Conversational interfaces enable a wide range of personas including non-technical line of business owners and executives to explore their data, investigate various KPIs, and derive valuable business insights without relying on external technical expertise to create a dashboard for them. As such, conversational interfaces democratise access to data significantly, and also allow dynamic and more intuitive explorations of data and derivation of valuable business insights.\nINTRODUCTION: Today's chatbot and voice assistant platforms (e.g., Google Dialogflow, Facebook Wit.ai, Microsoft Bot Framework, IBM Watson Assistant, etc.) allow users to interact through natural language using speech or text. Using these platforms, developers can create many kinds of natural language in-terfaces (e.g., chatbot, natural language search, etc.) for any kind of domain (e.g., weather, music, finance, travel, healthcare, etc.). These custom or domain-specific natural language assistants usually target a range of domain specific tasks, such as booking a flight, or finding a drug dosage. Such task-oriented agents limit the scope of the interaction to accomplishing the task at hand and hence are more tractable to design and build. However, these taskoriented agents fail to address the challenges involved in iterative data exploration through conversational interfaces to gain information and derive meaningful insights.\nINTRODUCTION: Recently, several business intelligence tools, such as Ask Data Tableau [2] , Power BI [8] by Microsoft, Microstrategy [6] , and the IBM's Cognos Assistant [3] , also explored exploiting natural language interfaces. These early systems have many restrictions in terms of the conversational interaction they provide, as they rely on the user to specify several parameters, and only offer a fixed set of patterns.\nINTRODUCTION: There are several challenges in creating a conversational interface for a BI application. The first challenge is creating a data model that captures the entities, and their relationships and associated semantics that are relevant to the underlying data and the common set of BI queries and operations. We have two options: Modeling the underlying data in the RDBMS, or modeling the cube definition. We chose the latter, because a cube definition provides important BI specific information, such as measures, dimensions, dimension hierarchies, and how they are related.\nINTRODUCTION: The second challenge is building the necessary capability of the conversation system to capture user intent, recognize and interpret the different workload access patterns. We explore three different approaches, which we explain in detail in Section 3.3. The first two approaches use only the information available in the ontology, capturing the structural relationships between measures and dimensions. The third approach also takes into account user's access patterns.\nINTRODUCTION: The third and the final challenge is the integration with the underlying BI platform to issue appropriate structured queries and render the intended visualizations.\nINTRODUCTION: In this paper, we explore the use of conversational interfaces for BI applications. In earlier work [22] , we developed an ontology-based approach to developing conversational services to explore the underlying structured data sets. In particular, we developed techniques to bootstrap the conversation work space in terms on intents, entities, and training samples, by exploiting the semantic information in an ontology. In this paper, we extend that work for BI applications. In particular, we observe that users follow certain B I patterns and operations when analyzing their data using BI tools. We exploit this information in the construction of the conversation work space, as well as the conversation design. We have implemented our techniques in Health Insights (HI), an IBM Watson Healthcare offering, providing analysis over insurance data on claims, and our initial feedback from users has been very positive.\nINTRODUCTION: We demonstrate the effective exploitation of the BI access patterns to provide a more dynamic and intuitive conversational interaction to derive business insights from the underlying data, without being tied to a fixed set of pre-existing dashboards and visualizations. We evaluate our approach and show that our conversational approach to BI not only covers the use cases supported by pre-defined dashboards, but goes way beyond to assist users in better understanding the insights from existing visualizations as well as discovering new and useful insights that are not covered by the pre-defined dashboards through the dynamic generation of structured queries and integration with the underlying BI platform.\nINTRODUCTION: The main contributions of this paper can be summarized as:\nINTRODUCTION: \u2022 We propose an end-to-end ontology-based framework, and tools to create a conversation service for BI applications.\nINTRODUCTION: \u2022 We create an ontology from a business model, capturing all the key information for the BI application, including measures, dimensions, dimension hierarchies, and their relationships.\nINTRODUCTION: \u2022 We exploit common BI access patterns and use the ontology to generate several conversation space artifacts automatically, including intents, entities, and training examples.\nINTRODUCTION: \u2022 We adapt the dialog structure to support the BI Access patterns and operations to provide an intuitive conversational interaction for BI applications.\nINTRODUCTION: \u2022 We implement and demonstrate the effectiveness of our proposed techniques for Health Insights, an IBM Watson Healthcare offering.\nINTRODUCTION: The rest of the paper is organized as follows. Section 2 provides a brief overview of our ontology-driven approach for building conversational interfaces for BI applications. Section 3 describes in detail our approach for data modeling and generation of conversational artifacts including intents, entities and dialog. We discuss the implementation of our proposed techniques in a healthcare use case Health Insights in Section 4 and provide a detailed system evaluation in Section 5. We discuss related work in Section 6 and conclude in Section 7.",
    "relatedWork": "RELATED WORK: We cover relevant related work in this section under three different categories described below.\nRELATED WORK: Natural language support in existing BI tools Several existing business intelligence tools, such as Ask Data Tableau [2] , Power BI [8] by Microsoft, Microstrategy [6] , and the IBM's Cognos Assistant [3] , support a natural language interface. However, these systems are restricted in terms of the conversational interaction they provide. A majority of these systems rely heavily on the user to drive the conversation. More specifically, they leave the onus on the user to select from a large number of options and parameters through user interfaces for getting to an appropriate visualization without much system support. Our system, on the other hand, uses information in the ontology to guide the user through meaningful conversational interactions and elicits further information to access appropriate visualizations. Further, unlike these systems our ontologydriven approach provides a formal mechanism for defining a semantically rich entity-centric view of the business model capturing both actual measures, dimensions and higher level groupings to support more complex queries catering to the querying needs of a wider range of personas. Further, our novel automated workflow for constructing a conversational BI system, enables rapid prototyping and building conversational BI systems for different domains.\nRELATED WORK: Current conversational systems Existing conversational systems can be classified into three different categories [15] based on the kind of natural language interaction they support. First, are one shot question answer systems, second are general purpose chatbots such as Microsoft Cortana [19] , Apple Siri [10], Amazon Alexa [9] , etc. that can converse on a range of different topics such as weather, music, news or can be used to accomplish general tasks such as controlling devices, timers etc. and are agnostic to any particular domain. The third category are task-oriented agents that target tasks in specific domains such as travel, finance, healthcare and are limited in scope to specific tasks such as booking a flight, finding account balance, etc. These task oriented chatbots however fail to address the challenges involved in data exploration and derivation of meaningful insights especially for business applications. We propose an ontology-based approach for building conversational systems for supporting BI applications through natural language interfaces.\nRELATED WORK: Approaches for dialogue management Recent advances in machine learning, particularly in neural networks, have allowed for complex dialogue management methods and conversation flexibility for conversational interfaces. The approaches that are commonly used in building the dialogue structure for a conversational interface are: (1) Rule-based approaches [18, 17] used in finite-state dialogue management systems are simple to construct for tasks that are straightforward and well-structured, but have the disadvantage of restricting user input to predetermined words and phrases.\nRELATED WORK: (2) Frame-based systems [14, 11, 16] address some of the limitations of finite state dialogue management by enabling a more flexible dialogue. Frame-based systems enable the user to provide more information as required by the system while keeping track of what information is required and ask questions accordingly. (3) Agent-based systems [12, 25, 23, 20] . Agent-based methods for dialogue management are typically statistical models and require to be trained on a corpora of prior user interactions for better adaptation. We found the frame based systems most suitable for adaptation for building a conversational BI systems to support the commonly observed BI query patterns.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: As intelligent assistants such as Siri (Apple), Facebook Messenger, Amazon Alexa, Google Assistant, enter the daily life of users, research on conversational information systems is becoming increasingly important. There are mainly three kinds of conversational systems (i.e. dialogue systems): chit-chat, informational chat and task oriented chat. Chit-chat systems are focusing on information social chat and try to interact with human-like reasonable or interesting responses [8] [25] . Informational chatbots try to help user find information or directly answer user questions. Task oriented chatbots try to help users finish a specific task, such as booking a flight or canceling a trip. And they are usually built for a close domain. This paper is related to both informational chat and task oriented chat.\nINTRODUCTION: Due to the big commercial potential, there are quite some activities on task oriented conversational chatbots that can interact with users to help them find products/services. Companies like Amazon, Google, eBay, Alibaba are all rolling out these chatbots. Most of existing works are focusing on natural language processing or semantic rich search solutions for dialogue systems. The most notable recent related work is [5] , which focuses on enabling user to query knowledge base interactively.\nINTRODUCTION: On the other hand, researchers have demonstrated the importance of recommender systems in e-commerce websites and applications. To improve the success or conversion rate of a shopping/sales chatbot, we argue that one should integrate recommendation techniques into conversational systems. Intuitively, this can benefit both recommender systems and dialog systems. For dialogue systems, good recommendations based on users' previous purchasing or rating history can better fulfill user's information need, and create more business opportunities. For recommender systems, dialogue systems can provide more detailed information about user intentions, such as user preferred price range or the location of a restaurant, by interactively soliciting and identifying user intentions based on multi-round natural language conversation. This motivates us to study how to build a conversational recommender system.\nINTRODUCTION: This paper tries to integrate search and recommendation techniques with conversational systems seamlessly. We build a chat agent that can assist users to find items interactively. With the recent breakthrough of deep learning technologies and a better understanding of search and recommendation, we can approach this problem with a new perspective and a set of enabling technologies. Similar to other dialog systems, our system has three major components. First, a natural language understanding (NLU) module for analyzing each user utterance, keeping track of the user's dialogue history and constantly updating the user's intention. This NLU module focuses on extracting item specific meta data. Second, we propose a dialogue management (DM) module that decides which action to take given the current state. This DM module has an action space defined specifically for this task. It is well integrated with an external recommender system. The third component is a natural language generation module to generate response to the user. This framework enables us to build a conversational search and recommender system that can decide when and how to gather information from users and make recommendations based on a user's past purchasing history and context information in the current session.\nINTRODUCTION: For the NLU module, we train a deep belief tracker to analyze a user's current utterance based on context and extract the facet values of the targeted item from the user utterance. Its output is used to update the current user intention, which is represented as a user query that is a set of facet-value pairs about the target. The user query will be used by both the dialogue manager and the recommender system. For the DM module, we train a deep policy network that decides which machine action to take at each turn given the current user query and long term user preferences learned by the recommender system. The action could be asking the user for information about a particular facet or recommending a list of products. The deep policy network selects an action that maximizes the expected reward in the entire conversation session. When the user query collected so far is sufficient to identify the user's information need, the optimal action usually is recommending a list of items that is personalized for the user. When the user query collected is not sufficient, the optimal action usually is asking for more information.",
    "relatedWork": "RELATED WORK: There are four lines of research that are closely related to and motivate our work: conversational dialogue system, recommender system, faceted search and the deep reinforcement learning.\nDialogue System: There have been three main streams of dialogue systems (DS): the chit-chat DS, informational DS and task-oriented DS. Early works of task oriented DS require large amount of labeled data [30] [27] and are very expensive. Recent works tend to apply deep learning techniques on each component of the dialogue system and have demonstrated significant improvements. Mesnil et al. [13] used LSTM and Conditional Random Fields networks to perform the slot filling. Wu et al. [29] developed an entropy based policy for the DS. Christakopoulou et al. [3] used bandit machine for the decision making. Zhao et al. [32] was among the first works of building an end-to-end dialogue system. Wen et al. [26] introduced an end-to-end task oriented dialogue system and a wizard-of-oz framework for data collection in the restaurant domain. Bordes et al. [1] built an end-to-end task oriented bot based on memory network. Dhingra et al. [5] built a goal oriented information access system based on reinforcement learning, trying to select related items with certain attribute values. However, most of those prior works focus on NLP challenges instead of commercial success metrics such as conversion rate. They either did not focusing on recommendation problems or did not model or utilize the user's past preferences when recommending items to users.\nRecommender Systems: Recommender systems have achieved much commercial success and are becoming increasingly popular in a wide variety of practical applications. For example, online stores such as Amazon, iTunes and Netflix provide customized recommendations for additional products or services based on a user's history. Most recommender systems are either content based [10] , collaborative filtering (CF) based [7] [14] or hybrid. Research in recommender systems usually focus on improving rating prediction or ranking measures (learning to rank for recommendation) [7] [18] . Few work has been done towards making recommendations in a dialogue system. Christakopoulou et al. [3] studied using a generative Gaussian model to recommend items to users in a conversation. However, their model does not target at maximizing the long-term benefits and their conversational agents' action space is very limited and doesn't include actions one would typically expect in a dialog system. Their dialog system only asks questions about whether a user likes an item or whether the user prefers item A to item B, while a typical task oriented dialogue system often solicits facets from users [26] [5] . In this work, we maximize the long-term utility by using the reinforcement learning framework, and our question types are requesting facets from users and recommending a list of items to users, which are better aligned with the typical dialogue systems.\nFaceted Search: Conversational recommendation agent interactively helps a user find candidate items. Faceted search, a commonly used web technique in the e-commerce domain, is also a technique that interactively helps a user find candidate items. On faceted-search-enabled websites, buyers can narrow down their list of products by adding constraints on a group of merchandise facets [24] . It has been shown that a well designed faceted search idea can be understood by the average user. Users might have preferences for certain types of facets. Movie viewers might have preferences on movie genres, directors or actors; shoe buyers might have preferences on brands and colors, while restaurant seekers might prefer a region of food or a price range, etc. To avoid overwhelming users with too many facet-value pair options per conversation, a faceted search engine selects a small set of facets or facet-value pairs for a user to choose from based on context [31][6] . Motivated by prior work, we introduce a particular type of machine actions into our conversational system: selecting a facet based on the context and asking user to provide information about her preferred facet value, such as \"What's the color you like?\", \"Which brand you prefer?\", \"Do you like small size, middle size or large size?\". Unlike prior work on faceted search, our facet selection decision is made by a deep reinforcement learning algorithm.\nDeep Reinforcement Learning: Deep learning techniques allow people to use deep neural networks for function approximation in reinforcement learning (RL) [22] . One of the most famous success of deep RL is Google's DeepMind research on the game of Go [20] [21] . Deep RL has been applied for better sequential decision making in various domains, including End-to-End dialogue systems using deep RL for information access [5] [32] , information extraction [16] , query reformulation [17] , real time ads bidding [2] . Shani et al. [19] is one of the early studies of applying RL techniques for the recommender system problems, however, not in dialogue systems. Inspired by these works, we build a deep RL based conversational recommender system. This combines the ranking and personalization ability of recommender system with the sequential decision making power of the RL models, thus can better serve a user.",
    "rq": "agent: what rating range do you want?"
}
{
    "intro": "INTRODUCTION: Natural language dialog has been used in many areas, such as for call-center/routing application (Carpenter & Chu-Carroll 1998) , email routing (Walker, Fromer & Narayanan 1998), information retrieval and database access (Androutsopoulos & Ritchie 1995) , and for telephony banking (Zadrozny et al. 1998) .\nINTRODUCTION: In this demonstration, we present a natural language dialog interface to online shopping. Our user studies show natural language dialog to be a very effective means for negotiating user's requests and intentions in this domain.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "Introduction: 'Natural Language Generation' also known as 'Automated Discourse Generation' or simply 'Text Generation', is a branch of computational linguistics, which deals with automatic generation of text in natural human language by the machine. It can be conceptualized as a process leading from a high level communicative goal to a sequence of communicative acts that accomplish this communicative goal (Rambow et. al., 2001) . Based on input representation, any NLG technique can be broadly classified into two paradigms viz. Template based Approach and Plan based approach. The template-based approach does not need large linguistic knowledge resource but it cannot provide the expressiveness or flexibility needed for many real domains (Langkilde and Knight, 1998) . In (Deemter et. al., 1999) , it has been tried to prove with the example of a system (D2S: Direct to Speech) that both of the approaches are equally powerful and theoretically well founded. The D2S system uses a tree structured template organization that resembles Tag Adjoining Grammar (TAG) structure. The template-based approach that has been taken in the system, enables the basic language generation algorithms application independent and language independent. At the final stage of language generation it checks the compatibility of the sentence structure with the current context and validates the result with Chomsky's binding theory. For this reason it is claimed to be as well founded as any plan-based approach. As another practical example of NLG technique, we can consider the IBM MASTOR system (Liu et. al., 2003) . It is used as speech-to-speech translator between English and Mandarin Chinese. The NLG part of this system uses trigram language model for selecting appropriate inflectional form for target language generation.\nIntroduction: When NLG (or NLP) technology is applied in assistive technology, the focus is shifted to increase communication rate rather than increasing the efficiency of input representation. As for example, CHAT (Alm, 1992) software is an attempt to develop a predictive conversation model to achieve higher communication rate during conversation. This software predicts different sentences depending on situation and mood of the user. The user is free to change the situation or mood with a few keystrokes. In \" Compansion\" project (McCoy, 1997) , a novel approach was taken to enhance the communication rate. The system takes telegraphic message as input and automatically produces grammatically correct sentences as output based on NLP techniques. The KOMBE Project (Pasero, 1994) tries to enhance the communication rate in a different way. It predicts a sentence or a set of sentence by taking sequence of words from users. The Sanyog project (Sanyog, 2006) (Banerjee, 2005) initiates a dialog with the users to take different portions (eg. Subject, verb, predicate etc.) of a sentence and automatically constructs a grammatically correct sentence based on NLG techniques.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": " our main research question for this work was: how can co-design participants be empowered in designing a chatbot supporting social integration within the context of migrants?\n these findings led to the first design question: what personalities does an anthropomorphic chatbot avatar need to possess to create an emotional engagement with migrants?\n this led to the second design question: what represents a natural conversation flow between the user and the chatbot, which enhances the efficiency of question answering?\n this provided us the last design question: how to motivate users to train the chatbot while engaging in a natural conversation?\n: what personalities does an anthropomorphic chatbot need to possess to create emotional engagement with migrants?\na second co-design session was planned and conducted to resolve the remaining two design questions: what represents a natural conversation flow between the user and the chatbot, which enhances the efficiency of question answering?"
}
{
    "intro": "INTRODUCTION: Recommender systems have become critically important for helping users quickly find ideal items among a large number of products [25] . However, personalized recommendations may lead users to increasingly narrower space of items over time (called \"filterbubble\" effects) [20, 30] . To mitigate this issue, several attempts have been made to encourage users to explore diverse sets of items, such as diversity-driven algorithms [41, 42] and visualizing recommendations [16, 32] . On the other hand, dialogue-based conversational recommender systems enable users to freely give feedback on recommendations through natural language [3, 11, 12] , which show considerable potential for promoting users' exploratory activities. However, so far little work has studied supporting user exploration through conversational interaction.\nINTRODUCTION: Recently, there is a work [12] that studied a chatbot for accommodating user control in music recommendations with two critiquing techniques (i.e., user-initiated critiquing (UC) and system-suggested critiquing (SC) [7] ). The user study of this work reveals that users tend to feel receiving more diverse recommendations when using the system with both UC and SC. Inspired by this observation, we consider to stimulate users' exploration of recommendations by strengthening the critiquing technique in conversational interaction.\nINTRODUCTION: Therefore, in the current work, we have designed two kinds of system-suggested critiquing technique: Progressive systemsuggested critiquing (Progressive SC) and cascading systemsuggested critiquing (Cascading SC) for facilitating users' exploration of music with two different directions: The former is preference-oriented, which provides critiques based on users' current preferences and incremental critiquing feedback [24] , while the latter is diversity-oriented, which suggests critiques to steer users into a cascade of diverse types of music using a strategical approach with the assumption of the cascading user behavior as inspired by [19] . Then, we have developed a music chatbot with three system variants, which are respectively featured with UC (i.e., users can make critiques on the recommended songs to explore songs they want), Progressive SC and Cascading SC. To investigate how these critiquing techniques influence users' music exploration with conversational interaction, we conducted a between-subject user study (involving 107 participants) to compare the three system variants in terms of both user perception of and user interaction with recommendations. We also examined how these critiquing techniques moderate the relationship between user interaction behavior and user perception of music recommendations.\nINTRODUCTION: In a short summary, we have mainly focused on answering two research questions as follows (see Figure 1 ): RQ1: How do critiquing techniques influence users' exploration of music in a conversational recommender?\nINTRODUCTION: RQ2: How do critiquing techniques moderate the relationship between user interaction behavior and user perception of music recommendations?\nINTRODUCTION: Our main contributions of this work are four-fold:\nINTRODUCTION: (1) We have proposed two kinds of system-suggested critiquing technique, in order to encourage users' exploration of music recommendations, and compared three variants of the system supported with different critiquing methods (i.e., UC, Progressive SC, and Cascading SC) in terms of users' perception of and interaction with recommendations. The experimental results show that users perceive higher diversity of recommendations with the system that offers Cascading SC and feel more serendipitous recommendations with the system that offers Progressive SC.\nINTRODUCTION: (2) We have investigated the moderation effects of critiquing techniques, and find that the critiquing techniques significantly moderate some relationships between interaction metrics (such as number of listened songs and number of dialogue turns) and user perception metrics (such as perceived helpfulness and serendipity). (3) We have analyzed users' interaction flow towards UC and SC, and find that users tend to use UC when they have gradually established their new preferences during the interaction with conversational recommendations, while users may be stimulated to request SC when they have benefited from the SC proactively offered by the system. (4) We have discussed our findings and provided practical implications for designing a critiquing-based conversational recommender system for supporting users' music exploration.",
    "relatedWork": "RELATED WORK 2.1 User Exploration in Recommender Systems: Prior work has shown various strategies to support user exploration by diversity-driven algorithms or visualizing recommendations. Diversity-driven algorithms typically generate recommendations that maintain the balance between accuracy and diversity [9, 18, [40] [41] [42] . For instance, some researchers proposed to increase the recommendation diversity based on items' attributes, such as book topics [42] , movie genres, and social tags [33] . In [30] , the authors proposed a way to help users take a gradual path towards the desired new music preference by traversing user preference graphs and generating a sequence of artists as guided transition. Most of the related studies have attempted to increase the diversity for a ranked list, but there are some limitations [6, 29] , such that users tend to pay less attention to the bottom of the list when exploring recommendations, which is a position bias. Besides, some works have visualized recommendations to support user exploration in recommender systems. For example, to raise users' awareness of exploration, [16, 32] highlighted the regions of the underrepresented recommendation space, so-called blind spots, which could help users to identify what is known and what is unknown in their profile. Some visualization systems, such as TalkExplorer, [34] , and Moodplay [1] , allow users to explore diverse items during the recommendation process. In [26, 27] , the authors introduced a shortlist as a short-term memory to reduce users' cognitive efforts and help users make better decisions when exploring diverse movies. The recommender systems discussed so far support user exploration by presenting diverse recommendations or visualizations. To the best of our knowledge, little work has been done to support user exploration with conversational interaction.\nConversational Recommender Systems: Conversational recommender systems aim to help users seek for their desired items through natural language [11] . Several studies have demonstrated this kind of conversational systems [28, 31] . For instance, ExpertClerk [28] is a conversational agent designed to interact with shoppers by asking questions to obtain their preferences and proposing recommendations to assist users to find their satisfactory products. Adaptive place advisor [31] provides personalized recommendations to assist users to find preferable places for traveling by considering both users' long-term preferences and short-term interests. Also, several studies show the superiority of conversational user interfaces over graphical user interfaces during the process of recommendations [10, 15, 38] .\nConversational Recommender Systems: In the broad area of recommender systems, critiquing-based recommender systems have been proposed to elicit users' critiquing feedback to help the system improve the recommendation [7] . In particular, there are two major types of critiquing technique, including user-initiated critiquing (i.e., users construct critiques by themselves) and system-suggested critiquing (i.e., the system generates a set of critique candidates for users to choose). A recent work [12] studied such kind of system with conversational interaction and found that critiquing techniques enable users to control recommendations in conversational user interfaces. They also observed that users tend to perceive higher diversity and efficiency when the conversational recommender presents system-suggested critiques compared to the system that only supports user-initiated critiquing. Inspired by this observation, we are interested in in-depth investigating how critiquing techniques can support users' exploration of recommendations with conversational interaction.\nConversational Recommender Systems: Different from [12] , in this work, for stimulating users' exploratory activities, we introduce two kinds of system-suggested critiquing: Progressive system-suggested critiquing that is preference-oriented (generating critiques considering both users' current preferences and incremental critiquing feedback [24] ); and cascading systemsuggested critiquing that is diversity-oriented (suggesting critiques in a strategic approach with the assumption of the cascading user behavior as motivated by [19] ). In addition, we consider the chatbot's proactivity in our designed systems (i.e., the ability of proactively offering SC to encourage users to explore music), since some previous studies have shown that the robot's proactivity may help people get rich information and reduce the decision space [23] .",
    "rq": " rq1: how do critiquing techniques influence users' exploration of music in a conversational recommender?\nrq2: how do critiquing techniques moderate the relationship between user interaction behavior and user perception of music recommendations?\nrq1: how do critiquing techniques influence users' exploration of music in a conversational recommender?\nrq2: how do critiquing techniques moderate the relationship between user interaction behavior and user perception of music recommendations?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: The population is growing older globally, with more older adults living alone [4] . However, as people age, the degree of social interactions often reduces drastically, and loneliness and social isolation become a reality for many living alone [5] . At the same time, the working population is under increased demands on their time, with many being spread out geographically far from their relatives. Taken together, this has the potential of limiting the social interaction older adults have with their relatives [10] . Research has already linked social isolation and loneliness to higher risks for various physical and mental conditions including heart disease [14] , depression [1] , and even death [6] . Many Western governments have seen these challenges as being a grand challenge for society to address [8] .\nINTRODUCTION: Designing conversational interfaces for older adults has received significant attention in the literature. For instance, Ring et al. [11] designed a conversational interface that addressed the social needs of older adults. Relatedly, Chi et al. [3] investigated the perceived acceptance and utility of a virtual agent embodied as a pet avatar. They found that participants enjoyed the companionship, entertainment and operational tasks, yet also voiced concerns regarding privacy, system's capabilities, dependence and costs. More recently, Sayago et al. [12] discussed some open issues and challenges of designing voice assistants for older adults including the way such agents should talk and appear.\nINTRODUCTION: Our work focuses on investigating how the design of the appearance and behavioral aspects of a conversational agent have on older adults' acceptance of these new technologies. To our knowledge, no studies have designed or tested conversational agents whose personalities are based on abstract embodiments such as a flower. However, such personalities may shift the focus from major concerns like privacy or technical capabilities to its intended purpose, that of casual conversations, companionship and entertainment to decrease social isolation.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: The productivity of information workers can be significantly influenced by information overload and stress. Due to the ubiquity of multiple devices, including desktops, laptops, phones, surfaces, smart watches and speakers, notifications, messages and other kinds of disruptions have become a serious problem for keeping focused on high priority tasks at work. A large body of work in human-computer interaction (HCI) research has concentrated on better understanding how workers manage their tasks, attempt to stay focused, and deal with distractions and interruptions throughout the day. In any given day, information workers are typically faced with multiple tasks that they need to complete, and often they devise unique strategies to remember and make note of these tasks [12, 15, 25] . A diary study of information workers found that workers had an average of 50 task-shifts over an entire work week [9] . Furthermore, information workers are usually faced with numerous interruptions throughout their day.\nINTRODUCTION: At work, people average 40 seconds on a computer screen before switching [23] , and it can take several minutes for an office worker to return to their original task after interruptions. The deleterious effects of notifications, email and face-to-face interruptions has been very well documented [9, 20] in terms of lowered productivity at work. Different factors like the type and duration of the interruption, the complexity of the task prior to interruption, and even the exact moment of the interruption can have a negative effect on workers' ability to resume a task, their perceived productivity and their satisfaction with their performance [2, 5, 25, 30, 38] . For instance, past work from Iqbal and Horvitz [14] found that when information workers were interrupted by conversation, they were more likely to return to work on more peripheral tasks like email and web searches, rather than resume their previous task.\nINTRODUCTION: Research suggests that more distractions can lead to higher reported stress and lower productivity in the workplace [20] [21] [22] . While there have been many attempts to design software to assist with this problem in order to reduce stress and improve focus (e.g., Freedom, Windows FocusAssist [29] , Tracktime [4] , see [16] for a review), few of these products have yet to be widely adopted in the Figure 1 : We present two productivity agent prototypes which help users schedule and block time on their calendar to focus on important tasks, monitor and intervene with distractions, and reflect on their daily mood and goals. Snapshots of the sample morning dialogue interactions for the text-based (left) and virtual agent (right) prototypes are shown. The face images are blurred to respect privacy.\nINTRODUCTION: workplace. Therefore, people continue to design their own methods and workarounds, but report having trouble nonetheless [16] .\nINTRODUCTION: These findings suggests that maintaining focus in the face of constant shifting priorities and interruptions in the workplace is a complex and important problem. Yet, to our knowledge, only one system, from Kimani et al. [16] , has yet been designed that helps knowledge workers with prioritizing their work, providing reminders on when to switch tasks or get back on task, to take breaks, as well as to reflect on how much they accomplished at the end of the day. While this effort was notable for its complex system design, the user interface was quite simple in its design, similar to a standard chatbot. In our work, we design two different agent prototypes which build upon this past work from Kimani et al. [16] (described further in the related work). Our paper offers three main contributions: 1) The design of two different conversational agents, one text-based (TB), similar to a chatbot, and one virtual, embodied, conversational agent that responds to the user's emotion (VA), 2) a longitudinal evaluation of these two agents against a shipping product that tries to help users schedule focus time in Microsoft Outlook, and 3) actionable insights from qualitative analysis of user feedback on agent design around anthropomorphism, user task scheduling, and the need for better back and forth negotiation and control with the user.",
    "relatedWork": "RELATED WORK: For task and time management, various different applications have been developed, most notably, MeTime [35] , RADAR [10] , TaskBot [34] , and Calendar.help [8] that each aim to assist users be more efficient in managing their work through different approaches. MeTime aims to provide real-time awareness of how users are spending their time through graphic visualization of their application usage in order to promote efficiency of time use achieving their task goals [35] . The authors showed that exposure to meTime decreased time spent on distractions (e.g., social media), and increased their feelings of productivity [35] . RADAR is an AI system that was designed to help reduce email overload by automatically producing tasks and to-do lists directly from users incoming emails [10] . Through an experimental setup, users that were aided by RADAR that were confronted with overload performed better at completing email intitiated tasks than without RADAR. [10] TaskBot, a chatbot agent was designed to mediate the creation and management of tasks within project teams [34] . Users found TaskBot useful for naturally transferring conversations into actionable tasks, but found that the system struggled when having to deal with multi-threaded conversations [34] . Finally, Calendar.help used an AI interface integrated with the user's email to automatically schedule user's meetings, and allowed the user or another human assistant to make adjustments manually when needed [8] .\nRELATED WORK: Applications developed within this domain have also aimed at helping block distractions to help users focus. One such example is Microsoft Focus Assist, which allows users to define \"whitelisted\" work related sites and applications and \"blacklisted\" non-work related sites and applications [29] . Focus Assist also then blocks access to non-work related sites and applications for a period of time determined by the user [29] .\nRELATED WORK: Finally, other applications have aimed at promoting workers to reflect on their emotions and goals throughout the workday, as well as to promote saving time for healthy and appropriate breaks. For instance, Robata was developed to be voice mediated agent that helped its users plan and organize their tasks, reflect on their motivation and satisfaction throughout the day, and also promote and reflect on their self-learning [18] . Users of Robata reported generally appreciating a way of reflecting on their planning and goal setting [18] . BreakSense is another application that was developed to help promote mobility for its users during their breaks in order to encourage healthy activity and lifestyle in office environments that can be often very sedentary [7] .\nRELATED WORK: Although each of these applications attends to a specific problem area within the broader domain of promoting workplace productivity and well-being, there has been little research on the development and evaluation of AI systems that incorporate task scheduling and management, distraction blocking and monitoring, and mood and goal reflection in a single, standalone application. To our knowledge, the only application that has attempted to integrate all these different functionalities into a single system is the work of Kimani et al. [16] . In their work, they presented Amber, a conversational desktop assistant whose purpose was to help workers in four main areas: (1) scheduling high priority tasks, (2) aiding workers in transitioning from one task to the next, (3) avoiding and intervening with distractions, and (4) reflecting on their work through a conversational AI interface [16] . In our work, we present our productivity agent, which builds upon the infrastructure and capabilities of this previous agent developed by Kimani et al. [16] . Similar to the work from [16] , we gave our agent a female gender.\nRELATED WORK: Our work extends this previous work in a number of ways. First, as Kimani et al. [16] found in their analysis that users desired an agent that took more control over the flow of interaction, we designed our agents to provide a series of dialogues that started at the beginning of each day with scheduling their tasks, helping users progress through these tasks until the end of the day. These dialogues were initiated automatically when appropriate without any extra user input. Second, in addition to incorporating the four major functionalities from Kimani et al. [16] , we also wanted to investigate the impact that a more human-like agent with more emotional intelligence would have on user perceptions, as well as their focus and productivity. In past studies of virtual agents, research has found that emotionally appropriate responses from an agent contribute to a more positive and satisfying experience during interaction [11, 36] . Furthermore, emotional intelligence in agents has been shown to help alleviate negative emotions, like frustration [17] . In a study of the effectiveness of different agent characteristics in an organizational setting, anthropomorphic appearance in agents was also shown to increase users perceptions of the agents usefulness [31] . As little research has investigated the effect of more human-like agents in the context of workplace focus and productivity, a primary goal of our study was to evaluate and compare its effectiveness to a typical chatbot agent. Therefore, in this work, we build two different prototypes of our agent: a text-based conversational interface prototype (TB) that employs a similar UI to the previous version developed by Kimani et al. [16] , and a prototype virtual agent conversational interface (VA) version with a video avatar that speaks to the user. The VA prototype incorporates the ability to detect user emotions through video input and adapt its responses to be appropriate and congruent with the users emotional state. We conduct a three week long within-subjects user study to evaluate and compare the VA prototype, the TB prototype, and a simple, non-conversational task scheduling tool integrated into users' email, similar in part to Calendar.help [8] , as a control. To our knowledge, our work presents the first multi-week user study evaluating different intelligent agent prototypes that aim to increase focus and productivity at work.",
    "rq": "rq1: are users more productive and less distracted during the time periods they scheduled for focused work through the agents?\nrq3: what features of the agent prototypes are most useful, and are there design improvement opportunities?"
}
{
    "intro": "",
    "relatedWork": "RELATED WORK: In order to offer a good travel experience, airports have to provide a good customer support. In particular it is paramount to ensure that customers can access all the needed information in a smooth and timely manner. Early in 1994, Kuhlthau stressed out how social interaction with service providers can help this process [16] . However, today's customer attitude is to search information online, dealing with the more structured and rigid interface offered by websites. The lack of flexibility could be an hindrance whereas a user's search is characterized by uncertainty and confusion [17] , still organizations should be able to provide an optimal customer experience. Papadopoulou [22] pointed out that for the online environment to be equivalent to the offline one, the same complete social experience can be supplied. Song and Zinkhan [26] suggested to improve the overall user experience by supplementing the information communication with side channels, such as live chats or social networks. While the adoption of mixed approaches to provide online customer service is increasing [28] , and some service providers are now embracing it [4] , interaction between humans in an online context can be both costly and limited [12] These shortcomings can be in part mitigated by exploiting humanmachine interactions, as individuals often see the computer as a social actor in conversation rather than a simple mean of communication [18] . To this end, offering an automatic and conversational interface to information is becoming more and more a viable support solution. However its effectiveness could vary in different application scenarios and thus it must be properly assessed.\nRELATED WORK: To evaluate support solutions we need to understand how to achieve a good customer experience. In recent years, research has focused on evaluating the quality of service and its impact on customer behavior and in particular on the analysis of the online customer experience (OCE) [18] . Note that, for airport customer support, it is crucial to keep track of the user's activity, in order to offer a better service of continuous assistance anticipating user's inquiries whenever possible.\nRELATED WORK: In the model proposed in this paper, we are adopting a service solution based on an human-machine conversational interaction. This kind of interface is usually called conversational agent or chatbot, i.e. computer programs that use natural language processing (NLP) for human-machine interaction (HMI) [15] to transform a series of self-initiated tasks into quasi-human conversation [29] .\nRELATED WORK: Historically, chatbots have been used for entertainment with the aim of inducing one human to talk to another [19] . Modern conversational agents are tools used to supply information [11] and their use has been adopted not only for customer service [10] , but also for education [13] , e-commerce [7] or health-care [6] . Kane [14] noted that chatbots, thanks to natural language processing and their ability to connect multiple models to a single response, are a natural fit for providing reference and instructional assistance.\nRELATED WORK: Given the high impact of easy information access in the airport environment, it is not surprising that several chatbot solutions have already been proposed for such context. Athens International Airport at the end of 2016 was the first airport in the world to release a chatbot service that provided live flight information, presently available both on Facebook and on Twitter [20] . The airport in Frankfurt has FRAnky, a chatbot based on Messenger that can search for flights, send updates on flight status and, can provide information on restaurants, shops and service facilities [9] . In Italy, Milan airports (Malpensa and Linate) have recently implemented their chatbot solutions [27] . For the creation of their chatbots, both the Frankfurt and Milan airports, have relied on Airport.ai [2] , a company specializing in the construction of airport chatbots. 21 are currently Airport.ai's partner airports for chatbot development; Melbourne, Edinburgh and Riyadh (Saudi Arabia) are just few examples.\nRELATED WORK: In addition, there are also airline companies that have developed their own chatbots. Aeromexico has developed a chatbot with nice features such as the possibility of buying tickets and or of finding destination-based recommendations [1] . The chatbot was developed with the help of its partners at Yalochat.com and IV.AI and was developed with proprietary platforms. Aeromexico affirmed that after only 6 months from launch, the chatbot had reduced the average time of resolution of customer service via chat from an average of 16 minutes to only 2 minutes. In the same period of time, customer service increased from 0 to 96 percent of automation. Other airlines such as British Airways or Iberia relied on the OpenJav company, that uses the IBM's platform for the development of chatbots [21] .\nRELATED WORK: As we have seen, there are many companies specialized in chatbots, some of them use proprietary software for development while others are partners with big technology giants such as IBM, Amazon or Facebook, which provide their own internal chatbot development platform. The companies that provide these services most of the time also offer a graphical and intuitive interface. On the other hand, our chatbot does not require the payment of a high fee, but has a less user friendly interface as it requires the user to directly act on the code of the chatbot.\nRELATED WORK: All the chatbots implementations that can be found both in literature and on the market can be roughly classified in two main categories: first-party and third-party [25] . Firstparty chatbots refers to conversational software developed independently by large companies to reduce licensing costs and improve the quality of customer service through very vertical implementations. This is the case in consumer-oriented sectors such as banking, telecommunications and e-commerce. Third-party chatbots refer to agents built using open source building blocks, such as Microsoft Bot Framework, Facebook Messenger, Google Assistant, and Amazon Lex [8] . These new technology tools and services enable companies to simultaneously meet customer expectations, achieve business goals and create value without requiring huge resources [5] . For these reasons, in the development of our chatbot we have adopted Microsoft Azure, the third-party platform that best met our requirements regarding performance and easiness of implementation. From a back-end point of view, the AI services that are offered by our chatbot and other chatbots are comparable since other companies rely on the services of IBM, Microsoft or Google.",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: The importance of self-disclosure-revealing personal or sensitive information to others [1, 38] -for mental well-being has been proved in substantial literature [45, 61] . For example, through selfdisclosure, people can release their stress [7, 18] , analyze themselves [44] , gain social support [48] and receive professional services [15] . But it is always challenging for mental health professionals by bringing the unique insight that trust may be transferable from human-AI to human-human through AIs.",
    "relatedWork": "RELATED WORK: In this section, we first present literature on how chatbots are a promising technological solution for promoting people's self-disclosure; then, we discuss related work about how people self-disclose with peers through ICTs, e.g., social media platforms. Finally, we provide background work on the remaining challenges of self-disclosure with medical health professionals (MHPs), often without ICTs. With the literature review, we propose our research questions.\nRELATED WORK: 2.1 Promoting Self-Disclosure to a Chatbot Chatbots have been broadly used in different areas [11, 20, 26, 78] . They can not only help people complete various tasks [70] but can also improve mental well-being (e.g., self-compassion [49] ). For example, chatbots are utilized in the workplace to assist team collaboration [70] , to improve workers' quality of life and work productivity [78] , and to reduce caregivers' workloads [25] . Park et al. [60] adopted Motivational Interview in the chatbot conversation to help users cope with stress and found that their design can facilitate a conversation for stress management and self-reflection. Lee et al. [49] designed a dialog to make the users take care of a chatbot's negative experience. After a two-week interaction with the chatbot, the user's self-compassion significantly increased. These studies have demonstrated the potential benefits of using a chatbot in different purposes, and our research aims at understanding how to use a chatbot to mediate sensitive information. The Computers Are Social Actors (CASA) paradigm indicated that people may apply social norms of human relationships when interacting with computer agents [58] . Thus, research has been focusing on advancing technological contributions for making computer agents to naturally chat and understand people; thus, some studies examined different strategies [6, 36] to enhance users' experience when talking with a chatbot. For example, Hu et al. found that the tone-aware chatbot could be perceived as being more empathetic than a human agent [36] .\nRELATED WORK: People's self-disclosure to chatbots can be used to detect symptoms, identify possible causes, and recommend actions to improve their symptoms by promoting people's self-disclosing, as well as to encourage interviewees to disclose themselves more openly in an interview session [53] . Scholars compared web surveys against chatbots and found that respondents tended to provide more highquality data when using the latter [41] . Fitzpatrick et al. utilized a therapy chatbot \"Woebot\" in their study to explore its feasibility to help reveal people's mental illness; their results showed that the chatbot helped relieve symptoms of anxiety and depression [26] . Additionally, chatbots can be deployed to various platforms using both speech and text; chatbots provide cost-effective [11] solutions for self-disclosure [53, 64, 80] or deliver education materials for self assessment (e.g., alcohol risks [21] ). However, most of the works focus on human-chatbot interactions, and little work studied:\nRELATED WORK: RQ1: Do people self-disclose to a medical professional through a chatbot differently from selfdisclosing with a chatbot alone? 2.2 Self-Disclosure with Peers through ICTs Self-disclosure behavior on social network sites has gained the attention of HCI scholars. For example, people freely disclose stress, depression, and anxiety through online social media platforms [3, 18, 53, 84] . It was found that such anonymous self-disclosure with their peers could help users maintain their mental well-being, as they may receive social support from their peers [4] . Similarly, Yang et al. [81] investigated the self-disclosure behaviors of online health support communities, and the study found the members' self-disclosure in private and public channels affected how they reciprocated with other and reached out for social support. Although self-disclosure on social media could help each other seek social support, people naturally avoid revealing their vulnerabilities to others [66] , as it might also cause social risks [2, 23] . Thus, Andalibi et al. [2] explored how people used throwaway accounts on Reddit to disclose their stigmatized issues (e.g., sexual abuse) and found that people using anonymous means engaged more in seeking support.\nRELATED WORK: Through interacting with the chatbot, people can search useful resources, i.e., self-help information, before reaching out for face-to-face counselling [11, 20] . Therefore, chatbots have become popular in response to the demand of mental health care in modern society [69] . A recent work shows that chatbots can play a role to inquiry users answering questions and convincing them to share diet information with their family members so as to support each other [54] . Also, coaching apps have been developed, not only for boosting users' awareness of their own mental well-being, but also for helping mental-health professionals gain more knowledge about their clients [40] . In this study, we investigate using a chatbot to facilitate self-disclosure to a MHP:\nRELATED WORK: RQ2: What is an effective chatbot design as a mediator for eliciting self-disclosure to a medical professional?\nSelf-Disclosure with Medical Professionals without ICTs: In the sphere of mental health care, journaling is a common practice of self-tracking that has been proven effective in terms of boosting mood and reducing anxiety [24, 73] . Prior studies [20, 26] have shown the positive effect of deploying a chatbot to facilitate journaling and for helping people to realize their mental issues and relieve their symptoms.\nSelf-Disclosure with Medical Professionals without ICTs: However, how people interacted with the chatbot differently when they self-disclosed sensitive topics to mental health professionals has not been investigated. According to social penetration theory [1] , self-disclosure is critical for the development of a successful interpersonal relationship that involves give-and-take between its parties. Self-disclosure is evaluated from two dimensions, breadth and depth [8, 59] . The breadth of self-disclosure can be demonstrated with a wide range of topics disclosed; on the other hand, the depth is more involved with personal experiences, intimate relationships, and possible negative feelings as a result of life difficulties. Prior works on chatbots often highlighted the volume of self-disclosure in terms of its breadth (e.g., [26, 53, 64] ); less was discussed on its depth. In order to assess mental well-being, a high depth of disclosure (deep self-disclosure) is needed [68] . To elicit self-disclosure at a deeper level, a higher level of trust is often associated in the relationship [77] .\nSelf-Disclosure with Medical Professionals without ICTs: Our work fills the void by focusing on evaluating self-disclosure depth by running an experimental study and by comparing participant's daily journaling and answers to sensitive questions before and after sharing with a real mental health professional via the chatbot. According to the norm of reciprocity, when someone discloses something deeply personal, his or her interlocutor feels pressure to share a similar level of information [79] , therefore, therapists often disclosed themselves to encourage partients' self-disclosure [28, 42] . In a recent work, reciprocity was found to happen in human-chatbot interaction as well, e.g., a self-disclosing chatbot received more self-disclosure from users [64] . Lee et al. [47] indicated that small-talk increased users' trust in the robot, and found that a user's greeting with the robot could predict the user's conversational strategies such as sociable interaction and self-disclosure. Thus, these studies demonstrated that a reciprocal social conversation may increase people's trust in a computer agent. Although mutual self-disclosure can, with time, facilitate intimacy, trust, and depth of self-disclosure by both parties [30] , whether and how a psychiatrist should self-disclose to clients is the subject of ongoing debate [14, 33] . Some studies have raised concerns that too much closeness with clients might derail their progress [32] . However, others have suggested that therapists' carefully selected self-disclosures could be beneficial as a means of building rapport with clients [33] and of building certain skills that can strengthen the counseling relationship, such as active listening [10] , gradually building trust [30] , and matching communication styles [46] . On the other hand, lack of trust in online applications may lead to inaccurate information being collected and deterred efficacy of services provided by the applications [69] . What if the MHP is not involved in the conversation directly, and instead, the MHP only receives self-disclosure content from people through a chatbot? This is the context of our study.\nSelf-Disclosure with Medical Professionals without ICTs: RQ3: How do people self-disclose to a medical health professional (MHP) through a chatbot? RQ4: What factors contribute to people' self-disclosing behavior to the MHP through a chatbot?",
    "rq": "rq2: what is an effective chatbot design as a mediator for eliciting self-disclosure to a medical professional?\nrq3: how do people self-disclose to a medical health professional (mhp) through a chatbot?\n rq4: what factors contribute to people' self-disclosing behavior to the mhp through a chatbot?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "Introduction: How the business industries handle product and services inquiries in person through a call center or email have to change as the cost of 24/7 support is too high. Therefore, the use of chat apps has exponentially increased with user friendly chat UI/UX (user interface and user experience). The chat apps are designed with Artificial Intelligence (AI) so they can automate the process to answer inquiries and increase customer satisfaction are popularly called Chatbots. They can manage any scenario of a conversation as they are aware of the requirements of the users [1, 2, 3] . Besides, discovering many question-and-reply patterns from past interactions via machine learning can be done by AI chatbots [4] . Chatbots have been used in business application such as customers service [5] , medical health [6, 7] , public transport [8] , tourism [9] , e-commerce website [10] and cultural heritage [11] . There are several types of Chatbots used for several purposes such as for businesses (Chat.io), marketing (Collect.chat), education (Cleverbot) [12] , entertainment (Reikobot) [13] .\nIntroduction: In hotel industry, chatbots help users to book and reserve a room, get the promotional offers, order advanced meal/drink preference, etc. The intelligent chatbots will then understand the topics and give a reliable response. Chatbots are recognized by their ability to understand and process natural language. Natural language processing exists to ease the work of users and also to satisfy the wish to communicate with the computer in common language. Assisting human-computer interaction and examining the behavior of the user is the advantage of the chatbot [14] . It will ask the questions and respond to the user's questions. Florian Daniel et al developed chatbot that is personal and helpful by providing services that are configured by the users themselves [15] . Therefore, the users can control their own personal bot that really respond to their needs.\nIntroduction: In this paper, we proposed a personalized chatbot hotels in Indonesia, called Bershca. The hotel industry needs the personalized chatbot hotel in order to increase the sales and customer satisfaction. This personalized chatbot is focus on the service recommendation that available at the hotel but relevant to the customers. The hotel offers in-room services such as in-room dining, in-room amenities offered and all pay-for-use products and services that related to the customers need. A hotel property with the right mixture of desired in-room amenities and services can charge higher rates for their customers and increase the revenues [16] . Therefore, Bershca will track the habits of their customers and to make sure that the hotels are readily available to cater to all the requirements of their customers at every step before arriving at the hotels. The chatbot mobile application will be chosen rather than chatbot website to increase the efficiency of users in using it.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: There were over 4 billion phone users in 2008 [ITU 2009] , and close to 60% of subscribers live in developing countries [UNCTAD 2008 ]. Thus, many entities with a global development focus have turned to the mobile phone as a potential platform for delivering development services, in sectors spanning education, finance, health, agriculture, and governance [Donner 2008 ]. One of the challenges of delivering such services, however, is that 41% of the population in the least developed countries is nonliterate [UNESCO 2007] , and even the literate among the poor are typically novice users of computer technologies. Previous research shows that nonliterate populations avoid complex functions, and primarily use phones for synchronous voice communication [Chipchase 2005 ]. This brings us to the question of how we can design mobile phone interfaces such that novice and low-literacy users can use more advanced services, if they were provided.\nINTRODUCTION: Mobile devices lend themselves to a rich design space of alternative user interfaces. As depicted in Figure 1 , it is natural to classify mobile interfaces along two axes, according to the flexibility of their input and output modalities. Rigorous comparisons between points in this space are only starting to be explored by researchers. Concurrently to the studies conducted in this article, researchers have examined the trade-offs between interactive voice response (spoken menus with keypad navigation) and spoken dialog systems (spoken menus with speech navigation) and have reached varying conclusions regarding the benefits of typed versus spoken inputs [Grover et al. 2009; Patel et al. 2009; Sherwani et al. 2009 ]. However, to the best of our knowledge, there has never been a quantitative evaluation of mobile interfaces that encompasses the full space of designs, spanning text, audio, and graphics. We present the first such evaluation in this article.\nINTRODUCTION: The contributions of this article are twofold. First, in order to understand the usage patterns of nonliterate and semiliterate mobile phone users, we conducted an ethnographic exploration involving 79 subjects and 100 hours of interviews in India, the Philippines, and South Africa. We also leverage 11 interviews conducted in Kenya by Ratan [2008] . This investigation revealed several barriers to using existing text-based interfaces, including difficulties understanding or utilizing hierarchical structures, soft keys, scroll bars, nonnumeric inputs, and specialized terminology. We formulate a set of design recommendations to improve the usability for low-literacy populations, including the provision of graphical cues, voice annotations, and local language support, as well as avoiding complex navigation and inputs.\nINTRODUCTION: To evaluate and refine these guidelines, we conducted two separate experiments that quantitatively assess the impact of various design elements on novice and lowliteracy users. Depicted graphically in Figure 1 and summarized in Table I , these studies incorporate over 70 subjects in India and represent the second contribution of the article. Both studies compare text-based mobile interfaces to richer alternatives. The first study considers automatic solutions (spoken dialog 1 and graphical, text-free UI [Medhi et al. 2007b] ), and the second study compares text input to a live human operator. In both scenarios, we find that the nontext interfaces greatly outperform their textual counterparts. In the context of mobile banking, not a single low-literacy subject could complete a transaction using the text interface, while 72% completed the task using spoken dialog and 100% were successful using a graphical UI. In the context of health data entry, literate health workers and hospital staff demonstrated approximately a 5% error rate using text interfaces, but performed ten times better using a live operator.\nINTRODUCTION: Our studies also uncover new trade-offs in designing mobile interfaces for novice and low-literacy users. While the spoken dialog system offered lower rates of task completion than the graphical interface, users that did succeed in completing the task did so more quickly and with less external assistance on the spoken dialog system. We attribute this to the fact that most users are more comfortable and familiar with providing spoken inputs, and thus can complete task more quickly; however, some users lack a basic understanding of the context, the terminology, or the interface and, being unable to acquire that understanding through an automated spoken dialog system, are unable to complete the task at all. For the graphical UI which required typed inputs to proceed, we observed that while task completion rates were higher, more time as well as more external assistance was required. We attribute this to the fact that users required significant prompting, encouragement, and time to press any key as they were nervous that they might \"break\" or \"spoil\" the phone. Also, our evaluation of the live operator interface suggests that it offers several benefits in addition to the high accuracy achieved, including the ability to change the interface over time and the ability for users to convey detailed, unstructured information over the phone. In environments such as India, an operator interface is also highly cost effective, and can be less expensive than providing programmable phones to field staff. For these reasons, the results of our study caused a community partner to change their plans for a rural tuberculosis treatment program from using electronic forms to using a live operator.\nINTRODUCTION: In the rest of this article, we describe related work (Section 2) before presenting our ethnographic investigation (Section 3) and each of our user studies (Sections 4 and 5). We conclude in Section 6. Our presentation summarizes and extends research whose ",
    "relatedWork": "RELATED WORK: Here we consider related work in evaluating the accuracy of data entry on mobile devices, as well as design techniques to improve usability for low-literacy users.\nEvaluating the Accuracy of Mobile Data Entry: As summarized in Table II , there have been several initiatives to apply PDAs and cell phones for mobile data collection in the developing world. While a fraction of the studies on PDAs include an experimental analysis of the error rate incurred, we are unaware of any prior study which systematically measures the accuracy of data entry (or the likelihood of completing a task) on a low-cost cell phone. This is a central novelty of our work. As detailed elsewhere [Patnaik et al. 2009] , previous studies of PDA entry accuracy have found that novice users can achieve an error rate of less than 2% (i.e., less than 2 errors per 100 entries) given that they receive at least an hour of training [Blaya and Fraser 2006; Forster et al. 1991; Missinou et al. 2005 ]. In two of these studies, error rates are as low as 0.1-0.6% [Forster et al. 1991 ] and 0.4% [Blaya and Fraser 2006] . However, in a context where novice users received only 2-3 minutes of training, error rates were observed to be 14%; entry accuracy was substantially higher for those who had completed secondary schooling [Bernabe-Ortiz et al. 2008] .\nEvaluating the Accuracy of Mobile Data Entry: Additional programs have applied PDAs for data collection in the developing world, but have not provided a rigorous analysis of entry accuracy [Anantraman et al. 2002; DeRenzi et al. 2008; Diero et al. 2006; Engebretsen 2005; Gupta 1996; Shirima et al. 2007 ]. In the case of SATELLIFE [Groves 1996 ], there are anecdotal reports that PDAs improved data quality [Ladd and Sewankambo 2004] and demonstrated decreased error rates as estimated on a five-point scale [AED Satellife 2004 ]. An additional report had users rate the usability of the system [Bridges.org 2003] . However, we are unaware of a quantitative assessment of the error rates incurred. DataDyne EpiSurveyor [Selanikio and Donna 2006] is also argued to be more accurate than paper forms [Selanikio 2009 ], though we are unaware of a controlled study.\nEvaluating the Accuracy of Mobile Data Entry: Cell phones have also found broad application for mobile data collection in the developing world, with interfaces spanning electronic forms [Balancing Act News 2007; Fynn 2006; Nokia 2007; Skinner et al. 2007; United Nations 2007] and interactive voice response [Curioso et al. 2005 ]. While we are unaware of previous evaluations of entry accuracy, there are three studies ( published concurrently to our own) that assess the accuracy of novice users in navigating interactive voice response systems [Grover et al. 2009; Patel et al. 2009; Sherwani et al. 2009 ]. The studies have varying results: one reports that task completion is higher with dialed inputs than with speech inputs [Patel et al. 2009] , one reports comparable task completion but a preference for dialed inputs over speech inputs [Grover et al. 2009] , and one reports that speech inputs significantly outperform dialed inputs [Sherwani et al. 2009 ].\nEvaluating the Accuracy of Mobile Data Entry: To avoid the complexities of navigating electronic forms, the CAM framework offers a hybrid system in which paper forms are used for organization while phones are used for data entry [Parikh 2005 ]. Each field on the paper form is annotated with a barcode which is recognized by a camera on the phone prior to data entry. Users that lacked prior camera or computer experience were trained to a level of comfort within 5-15 minutes. A separate study measures error rates of 1% or below using the CAM system [Parikh et al. 2006 ]. This represents an interesting and useful design point, especially in cases where paper forms are already ingrained into the workflow. We focus on solutions that are independent of any paper workflow and which do not necessarily require a camera-phone. While phones that support electronic forms (e.g., via Java) often have cameras, our SMS, USSD, spoken dialog, and live operator solutions are suitable to the most inexpensive phones.\nDesign Principles for Low-Literacy Users: Because for the most part illiteracy correlates strongly with poverty, nonliterate users are very different from the target users of typical UI designs [Cooper and Reimann 2003] . Most previous work with nonliterate users focuses on the mechanics of the interface, and on PCs or PDAs. Many researchers have recognized the value of imagery, and have advocated extensive use of graphics [Grisedale et al. 1997; Havelock 1971; Medhi et al. 2007a Medhi et al. , 2007b Parikh et al. 2003a Parikh et al. , 2003b . More specifically, it appears that static hand-drawn representations are better understood than photographs or icons [Medhi et al. 2007a ]. Voice instructions and audio annotations are also powerful, and much of the interesting work in this area focuses on the interplay between graphics and audio to generate a usable interface, as reviewed elsewhere [Medhi et al. 2007a [Medhi et al. , 2007b . Some authors note that the use of numbers is acceptable, as many nonliterate people can read numerical digits [Medhi et al. 2007b; Parikh et al. 2003a Parikh et al. , 2003b .\nDesign Principles for Low-Literacy Users: Other work has focused on ultra-simple navigation as a design goal [Grisedale et al., 1997] , or on removing anxieties about technology use. For example, looping video clips which include dramatizations of the overall usage scenario have been found effective in reducing barriers to usage by first-time users [Medhi and Toyama 2007] . Voice recordings of \"help\" information have also been shown valuable [Medhi et al. 2007b ]. These principles have been applied to application domains such as job information systems [Medhi et al. 2007b ], health information dissemination [Medhi et al. 2007a ], and microfinance [Parikh et al. 2003a [Parikh et al. , 2003b .\nDesign Principles for Low-Literacy Users: Interfaces for low-literacy users have also been studied in the context of Automatic Teller Machines (ATMs). Two studies propose an icon-based approach for ATMs [Ivatury 2004; Thatcher et al. 2006 ]. Another study looks at attitudes in literate and semiliterate bankaccount holders towards ATMs and alternative ATM interfaces (speech based and icon based) [Thatcher et al. 2005 ]. Overall, groups showed a tendency to prefer an icon-based alternative ATM interface over the other choices. Evaluations of a pilot trial by one large bank in India make various recommendations for ATMs for low-literacy users: avoid use of text altogether; loop voice instructions in simple, slow vernacular; provide biometric authentication; use consistent visual cues [Medhi and Ratan 2006] .\nDesign Principles for Low-Literacy Users: Apart from work that focuses on PCs, PDAs, and ATMs, there is some amount of research that looks at mobile phone UIs for low-literacy users. Researchers have recognized the value of voice feedback [Parikh et al. 2006; Plauch\u00e9 and Prabaker 2006] and speech interfaces [Boyera 2007; Plauch\u00e9 and Prabaker 2006; Sherwani et al. 2007 ]. Others have questioned suitability of menu-based navigation for novice users [Jones et al. 2000 ] and have discussed designs that advocate fewer menus and dedicated buttons for this target group [Lehrman 2007 ]. Again there is work that looks beyond the UI at coping mechanisms of illiterate and semiliterate users when confronted with traditional mobile interfaces [Chipchase 2005 [Chipchase , 2006 .",
    "rq": " we divide our analysis according to two orthogonal questions: why were users faster and more independent on the voice ui, and why were they also more likely to give up?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Deception in computer-mediated chat is becoming more prevalent and has real consequences for individuals, employers, and society [Zhou and Zhang 2007] . Several recent studies and events have clearly shown the impact of online deception. These include US Congressional action against online predators [Fitzpatrick 2006 ], online dating studies [Epstein 2007 ], and analysis of terrorist networks [Chen and Wang 2005 ]. In a dramatic example, Megan Meier was a thirteen-year-old girl who took her own life after being deceived by a series of chat-based messages and email communications. A fictitious boy (actually an adult woman) developed an online relationship with the girl and then turned hostile. Megan ended her life in October 2006 not knowing of the deceit [ABC News 2007] .\nINTRODUCTION: Computer-mediated chat is a very fertile ground for deception because people can easily conceal their identities and computer-mediated messages often appear credible [Zhou and Zhang 2007] . Deception in computer-mediated chat refers to deceit that occurs in synchronous text-based communication conducted over a computer network. As with most forms of synchronous communication, deceptive messages must often be formulated on the spot [Whitty et al. 2012] . Such deceptive communication is very common in organizational and individual settings [Zhou and Zhang 2007] and can include applications such as instant messaging, online forums, chat rooms, virtual worlds, live customer service sites, product review sites, and social networking sites (e.g., Lau et al. [2012] and Twitchell et al. [2005] ). Compounding the problem is the fact that a large and consistent body of research shows that without special training or technological aid, human deception-detection accuracy is near chance (e.g., Jensen et al. [2011] ). A meta-analysis summary of over 200 experiments shows that people are, on average, correct 54% of the time at distinguishing truths from lies [Aamodt and Custer 2006; Bond and DeP \u00e1ulo 2006] . This level of accuracy is statistically greater than chance but is extremely poor in a practical sense. Further, specific individual differences such as age, professional experience, education, cognitive ability, and social skills have little impact and there is little variance overall in a person's ability to detect deception according to a substantial meta-analysis [Bond and DeP \u00e1ulo 2006] , although a few individual studies suggest that training/experience can make a difference (e.g., Smith [2001] ).\nINTRODUCTION: In a seminal study on synchronous computer-mediated communication, Zhou and Zhang [2007] identify two channels for detecting deception in computer-mediated chat-messaging and typing. Messaging refers to the content and characteristics of the sequential messages displayed on the screen in a chat application; typing refers to the deceiver's behavior while typing the message. These two categories were derived from interpersonal deception theory [Buller and Burgoon 1996; Zhou 2005] , which posits that cues of deception may be categorized as verbal or nonverbal in interpersonal communication. Several cues of verbal interpersonal communication (messaging cues) may be relevant to computer-mediated chat such as lexical diversity, word count, and message content ]. These cues are important because empirical evidence suggests that deceivers may use language differently than truth tellers [Buller et al. 1994; Newman et al. 2003 ]. Deceivers may also demonstrate less language diversity, less complexity, and use more negative words [Newman et al. 2003 ]. Numerous linguistic features may be used to gauge the credibility of messages. For example, average word lengths may be used to judge language complexity, and linguistic features may be used to uncover defensive speech routines [Kuo and Yin 2011] . These cues are additionally relevant because they are used with synchronous and asynchronous persistent conversations (e.g., chat, email). Additionally, messaging cues that use criteria-based content analysis (CBCA) [Vrij 2005 ] or reality monitoring [Johnson and Raye 1981] analyze the content of speech, rather than the form, in terms of message length or count. CBCA is a widely used veracity assessment technique for discriminating between true and fabricated events through systematic analysis of witnesses' spoken accounts [Vrij 2005 ]. Reality monitoring is a technique for detecting deception based on the postulate that imagined memories involve more information about the cognitive process that produced them than true memories [Johnson et al. 1981] . In this type of assessment, attention is given to types of terms such as spatial terms (or Detecting Deceptive Chat-Based Communication 9:3 their absence) and particular words such as modifiers or hedges (e.g., maybe, possibly, might, could) [Vrij 2000 ]. All of these studies focus on the cues of the message itself.\nINTRODUCTION: Conversely, important interpersonal non-verbal cues of deception such as proxemics (distance) and kinesics (body movement) [Burgoon et al. 2002] are not applicable to text-based communication. Computer-mediated chat, however, does offer other unique and unexplored paralanguage cues relating to people's typing behavior that can be captured, analyzed, and measured. These cues can include the number of edits, deletions, and response time and can provide valuable insights into meaning, emotion, and intent [Zhou and Zhang 2007] . In interpersonal, face-to-face communication, \"how something is said\" is often as important as the message itself. Message construction is an important part of the communication process, and cues of deception may be manifested in this process. In face-to-face interactions, cues related to message construction may be manifested in response latency, vocal changes, and other nonverbal behavior [DeP \u00e1ulo et al. 2003; Sporer and Schwandt 2006] . So, it holds that message construction behavior (i.e., typing behavior) in computer-mediated communication may provide insights into deception. In other words, message creation is an important part of the deception process that can be analyzed independent of the actual message. As an example of this, unaided chat participants that observe messaging behavior have been shown to exhibit higher deception-detection accuracy than unaided chat participants who observe typing behavior. Participants were able to correctly classify on average 20.4% (sd = 51%) of deceptive video clips based on observing typing behavior and 35.7% (sd =41%) based on messaging behavior [Zhou and Zhang 2007] . Although observing typing behavior did not improve humans' deception detection, we cannot conclude that typing behavior is not diagnostic of deception. Rather, it may suggest that humans have difficulty identifying and interpreting theoretically sound cues of deception from typing behavior and combining this information with theoretically sound cues of messaging behavior. To our knowledge, extant research has not identified theoretically sound cues of deception from typing behavior nor has it confirmed the applicability of messaging cues of deception in computer-mediated chat.\nINTRODUCTION: This article addresses this limitation by creating a theoretical model of chat-based deceit that identifies two typing cues of deception (response time and the number of edits) and two messaging cues (lexical diversity and word count) and empirically tests them in a chat-based environment. These selected cues are noteworthy because they are easily implemented in most current chat-based applications. We focus on spontaneous deception in chat-based communication because this type of deception is most common in synchronous computer-mediated contexts [Whitty et al. 2012] . We attempt to demonstrate that increased cognitive load [Arnsten and Goldman-Rakic 1998; Hains and Arnsten 2008] and other associated psychological processes (e.g., inhibition of the prefrontal cortex) associated with deception can be operationalized and measured in a distributed, online environment. We use a custom chat-based instrument with precise timing measurements to capture and test the typing and messaging behaviors. In summary, we answer the following research question: What is the relationship between spontaneous deception and the number of edits (e.g., backspaces, deletes), response time, word count, and lexical diversity in chat-based communication?",
    "relatedWork": "",
    "rq": " we answer the following research question: what is the relationship between spontaneous deception and the number of edits (e.g., backspaces, deletes), response time, word count, and lexical diversity in chat-based communication?"
}
{
    "intro": "INTRODUCTION: The embodiment of the virtual conversational agent requires the following key components: sensory information acquisition, believable non-verbal behavior generation model(s), verbal utterance generation, decision making, dialogue contents, character animator, and a middleware framework to connect all of the components and enable real-time processing. Our previously proposed GECA Framework [3, 4] is a middleware framework designed for this purpose, and the various developments up to now has covered most of these aspects. This specific work aims to develop a model to generate fine grained and reactive non-verbal behaviors of the virtual character when the human user is talking to it. The target micro non-verbal behaviors are facial expression, head movements, and postures. Deep neural networks and data driven approaches have been proven to be effective in various learning tasks. Facial expressions, head movements, and postures involve dozens of parameters simultaneously, this is supposed to be an appropriate application for neural networks, which can generate multiple outputs in nature.\nINTRODUCTION: In this paper, we presents the integration of recurrent neural networks (RNN) based reactive micro behavior generation modules which the existing framework. The generated animation is expected to be fine-grained both temporally and spatially, no identical sequences, and reactive to the user's behaviors. As an example application for this research issue, we developed a virtual listener agent system on our proposed platform with the generation models derived from an active listening data corpus. Simply switching Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). IVA '19, July 2-5, 2019 , PARIS, France \u00a9 2019 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-6672-4/19/07. https://doi.org/10.1145/3308532.3329448 the behavior generation NN models trained from the data corpus as well as the dialogue manger component in a different dialogue context, another agent can be rapidly reconfigured to work. This paper is organized as the follows: section 2 introduces related works, section 3 briefly introduces the multimodal interaction platform, section 4 introduces the data corpus used in the machine learning experiment and describes the development of RNN models. Finally, section 5 concludes the paper.",
    "relatedWork": "RELATED WORKS: Deep neural networks (DNNs) have been shown their performance in generalizing the learning process of complex contexts including the multimodal classification of human behaviors. Despite of the time consuming process in the learning phase of DNNs which usually requires large dataset, the application of learned models is fast. Therefore, DNN is a suitable tool for the generation of multiple parameters with large number of input variables. DNNs have been shown their power in image or speech recognition and are also gradually getting popular in human-agent interaction and social computing fields. They are most often used in prediction and estimation of human state such as visual focus [5] or sentiment [1] by using low-level multimodal features. Recently, DNNs are also seen in the generation part such as utterance [8] and gesture [2] . However, there is no direct previous work in generating facial expression parameters to animate virtual characters in active listening context yet.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: The process to realize a new project in the area of multimodal interaction systems should be straightforward: you write down your list of needed functionalities and technical requirements, read the relevant literature and collect all the technology you have on your shelves or that is available to you. Then you devise a good interaction metaphor, make some user studies and build your system.\nINTRODUCTION: In this contribution we describe the first steps in the development process for the project SmartWeb (www.smartwebproject.de, [21] ). Together with a number of partners from industry and academia in Germany we started in 2004, and aim at the development of a context-aware, mobile, and multimodal user interface to the Semantic Web [8] . In our main scenario, the user carries a smartphone as interaction device -other scenarios are the integration in a motor-bike interaction device and in a car. The user poses open-domain multimodal questions in the context of a visit to a Football World Cup stadium in 2006. Using speech and gestures, she can ask for information about players, events, weather information and other services available through web services, Semantic Web pages, and the plain Web. The user can interact with the result by speech and gestures on the device, and browses multimodally through alternative results.\nINTRODUCTION: The multimodal input is interpreted and transmitted using UMTS or wireless LAN to a back-end server system. The multimodal recognizers, the dialog system, and the Semantic Web access subsystems are located on this server. They should be able to serve multiple end-users. The response to a question is presented on the mobile device and rendered on its screen.\nINTRODUCTION: The partners in the project share experience from earlier projects [20, 16, 17] where different sub-components were integrated to multimodal interaction systems. Like others [15] we learned some lessons which we use as guidelines in the development of our system [2] :\nINTRODUCTION: Multimodality More modalities allow for more natural communication, which normally employs multiple channels of expression. It is also the case that more modalities constrain interpretation and hence enhance robustness. However, the use of multiple modalities not only requires a lot of technical effort in the recognizers or generators, but also in the interaction design to get full leverage out of the employed technologies.\nINTRODUCTION: Representation Representation is important in two aspects. Firstly, in a complex interaction system a common ground of terms and structures is absolutely necessary. A shared representation and a common knowledge base ease the data flow within the system and avoid costly and error-prone transformation processes. Secondly, the representation is part of the global dialog history residing in the discourse module and can be accessed by any module, e. g., for multimodal reference resolution or generation at any time during the course of the dialog. (c.f. \"No presentation without representation\" [14] )\nINTRODUCTION: Standards Standards ease scalability. For some representations and tools, we have previously developed custombuilt software providing short-lived solutions. With the advent of standards like EMMA (see below), adopting these standards opens up the door for scalability since we can re-use ours as well as other's resources.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": "this experiment will seek to answer the following question: is there a significant difference between the effectiveness of virtual mentoring from a twitter conversational agent (tca) and an embodied conversational agent for hbcu undergraduate students interested in pursuing graduate school?"
}
{
    "intro": "Introduction: During the last few years the recognition of spontaneous speech in telephone applications \"has greatly improved; nevertheless spoken dialogue between computers and inexperienced users still presents some problematic issues that reduce the user satisfaction in interacting with spoken language systems.\nIntroduction: The occurrence of errors in the acoustic decoding of users' utterance is the potential cause of miscommunication in oral interaction with spoken language systems. Some of these errors have important consequences in reducing the naturalness of humanmachine dialogues. Sometimes a robust approach in parsing and the use of language models during recognition are not sufficient to avoid recognition breakdowns. The recognition performance has a direct impact on the requirements that the dialogue modules of spoken language systems have to meet. In order to increase the usability of the applications, dialogue management modules have to deal with partial or total breakdowns of the lower levels of analysis by preventing and detecting miscommunication sources.\nIntroduction: In this paper we identify some typologies of recognition errors that cannot be recovered during the syntactico-semantic analysis, but that may be effectively approached at the dialogue level. Our analysis and the methodologies we describe have been tested in a task-oriented telephone application, but we deem that some considerations may also be useful for other display-less human-machine communication applications. We will describe how nonunderstanding and the effects of misrecognition are dealt with in Dialogos, a real-time spoken language system that allows/users to access a database of railway information by using the telephone. A detailed description of the different modules of Dialogos may be found in (Albesano et al., 1997) . In this paper we will discuss the importance of supporting confirmation turns, and clarification and correction subdialogues. The dialogue module of Dialogos makes an extensive use of context knowledge: contextual information is used not only for validating or rejecting semantic interpretations, but it is also sent to the lower levels of input analysis for helping the recognizer. We will show that the positive effects of robust dialogue management and dialogue state dependent language modeling may be evaluated by taking into account both the recognition and understanding performance, and the success rate of dialogue transactions.\nIntroduction: From our experience we may conclude that if we provide robust behaviour in our dialogue systems, speech is a viable interface even with relatively low word accuracy rates. Nevertheless we believe that some important issues are still unexplored, and one of these is related to the weight that recognition errors have with respect to the degree of co-operativeness of the users. These open issues and some experimental data that emphasize their urgency will be discussed in the section on experimental data.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Developers of conversational systems are diverting an increasing amount of attention towards designing conversational agents that are more human-like. This includes incorporating and maintaining consistent personalities as is done for most commercial chatbots [19, 42, 64, 65] , mimicking nuances and quirks of human communication (notably the now-famous Google Duplex \"uh-hmm\") [62] , but more broadly, retrofitting behavioral intricacies of human-human communication to human-agent communication. However, exploring the entire space of behaviors that emerge in human conversations is prohibitively large and consequently, research, with this thrust towards anthropomorphization, focuses on iteratively incorporating more of such nuances into agent design.\nINTRODUCTION: A major challenge in making text-based agents more human-like is that humans are adaptive and flexible in conversations, and conversational behavior is largely dictated by the interests of people involved, their shared context, their social distance, and the cultural contexts. While humans are accustomed to adapting to people and contexts, agents need special provisions for such adaptations. Along these lines, work has looked at how responses of chatbots can be conditioned on information about individual users [35, 40] , how often agents should switch topics and ask questions [57] , and how more diverse responses can be generated [7] . The overarching theme, in most previous work, is that content in human conversations is dynamic and we need to explore how to build agents that can adapt and navigate through a range of topics, subject to the user and context. While these approaches try to capture variations in \"what\" a chatbot should say at a particular turn in the conversation, they leave unexplored the question of \"how\" a chatbot should phrase the message.\nINTRODUCTION: In order to move towards the goal of building human-like chatbots, we argue that it is insufficient to look at content alone. The fact that style in human conversations is dynamic too needs to be addressed. Human languages are rich and provide us several ways (styles) to express the same thought-we can choose to be more formal and ask, \"Can you give me the directions to the restaurant, please?\" or assume a more casual tone-\"uh, directions?\". We can choose to be more or less verbose, personal or impersonal, introduce varying amount of humour and possibly make infinitely many such choices in how we want to communicate our thoughts. Context, both interpersonal and cultural, finds its way into not just the content of conversations but also the style of utterances. Should we then design chatbots that are mindful of style and not just content, in a conversation? Would this make our chatbots more human-like?\nINTRODUCTION: A form of stylistic variation that has assumed special importance is that of language mixing. Speakers in multilingual societies have been known to fluently and frequently alternate between languages they are proficient in-a phenomenon known as code-mixing (CM) [30, 44] . Despite an increasing number of users of conversational systems coming from multilingual societies [2] , conversational systems still remain predominantly monolingual, even as they are being developed in new languages [58] . In such multilingual markets, does a monolingual conversational system, capable of handling interaction in one language that the user speaks, suffice? It becomes important to understand the need for conversational systems to move beyond monolingual interactions towards such mixed language interactions. Backed by our reasoning about the importance of style, we seek to understand if chatbots that can handle code-mixing will be perceived as more human-like by such users. This paper presents a user study with the following question:\nINTRODUCTION: Research Question: Do multilingual users prefer chatbots that code-mix systematically, and if so, what is the optimal way to code-mix systematically?\nINTRODUCTION: A prominent theory that seeks to explain the dynamics of style in human conversations is the Communication Accommodation Theory (CAT) [28, 29] which posits that speakers, in co-operative communication, end up reciprocating each others' styles, attempting to converge to a common style -a process called accommodation. A computational study of style-accommodation [20] shows that style-accommodation is highly prevalent and exhibits complexity in Twitter conversations. In this paper, we explore whether chatbots should accommodate for the CM of the individual users they interact with. We evaluate if users express a marked preference for a chatbot that reciprocates their CM over an otherwise similar chatbot that does not accommodate for their CM, the difference being only in the surface form realizations of the response. We choose to focus on users in India for pragmatic reasons including proximity and the presence of a large population of multilingual users. 10.6% of the population of India is English literate with 6.8% reporting it as their second language and 3.8% reporting it as their third language [27] . Given a smartphone penetration rate of over 25% [1] , there is bound to be a sizeable population of multilingual users [36] . Thus the Indian market has been the focus of other research in multilingual interfaces as well [10, 36] . Since Hindi is the most widely spoken language in India, with English being second, we choose to work with the frequently-mixed English-Hindi (En-Hi) language pair while studying preferences of participants from India.\nINTRODUCTION: We devise two prototype CM policies-one that always code-mixes and another, rooted in the CAT, that does so intelligently by nudgingimplemented as algorithms that take into account the user's CM. These CM policies run in parallel with the conversational system's response generation system and when both the response and the CM policy output are ready, a paraphrasing system introduces CM in the response in accordance to the policy's output. Using a human-in-the-loop prototype of our conversational system, we carry out a study with 91 participants from India to understand the impact of CM accommodation on user evaluations of the conversational system.\nINTRODUCTION: We find that participants rate bots that can code-mix higher than a monolingual English bot, in terms of human-likeness and conversational abilities. We observe that individual differences between participants are a strong predictor of their evaluations. Participants with higher fluency in the two languages, tend to reciprocate the chatbots' CM more often and evaluate the CM chatbots more favorably than participants that don't reciprocate the chatbots' CM. Similarly, when participants perceived that the chatbot was reciprocating their CM, they evaluated the chatbots' CM as more natural. Between the Always CM and the Nudge policy, we find that the Always CM policy is a high risk-high gain choice. On the other hand, the Nudge policy is a lower risk design choice with slightly lower but more consistent ratings across users.\nINTRODUCTION: The primary implication of our findings, for the design of chatbots that code-mix, is that in the absence of knowledge about the users' fluency and attitudes towards CM, it is better to adopt the Nudge policy while the Always CM and monolingual English systems are a good fit when it is, a priori, known whether or not the users are fluent in the constituent languages and have a positive predisposition towards CM. Our results galvanize the need to divert increasing attention towards developing language understanding and generation systems for CM language due to the unique utility these serve in a conversational setting. The accommodation policies devised in this work also extend to style dimensions beyond just CM and can be incorporated into systems that accommodate for politeness, formality and similar style choices.",
    "relatedWork": "RELATED WORK: Our work is situated in the context of previous efforts that seek to understand how humans communicate with technological systems, how language technologies have evolved, how norms from human-human communication extend to human-computer interaction, and how to develop systems that are informed by all these in the specific domain of mixed language technologies.\nRELATED WORK: We give an overview of themes most relevant to our work, from literature in linguistics, humancomputer interaction and natural language processing.\nComputers as Social Actors: The Media Equation [52] proposes that norms from human-human communication carry over to the realm of human-machine interaction. People were found to be naturally inclined to be polite to a computer despite it's inanimacy [52] . Subsequent research has gone on to show that: people have similar processes of impression formation with robots as they do with other humans [37] , derive similar psychological outcomes from disclosing to a chatbot when compared to another human [32] , and even co-operate more with a computer when they are primed to think that the computer is on their \"team\" [48] . It is not a straightforward parallel, however -social norms around human interactions do not entirely shape expectations for human-agent conversations [17] . But largely, humans have a subconscious tendency to treat computer systems as they do other humans, and this has lead to the paradigm of \"Computer as Social Actors\" (CASA).\nComputers as Social Actors: A major implication of the CASA framework, for the design of human centered systems, is that \"people should be able to use what comes naturally -rules for social relationships and rules for navigating the physical world,\" [52] [sic]. People have existing notions of how to navigate the world-being polite, co-operating, resolving conflicts. Designing systems, from the lens of the CASA framework, should take into account these behavioural tendencies. For the design of conversational systems, this implies building systems that enable the user to interact with them as they would with other humans, thus justifying the need to build more human-like agents. Users in bilingual communities tend to code-mix in conversations with each other and have a natural tendency to do so in interactions with computer systems. Thus we reason that users would evaluate systems that afford such interaction, more favorably. More concretely, we hypothesize: Hypothesis 1 (H1). Chatbots that code-mix will be rated as more human-like and as having better conversational ability than monolingual chatbots.\nComputers as Social Actors: This thought has been strengthened by findings of studies that show that people tend to favor human-like conversational systems. People feel a stronger urge to resolve misunderstandings with more human-like agents [18] . Similarly, people look more favorably upon conversational agents that nod and provide envelope feedback [16] . Our work seeks to contribute to this line of work by specifically exploring how text-based agents can be made more human-like by understanding and reciprocating users style choices-specifically those concerning language mixing.\nConversational Systems and Human-likeness: As conversational agents become prevalent with designers aiming to broaden their capabilities, research in the design of conversational systems has grown from developing better language generation and language understanding models to include more human-centered considerations [34, 41, 63] . In pursuit of human-likeness, the design of conversational systems has explored the use of personalization [35, 40] , personas [8, 19] , visual cues including animations and embodiment [12, 15, 16] , and voice to signal human traits [39, 49] . Recent work has highlighted design considerations of the \"voice\" of voice-based conversational systems [13] from the perspective of the users and context, including the thrust towards individualization and adapting to context. Voice-based systems and embodied systems, however, are usually built on top of a core text-based system with a layer to switch in and out of the appropriate input-output modality (a speech recognition and accompanying text-to-speech system in the case of voice-based systems). Thus, we concern ourselves with understanding what contributes to human-ness in text-based systems-at the level of language alone-while the inquiry itself extends to other modalities such as voice-based and embodied agents.\nMultilingual Conversational Systems: We are not the first to acknowledge language preferences of multilingual users. Developers of commercial systems and researchers, both, have looked at designing conversational systems for multilingual users. Systems have been developed to support interactions in multiple languages [33, 50] , however, their primary focus is on enabling users to use the language they are most comfortable in while not making specific efforts to mimic human conversation or to handle code-mixing within a message. These systems require that the users' message be, entirely, in one of the supported languages and the system responds in the same language. If the user opts to change the language of the conversation, the system follows the user and responds in the new language. Our inquiry is fundamentally different in that we seek to understand how conversational systems can afford more natural and human-like interaction by appropriately accounting for users' language mixing choices. Similarly, Microsoft developed Ruuh [31] -a chatbot for the Indian market with a specific focus on human-likeness and conforming to cultural norms. Ruuh, at it's core, had a deep learning based response generator trained on code-mixed tweets from India and thus could generate code-mixed responses, however, Ruuh did not account for the language preferences of the users and would code-mix regardless of whether the user welcomed code-mixing or not. In contrast, we work within the framework of the CAT to systematize language mixing through the conversation and we concern ourselves with controlled language mixing with the goal of taking a step towards making human-agent interaction similar to human-human interaction. CAT suggests that for goals including social approval and communication efficiency, people in co-operative communication adapt to each others' style choices and attempt to reduce the differences in their communicative behaviors [28, 29] . This suggests that chatbots that adopt such strategies to adapt to and reciprocate users' style choices might appease more users as opposed to a one-fits-all solution, by virtue of adapting to the individual styles of users. We hypothesize: Hypothesis 2 (H2). A chatbot that adapts to users' individual styles and accommodates for their language-mixing behaviors would appease more users than a chatbot that does not accommodate for users' language-mixing tendencies.\nCode-choice as linguistic style: Code-choice is known to be associated with various sociopragmatic functions and considerations [3, 6, 11, 47, 56] that influence speakers' decisions on which code they use and when. Code-mixing (CM) can indicate a shared linguistic identity [5] and make a conversation sound more natural or engaging, convey informality, and reduce perceived social distance between speakers [14, 22, 45] . While CM is known to be similar to other dimensions of linguistic style [9, 59] in its cohesive and accommodative characteristics in human conversations, it also differs from them in being a strong sociological indicator of identity [5] . This association and dependence on sociolinguistic variables beyond the textual surface form is precisely what makes it hard for a data-driven system to capture the dynamics of this linguistic phenomenon.\nCode-choice as linguistic style: Another important consideration is the wide range of differing attitudes towards CM that exist at the level of an individual or community of speakers [24] -not only sociolinguistic factors like age, gender, education and language proficiency, but also personality types of speakers (levels of emotional stability, tolerance to ambiguity, cognitive empathy and neuroticism) [23] . Thus, the preference towards a CM agent might widely vary across individuals, communities and multilingual geographies, and we cannot a priori comment on the usefulness of CM agents.\nCode-choice as linguistic style: Instead of delving further into the functions and motivations for CM, we formulate our questions around its effects on a user of a chat system. Specifically, when a user interacts with two identical chatbots that differ only stylistically, specifically in the language of their messages -one always responds in English whereas the other code-mixes fluently in English and Hindi -do users systematically prefer one over the other? Does the code-mixing chatbot get consistently better or worse ratings than the monolingual chatbot?\nCode-choice as linguistic style: If there is a systematic difference, we explore how much of it is due to:\nCode-choice as linguistic style: \u2022 the users' demographics (their native language, proficiency in either language, ...)\nCode-choice as linguistic style: \u2022 individual differences in the patterns of CM -what surface forms are considered more natural varies across speakers \u2022 expressed attitude towards CM -not all bilingual speakers consider CM as equally grammatical or valid in written/text-based communication \u2022 inadvertent differences in the 'persona' of the chatbot conveyed by virtue of their mixing [14, 22, 45] \u2022 the sheer novelty of a chatbot that can converse in a mixed language \u2022 other confounding differences in the conversations with each chatbot, like the content or topic of the conversation, response time, etc. Since this particular form of stylistic variation has not been studied much in a human-agent conversational setting, we first explore the effects of merely introducing this style into our chatbot. We then move on to a further exploration of the effect of adding reciprocity and reciprocal variation into this style dimension.\nCode-choice as linguistic style: The latter leads to a follow-up question. In a typical human-human conversation between bilinguals, the choice of code is neither premeditated nor fixed. Rather, it is known to exhibit cohesiveness and interpersonal accommodation, as shown empirically [9] . Given this knowledge, we explore if a chatbot that follows an online reciprocative CM strategy is judged differently from a chatbot whose CM is fixed and independent of whether or not the user code-mixes.",
    "rq": " 1) what did you like and dislike about this conversation?"
}
{
    "intro": "INTRODUCTION: After an American actress Alyssa Milano tweeted a request to those who have been sexually harassed or assaulted to reply with \"me too\", 1.7 million tweets were made within ten days in at least 85 countries [44] . According to a survey in 2018, 81% of women and 43% of men in the US reported experiencing sexual harassment or assault in their lifetime [29] . Incidents of SH continue to be reported on a regular basis and many organizations are seeking ways to build a proper anti-harassment culture such as offering repeated and mandatory SH prevention training. A typical training program is comprised of presenting information and resources online with case studies and evaluating learner's comprehension using surveys and quizzes. In-person classes are sometimes held to supplement the online training and discuss SH issues [53] .\nINTRODUCTION: Despite prevalent training, recurring SH incidents are raising doubts about the effectiveness of existing SH prevention programs [21, 46] . Some researchers assert that current methods have little impact on changing one's actual behavior [21, 47] and may even have the opposite effect [9, 12, 52] . As our results will later show, the existing designs are perceived to be tedious, overwhelming, uncomfortable to express honest opinions, and un-motivating. Our work envisions a new class of interactive training delivered by an intelligent conversational agent. This paper progresses toward that vision by providing a realistic experience of having a conversation with an intelligent agent representing a person who has been sexually harassed.\nINTRODUCTION: In this paper, we explored the design of a text-based conversational interface (CI) to incorporate design principles that underlie effective SH training. We derived three key principles from the literature about how to design effective SH prevention training [16, 45, 48, 49, 53] : 1) Foster empathy towards SH targets through the use of first-person narratives, 2) use interactive and experiential methods (e.g., role-play scenarios), and 3) utilize synchronous delivery methods (e.g., online chat). While prior studies in the design and education research communities have tested different subsets of these principles [16, 20, 32, 37, 58, 63] , we designed and implemented a CI that demonstrates a novel synthesis of all three principles for SH prevention training. Our proof-of-concept interface was designed to have a persona of a woman, named Jane, who has been sexually harassed in the workplace and engages the learner in a conversation about her experience from a first-person perspective.\nINTRODUCTION: We conducted a mixed-methods study (N=32) to explore the benefits and limitations of the CI design for the purpose of SH prevention training. Participants were randomly divided into two groups, either interact with our interface (CI group) or read the same vignette on a web page (Control group). In both groups, we measured empathy using an 8-item scale [5] and Inclusion of the Other in the Self (IOS) scale [4] , and SH attitude using Sexual Harassment Myth Acceptance (SHMA) scale [39] to evaluate how experiencing the vignette through the interfaces affects learners' empathy towards the target and attitude towards SH. We interviewed participants and extracted the themes that emerged from their responses.\nINTRODUCTION: We compared the themes between the CI and the Control group and identified the themes that appear in the CI group only. The participants in the CI group reported feeling engaged due to the designed interactivity (N C I =14). Participants also reported that reading individual messages was less overwhelming than being presented with an entire article and created suspense of how the conversation would unfold (N C I =9). Participants appreciated the CI being a realistic simulation because this allowed them to feel comfortable discussing a sensitive topic (N C I =8). They also felt immersed in the situation and motivated to help the target (N C I =15). These benefits favorably contrasted with the limitations of the traditional SH training that participants had previously experienced. In contrast to our expectation that the virtual interaction with a SH target would increase empathy, the quantitative results revealed that there was no significant difference between the conditions on empathy.\nINTRODUCTION: Our study makes three contributions. First, we identify design principles from prior literature for effective training and demonstrate how to implement a subset of these principles within an intelligent CI. Second, our results provide a deeper empirical understanding of how our interface design affects a learner's experience relative to the status quo approach for the purpose of SH training. Lastly, we provide design implications for building intelligent interfaces that aims to arouse empathy and support experiential learning. Our work is original because we reveal insights on how our interface can complement the current practices through systematic analysis, and initiate thought-provoking discussions on how to improve the proposed design and the training. We anticipate that the results and the implications generalize to other training programs (e.g., ethics, inclusiveness, and security training), contexts dealing with sensitive issues (e.g., stigmatic diseases), and domains that value empathetic responses from users (e.g., medical crowdfunding).",
    "relatedWork": "RELATED WORK: SH refers to unwelcome sexual advances, requests for sexual favors, and other verbal or physical harassment of a sexual nature [15] . Effective interventions are critical for reducing the prevalence and severity of SH. Our research focuses on advancing the use of training as an intervention for SH prevention [15] . Our work complements other HCI research and interventions that encompass sexual misconduct problems including dating and domestic violence [18] , stalking [11] , and online harassment [17] .\nSexual Harassment Prevention Training: SH prevention training seeks to achieve two goals: 1) inform learners about anti-harassment policies and resources, and 2) educate learners about appropriate conduct and improve attitudes towards SH prevention [15] . Achieving the second learning goal is known to be more challenging but also less studied in the research community [47, 53] . A consistent finding in the literature is that training designed to promote empathy toward the target of SH improves a learner's attitude towards SH [53] . Diehl et al. showed that reading a SH case from a target's perspective increased empathy and reduced the acceptance of SH misconception compared to reading the case from a perpetrator's perspective [16] . Schewe and O'Donohue found that presenting empathy-arousing materials decreased men's self-reported likelihood of committing sexual abuse [55] . Empathy is also related to experience-taking, an imaginative process of spontaneously assuming the identity of a character in a narrative and simulating that character's thoughts, emotions, behaviors, goals, and traits [28] . A story written in first-person, where the main character relays a story from his or her point of view, is effective for experience-taking. Prior work shows that first-person narratives lead to favorable changes in participants' behavior and attitudes toward the character's group [14, 28] and positive perceptions of the learning experience [36] .\nSexual Harassment Prevention Training: The form of the instructional method is critical for arousing empathy and achieving the desired outcomes of SH training [54] . Providing multiple methods for training, such as video-based episodes combined with case analyses in text, correlates with increased sensitivity to SH scenarios [63] . However, studies on how to design effective instructional methods are limited [9, 46, 53] . Existing research focuses on text and videos [53] and these methods have shown success in clarifying the gray area of unwanted sexual behavior [3] and improving knowledge [47] . However, such methods have had little impact on changing actual behavior [21, 47] and even lead to adverse effects such as reinforcing gender stereotypes [52] . Researchers suggest that attitudinal change requires more interactive or experiential training [6, 47] .\nSexual Harassment Prevention Training: HCI researchers have primarily focused on creating technology that prevents SH in-situ such as developing panic buttons to draw attention of bystanders or notify emergency contacts [2, 27] , crowdsourcing maps to show locations of SH incidents [2, 60, 64] , and evaluating recording probes that collect contextual data on negative behaviors [7] . However, how to design technology that delivers effective SH prevention training has received little attention in the HCI community. Our work contributes to this limited body of literature by reporting on the design and evaluation of a conversational agent for SH prevention training.\nLeveraging Conversational Interfaces for Sexual Harassment Prevention Training: A conversational interfaces (CI), an interface that allows a user to interact with a computer as if it were a conversational partner [66] , has been increasingly leveraged in training and education [25, 33, 56, 61] as well as in other domains such as healthcare [22, 42] . Previous studies showed that the technology has the potential to assist attitude learning, which involves cognitive, affective, and behavioral aspects. Regarding the cognitive aspect, prior works have revealed the effectiveness of the CI in memory retention [1] , critical thinking, and inquiring mindsets [23] . A CI is known to improve students' affective learning outcomes [62] and influence users' behaviors through distraction and encouragement [32] .\nLeveraging Conversational Interfaces for Sexual Harassment Prevention Training: Prior work has also investigated the use of a CI for storytelling. Emile [41] was created to discuss social theories in first-person narrative. A Freudbot [25] was designed to represent Sigmund Freud, a famous historical figure in psychology, and informed his theories and biographical events in Freud's voice. These studies reported that students endorsed the idea of using the interface as a promising direction in online education. Our work builds on these prior successes by extending and studying the use of a CI for SH prevention training. The goal is not only to acquire knowledge on the topic, as in prior uses of the interfaces, but also to arouse learners' empathy towards the targets of SH and change attitudes about what type of behaviors are considered as SH.",
    "rq": "rq1. what strengths and weaknesses do learners perceive in the use of the ci for sh prevention training?\n rq3. what limitations do learners perceive in traditional sh prevention training?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": " who then had to work with the system in directive mode in session 3. what do we find in the results?"
}
{
    "intro": "INTRODUCTION: The concept of a digital agent that assists users by performing simple tasks (e.g., scheduling, writing notes) first appeared in the 1980s [8] . In recent years, owing to the progress of natural language processing technology, agents are able to communicate with users by voice and even have a simple conversation. These types of agents are known as conversational agents [8, 13] . Meanwhile, conversational agents can assist the life of old people in a variety of ways [4, 9, 16, 17] ; for example, they can help reducing the sense of loneliness through emotional exchanges or remind the intake of medicine [1] . However, there\nINTRODUCTION: are not yet enough studies on the distinct features of old people when interacting with a conversational agent. In this study, we conducted interviews with young and old participants who had used a conversational agent for two weeks and investigated the different interaction of the two user groups.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Service robots are expected to be installed in various locations in future society, such as public spaces, hospitals, and shopping malls. As a social robotics challenge, we are developing a robot caf\u00e9, where the robot's communication functionality is indispensable in serving customers. For example, a robot can recommend dishes by interviewing a customer for his/her food preferences, and collect survey responses related to the dish and service before he/she leaves. It would also be possible to ask the chef to modify the recipe if the robot can elicit the customer's food preference through a conversation. Therefore, this study aims to develop an interview dialogue system that can elicit user food preferences in terms of taste and texture.\nINTRODUCTION: With respect to the research on recommendation systems, there has been substantial interest in augmenting recipe data [4, 18] . Druck [3] focused on the taste and texture of the dish mentioned in the recipe review text, and proposed to augment recipe representations with such information. In a similar study, Watanabe and Nakamura [21] proposed a recipe search system that exploited onomatopoeia to express taste and texture. Moreover, various types of recommendation methods and algorithms were applied to such user feedback [19] .\nINTRODUCTION: By contrast, this study focuses on eliciting user feedback rather than on the recommendation algorithm itself. We aim to collect user preferences through dialogue with a robot. Compared to preference ratings, we believe that this approach can allow customers to enjoy communicating with a robot, and has the possibility of extending the conversation to elicit richer user feedback.\nINTRODUCTION: Studies on interview dialogue systems aimed to obtain information from the user, e.g., conducting survey interviews [7] and academic course ratings [16] . In such domains, it is sufficient for the users to select their answers from a limited number of choices: yes/no and numbers. By contrast, there are a variety of expressions for the preference of dishes. For example, some people prefer crispy bacon for carbonara, whereas others prefer juicy bacon. As discussed in [14] , in such a situation, dialogue systems are required to help the users to verbalize the nuances of the tastes and textures of dishes. Komatsu [10] reported that onomatopoeia is a useful Based on the above discussion, in this study, we propose an interview dialogue system that acquires a user's preference for the tastes and textures of dishes through a conversation facilitated using onomatopoeia. Moreover, in generating the dialogue, the system utilizes the default knowledge base obtained from a recipe database and an analysis of a large number of Twitter messages [17] .\nINTRODUCTION: The proposed dialogue system was installed in a Pepper (a humanoid robot of the SoftBank Group). This robot is used at shops and stores to provide information about their services. Therefore, we believe this robot is also suitable for caf\u00e9 and restaurant situations. Moreover, it is known that the humanoid interface has the power to elicit human communicative behaviors [2, 15] , and people are more willing to disclose information to virtual agents than to real humans [5, 12] . Therefore, it is expected that a customer will be more honest in answering questions about his/her preferred taste.\nINTRODUCTION: In the following sections, we first explain how to create the knowledge base, and then propose our interview dialogue system and a food vector space that characterizes the user's food preferences.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Chatbots have come a long way since 1966, when Joseph Weizenbaum created one of the earliest natural language processing computer program -ELIZA [11] . What would be simply called a chatbot today, ELIZA used fixed scripts and rules to react to user inputs. The most famous of these scripts was called DOCTOR and simulated a psychotherapist, allowing users to interact with ELIZA as they would be doing a psychiatric interview with another human.\nINTRODUCTION: Most chatbots today are used in the customer service, marketing or sales. For example they can help you book a flight, or provide support if you have questions about a product or service. These chatbots are usually rudimentary, providing information or answering questions without really showing interest in a user's wishes or reacting to emotions in the same way a salesperson or customer care agent would. This lack of emotions and empathy can lead to unsatisfying answers, especially if the user expresses certain emotions and expects an appropriate, potentially empathic response.\nINTRODUCTION: Empathy is the ability to understand and share the feelings of another. It is considered a difficult task for a conversational agent, or an artificial intelligence in general. Being empathic means to have the ability to recognize another person's emotions and to respond emotionally to it (affective or emotional empathy) and to understand another person's feelings by taking their perspective (cognitive empathy) [7] . It becomes apparent why empathy is difficult to achieve in chatbots, as a chatbot cannot simply take the perspective of the user the same way a person would be able to.\nINTRODUCTION: However, recent research has shown that it is possible to generate empathic responses which are preferred by users over more traditional non-empathic responses, for example in customer service [12] . There are currently many different techniques that can be used to simulate empathy, by detecting the user's emotion and creating an empathic respond based on fixed rules, non-neural machine-learning such as Support Vector Machines or deep learning approaches [10] .\nINTRODUCTION: In this work, we conducted a preliminary study where we have compared an English non-empathic advice-only chatbot and an English empathic physical healthcare chatbot by having them tested by participants and by having them answer questionnaires. We first present the related work and the research conducted over this subject. The methods followed by the results and discussion will then be presented. At the end we present the conclusion and future work.",
    "relatedWork": "RELATED WORK: Feng et al. [5] studied the relationship between empathy and trust between two people communicating via instant messaging. By comparing different communication styles they found that both response type (i.e. supportive responses) and empathic accuracy (correctly inferring the other's feelings) have statistically significant influences on interpersonal trust. Contradictory messages, where response type and empathic accuracy are inconsistent, have a highly negative impact on trust.\nRELATED WORK: It is generally easier for a chatbot to show empathy in longer conversations, as there is more data to work with. However, Xu et al. [12] developed a chatbot capable of showing empathy for answering customer service requests on social media. The chatbot uses Deep Learning techniques such as Long Short-Term Memory (LSTM) and was trained on almost 1M Twitter conversations between users and agents from 62 brands. The evaluation showed that it is almost as good as human agents in showing empathy for emotional requests, even for short Twitter interactions. There was a decrease in empathy ratings for purely informational requests, probably due to a lack of emotional words. It was mentioned that this could be solved by considering additional contextual information such as a users' social media profile.\nRELATED WORK: In terms of healthcare, Liu and Sundar [8] evaluated whether a health advice chatbot should have empathy or if that leads to strong uncanny perceptions called the uncanny valley of mind (UVM). They did two studies, where in the first one participants were asked to read chatbot conversations with and without different kinds of empathy (sympathy, cognitive empathy, affective empathy), imagining themselves as the user. Expressions of sympathy, cognitive empathy and affective empathy by the chatbot were perceived as compassionate, and except for cognitive empathy also more supportive than advice-only. In a second study four real script-based chatbots were used based on keyword recognition. The instructions were standardized to ensure the script was followed, and the participants were told what to write. They've found no significant differences between these different chatbots except for perceived sadness between sympathy and advice-only. For both studies, however, the findings are more in favor of the paradigm computers are social actors (CASA), meaning empathy is useful for chatbots in a medical setting. Even though there was no statistical significance for that in the second study, the pattern was still similar to the first study.\nRELATED WORK: Another important example of a chatbot is Owlie, created by Falala-S\u00e9chet et al. [3] , mainly created for psychological support. It follows different psychological theories, as Cognitive Behavioral therapy (CBT), Dialectical Behavior Therapy (DBT), Mindfulness, and more. Woebot [6] , developed by Fitzpatrick et al., was dedicated for college students in the United States who are experiencing anxiety and depression. The results have shown a significant difference on depression while delivering and applying CBT principles.\nRELATED WORK: On another hand, for physical health, chatbots appear mainly as simple symptom checkers with pre-determined questions without reacting to the user's mental state or emotions, thus not responding in an empathic way.\nRELATED WORK: In contrary to human-human interaction, there are currently no widely used and accepted measurements for perceived empathy in robots in human-robot interaction (HRI). Most measurements are based on self-made questionnaires or self reported measures. In order to measure how empathic a robot is conceived, Charrier et al. [1] developed the Robot's Perceived Empathy (RoPE) scale. It is based on human empathy metrics adapted to robots and was evaluated by 15 experts in cognitive and other social sciences, and 10 experts in robotics or artifical intelligence.\nRELATED WORK: For this work, we create and evaluate two different medical assistant chatbots capable of providing a medical diagnosis based on detected symptoms from the user input. The first chatbot detects the emotions of the user and, depending on the emotion, replies in a supportive and empathic way while giving medical advice. The second one is advice-only by just providing the diagnosis without reacting to the user's emotions. The RoPE scale is used in the form of a questionnaire at the end of the tests to evaluate the empathy of the two chatbots.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Recently, dialogue systems such as Siri, and Google Assistant, have been widely deployed. These systems are able to answer simple questions, and also chat with users. While such systems are able to respond to questions and orders from the users, they lack the ability to express emotions in their dialogues. Therefore, the user engagement towards these systems is limited. In human-human conversations, psychological studies have shown that people experience positive mood [13] or have friendly feelings toward their partner when the partner shows his/her empathy [3] [11] .\nINTRODUCTION: Dialogue systems that can express emotions in dialogues have been proposed. Emotional Chatting Machine generates six types of dialogues including six emotions (e.g. happy, sad) [16] . Yoshino et al. propose a dialogue system for persuading users to take good actions such as going to bed earlier [15] . Their dialogue system expresses anger and happiness depending on the degree of user acceptance. Berteto et al. propose an emotion recognition system from user's voice utterance with deeplearning based approach [1] .\nINTRODUCTION: Empathic dialogue systems have also been attracting attention. Zara is an embodied agent that expresses empathy with her face and gestures after recognizing the user's emotions from his/her face and voice [5] . As a dialogue system showing empathy for the user utterance in dialogues, Kawahara et al. propose a prediction method of appropriate moments to respond backchannels such as \"okay\" by analyzing the conversations between university students and a counselor [8] . Their experimental results show that their prediction method increases the participants' degrees of empathy. An autonomous android Erica generates backchannels for the user utterance to express her empathy to the user [9] . DeVault et al. propose a virtual interviewer that responds backchannels and general phrases such as \"I'm sorry\" for the user utterance about his/her sad story [4] . While these approaches are expected to be effective for increasing user empathy, they were applied to limited domain dialogues such as counseling or health care. In the limited domain dialogues, dialogue systems can utilize fixed phrases such as backchannels corresponding to the observed user emotions. In open domain conversations, however, these approaches are inapplicable due to the unlimited variety of topics and lengthier dialogues, thus, more flexibility is required for system utterances.\nINTRODUCTION: In this paper, we propose an empathic dialogue system, which generates empathic utterances for a wide variety of topics based on their observed emotions in dialogues. Our dialogue system provides topics from collected Web news articles, and creates utterances from tweets related to the news articles. To investigate what types of emotions the system should express against observed user emotions, we draw on psychological studies about empathy, and established a hypothesis that the degree of a user's empathy can be increased when the system creates an utterance from tweets that contain the same emotion as the user's utterance. We have implemented this hypothesis to our system, after confirming its validity by a preliminary experiment. In order to evaluate effectiveness of the proposed method, we ask the participants to answer a questionnaire about the degree of empathy for dialogue in seven grades, -3(not at all) to 3(very much). The results show that in cases where the user utterance was classified into either positive or negative, the degree of empathy in the proposed method had significantly increased, and was the highest among all tested methods.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "Introduction: In recent years there has been a series of successful results in the area of conversational agents to support learning in chat environments [2] [4] [6] [7] [8] [9] [10] [11] . Such agents have provided social support, affording the agents a more credible social standing in the group and helping to diffuse tension and create a productive learning environment. Furthermore, they have provided conceptual support, designed to elicit more depth by leading students through directed lines of reasoning, referred to as knowledge construction dialogues (KCDs). While KCDs have been shown to lead to increased learning gains, particularly in situations where the conversational agents also provide social support [8] , the necessity of designing them statically, with a pre-defined line of reasoning in mind both makes them hard to adapt to new subject material and does not fully exploit the benefits of collaborative learners following their own spontaneous lines of reasoning.\nIntroduction: We have therefore drawn on extensive work related to support of classroom discourse [12] [13] [14] and collaborative learning [3, 15] to investigate the use by conversational agents of facilitation moves that promote academically productive talk (APT). The aim of APT facilitation moves is to increase the amount of transactivity [3] , by dynamically reacting to student discussions, encouraging them to build on each other's reasoning. Furthermore, as APT refers both to learners social positioning to each other and their conceptual positioning to knowledge, this provides us with a theoretical framework to better integrate the social and conceptual support aspects of conversational agents.\nIntroduction: In this paper, we analyse our first study involving an agent performing APT moves in the context of a 9 th grade biology classroom. We contrast two forms of support (one in which the agent performs the facilitation and a second in which the agent prompts another student to perform these moves) and a null condition with no support. We show that the presence of APT moves is correlated with improved student reasoning but also discover that while the first form of APT support shows promise, the second produces much less reasoning than would be expected. In order to better understand how the agents shape the conversation, both productively and unproductively, we employ a linguistic style process analysis to inform the next iteration of development of academically productive talk agents.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": "taskbot: are you done with \"please remind bob to submit the proposal.\"?\np7: how do i reply to a question?\n p8: can you send the consent form again?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Distributed teams are widespread in daily work settings [24] . Recent global developments due to the COVID-19 pandemic have highlighted this trend more than ever expected [52] . The way teams work has experienced substantial change in recent years with the emergence of tools like Slack or Microsoft Teams, specifically leading to increased text-based communication [24, 51] . However, these tools only provide limited capacity for traversing socio-emotional information (e.g. non-verbal cues) [27] . This reduction has an adverse effect on the management of emotions (EM) -the ability to reason about and use emotions to enhance thought through perception, understanding and regulation -which is important for team collaboration. Simultaneously, EM is highly challenging for self-managing teams in distributed conditions [56] . While successful EM exerts a strong positive influence on team-level outcomes like team cohesion and performance [31] , poor EM, in contrast, may lead to decreases in decision-making, communication flow, and well-being [2, 11, 40] . Ergo, the lack of adequate EM is a major challenge for text-based team communication. For these reasons, emotions in teams and their influence on team work has gained increasing interest in research [70] . Beyond the primary function of text-based communication, modern collaboration tools allow for the integration of third-party applications [42] . Chatbots are one particularly interesting instance of such applications since they have been suggested as team facilitators for task support [1, 69] . In this function, chatbot applications represent a novel opportunity for EM within teams as they can serve as a replacement for missing human emotion managers [58] . Due to their ability for natural language processing, precise information retrieval [44, 75] and emotion recognition [60] , they are particularly suited for this role of managing team emotions and to excel better in this role than other interaction interface, e.g. like notifications. Previous research has followed similar avenues and proposed an exploratory prototype [56] in order to create a positive affective tone within a group chat. This work provides initial evidence for the basic ability of chatbots to support emotion regulation. However, the study results also showed no usefulness and partial annoyance perceived by its users through the chatbot design. One well-known solution to overcome these problems is to involve end-users in the design process [34] .\nINTRODUCTION: In this study, we expand early research [56] by leveraging a participatory design approach [34] . We conducted participatory design workshops with end-users to investigate chatbot-based EM for teams in order to overcome previously identified challenges. For the scope of the design space, three relevant entities are considered which guided the design process, namely the messaging system, the interacting team, and the chatbot. Three design workshops with 16 participants amassed to 153 design sketches. Through a user-informed selection process, three final design prototypes were derived subsequently: NeutralBot (NBT) which is neutrally reporting, SocialBot (SBT) which embeds anthropomorphic and social design features, and ActionBot (ABT), which acts as immersive moderator that actively intervenes (e.g. through break suggestions). These prototypes were subsequently evaluated within a laboratory experiment and ensuing interviews in order to receive detailed insights with 27 participants (9 three-member groups).\nINTRODUCTION: Both, quantitative and qualitative results provide evidence for the positive influence of all chatbot designs (NBT, SBT, ABT) of increased emotion awareness and communication efficiency. Beyond that, SBT and ABT present positive development in emotion regulation and compromise facilitation through stimulating effects of social (e.g. anthropomorphic appearance) and interactive (e.g. break) design features. While general chatbot experience was positive, the findings report the importance to consider contextual factors (timing, accuracy, time-pressure), and limit too obtrusive (content deletion, large images) and too neutral interventions (missing explanations) in order not to undermine these effects. Finally, since emotions are a sensitive domain for the users, we also find first evidence for threats of chatbot-based EM with a described feeling of surveillance and loss of control.\nINTRODUCTION: In summary, the present work contributes to the literature on chatbot-based EM in distributed team communication in a three-fold manner: (1) We present three user-informed design concepts for chatbot-based EM for distributed teams (Section 3, 4). (2) We evaluate these design concepts regarding the participants' experience and the EM abilities (Section 5, 6). (3) We discuss in detail the design implications arising and highlight negative pitfalls of the designs (Section 7). Based on this, we suggest a selective combination of design features to improve positive effects and mitigate downsides of the designs for the future of distributed teams, especially in times of new workplace realities like COVID-19 created. Finally, our findings reveal the importance to consider feelings of loss of control in the future design of chatbot-based EM for distributed teams.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Our research group is building embodied conversational agents (ECAs) to support human-human communication and relationship building in virtual environments. We have created an online digital city resource for real-life inhabitants of Kyoto, including virtual reproductions of parts of the city itself, which people can log into to explore and converse [7] . We believe ECAs can play important roles in this online community, providing assistance and information, helping to direct the flow of visitors to this online world, and assisting in making connections between these virtual Kyoto citizens. In online worlds, such agents can have special information and knowledge, and powers of movement and activity that can complement the abilities and powers of actual human visitors. We believe novel, blended interaction styles and relationships will form in these contexts. We call such environments 'agent-mediated communities'. Within agentmediated communities, we believe ECAs could be quite powerful-even going so far as to influence visitors' opinions of one another as well as of the ECA. Shared virtual environments with ECAs are becoming increasingly common (for a large-scale commercial example, see [11] ), and we (and others-see [19] ) see this as an important application area for ECAs. There is already evidence that nonembodied software agents that communicate with human users in text-chat virtual worlds are engaging [2] . Some would argue that ECAs could play a vital role in shaping community in online virtual worlds [8] . However, this belief presumes that ECAs can in fact be influential members of such communities.\nINTRODUCTION: Yet the ECAs' effectiveness will depend upon their ability to work with visitors not just as individuals, but as members of conversational and other groups (such as crowds or tour groups), within the environment. Techniques for interacting with small or large groups socially can be very different than those used in oneon-one conversations and relationship formation. For example, an ECA might try to completely align its opinions with any individual it is working with, in a one-on-one situation. However, in a group, it needs to be careful of how it uses such agreement with each person, in order to preserve the harmony of the group, and avoid inconsistency or perceived favoritism. There are many findings of this nature in the social sciences, about the workings of groups. Developers of ECAs need to take these findings into account when designing for these contexts.\nINTRODUCTION: In our own efforts, we are focusing on crafting group interaction skills and competence for ECAs, so that they can have a role in supporting and sustaining human-human relations. We call an agent with such skills a social ECA. In this paper, we present preliminary studies that use principles from the study of group communication dynamics in the social sciences to craft charismatic and effective social ECAs.",
    "relatedWork": "SOCIAL EMBODIED CONVERSATIONAL AGENTS 2.1 Related Works: Much research has explored creating embodied conversational agents for one-on-one interaction applications, such as tutoring [9] , training [17] , and sales [1] . Less work has been done to investigate how best to create ECAs for group contexts. As far as documenting practical effectiveness of ECAs, there are as yet few results even in one-on-one application contexts (Lester et al's results are a notable exception [9] ), and only one that we know of for group contexts [6] .\nSOCIAL EMBODIED CONVERSATIONAL AGENTS 2.1 Related Works: There are, however, research findings about the general social effectiveness of one-on-one ECAs. People respond to the facial expressions, gaze, head movements, and gestures of agents as if they were human beings [1] [18] . There is a body of work (summarized in [16] ) that demonstrates that people will respond similarly to characteristics (such as gender cues) and interpersonal tactics (such as flattery or reciprocity) coming from computers and ECAs as they do toward other people. Researchers have also demonstrated that people will behave in a familiar social science task (the prisoner's dilemma) with ECAs similarly to how they'll behave with a human partner in a videoconferencing system [14] .\nSOCIAL EMBODIED CONVERSATIONAL AGENTS 2.1 Related Works: Our research follows this approach-building ECAs that use strategies and cues known to be effective in human interaction situations, and confirming their effectiveness in the agent-human interaction. We are interested in cues and tactics that enhance or dampen persuasiveness and charisma in our agents, in a group context.\nWhat Makes a Persuasive and Effective Social ECA?: In an earlier study [6] , we noticed an interesting effect-an ECA that made itself disagreeable to both conversation partners in a group (by bringing up an embarrassing subject), tended to cause them to have a more interesting and engaging experience, and to have more positive impressions of their conversation partner. Upon investigating the literature on impression formation, we learned of a social science theory that may help explain this effect, called balance theory. Balance theory, formulated by Heider [4] , states that people prefer to have harmoniously aligned attitudes toward a third party or object. For example, if I dislike a third person, I will feel more in balance with you if you feel the same way toward this person. Such an alignment can cause the kinds of positive feelings and outcomes that we observed in this prior study, and is used in advertising contexts to elicit positive impressions of products [15] . We felt this would be a helpful tactic for a social ECA to use in group situations, to drive human interaction partners toward forming particular impressions of one another, and of the agent itself.\nWhat Makes a Persuasive and Effective Social ECA?: We were also interested in what factors would affect the ability of the social ECA to influence people in this manner. In human conversation, a person's power in a group situation can be gauged by looking at how much he or she holds the conversation 'floor'. A person who maintains a higher degree of control over the interaction will tend to have more influence [13] . From this, we drew the conclusion that the more our social ECA could control the floor by mediating the conversation, the more effective it would be at driving peoples' impressions of it and of the other conversation partners. Conversely, allowing the human interaction partners to control the floor more would inoculate them against influence from the social ECA.\nOur Hypotheses: We set up experiments to investigate whether we could replicate balance theory effects with our social ECA, and whether we could modulate the effects based on control of the conversation. Our hypotheses were:\nOur Hypotheses: 1. A social ECA can create balance theory effects.\nWidening the conversational channel (reducing the social: ECA's mediation) between the human conversation partners will lessen the conversational control of the ECA, and correspondingly lessen its effectiveness.\nWidening the conversational channel (reducing the social: In order to test hypothesis 1., we needed to be able to manipulate the participants' attitudes toward the social ECA, so that they could then be in alignment (or not in alignment). To do this, we manipulated whether the social ECA held the same opinion as the participant-a common and effective tactic in persuaders' attempts to influence [20] . Thus, we include two additional hypotheses:\nWidening the conversational channel (reducing the social: 3. A person will like the social ECA more if it shares his/her opinions, and will dislike it more if it does not.\nWidening the conversational channel (reducing the social: 4. A person will assume that his/her conversation partner likes the social ECA more if it shares that person's opinion, and will assume that s/he dislikes the social ECA more if it does not.",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Bots deployed in online communities are a combination of tools designed to serve a specific purpose and social actors intended to join conversations [25] . They are designed to do many things that humans had previously done or would otherwise do, but are also created with social elements in mind -their own names, grammatical styles, and sometimes even personas. Many are run alongside rather than within sites, a concept that Geiger calls \"Bespoke Code\" [12] . In certain situations, bots can create and contribute their own content to a community [8] in patterns designed by their creators. To the outside observer, limited affordances on certain sites allow cleverly designed bots even to pass as human for extended periods of time [1] . Beyond their variable humanity, bots give us new capacities, particularly by amplifying our efforts in speed or scale. They help us collect data [40] , attract attention for a cause [36] , and detect and respond to certain behaviors [13] . At scale, bots can even help shape or sway political conversations [1] .\nINTRODUCTION: In this paper we explore the variety of types of social actions that can be taken by bots on Twitch 1 , a growing platform that reports more than 140 million unique users 2 and hosts thousands of diverse communities. As we find in our analysis, Twitch bots are extremely active participants in Twitch chatrooms; though the overwhelming majority of accounts that participate on Twitch are human, the average bot sends well over an order of magnitude more messages than the average human, making them important actors in the space. While recent research has found similar patterns of participation for both malicious and user-invited bots in spaces like reddit and Twitter [8, 35] , Twitch is both a new context for the study of bots and a space with a different conversational structure than Twitter or reddit, with bots participating in a public chatroom rather than a threaded conversation or Twitter network. We show here Twitch bots' importance to the social discourse on Twitch both through analysis of their rates of participation across different types of communities and through description of the types of engagement they have with their communities.\nINTRODUCTION: In order to guide our exploration of the roles of bots in Twitch communities, we aim to answer the following research questions for a number of different types of communities on Twitch:\nINTRODUCTION: (1) How frequently do bots send messages on Twitch? (2) What kinds of language do bots on Twitch use, and for what purposes? (3) What do bots and users say to each other?\nINTRODUCTION: The first of these questions addresses whether bots on Twitch are active participants in social spaces, placing their social dynamics in conversation with prior work [8, 35] . The second question supplements previous work that identified patterns of bot language in different contexts, e.g., a particular subreddit [8] . While some work has looked at users' desired features for bots in community-style social platforms [28] , most language-specific analysis of bots has been done on networked spaces like Twitter, or specific spaces on a single platform. Here we analyze the full breadth of communities on Twitch, identifying ways that bot usage varies across types of communities. The third question addresses the complexity of human-bot interactions on Twitch. Prior work has found that bots have had a variety of roles, both in completing particular automated tasks (e.g., [40] ) and in holding human-like conversations with users (e.g., [42] ), and in this work we aim to situate Twitch bots in this space. We answer each of these three questions across different sizes of communities and across communities built around different types of content, identifying how community characteristics contribute to bot behaviors.\nINTRODUCTION: We begin this paper with a brief description of the Twitch platform and then review relevant work. Next we describe prior research that has identified roles among humans online, focusing specifically on the methods that have been used. Third, we briefly review some of the literature on chatbots online and consider how the methods used to study human roles online might be applied to the study of bots. Following our review of the literature, we identify the functions for which bots have been developed on Twitch and present example interfaces for management of the two most popular and oldest third-party bots on Twitch, which have set the standard for bot designs. In the main section of this paper we quantify the influence of bots on conversations using methods influenced by structural role theory [4] and based on prior analysis of human roles to identify what bots types of messages send, how frequently they send each type, and how much they interact with regular users. We conclude with a discussion of opportunities for further development of bots' social roles on Twitch and beyond.\nINTRODUCTION: The Social Roles of Bots 157:3",
    "relatedWork": "",
    "rq": "(1) how frequently do bots send messages on twitch?\n (2) what kinds of language do bots on twitch use, and for what purposes?\n (3) what do bots and users say to each other?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": " the gain strategy had the question: how do we fight corruption in our cities & thus improve them?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": "the research questions that the current study poses are: can a linking contributions (lc) mode of conversational agent interventions, which is operated in the context of an online collaborative learning activity in higher education settings, improve students' learning performance?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "Introduction: Apart from their conversational skills, the non-verbal behavior and the appearance of embodied conversational agents become more and more realistic. Thus, embodied conversational agents offer great promise to more natural interaction in social settings like tutoring or gaming. Prominent examples of such agent systems include the Steve Agent (Rickel and Johnson, 1999) , the real estate agent REA (Cassell et al., 2000) , the GRETA Medical Advisor (Pelachaud et al., 2002) , the agent MAX (Kopp et al., 2003) , the virtual patient (Hubal and Day, 2006) or the tactical language training agent (Core et al., 2006) .\nIntroduction: Despite these successful examples, virtual agents are still research prototypes and seldom tested with real users in real settings. Thus, not much is known about the acceptance or behavior towards such artifacts that goes beyond small-scale user studies in laboratory settings. Reeves and Nass (1996) claim that every type of media and especially new media like virtual agents is treated as a social communication partner in one on one interactions. They call this the CASA paradigm-Computers Are Social Actors-and have shown in a number of experiments derived from typical human-human interaction studies that people indeed seem to treat computers and other media like they treat real people. For instance, to attribute technical expertise to a system it matters if a male or female synthesized voice is used. Although the CASA paradigm seems to hold true for certain aspects of interactions with virtual characters, results of actual user studies are not conclusive and sometimes even contradict each other. Kra \u00a8mer (2005) gives an excellent overview of a number of different studies. To give an example, it was shown that the use of an embodied conversational agent can have both positive and negative effects on solving a given task.\nIntroduction: All of these studies and all of the above mentioned systems focus on dyadic interactions between one user and one agent. If we turn to communications with more than two interactions partners, we find systems where one user 0953-5438/$ -see front matter \u00d3 2008 Elsevier B.V. All rights reserved. doi:10.1016/j.intcom.2008.02.005 engages in the interaction with several agents. A number of approaches to such multiparty conversations have been inspired by research on interactive drama that aims at integrating a user in a scenario-either as an audience member or an active participant. An example includes Avatar Arena (Rist et al., 2003) , where the agents perform a presentation for the user interacting among each other. An interesting feature of Avatar Arena is the simulation of listener as well as speaker behaviors based on empirical studies of human-human conversations. Traum and Rickel (2002) have addressed the issue of automatically generated multiparty dialogues in immersive virtual environments. The characters are based on the Steve architecture which has been enhanced by a multi-modal dialogue model to handle turn taking in such a challenging scenario. The Vic-Tec system (e.g., Paiva et al., 2004 ) realizes a multi-agent learning environment to teach kids strategies against bullying relying on a Forum Theatre metaphor. The user is able to interact with one of the agents and suggest plans of action that will influence the storyline. In the NICE fairy tale game (Gustafson et al., 2004) , a user can enter the fairy tale world of H.C. Andersen where she may meet three different types of agents like the helper agent that accompanies her through the world, suggesting lines of action.\nIntroduction: Hardly any work so far has been conducted on the realization of scenarios with multiple users and a synthetic agent. An exception includes the work by Isbister et al. (2000) who concentrate on social interactions between several humans in a video chat environment which is supported by a so-called Helper Agent. In contrast to Isbister et al., we focus on a game scenario in which the agent does not appear in the role of a moderator, but takes on a similar role as the human users. The questions arise, if users regard an artificial agent as an equal conversational partner worthy of being attended to even if there are other human interlocutors around and what kind of behavior the users will show towards the agent.\nIntroduction: In this article, we are still not moving outside the laboratory setting but our scenario differs in an important aspect from the standard user evaluation. Instead of one user and one agent who engage in a task-oriented dyadic interaction, our game scenario features two users and an agent who play a game of dice together. This offers completely new ways of evaluating the interaction with virtual agents. On the one hand this setting allows us to directly compare the social behavior of the users towards each other with their social behavior towards the virtual characters at the same time in the same situation. On the other hand, users are free to talk with one another which gives us additional insights into what they think about the agent and how they interpret its behavior.\nIntroduction: In the next section, the game environment is shortly introduced (Section 2). Section 3 gives an overview of the verbal and nonverbal behavior that is examined in the evaluation studies. Section 4 then deals with the evaluation of gaze behavior which is followed by an evaluation of the users' verbal behavior (Section 5). After a general discus-sion (Section 6) the article closes with a conclusion (Section 7).",
    "relatedWork": "",
    "rq": "2. what does she want now?"
}
{
    "intro": "INTRODUCTION: Immersive virtual worlds offer enormous potential and opportunity to human users for the interactive experience and realistic three-dimensional graphical environment that they provide. Advanced agent technologies allow Embodied Conversational Agents (ECAs) to be built, in order to create more customized and user-oriented virtual experiences. To date, most of the agents effectively operate at one-to-one level. Meanwhile, Collaborative Virtual Environments (CVE) technology is becoming more popular. It aims to transform today's computer networks into the networks that support collaborative work and social play. CVE represents a natural extension of current single-user technology to support multiple-users. Furthermore, CVE can also support some aspects of social interaction which are not readily accommodated by single-user ECA technology.\nINTRODUCTION: In previous research, we built an embodied tour guide, Elva [1] , that is able to engage conversationally with a system user about gallery exhibits, and capable of behaving according to social norms in terms of gestures and facial expressions (see Figure 1 ). Its layered architectural design has ensured appropriate coupling between the agent's perception and action. We utilized the notion of \"schema\" to support structured and coherent verbal behavior. Elva's nonverbal behavior is dynamically generated from communicative goals to mimic a face-to-face conversation.\nINTRODUCTION: While the previous research has focused on Elva's interaction with a single user, multiparty interaction support is a desirable feature for the further development of this project. A multiparty scenario not only extends the questions from a two-party scenario, such as modeling of interaction and coordination of multimodal behaviors, but also presents entirely new challenges posed by the large number of users. How to embed relevant social rules in a multiparty system, how to exhibit leadership, and how to establish appropriate interaction style impose great challenges to the designer of the virtual agent. The successfully implemented multi-party interaction support aims to improve agent autonomy and believability and to achieve higher user satisfaction.\nINTRODUCTION: In this paper, we present an integrated agent design with multiparty interaction support. Section 2 reviews agent technology and social theory development. Section 3 discusses the specific issues in a facilitated casual social group. Section 4 focuses on system design and architecture. Section 5 presents a demonstration and evaluation, and section 6 concludes the paper.",
    "relatedWork": "",
    "rq": " joyce: what is it made of then?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "Introduction: Conversational interfaces including dialogue systems and conversational agents have been typically used as a single interface to a single user (Zue et al., 1994; Allen et al., 2001; Cassell et al., 2000) . On the other hand, a new area of research in conversational interfaces is dealing with multi-party interaction (Traum and Rickel, 2002; Liu and Chee, 2004; Zheng et al., 2005) . Multiparty conversational interfaces have been applied to such tasks as training decision-making in team activities (Traum and Rickel, 2002) , collaborative learning (Liu and Chee, 2004) , and coordinating and facilitating interaction in a casual social group (Zheng et al., 2005) .\nIntroduction: The advantage of such multi-party dialogues over two-party cases is that the multi-party case encourages group interaction and collaboration among human users. This advantage can be exploited to foster such human activities as student learning in more social settings and to build and maintain social relationships among people. However, unless users actively engage in the interaction, these multi-party dialogue qualities cannot be adequately exploited. Our objective is to stimulate human communication in multi-party dialogues between multi-users and multi-agents by raising user willingness to engage in the interaction and increasing the number of user utterances.\nIntroduction: As the first step toward this objective, we exploit a new style of dialogue called thoughtevoking dialogue and experimentally investigate the impact of a peer agent's presence and agent emotional expressions on communication activation in thought-evoking multi-party dialogues. A thought-evoking dialogue, an interaction in which agents act on the willingness of users to provoke user thinking and encourage involvement in the dialogue, has the potential to activate interaction among participants in multi-party dialogues.\nIntroduction: Previous work proposed a quiz-style information presentation dialogue system (hereafter quizstyle dialogue system) (Higashinaka et al., 2007a) that is regarded as a kind of thought-evoking dialogue system. This system conveys contents as biographical facts of famous people through quizstyle interaction with users by creating a \"Who is this?\" quiz and individually presenting hints.\nIntroduction: The hints are automatically created from the biographical facts of people and ordered based on the difficulty naming the people experienced by the users (Higashinaka et al., 2007b) . Since the user has to consider the hints to come up with reasonable answers, the system stimulates user thinking. This previous work reported that, for interaction between a single user and a computer, a quiz-style dialogue improved user understanding and willingness to engage in the interaction. In this paper, we focus on a quiz-style information presentation multi-party dialogue (hereafter quiz-style multi-party dialogue) as an example of a thought-evoking multi-party dialogue.\nIntroduction: A peer agent acts as a peer of the users and participates in the interactions in the same way that the users do. We are interested in the peer agent's role in quiz-style multi-party dialogues since the positive effects of a peer agent on users have been shown in the educational domain (Chou et al., 2003; Maldonado et al., 2005) , which is a promising application area for quiz-style dialogues. In the educational domain, a user could benefit not only from direct communication with a peer agent but also from overhearing dialogues between a peer agent and a tutor. Learning by observing others who are learning is called vicarious learning and positively affects user performance (Craig et al., 2000; Stenning et al., 1999) . To the best of our knowledge, detailed experimental investigations on the effect of a peer agent on communication activation have not been reported in multi-party dialogues between multi-users and multi-agents, which are our main concern in this paper.\nIntroduction: The topic of emotion has gained widespread attention in human-computer interaction (Bates, 1994; Picard, 1997; Hudlicka, 2003; Prendinger and Ishizuka, 2004) . The impact of an agent's emotional behaviors on users has also recently been studied Maldonado et al., 2005; Prendinger et al., 2005) . However, these previous studies addressed scenario-based interaction in which a user and an agent acted with predetermined timing. In this paper, we investigate the impact of agent emotional expressions on users in multi-party dialogues in which multiple users and agents can make utterances with more flexible timing. , we classify agent emotional expressions into empathic and self-oriented ones and investigate their impact on users in a thought-evoking multi-party dialogue system. As stated above, addressed scenario-based Black-jack interaction, but we deal with multi-party dialogues that enable more flexible turn-taking. Previous studies (Bickmore and Picard, 2005; Higashinaka et al., 2008) showed that agent empathic expressions have a positive psychological impact upon users, but they only examined two-party cases. Although Traum et al. (2002) and Gebhard et al. (2004) exploited the role of agent emotion in multi-party dialogues, they did not adequately examine the effects of agent emotion on communication activation by experiment.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Group chat applications have seen considerable growth in recent years, especially for coordinating information work. By enabling quick, team-wide message exchange in different channels, these applications promise to minimize the frictions of group communication, particularly for distributed and remote teams. Many organizations use systems such as Slack [6] , HipChat [3] , Internet Relay Chat (IRC) [59] , Microsoft Teams [5], and Google Hangouts Chat [2] to make decisions, answer questions, troubleshoot problems, coordinate activity, and socialize. As of 2016, Slack alone reported having over 4 million daily users [47] .\nINTRODUCTION: However, chat systems can have a number of downsides. Unlike email or forums, chat is predominantly synchronous, with a heightened expectation for quick responses and a high volume of back-and-forth messages exchanged in rapid succession [10] . As a result, chat logs are often comprised of a great many short messages forming multiple distinct yet intertwined conversation threads, with little distinction made between messages that are important and those that are not. 196:2 A. X. Zhang and J. Cranshaw This can make it difficult for members of the group who are not present in the conversation in real-time to make sense of it after the fact-for example, when someone falls behind, goes on vacation, revisits old chat logs, or is a newcomer to the group. Perhaps because of this burden of sifting through chat conversations, users have criticized group chat as encouraging an overwhelming \"always on\" culture, and some organizations have chosen to cast it aside [38, 41] .\nINTRODUCTION: To make group chat conversations more comprehensible, we can build off of sensemaking affordances designed for other textual domains, such as email, online forums [53] , or documents and general information management [31] . For instance, tags can be added to important messages to contextualize them or differentiate them from unimportant messages, similar to labels in emails or highlighted sentences in long documents. Furthermore, adding structure to the conversation could allow related messages to be grouped, much like distinct threads in an email inbox. Finally, both of these affordances facilitate the summarization of long back-and-forth conversations into a condensed format, much like notetaking in live meetings. Although these approaches to sensemaking have been explored in asynchronous discussion [36, 57, [89] [90] [91] , little work has explored how to enrich synchronous chat conversations, which has additional challenges.\nINTRODUCTION: In this work, we consider how to apply these techniques in situ, enabling participants to enrich their discussions while they are conversing. We explore a variety of ways chat participants can mark up portions of their chat to create enriched, structured representations that allow users to get a high level overview of a full conversation and to dive in to parts of interest. Furthermore, our approach does not require a dedicated notetaker, allowing our design to conform to the spontaneous nature of group chat discussions. We conduct our analysis through an iterative design process, beginning with needfinding interviews and design mock-ups, and culminating in lab studies and a field study of a prototype system.\nINTRODUCTION: From interviews, we learned about the information management practices of 15 active group chat users, finding that many interviewees have trouble keeping up with chat and often miss important messages while scrolling up to read through their backlog. To ground the interviews, we created mock-up illustrations of different synthesized representations of a chat conversation, each emphasizing different information extracted from the conversation and varying degrees of structure. Some designs made use of tags on individual messages, others focused on extraction of important quotes, while still others involved written abstractive summaries. From showing the designs to our interviewees, we found a preference for more structured designs as well as signals such as major discourse acts [68] in a conversation, where discourse acts are categories of statements that characterize their role in the discussion (e.g. \"question\" or \"answer\").\nINTRODUCTION: Based on these findings, we developed Tilda, a prototype system built for Slack that allows discussion participants to collectively tag, group, link, and summarize chat messages in a variety of ways, such as by adding emoji reactions to messages or leaving written notes. Tilda then uses the markup left by participants to structure the chat stream into a skimmable summary view accessible within the chat interface. The summaries become live artifacts that can be edited, referenced, and posted to particular channels and individuals. Users can dive in to points of interest by following links in a summary to its place in the original chat stream.\nINTRODUCTION: We evaluated Tilda through three studies. First, we performed a within-subjects experiment to measure the effort required for groups to mark their chat while executing a shared task. We compared Tilda to using Slack alone and using Slack with a shared online document for notetaking. From 18 participants, we found evidence that Tilda was the better tool for taking notes while participating in the conversation. In a second experiment, we used the artifacts created in the first study to investigate the effort for a newcomer to comprehend the past conversations. From 82 participants, we found that users looking over summaries and chat logs enriched by Tilda felt less hurried when catching up compared to the other conditions. Additionally, those who utilized the links within Tilda summaries to dive into specific chat messages had a lower mental load and performed better at finding information from the chat log while still taking less time overall. Finally, we conducted a week-long field study of Tilda within 4 active Slack teams of 16 users total, and observed that teams actively used Tilda to mark up content and also found Tilda to be effective for catching up or looking back at old content.",
    "relatedWork": "RELATED WORK: 2.0.1 Group Chat and Instant Messaging. The first group chat was developed at University of Illinois to connect users of an instructional system [82] . Since then, group chat, and its close relative instant messaging, have amassed billions of users world-wide [16, 18, 47, 83] . Chat has been extensively studied in numerous application areas, including how it can foster intimacy among friends and family [37, 81] , how social norms form in online chat communities [65] , how firms collaborate with chat in the workplace [32, 35, 63] , how open source software developers coordinate in distributed teams [71] , and how chat can lead to unintended consequences, such as a reduction in face-to-face communication, and increased interruption and distraction [10, 21, 29, 39] . Echoing this work, we also find unintended consequences and side effects in today's group chat, in particular, that people like the simplicity of having a single tool for rich interactions with their entire team, but struggle to keep up with the demands of information management.\nRELATED WORK: 2.0.2 Sensemaking of Online Conversations. Due to issues of scale and the lack of structure in online discussion, researchers have developed tools for making sense of large conversations, including tools to produce more structured and rich representations of discussion as well as tools for higher level synthesis. Techniques such as highlighting [91] or tagging [89] can assist with \"foraging loops\" during the sensemaking process [62] , by providing more contextual signposts to readers while navigating through and diving in to the discussion space. Tools for higher level synthesis include visualization, clustering, and summarization techniques to more easily gain an overview. Some work has focused on the production side, including tools to turn groups or threads of discussion into short summaries [57, 90] , or organize comments into topics [36] . Others automatically present more visual representations of the discussion, such as displaying opinions in a two-dimensional space [24] or portraying relationships between discussants [79] , temporal activity [23] , or reply structure [43] . However, most of these tools have focused on threaded discussion forums, while few exist for discussions with no reply structure. When it comes to chat, some work focuses on new chat representations, such as allowing people to have threaded conversations in chat [73] or time-based views [28] . Researchers have also looked at high-volume chat feeds as backchannels to live events, exploring how restricting feed visibility to virtual neighborhoods can help people manage the volume, enabling more participation [54] . However, such chats are rarely maintained as archival knowledge for the group, which is the scenario we study.\nNotetaking and Live Meeting Notes.: A common technique for synthesis when it comes to synchronous conversations in particular is the practice of notetaking during meetings. Research has demonstrated that notetaking is beneficial both to individuals, in improving learning and comprehension [34, 44] , and to teams and organizations, in improving knowledge management practices and fostering collaboration [52] . During live meetings, it is common for teams and organizations to assign someone the role of designated notetaker [26] , who may find it difficult to participate in the conversation due to the cognitive effort and split attention required to take notes [60, 61, 85] . Due to the cognitive load of synthesizing conversation, we consider how more lightweight techniques such as tagging or inline notes in the chat could make notetaking easier. We also consider how the work could be broken down and distributed among participants, both to lower individual load and spread the benefits of participation.\nNotetaking and Live Meeting Notes.: Many tools have been developed to improve notetaking in live meetings and lectures, including tools that enable participants to collaborate with shared notes [22, 42, 48, 66] , tools for embedding notetaking within multimedia experiences [14, 15] , and tools for leveraging meeting recordings to bootstrap notetaking [30, 55] . However, little research has been done looking specifically at notetaking during group chat, where conversations can often occur spontaneously.\nNotetaking and Live Meeting Notes.: 2.0.4 Conversational User Experiences. In order to integrate seamlessly into chat conversations as they are ongoing, our Tilda prototype is developed as a Slack bot [51] , exposing its functionality to the participants within their conversation. Chatbots have a long history in research [69] , from initial explorations for fun and entertainment [80] , to modern assistants offering a conversational interface to complex tasks [8, 11, 20, 25, 78] . Our system differs from many of these bots, in that it does not rely on natural language understanding [70] , and is instead command driven, reacting only to specific user-input commands and actions. Several command-driven chatbots initially gained popularity in IRC communities [9] , including Debian MeetBot [4], which is still actively used by organizations such as Ubuntu and Wikimedia to take notes during IRC meetings, or Zakim [7] , which is in use at the W3C. MeetBot allows the leader of a chat meeting to designate the start and end of the meeting and enables participants to add different forms of notes to a running list of notes using hashtag commands. Similarly, Zakim is used during meetings for setting agenda items, reminders, speaking queues, and meeting scribes. While inspired by MeetBot, our prototype tool does not require scheduled meetings but can be used for more informal group chat conversations, with topics shifting continuously and people coming in and out throughout the day.\nNotetaking and Live Meeting Notes.: 2.0.5 Microtasks. Microtask workflows are an effective way to break down complex tasks into manageable, independently executable subtasks that can be distributed to others and executed over time [46, 74] . They have been successfully used for taxonomy-creation [13] , writing [76, 77] , holding a conversation [50] , transcription [49] , and scheduling meetings [20] . In examining sensemaking of chat conversations, we were inspired to embed the concept of a microtask as way to \"teamsource\" the synthesis of a long chat conversation, a difficult task that often takes a dedicated notetaker.\nNotetaking and Live Meeting Notes.: 2.0.6 Automatic Summarization. Finally, there is a long history of natural language processing research on automatic summarization [58] . While less work has focused on group chat, several projects look at summarization, including work on summarizing threads [64, 86] , extracting important information from email conversations [67, 84] , and analyzing audio conversations [56] . Building on this work, we provide insights into the level and type of structure people desire in synthesized representations of chat conversations. Our work points to the importance of discourse act tags for providing structure and context to chat messages. This finding has implications for prior work towards characterizing discussion using common sequences of major discourse acts [88] . Other work has looked at automatic extraction of major discourse acts such as questions and answer pairs [72] or tasks [19] from email, forums [17, 45] , and chat [27] . However, a great deal of prior work builds models from data labeled by dedicated paid annotators. In this work, we examine lightweight ways to collect discourse act and other signals while collectively chatting, which could be used as richer training data towards automatic summarization of chat.",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": " rq1: how do different designs of the agent's embodiment (voice vs. embodied) influence subjective social perceptions of the agent (rapport, trustworthiness, intelligence and power) in a group setting?\n rq2: how do different designs of the agent's embodiment (voice vs. embodied) impact: a) the group decision outcome, b) participants' interaction with other group members, and c) participants' interaction with the agent in a group setting?"
}
{
    "intro": "INTRODUCTION: As the verbal communication channel is fundamentally serial, people engaged in conversation need to coordinate with one another on turn taking. They do this with the presentation and recognition of non-verbal and verbal cues such as establishing or breaking eye contact, head and hand gestures, changes in speech prosody, and verbal affirmations [7, 8, 13, 14, 15, 19] . These cues are produced and attended to in an effortless manner, and are used, along with predictions about the natural or intended end of another's turn, to guide the dynamics of an evolving conversation.\nINTRODUCTION: The work described in this paper is part of a larger effort aimed at endowing situated spoken dialog systems with the core competencies required to engage in fluid, multiparty turn taking [4] . Key research challenges in this area include: tracking in realtime the conversational dynamics and the floor management actions that control these dynamics, making turn-taking decisions, and rendering these decisions into appropriate behaviors that accurately convey the system's turn-taking intentions and allow it to effectively shape the conversational dynamics.\nINTRODUCTION: We focus here on the implementation of a behavioral control model that leverages verbal and non-verbal cues to enable an embodied agent to influence the flow of conversation in multiparty settings. We conduct group interaction experiments in a shared task setting with a system that implements this model, and empirically investigate the degree to which an avatar's gaze, gesture and speech allow it to shape the conversation on a turnby-turn basis. We also discuss how a number of contextual factors, like dialog act type, previous speaker context, the presence of deictic markers, time elapsed, etc. relate to the system's ability to shape the flow of the interaction.",
    "relatedWork": "RELATED WORK: The process of turn taking in human-human interaction has attracted attention from researchers in the sociolinguistics and conversational analysis communities. Sacks, Schegloff and Jefferson [13] introduce a model for turn taking centered on a notion of turn-constructional-units, separated by transitionrelevant-places which provide opportunities for speaker changes. Other researchers have highlighted the importance of non-verbal signals such as gaze and gesture in turn taking. For instance, Duncan [7] proposes that turn taking is regulated via both verbal and non-verbal cues and highlights correlations between the direction of the participants' eye gaze and turn taking. Wiemann and Knapp [19] report results from a quantitative analysis of verbal and non-verbal cues across a variety of dyadic settings. Goodwin [8] also investigates various aspects of the relationship between turn taking and attention.\nRELATED WORK: One of the first implementations of a multimodal turn-taking model in a conversational agent was done by Thorisson [17] , using a layered architecture with several update loops operating at different speeds. More recently, Raux and Eskenazi [12] describe and perform experiments with a turn-taking model for dyadic interactions based on a six-state non-deterministic finite-statemachine. Moving beyond dyadic interactions, Traum and Rickel [18] describe a turn management system that supports dialogue with multiple virtual humans in immersive environments.\nRELATED WORK: Given the prominent and multiple roles of gaze, a variety of models for controlling gaze in physically or virtually embodied conversational agents have been previously proposed [1, 5, 9, 10, 11] . Gaze in these systems is frequently driven by a computed notion of -salience\u2016 in the scene and is based on the goal of maximizing realism or similarity to human behavior. For instance, Bennewitz et al. [1] propose a model which directs the attention of a robot in multiparty settings based on a measure of interest that takes into account the proximity of a detected face to the robot, and the probability that the face is correctly detected. Others, like Picot et al. [11] and Itti et al. [9] , have used image saliency and cognitive attention maps in an effort to produce a biologically motivated models that closely mimic the behavior as well as limitations of the human visual attention system. Cassell [5] has shown that beyond turn taking, other aspects of the interaction, such as information structure correlate with gaze direction. Mutlu [10] has also investigated with a wizard-of-Oz study the degree to which the gaze of a robot can be used to shape engagement and participant roles in multiparty interactions.\nRELATED WORK: Building on these earlier works, we have described and demonstrated in [4] a computational framework for managing multiparty turn taking in situated spoken dialog systems. The framework, which we shall briefly review in Section 3, subsumes models for tracking multiparty conversational dynamics, for making floor control decisions, and for rendering these decisions into appropriate behaviors. We shall focus on the behavioral subcomponent of this model, and empirically show how, by orchestrating gaze with other verbal and non-verbal turn-taking cues, we can enable a conversational agent to effectively participate in multiparty interactions.",
    "rq": ""
}
{
    "intro": "Introduction: We address the challenge of predicting the forthcoming engagement of people with open-world conversational systems (Bohus and Horvitz, 2009a) , i.e. systems that operate in relatively unconstrained environments, where multiple participants might come and go, establish, maintain and break the communication frame, and simultaneously interact with a system and with others. Examples of such systems include interactive billboards in a mall, robots in a home environment, intelligent home control systems, interactive systems that provide assistance and support during procedural tasks, etc.\nIntroduction: In traditional closed-world dialog systems the engagement problem is generally resolved via simple, unambiguous signals. For example, engagement is generally assumed once a phone call is answered by a telephony dialog system. Similarly, a push-to-talk button can provide a clear engagement signal for a speech enabled mobile application. These solutions are however inappropriate for systems that must operate continuously in open, dynamic environments, and engage with multiple people and groups over time. Such systems should ideally be ready to initiate dialog in a fluid, natural manner. They should manage engagement with participants who are close by, and with those who are at a distance, with participants who have a standing plan to interact with a system, and with those whom opportunistically decide to engage, in-stream with their other ongoing activities. In recognizing engagement intentions, such systems need to minimize false positives, while also minimizing the unnatural delays and discontinuities that come with false negatives about engagement intentions.\nIntroduction: The work described in this paper is set in the larger context of a computational model for supporting fluid engagement in open-world dialog systems that we have previously described in (Bohus and Horvitz, 2009b) . The above mentioned model harnesses components for sensing the engagement state, actions, and intentions of multiple participants in the scene, for making engagement control decisions, and for rendering these decisions into coordinated low-level behaviors, such as the changing pose and expressions of the face of an embodied agent. In this paper, we focus on the sensing subcomponent of this larger model and describe an approach for automatically learning to detect engagement intentions from interaction.",
    "relatedWork": "Related Work: The challenges of engagement between people, and between people and computational systems, have already received some attention in the conversational analysis, sociolinguistics, and human-computer interaction communities. For instance, in an early treatise Goffman (1963) discusses how people use cues to detect engagement in an effort to avoid the social costs of engaging in interaction with an unwilling participant. In later work, Kendon (1990a) presents a detailed investigation of video sequences of greetings in human-human interaction, and identifies several stages of complex coordinated action (pre-sighting, sighting, distance salutation, approach, close salutation), together with the head and body gestures that they typically involve. In (1990b), Kendon also introduces the notion of an F-formation, a pattern said to arise when \"two or more people sustain a spatial and orientational relationship in which they have equal, direct, and exclusive access,\" and discusses the role of F-formations in establishing and maintaining social interactions. Argyle and Cook (1976) as well as others (e.g., Duncan, 1972; Vertegaal et al., 2001) have identified and discussed the various functions of eye gaze in maintaining social and communicative engagement. Overall, this body of work suggests that engagement is a rich, mixed-initiative, and well-coordinated process that involves non-verbal cues and signals, such as spatial trajectory and proximity, gaze and mutual attention, head and hand gestures, and verbal greetings.\nRelated Work: More recently, several researchers have investigated issues of engagement in human-computer and humanrobot interaction contexts. Sidner et al. (2004; 2005) define engagement as \"the process by which two (or more) participants establish, maintain and end their perceived connection during interactions they jointly undertake,\" and conduct a user study that explores the process of maintaining engagement. They show that people direct their attention to a robot more often when the robot makes engagement gestures throughout an interaction, i.e. tracks the user's face, and points to relevant objects at appropriate times in the conversation. Peters et al (2005a; 2005b) use an alternative definition of engagement as \"the value that a participant in an interaction attributes to the goal of being together with the other participant(s) and of continuing the interaction,\" and present the high-level schematics for an algorithm for establishing and maintaining engagement. The proposed algorithm highlights the importance of eye gaze and mutual attention in this process and relies on a heuristically computed interest level to decide when to begin a conversation. Michalowski et al (2006) propose and conduct experiments with a spatial model of engagement, grounded in proxemics (Hall, 1966) . Their model classifies relevant agents in the scene in four different categories based on their distance to the robot: present (standing far), attending (standing closer), engaged (next to the robot), and interacting (standing right in front of the robot). The robot's behaviors are in turn conditioned on these categories: the robot turns towards attending people, greets engaged people and verbally prompts interacting people for input. The authors discuss several lessons learned from an observational study conducted with this robot in a building lobby. They find that the fast-paced movements of people in the environment pose a number of challenges: often the robot greeted people too late (earlier anticipation was needed), or greeted people that did not intend to engage (more accu-rate anticipation was needed). The authors recognize that these limitations stem partly from their reliance on static models, and hypothesize that temporal information such as speed and trajectory may provide additional cues regarding a person's engagement with the robot.\nRelated Work: In this paper, we expand on our previous work on a situated multiparty engagement model (Bohus and Horvitz, 2009b) . Specifically, we focus on a key subcomponent in this model: detecting whether or not a user intends to engage in an interaction with a system. We introduce an approach that improves upon the existing work (Peters 2005a (Peters , 2005b Michalowski et. al, 2006) in several significant ways. First, the approach is datadriven: the use of machine learning techniques allows the system to adapt to the specific characteristics of its physical location and to the behaviors of the surrounding population of potential participants. Second, we leverage a wide array of observations, including temporal features. Finally, no developer supervision is required for training the model: the supervision signal is extracted automatically, in-stream with the interactions, allowing for online learning and adaptation.",
    "rq": " we plan to focus on the more general problem of controlling the engagement process: how should the system time its actions (i.e. gaze and sustained attention, smiles, greeting, etc.) to create natural, fluid engagements in the open world. introducing mobility to dialog systems brings yet another interesting dimension to this problem: how can a mobile system, such as a robot, detect engagement intentions and respond to support a natural engagement process?"
}
{
    "intro": "INTRODUCTION: The Internet promotes a public sphere where people gather to exchange ideas, form opinions, and mobilize social movements [62] . Discussions in online chat spaces like Messenger, Telegram, and WhatsApp allow people to share different perspectives and opinions, free from time and place constraints. In certain online chat spaces, the guarantee of anonymity can facilitate greater openness about opinions and experiences [77] . Because of these advantages, online chats have emerged as a channel for discussing diverse social issues and driving social change [28] .\nINTRODUCTION: However, the fact that these spaces can host discussions does not guarantee that they will properly function as a segment of the public sphere [56] . A long history of empirical work has shown that rational debate and deliberation do not always occur in online discussions. Many people do not actively participate in discussions [29, 65] . People join groups and seek information consistent with their own perspectives, which can make it difficult for them to understand or respect others' contrasting viewpoints [53] . Due to these problems, consensus-reaching can be difficult in online discussion [30] . Despite the above, consensus reaching is highly important for situations in which a society is required to make a decision regarding an issue with major social consequences (e.g., How should self-driving cars make decisions in complicated situations?, Who owns the copyrights for AI created art?, What kind of harmful online content should be moderated?) [54] . It benefits both community members who have a stake in the outcomes of these decisions and society as a whole if a consensus is reached through iterative and deliberative discussions that are perceived as legitimate and fair [24] , and attempts at such a discussion are referred to as 'society in the loop' [60] .\nINTRODUCTION: Existing studies tend to pay attention primarily to discussion results, which are measured on the basis of whether or not a consensus has been reached. This perspective leads discussions toward being regarded as a means of obtaining a majority consent [63] . However, rather than the mere results of a given consensus, there are significant elements which constitute deliberative discussion including authenticity, substantive balance, diversity, and reasoning processes [21, 25, 72] . Thus, in this work we do not assess the success or failure of a discussion based on whether an agreement has been arrived at, instead distinguishing between deliberative consensus and mere agreement. We investigate whether the discussion includes both a deliberation process that matches the above criteria and an outcome where discussants actually agree with or concede to a consensus (authenticity) [25] .\nINTRODUCTION: The HCI and CSCW community has explored methods for prompting constructive and balanced discussion. Previous studies have developed systems to enable reasoned argumentation [18, 52, 64] and a balanced and valid perspective [40, 45] and to help human moderation [47] . Furthermore, a multi-turn argumentation system for crowd workers has been shown to improve data accuracy [11] along with worker engagement [59] . Our work draws inspiration from this prior work, building on findings related to effective discussion facilitation, but translates these findings into the integration of a computerized \"facilitator\"-a conversational agent-into a discussion platform rather than transforming or adding elements of the platform's front-end interface. We treat this chatbot as a member of the host community [68] . In line with recent work [69] , we argue that chatbot agents can foster positive group dynamics by playing specific social roles that human agents may not want to perform or may be naturally disadvantaged in performing relative to a chatbot.\nINTRODUCTION: What role can chatbot agents play to promote deliberative discussion? Unlike official discussions managed by professional moderators [75] , many informal discussions between people with common interests in online spaces take place without moderators, i.e., group chats or chatrooms. In situations where a moderator might have been able to manage a heated conversation, the absence of such a moderator can intensify the natural drawbacks of unstructured, unthreaded discussions [23, 41] . Moreover, absent a moderator monitoring a discussion, the right or power to speak may not be evenly distributed among the participants [41] , potentially leading to a \"spiral of silence\" [46] . Moderators distributing the right to speak and structuring discussion may induce more even and active participation [14] , given a shared group goal of achieving consensus, enabling more effective deliberative discussion and allowing groups to reach a more authentic consensus.\nINTRODUCTION: In this paper we present findings from the process of designing and testing a chatbot to facilitate deliberative discussion. We propose \"DebateBot\", which is designed to (1) structure discussion and (2) request opinions from reticent discussants. DebateBot structures discussion based on the thinkpair-share framework, which helps to maintain opinion independence and strengthen reasoned arguments (Figure 1 : S1-5) [4, 54] . It also encourages participation from lurkers and thus can solicit a broader variety of opinions (Figure 1 : F1-3).\nINTRODUCTION: In our tests we focused on discussion topics related to ethical dilemmas (i.e., the trolley problem of self-driving cars and the rights of AI), in which consensus-reaching and deliberative discussion are requisite. We predicted that the chatbot agent could facilitate deliberative discussion by encouraging more active and more balanced participation, greater opinion diversity, and clearer arrival at a mutually agreed-upon consensus. To evaluate the feasibility of the chatbot agent, we conducted a 2 (discussion structure: unstructured vs. structured) \u00d7 2 (discussant facilitation: unfacilitated vs. facilitated) experiment. In the structured condition, the chatbot agent structured discussion to encourage independent thinking and facilitate members' understanding of different perspectives using methods based on prior research [54, 64] including a think-pair-share strategy [4] . Participants in the unstructured condition engaged in free discussion without a predefined format. In the facilitated condition, DebateBot encouraged participants who had been less involved in the discussion to express their opinions; this intervention did not occur in the unfacilitated condition. We ran experiments with 12 groups of five or six members each (N = 64). We measured deliberative discussion based on authentic consensus reaching (discrepancy between group's and individual's opinions), group behavior (active participation, even participation, lexicon diversity), and discussants' attitudes (opinion alignment, opinion authenticity, communication quality, and usefulness). We also collected and analyzed users' qualitative feedback.\nINTRODUCTION: We found the following:\nINTRODUCTION: \u2022 In general, a chatbot-moderated discussion structure positively affects the quality of the discussion. Facilitating lurkers to speak drives increased opinion alignment, equality of contribution, and group members' perceived satisfaction. \u2022 There was no difference in the overall magnitude of participation across the four conditions, but the distribution pattern of participation was different. Participants in the facilitated group participated more equally in the discussion.\nINTRODUCTION: \u2022 Participants in structured discussions produced more diverse opinions (i.e., lexicons), generating a breadth of opinions. However, discussant facilitation did not accelerate this effect. This might be because one group, under the facilitated and structured condition, exhibited a unanimous prior opinion; this may have prevented the emergence of diversity. \u2022 In the facilitated and structured discussion condition, the highest proportion of participants reported that the group's consensus matched their personal opinions, resulting in authentic consensus reaching.\nINTRODUCTION: Based on these findings, we discuss the design implications of the online chat system for deliberative discussion. The main contributions of this work are as follows:\nINTRODUCTION: (1) We present a chatbot that we designed and built to enable deliberative discussion by structuring discussion and facilitating even participation. We demonstrate that the agent can perform the role of moderator in the group discussion process. (2) We present findings from an evaluation of deliberative discussion in terms of active and even participation, opinion diversity, and authentic consensus reaching based on behavioral log data, finding significant impact from the use of the chatbot agent. (3) We discuss the implications of a chatbot agent that can facilitate online discussion and present considerations for future work.\nINTRODUCTION: It should be noted that the work we present here may not be appropriate for certain sensitive and divisive issues such as racial, sexual, religious, or political topics, as the power dynamics and emotional intensity of these topics could be beyond the facilitation capabilities of the system we present here [10] ; for some topics within these categories, it is unclear whether a negotiated consensus is even the desired outcome [7] . For these topics, a more specialized intervention may be required.",
    "relatedWork": "RELATED WORK: This study aims to explore the feasibility of a text-based chatbot agent as a moderator in online discussions. We first look at how and where chatbots have been applied, then identify their advantages over other systems. Next, we explore the factors that enable deliberative discussion and their effects in face-to-face and computer-mediated contexts, and discuss how these may be integrated into the design of the chatbot agent.\nChatbots in Group and Community: Research related to chatbots has mainly focused on dyadic chatbots, where users and chatbots have one-on-one conversations. Studies related to dyadic chatbots have focused on their applicability to various domains such as health care [43] , customer support [33] , news consumption [36] and user research [42] . The effectiveness of dyadic chatbots has mainly been assessed by manipulating message-level variables such as conversational style [42] , empathic responses [33] , typeface [8] , and self-disclosure [48] . Recent research has explored potential roles for chatbots in multiparty interactions involving groups and community interactions [68] . Multiparty chatbots can play a role in a group by performing specific functions. For example, a task assistance chatbot can automate routine tasks. They can arrange group schedules [15] , manage tasks [76] , and help collaborative information-seeking [2] .\nChatbots in Group and Community: On the other hand, in addition to these task-based agents, chatbots also perform social roles by engaging in group dynamics and interacting with group members. In empirical research, researchers identified the social role of bots on the Twitch community, such as engaging users and running mini-games [66] . An analysis of14,822 comments on Reddit community revealed that bots are seen to perform functions including administration of content (e.g., scheduling and automization of postings), provision of fun (e.g., playing of games), ensuring functionality and quality (e.g., translating language), supporting community (e.g., pre-banning black-listed users, welcoming new comers), and archiving [50] . Experimental work has shown that chatbots that promote discussions in social chat groups by encouraging reticent members to speak and organizing opinions have helped members contribute more evenly to the discussion, leading to improved satisfaction [41] . In another study, compared to a voice-only agent, an embodied agent had a positive effect on the interaction between group members by conveying a sense of presence [70] . Finally, in another study that used research-through-design methods, a chatbot raised and grown by a community changed the way members interacted, and eventually the chatbot became accepted as a community member [69] .\nChatbots in Group and Community: These studies provide solid evidence that a chatbot can shape a group or community by playing a particular social role. So far, however, too little attention has been paid to how to apply these types of chatbots for deliberative discussions. If the lack of a moderator hinders deliberative and productive discussion [23, 41] , we might ask whether a conversational agent can partially perform the role of a human moderator, leading to a more deliberative form of discussion. Moreover, conversational agents can more deeply permeate group dynamics than many other interfaces due to their interactive and integrated nature. This integration has driven our decision to choose a chatbot as the format for an intervention into deliberative discussion, as we believe that the effects of a given approach may be greater when presented through a virtual agent than in a more socially-distant front-end interface. In particular, adding a single agent in a situation where multiple parties interact (as in a discussion) can be more intuitive and comfortable than adapting to a new interface. Based on the applicability and advantages of chatbots in the group interaction context, this study explores whether they can promote deliberative discussion.\nStructured Discussion: Structured discussion enables deliberation by promoting reasoned arguments [64] , reducing deviation from the topic [19] , and enabling independent thinking [4, 54] . In deliberative discussion, it is crucial to support claims with both evidence and reasoning [40, 44] and to understand other participants' opinions before engaging in full-scale debate [4, 54] . While it is easy to express opinions spontaneously without elaboration in many online contexts, constructive discussion is only possible if arguments are based on solid rationales established prior to the discussion [18, 64] . This is consistent with Cohen's concept of reasoning, an important component of deliberative democracy. Cohen [13] stated that in deliberative discussion, arguments must be based on reasonable and logically sound evidence. High-quality discourse can be achieved and rational decisions can be made when debaters conduct discussions based on reason and proceed with debate in a structured manner [26] , particularly when independent judgments are encouraged rather than overshadowed by majority opinion (groupthink) [54] .\nStructured Discussion: A number of studies applied structured discussion to facilitate online group communication by introducing multiple stages by, for example, allowing users to exchange opinions and achieve goals productively by conducting discussions in an order provided by the system. LeadLine enables structured discussion by allowing people to create predefined scripts [19] . LiquidFeedback introduces four stages--admission, discussion, verification, and voting--to support online deliberative processes for policy-making [16] . SolutionChat provides a flexible structure that allows moderators to use a personalized structure and control step transitions [47] . These studies provide supporting evidence for the effects of structured discussion. However, these studies have structured the discussion at the level of the graphical user interface, and none has verified the feasibility of a conversational agent that structures a multi-stage discussion like a human moderator. Designing the stages of deliberative discussion that enable reasoning into the protocol of a chatbot can facilitate deliberative discussion. In synchronous discussion, chatbots can structure discussions by guiding discussants to the discussion stages considering a predetermined time.\nEquality and Diversity: One of the basic elements of deliberative discussion is that every participant has equal standing [13] . Deliberative discussion requires an equilibrium of substantially equal opportunities for people with different perspectives to present their opinions [21] . However, it is often observed that equal participation does not often occur in online spaces. The influence of minority opinions can be repressed, and decision-making can be dominated by influential users; online discussions with a more democratic power balance can be difficult to hold [74] .\nEquality and Diversity: Unequal participation in online discussions can have two interrelated consequences: a \"spiral of silence\" and social loafing. When a person is on the side of a minority opinion, a spiral of silence can arise because of fear of receiving bad evaluations or being isolated from others [55] . Since expressing opinions is a social act that reflects a social climate and not simply an independent action, it is possible to express agreement with dominant opinions even when they are not in accord with individual opinions. Social loafing, or a reduction of individual input, can occur when users are collaborating in a group, particularly when incentives to contribute are low [37] . In this case, a form of social loafing may occur when a user believes that there is little reason for them to contribute to a deliberative discussion. Reducing individual input within a group lowers the motivation of other members and has a long-term negative effect on the group and organizational level [37] .\nEquality and Diversity: Although uneven participation among the users has been criticized as an obstacle to positive group dynamics, far too little attention has been paid to solving this problem using technology. Our design aims to overcome these challenges by encouraging members who are less involved in discussion to express their opinions. We incorporate this principle into the design of the chatbot, allowing it to identify members in real time who are passive in expressing their opinions and encourage them to participate, potentially leading to a greater diversity of opinions and making arrival at a representative understanding more likely. Thus, we focus on the following research questions:\nEquality and Diversity: \u2022 RQ1. How can a chatbot be designed to facilitate deliberative discussions? \u2022 RQ2. Can a chatbot designed to structure discussion and facilitate discussants have a positive effect on the deliberative discussion in terms of consensus reaching (behavioral and perceived opinion alignment), opinion expression (active participation, even contribution, outspokenness), discussion quality (lexicon diversity, deliberative quality), and discussant satisfaction (task cohesion, communication efficiency/fairness/effectiveness)?",
    "rq": " rq1. how can a chatbot be designed to facilitate deliberative discussions?\n rq2. can a chatbot designed to structure discussion and facilitate discussants have a positive effect on the deliberative discussion in terms of consensus reaching (behavioral and perceived opinion alignment), opinion expression (active participation, even contribution, outspokenness), discussion quality (lexicon diversity, deliberative quality), and discussant satisfaction (task cohesion, communication efficiency/fairness/effectiveness)?"
}
{
    "intro": "Introduction: To date, nearly all spoken dialog systems research has focused on the challenge of engaging single users on tasks defined within a relatively narrow context. Efforts in this realm have led to significant progress including large-scale deployments that now make spoken dialog systems common features in the daily lives of millions of people. However, research on dialog systems has largely overlooked important challenges with the initiation, maintenance, and suspension of conversations that are common in the course of natural communication and collaborations among people. In (Bohus and Horvitz, 2009) we outlined a set of core challenges for extending traditional closed-world dialog systems to systems that have competency in open-world dialog. The work described here is part of a larger research effort aimed at addressing these challenges, and constructing computational models to support the core interaction skills required for open-world dialog. In particular, we focus our attention in this paper on the challenges of managing engagement -\"the process by which two (or more) participants establish, maintain and end their perceived connection during interactions they jointly undertake\", cf. Sidner et al. (2004) in open-world settings.\nIntroduction: We begin by reviewing the challenges of managing engagement in the open-world in the next section. In Section 3, we survey the terrain of related efforts that provides valuable context for the new work described in this paper. In Section 4, we introduce a computational model for multiparty situated engagement. The model harnesses components for sensing the engagement state, actions, and intentions of people in the scene for making high-level engagement decisions (whom to engage with, and when), and for rendering these decisions into a set of low-level coordinated behaviors (e.g., gestures, eye gaze, greetings, etc.). Then, we describe an initial observational study with the proposed model, and discuss some of the lessons learned through this experiment. Finally, in Section 6, we summarize this work and outline several directions for future research.",
    "relatedWork": "Related Work: The process of engagement between people, and between people and computational systems has received a fair amount of attention. Observational studies in the sociolinguistics and conversational analysis communities have revealed that engagement is a complex, mixedinitiative, highly-coordinated process that often involves a variety of non-verbal cues and signals, (Goffman, 1963; Kendon, 1990) , spatial trajectory and proximity (Hall, 1966; Kendon, 1990b) , gaze and mutual attention (Argyle and Cook, 1976) , head and hand gestures (Kendon, 1990) , as well as verbal greetings.\nRelated Work: A number of researchers have also investigated issues of engagement in human-computer and humanrobot interaction contexts. Sidner and colleagues (2004) define engagement as \"the process by which two (or more) participants establish, maintain and end their perceived connection during interactions they jointly undertake\", and focus on the process of maintaining engagement. They show in a user study (Sidner et al., 2004; 2005) that people directed their attention to a robot more often when the robot made engagement gestures throughout the interaction (i.e. tracked the user's face, and pointed to relevant objects at appropriate times in the conversation.) Peters (2005; 2005b) uses an alternative definition of engagement as \"the value that a participant in an interaction attributes to the goal of being together with the other participant(s) and of continuing the interaction,\" and present the high-level schematics for an algorithm for establishing and maintaining engagement. The algorithm highlights the importance of mutual attention and eye gaze and relies on a heuristically computed \"interest level\" to decide when to start a conversation. Michalowski and colleagues (2006) propose and conduct experiments with a model of engagement grounded in proxemics (Hall, 1966) which classifies relevant agents in the scene in four different categories (present, attending, engaged and interacting) based on their distance to the robot. The robot's behaviors are in turn conditioned on the four categories above.\nRelated Work: In our work, we follow Sidner's definition of engagement as a process (Sidner et al., 2004) and describe a computational model for situated multiparty engagement. The proposed model draws on several ideas from the existing body of work, but moves beyond it and provides a more comprehensive framework for managing the engagement process in a dynamic, open-world context, where multiple people with different and changing goals may enter and leave, and communicate and coordinate with each other and with the system.",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: The field of human-computer interaction has made the Internet a viable environment for collaborative learning and large-scale collaborative work. Increasingly, it is becoming common place for groups of various sizes and compositions, whether co-located within close proximity to one another, or in distributed, multinational teams in professional or instructional settings to employ Internet-based collaboration environments to house their endeavors. Although great strides in design of online collaboration environments have been made, issues with communication and coordination continue to pose challenges for these efforts. Interactive, supportive technologies have the potential to contribute toward improving distributed collaborative learning and work.\nINTRODUCTION: In particular, in this article we explore the use of intelligent Conversational Agents (CAs) as a form of interactive, context-sensitive, task-oriented support for small groups communicating using common Internet-based technologies such as instant messengers and chatrooms. CAs are interface agents that offer natural language as a medium of interaction with machines. The most popular current application of such agents is information access typically using voice-based IO over telephones and mobile devices. At the core of these agents are models that represent the structure of the interaction [Rudnicky and Xu 1999; Freedman 2000; Bangalore et al. 2006 ] and perform functions such as belief update [Williams 2007 ], confidence annotation [Bohus and Rudnicky 2007] , timing [Raux and Eskenazi 2009] , and action selection [Paek and Horvitz 2000; Bohus et al. 2006; Williams and Young 2007] .\nINTRODUCTION: There has been much prior work investigating the use of these agents for supporting individuals in a variety of tasks, including information access [Apple 2013 ], learning [Graesser et al. 2003] , therapy [Bickmore and Cassell 2001] , marketing [Cassell et al. 1999] , and entertainment [Foner 1997 ]. Some studies [Wang and Johnson 2008; Ros\u00e9 et al. 2001 ] have evaluated the effect of these task-related conversational behaviors in tutorial dialog scenarios. Work in the area of affective computing and its application to tutorial dialog has focused on identification of student's emotional states [D' Mello et al. 2008 ] and using those to improve choice of task-related behavior by tutors. However, recently CAs have been used to support multiparty activities such as collaborative learning [Kumar et al. 2007b] , communication [Isbell et al. 2001 ], information assistance [Zheng et al. 2005; Bohus and Horvitz 2009] , and games [Dohsaka et al. 2009 ]. Specifically in the application area of collaborative learning, systematic evaluations of the use of CAs to support small groups of students have shown improvements of more than a letter grade in learning attributed to the support offered by these automated agents [Kumar et al. 2007b ]. In the case of these conversational tutors, the task (or work) related interaction includes aspects such as instructing students about the task, delivering appropriate interventions in suitable form (e.g., Socratic dialog, hints), providing feedback, and other such tactics [Graesser et al. 2001 ]. There has been only limited study of expressive (social-emotional) aspects of the tutor's behavior on group interactions.\nINTRODUCTION: The specific form of support that we investigate in our work is motivated by the literature on group interaction processes. McGrath [1984] reviews various theories that address the functions of group interaction processes. Of particular interest to our work among these are the theories proposed by Robert F. Bales [1950] and Wilfred R. Bion [1961] . Their theories propose that two fundamental processes operate within groups-that is, instrumental (task-related) versus expressive (social-emotional) in the case of Bales and work versus emotion in the case of Bion. Overattention on one of these processes causes lapses on the other. Hence, interaction shifts between these two in order to keep the group functional.\nINTRODUCTION: Most current state-of-the-art conversational tutors lack the ability to effectively perform the social-emotional function of interaction that is known to be a fundamental Triggering Effective Social Support for Online Groups 24:3 aspect of group interaction. Hence, we hypothesize that socially capable tutors will be able to offer more effective support both in collaborative learning and collaborative work scenarios. In order to further specify social capability, we use the Interaction Process Analysis (IPA) schema developed by Bales [1950] . Beside the influence and popularity of IPA over the past five decades, our choice is based on the unit of analysis at which IPA is applied, which is individual utterances, compared to Bion's units of analysis (sessions), which are typically much larger (10 to 50 utterances). The objective of the interactive support approach that we present is to achieve better group outcomes through keeping these processes in balance. We do this by employing CAs designed to employ social behaviors characterized by the IPA of Bales. The challenge is to do so in a way that is tuned properly for each group. In an earlier study [Kumar et al. 2010a] , two varieties of support for collaborative learning, both implemented by CAs, were contrasted in terms of effectiveness at increasing learning gains. In both conditions, the CAs supported task-related behaviors. However, in the experimental condition, the CAs also employed social strategies motivated by Bales's IPA in order to maintain balance between the task and social functions of the groups. Students in the experimental condition learned significantly more than students in the control condition. Although this foundational study is suggestive, it leaves two important questions open that are addressed in this article. The first question is one of generality. Specifically, are the positive effects of the designed support specific to learning contexts? Or are they in evidence in other collaborative task domains as well? And, second, is it possible to achieve additional benefits by increasing the context sensitivity with which the social strategies are triggered?\nINTRODUCTION: In the remainder of the article, we first describe the CA-based support for group interactions that we evaluate in this article. We then present Study 1 in which we test the generality of the Balesian social support in a collaborative work context, and thus demonstrate the generality of the positive impact of the social strategies across contexts. We then address the context sensitivity issue by first presenting a machine learning approach to learning greater context sensitivity in triggering support from human-administered support behavior, which we evaluate in Study 2. Next, we evaluate the learned triggering policy in Study 3, demonstrating advantages of the learned policy for context-sensitive triggering. We conclude with a discussion of the limitations of this research and directions for our continued research.",
    "relatedWork": "",
    "rq": " strategy 1e is triggered when one or more students in the group are found to be inactive for more than 5 minutes. in this event, the tutor chooses to raise the status of the inactive students by eliciting contributions from them through a prompt such as do you have any suggestions mike?"
}
{
    "intro": "INTRODUCTION: Most spoken dialog systems research to date can be characterized as the study and support of interactions between a single human and a computing system within a constrained, predefined communication context. Efforts in this realm have led to significant progress culminating in wide-scale deployments that now make telephony-based spoken dialog systems commonplace in the lives of millions of people. Nevertheless, numerous and important challenges remain with enabling computational systems to engage in fluid conversations in open, unconstrained environments, where multiple people with different and varying intentions enter and leave, and communicate and coordinate with each other and with interactive systems. We focus in this paper on these challenges.\nINTRODUCTION: We begin by reviewing several aspects of open-world interaction that represent key departures from assumptions typically made in traditional spoken dialog systems and we highlight a set of related research challenges and opportunities in Section 2. Then, in Sections 3 and 4, we present details of a framework for dialog systems that addresses several of these challenges. The approach integrates several core technologies, including speech recognition, machine vision, probabilistic models for scene analysis, multiparty engagement, turn-taking, and behavioral models for controlling an avatar, to support fluid dialog in open, dynamic environments.\nINTRODUCTION: We have explored three different applications on this platform, allowing us to investigate differences and similarities in openworld dialog across different domains. We discuss these different conversational agents in Section 5. We showcase by means of a recorded interaction how the different models work together to support mixed-initiative engagement and dialog with multiple parties. We also review results from an initial in situ observational study of multiparty interaction performed with one of these systems. Finally, in Section 6 we conclude and outline current and future planned research in this realm.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: As the advance of embodied conversational agent (ECA) technology, there are more and more real-world deployed applications of ECA's [9, 17] . Various kinds of kiosk information systems are used in public places, such as shopping malls, museums, and visitor centers. The typical situation in which such systems are used is that a group of people stand in front of the kiosk and operate it in order to retrieve the information while talking with one another. Therefore, in order to implement an ECA that can serve as an information kiosk in public places, multi-party conversation functionality for simultaneous interaction with multiple users is indispensable.\nINTRODUCTION: In dyadic dialogs where only two participants are involved, it can be assumed that when one participant is speaking then the other one is the addressee in most cases. In multi-party dialogs, however, the distinction among conversation participants' roles, addressee, overhearer, and speaker is necessary. Who is talking to whom about what topic should be identified and traced, the obligation to answer a question when being asked needs to be fulfilled, and whether an utterance has been grounded [3] with the addressee(s), i.e. became a shared belief between the speaker and the addressee(s), needs to be identified. Since there are potentially more interlocutors to acquire dialog turns from and transfer dialog turns to, managing the communication flow in a multi-party conversation is much more sophisticated than a dyadic one. Traum [16] provided the principal literature identifying some of the general issues in realizing multi-party human-agent interactions.\nINTRODUCTION: The conversation style of most contemporary ECA systems are either agent-initiative (e.g. the agent always asks questions and the user answers) or user-initiative (e.g. the user always ask questions and the agent answers), because these interaction designs are simpler and are easier to be implemented. However, the natural conversation occurred between human and human is mixed-initiative, i.e. both of the agent and the user may take initiative during the conversation. In this study, we are proceeding with a project that aims to build an ECA capable of mixed-initiative conversation with multiple users in a typical application, information providing for users' collaborative decision making.\nINTRODUCTION: Considering the case where a real estate agent is serving a recently married couple who are planning to buy a new house, the married couples consider the location, layout, or the price of the houses suggested by the agent, discuss with each other and collaboratively make the final decision. In this task, the decision can be made efficiently if the agent can actively intervene the discussion between the users to provide timely information while the agent is listening to the users' conversation as an overhearer. For example, the agent may suggest new candidates as \"how about this apartment? It is only a five-minute walk to the train station.\" if it hears that the users are discussing about the transportation issue, or \"how about this apartment? There is a super market which is just five-minute away from it.\" if it hears that the users are discussing about meal issue. This kind kind of interaction is nature and frequently occurs in human-human conversation. However, for a virtual agent to deal with this, several functions have to been incorporated to the agent. The agent needs to reason what information to provide from the users' conversation even their demands are not clearly described. The agent also needs to identify the timings when the user may be interested in the information being provided without making them feel disturbed. In order to realize this, we focus on nonverbal information. It has been known that nonverbal information is an important element in a conversation scene. In particular, gaze has been found to have a major impact on multi-party conversation [14] .\nINTRODUCTION: This paper describes the development of the method dealing with the timing finding issue. In order to realize the feature to find the appropriate timings for the agent, at first, a Wizard-of-Oz (WOZ) experiment was conducted for collecting human interaction data. The experiment participants were instructed to collaboratively make decisions in three tasks: travel planning, lecture registration, and part-time job hunting. They did the tasks with the information provided by a virtual agent who is controlled by one of the experimenters from remote (Section 3). Then an analysis was conducted on the collected data corpus to find the eight possible timings and the corresponding user behaviors (Section 4). A method was developed to automatically identify four of the eight kinds of timings only by using nonverbal cues, gaze, body posture, and acoustic information. Finally, the paper is concluded with the evaluation of the performance of the proposed method and the discussion on the results (Section 5).\nIdentification of individual utterances from raw audio: track. First, the speech audio was divided into utterances as the unit for further analysis. The speech was automatically separated using the voice source information from one of the two Kinect sensors. The speaker of each utterance is identified at the same time.\nIdentification of individual utterances from raw audio: As the results, there are averagely 108 user utterances and 49 agent utterances in one session.\nIdentification of individual utterances from raw audio: 2. Manual labeling on the addressee of each utterance.\nIdentification of individual utterances from raw audio: Using the video annotation tool Anvil 5.1 [8] , one of the experimenter annotated the addressee for each utterance identified by the process in Step 1, and used these labels as the ground truth.\nIdentification of individual utterances from raw audio: 3. Manual labeling on body posture. The body posture was labeled to stand in five directions (front, left, right, slightly left, slightly right). Averagely there were 89 posture shifts in each session as the results.\nIdentification of individual utterances from raw audio: 4. Gaze direction. In this study, we used head direction as the approximation of gaze direction.\nIdentification of individual utterances from raw audio: In order to eliminate the difference among individual users (their distance to the screen, personal habits when speaking, etc.) and to prevent overfitting to slight head movements, only three conditions of the participants' gaze targets: gazing at the agent, gazing at the partner (the other participant), or gazing elsewhere were considered relevant to the final results. Precise eye gaze detection usually requires a dedicated hardware which can be very expensive (several dozens of thousand dollars), and is not practical to the systems which are meant for public use. Approximating gaze direction with head direction cannot be very precise. However, regarding to the standing positions of the experiment setting where the view angle between the agent and the other participant is 90 degrees. It should be large enough that the participants have to turn their head to switch their gaze targets between the agent and the partner. Head directions were acquired by using the face tracking software, FaceAPI 1 , which can measure the head position and rotation angles in terms of x, y, and z axes.\nIdentification of individual utterances from raw audio: The head directions of four randomly chosen pairs (or eight subjects) were manually labeled as target values (forward, left, and right for each subject). For example, in the case of the subject who is standing on the left side, he/she is evaluated as looking at the agent if he/she gazes forward, as looking at his/her partner if he/she gazes to the right, or as facing elsewhere if he/she gazes to left. All of the six head pose data and their measurement confidence scores were used as input teaching data. A C4.5 [11] decision tree was then trained with the Weka [5] data mining tool. As a result of 10-fold cross-validation, the classification accuracy of the learned model was 97.2%. Since this is a good enough degree of accuracy, this model was used to automatically label the head direction of the entire corpus (the four manually labeled groups were relabeled with the decision tree). The result was: averagely there were 489 Gaze direction shifts in each session.",
    "relatedWork": "RELATED WORKS: Many studies on the processes of recognition and interpretation in multi-party meetings have been done in the AMI and AMIDA projects [12] and in the projects of DARPA [18, 19] . By applying speech and natural language processing technologies, a number of useful components were developed in these projects, such as speech transcription, speaker diarization, and meeting summarization.\nRELATED WORKS: The management of speaking turns in multi-party dialogs is very related to the goal of this work. The intervention for active information providing is a higher level task that needs to consider the situation and the context of conversation in addition. In the literature of human communication, [7, 4, 13] , it has been reported that in addition to explicit verbal utterances, people also use nonverbal signals such as their gaze, nods, and postures to regulate their conversation, e.g., to show their intention to yield or the willingness to take next speech turn. The speaker looks at the addressee to monitor her/his understanding or attitude, and, the addressee looks at the speaker in order to be able to offer positive feedbacks in return [7, 1] . When yielding his/her turns to speak, the speaker looks at the next speaker at the end of his/her utterances [4] . Takemae [15] provided evidence that the speaker's gaze indicates addressee-hood and plays a regulatory role in turn management. The Japanese spoken dialog system, DUG-1 [10] realized rich turn-taking with rules based on linguistics features. Jonsdottir and Thorisson [6] proposed a system that learns turn-taking behavior on-line with an individual user in less 30 turns by utilizing reinforcement learning techniques. Bohus and Horvitz [2] proposed a turn management framework in the virtual receptionist application. They showed the verbal and non-verbal cues used by the avatar can shape the dynamics of multi-party conversation. However, the application task was relatively simple, a question-answering game with trivial questions.",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Every semester, a few hundred new computer science students start their academic studies at the University of Vienna. They all face the first difficult task -the transition from high school to university. Many first-year students struggle with this transition, and some even fail [4, 10] . The transition from a predetermined, structured school system to the independence of a university system leads to students feeling disoriented and often having difficulties motivating themselves [3, 9] . They must also adapt to higher academic demands and different teaching methods [10] . A smooth transition is, therefore, more likely if students have access to the information they need and feel socially connected [3] . Concerning the social aspect of transition, social presence is consistently linked to the motivation of the students and is also supposed to influence motivation [13] . The right support for freshmen in this transition phase can help the university to achieve a significantly higher persistence rate of first-year students [5] .\nResearch Interest: Q1: How can a virtual companion, based on Chatbot technology, support the onboarding process of freshmen?\nResearch Interest: In this context, we will consider the following possible approaches, where a virtual companion could help new students:\nResearch Interest: \u2022 getting to know the faculty building, the university campus, and the vicinity \u2022 collecting knowledge, best-practices, tips & tricks for everyday student life \u2022 network building with other freshmen Q2: Is a Chatbot suited for this problem? Some problems are suitable for conversational interfaces and some are not adequate for them. In this project, we want to find out whether the chatbot technology is a good fit for our use cases.\nResearch Interest: The goal of this project is to develop a concept and to implement it to ease the transition from high school to university for first-semester students, thus minimizing transition difficulties and increasing the retention rate; as a technology-based supplement and improvement to the already existing offers. Another goal is in-depth research on Chatbots and their possible applications. Since the past several years, Conversational Agents (CAs) such as Chatbots continue to experience increasing popularity as an interaction modality alternative or augmentative to classical graphical user interfaces (GUIs), prompted, among other developments, by advances in the fields of Artificial Intelligence (AI) and Natural Language Processing (NLP). The increasing availability and capabilities of this low-threshold modality of interaction with a conversational agent hold transformative potential in many scales and sectors of smart tools for humans, e.g. a potential in democratization of technology access for social good ( [6] ) or as a new basis of exploration of the notion of Companion Technology ( [1] ). It is subject to current debate, what role(s) Chatbots can play as interactional modality, as e.g. approached by [7] in pursuit of a typology of Chatbots (see Figure 1 ).",
    "relatedWork": "",
    "rq": "q1: how can a virtual companion, based on chatbot technology, support the onboarding process of freshmen?\n network building with other freshmen q2: is a chatbot suited for this problem?"
}
{
    "intro": "Introduction: Much prior work demonstrates the advantages of group learning over individual learning, both in terms of cognitive benefits as well as social benefits [1] [2] . From a cognitive standpoint, a major advantage to learning in a group is that when one is exposed to an alternative perspective, it provides the opportunity to question one's own perspective, which in turn offers an opportunity for cognitive restructuring. In order to achieve this benefit, a major emphasis of work on scaffolding collaborative learning [3] has focused on drawing out aspects of an issue where there is a disagreement between students so that they will address the disagreements explicitly and benefit from that negotiation process. In line with this, work on formalizing the process of collaboration in order to identify events that are valuable for learning has in many cases focused on formalization of argumentation [4] .\nIntroduction: In this paper, we again investigate how conflict and negotiation relate to learning by characterizing spans of text as exhibiting a bias towards one stance or another. As a methodological contribution, we discuss how we use as a tool for quantifying bias a state-of-the-art topic modeling technique from the field of language technologies referred to as ccLDA [5] . On another dimension, we examine the impact In Proceedings of the Tenth International Conference on Intelligent Tutoring Systems, 2010, Pittsburgh, USA.\nIntroduction: of the tutor's social behaviors on student learning. As we have seen from recent work, tutors capable of engaging in social interactions with groups can be significantly more effective than tutors that have no social capability [6] . However, there is a trade-off between spending time on social behaviors and spending more time talking about taskrelated content. In this study, we manipulate the extent to which the agent exhibits social behaviors designed to build solidarity between the student and the agent in order to investigate whether these solidarity building behaviors either magnify or dampen the effect of the bias manipulation.\nIntroduction: We begin with a classroom study of collaborative engineering design where students work in pairs on the design of a power plant. This learning task involves negotiating between two competing objectives. Specifically, one student in the pair is assigned to the goal of maximizing the power output of the power plant. The other student, in contrast, is assigned the goal of minimizing the negative environmental impact of the design. Similar to our prior studies of collaborative learning [6] [7], a conversational agent participates with the students in the design task in order to provide support. The unique contribution of this study is that we explore the introduction of bias in the way the agent presents information towards one student's stance or the other. In addition to investigating the effect of the manipulation on learning, we investigate the extent to which students are sensitive to displays of bias in the language of their human partner and that of the agent, to what extent the agent's displayed bias affects the bias displayed by the individual students, the interaction between the individual students and the agent, and finally the interaction between the pair of students themselves.\nIntroduction: In the remainder of the paper we first review the literature on the connection between conflict and learning. We then describe our experimental study. Next we explain our methodology for measuring bias. We then detail our results. We conclude with discussion and directions for future work.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Conversational Agents (CAs) 1 are becoming increasingly integrated into various aspects of our lives, providing services across healthcare, entertainment, retail, and education. While CAs are relatively successful in task-oriented interactions [82, 96] , the initial promise of building CAs that can carry out natural and coherent conversations with users has largely remained unfulfilled due to both design and technical challenges [3, 18, 87] . This \"gulf\" between user expectation and experience with CAs [61] has led to constant user frustration, frequent conversation breakdowns, and eventual abandonment of CAs [3, 61, 98] .\nINTRODUCTION: Conducting smooth conversations with users becomes even more crucial when CAs are deployed in online communities, especially those catering to vulnerable populations such as online health support groups [71] and student communities [95] . These community-facing CAs often serve as a critical part of the community to ensure smooth interactions among community members and provide long-term informational and emotional support. However, these community-facing CAs face two unique challenges: the need to carry out smooth dyadic interactions with individual community members, and the need to respond accordingly based on the community's shifting perceptions [53, 86] . In fact, the community-facing nature of the CA adds new complexity-each dyadic interaction with individual members is visible to other community members, which can not only change the community's perception of the CA, but can also impact other community members, i.e., unsatisfactory interaction with one individual might also frustrate others [42] . However, humans are able to gracefully conduct smooth interactions with each other and behave according to a community's expectations and norms at the same time. This process is based on a uniquely humane characteristic called \"Theory of Mind\" [7, 12, 78] .\nINTRODUCTION: Scholars posit that the Theory of Mind (ToM) is a basic cognitive and social characteristic that enables us to make conjectures about each others' minds through observable or latent behavioral and verbal cues [6, 12, 37, 38, 94] . This characteristic spontaneously drives our understanding of how we perceive each other during social interactions. This enables us to employ social techniques such as adjusting our appearances and behaviors to align others' perceptions about us based on our self-presentation [36] . In typical human-human interactions, having a Mutual Theory of Mind (MToM), meaning all parties involved in the interactions possess the ToM, builds a shared expectation of each other through behavioral feedback, helping us to maintain constructive and coherent conversations [36, 75] . MToM is increasingly used as a theoretical framework for the design of human-centered AI, such as robots, that can be perceived as more \"natural\" and intelligent during collaborations with human partners [26, 57, 59, 75] .\nINTRODUCTION: While MToM is influencing the design of human-centered AI in task-oriented interactions, its role in informing the design of human-AI communicative interactions remains unexplored. Existing approaches to designing human-AI interactions also lack a theoretical framework and a unified design guideline to design human-centered CAs, especially in communicative interactions. Consequently, researchers and designers turn to traditional HCI design guidelines intended for Graphical User Interfaces, which is not always the optimal perspective to look at designing the interactions between humans and often anthropomorphized CAs [89] -researchers and designers face major obstacles in balancing unrealistically high user expectations [61] while providing an adequate amount of social cues to facilitate long-term natural interactions [56] .\nINTRODUCTION: In analogy to human-human interactions, we propose designing towards MToM as a theoretical framework to guide the design of adaptive community-facing CAs that can cater to users' changing perceptions and needs. The first step towards building MToM in human-CA communications is thus equipping the CAs with an analog of ToM that can automatically identify user perceptions about the CAs. With this ability, CAs would be able to monitor users' changing perceptions and provide subtle behavioral cues accordingly to help users build a better mental model about CA's capability. This would also help alleviate the current one-sided communication burden on users, who had to constantly adjust their mental model of the CA through an arbitrary trial-and-error process to elicit desired CA responses [4, 9] .\nINTRODUCTION: Research has explored ways along the realm of identifying user perceptions of CAs to facilitate dyadic human-AI interactions, including examining an individual's mental model of CAs in a variety of contexts [31, 54, 61] . These studies, most of which are qualitative in nature, are not only difficult to scale, but also lack directly feasible algorithmic outcomes that could be integrated into CA architecture to automatically recognize user perception about the CA. For community-facing CAs that are known to have fluid social roles in online communities [87] , we presently lack a clear understanding of how community perception of CAs evolve over time, and whether the very dyadic interactions between humans and CAs in community settings reveal any signal related to user perceptions.\nINTRODUCTION: We thus note a gap in theory and practice in automatically and scalably understanding human perceptions of a community-facing CAs at both individual and collective level. Drawing on the dynamics of human-human interactions, this paper explores a first step towards designing for MToM in long-term human-CA interactions by examining the feasibility of building community-facing CAs' ToM. Specifically, we target two research questions: RQ 1: How does a community's perception of a community-facing CA change over time? RQ 2: How do linguistic markers of human-AI interaction reflect perception about the community-facing CA?\nINTRODUCTION: We examine these research questions within the context of online learning, where community-facing CAs are commonly seen to provide informational and social support to student communities [1, 92, 95] . We deployed a community-facing question-answering (QA)CA named Jill Watson [24, 34, 35] (JW for short) in an online class discussion forum to answer students' questions for 10 weeks over the course of a semester. We collected students' bi-weekly self-reported perceptions and conversations with JW for further analysis. We discuss changes in the student community's long-term perception of JW and examine the relationship between self-reported student perceptions of JW and linguistic attributes of student-JW conversations such as verbosity, adaptability, diversity, and readability. Regression analyses between linguistic attributes and student perceptions of JW reveal insightful findings such as readability, sentiment, diversity and adaptability positively vary with desirable perceptions, whereas verbosity varies negatively.\nINTRODUCTION: Our contributions are three-fold: First, we propose MToM as the theoretical framework to design prolonged human-AI interaction within online communities. Second, our work provides a deeper understanding of how a community's perception of a communityfacing (QA)CA fluctuates longitudinally. Third, we provide empirical evidence on the potential of leveraging computational linguistic approach to infer community perception of a community-facing CA through accumulated public dyadic interactions within the community context. We discuss the implications of our work in designing adaptive community-facing (QA)CAs through theory-driven computational linguistic approaches, where our ultimate goal concerns building natural, long-term human-AI interactions.\nINTRODUCTION: Privacy, Ethics, and Disclosure. We are committed to ensuring the privacy of students' data used in this study. This study was approved by the Institutional Review Board (IRB) at Georgia Tech. We collected the survey and discussion forum data (limited to only student-JW interactions) by seeking student consent and the data was anonymized. We offered extra credits to students for filling out each survey, and bonus extra credits if they completed at least five out of the six surveys. This work was in collaboration with the class instructor and we took measures to avoid coercion. The maximum number of extra credits students could earn by participation was less than 1% of the total grade, and these extra credits could also be earned in other ways as part of the standard class structure. We clarified to the students that survey responses would not be shared with the instructor, and would not have any impact on grades.",
    "relatedWork": "",
    "rq": " rq 1: how does a community's perception of a community-facing ca change over time?\n rq 2: how do linguistic markers of human-ai interaction reflect perception about the community-facing ca?"
}
{
    "intro": "Introduction: Conversational Agents are autonomous interfaces that interact with users via spoken or written conversation. Automated tutoring is a widely studied application of such agents. Various research groups have developed conversational tutors for a variety of educational domains including algebra, calculus, computer literacy, engineering, foreign languages, geometry, physics, reading and research methods. Many evaluations show that these tutors can be effective support for learners [1] [2] [3] .\nIntroduction: While most of the work on conversational tutors has focused on one-on-one tutoring involving only one learner, use of such tutors in collaborative learning situations involving two or more human students has been investigated. Our previous work [2] has shown that tutors in a collaborative learning situation can lead to over one grade improvement. Other work [4] [5] [6] [7] has explored a variety of interaction pattern / tactics that could be used in multi-party educational situations.\nIntroduction: However, despite the effective support that automated tutors offers to students learning in groups, it has been reported that groups of students often ignore and abuse the tutor, unlike the case where students are individually tutored [2] [8] . We reason that the presence of other students in collaborative learning scenarios causes the tutors to compete for the attention of the students. Since the tutors are not capable of initiation or participating in social interaction which makes up the bulk of formative interaction in the group, they are pushed to the periphery of the learning group.\nIntroduction: Research in the area of small group communication has shown that humans employ both task-related as well as social interaction strategies while interacting in groups. However, research on conversational tutors has focused on presenting only taskrelated information, i.e., lessons and instructions in case of tutors. In this paper we report the first study in our investigation on the effects that conversational agents in general can achieve if they are equipped with social conversational skills.\nIntroduction: The rest of the paper is organized as follows: In the next section, we motivate social interaction strategies for agents based on relevant literature from small group communication research. Section 3 describes our flexible architecture and implementation details for a tutor with social conversational skills. Results from the evaluation of the tutor against a baseline and a human gold standard are presented in section 4 before conclusion.",
    "relatedWork": "",
    "rq": " strategy 1e is triggered when one or more students in the group are found to be inactive for over 5 minutes. in this event, the tutor chooses to raise the status of the inactive students by eliciting contributions from them through a prompt like: do you have any suggestions mike?"
}
{
    "intro": "",
    "relatedWork": "RELATED WORK: Our work builds on two areas of prior research: (1) collaborative search and (2) dynamic help systems and interruptions.\nRELATED WORK: Collaborative Search: Collaborative search happens when multiple people work together on an information-seeking task. Collaborative search is often investigated with two dimensions in mind: time and space. The time dimension focuses on whether the collaboration happens synchronously or asynchronously, while the space dimension focuses on whether the collaborators are colocated or remote. A large body of prior work has focused on understanding collaborative search practices along these two dimensions [24, 25, 34, 35] . In this paper, we focus on synchronous collaborative search in situations where the collaborators can only communicate via the Slack messaging platform.\nRELATED WORK: A number of different systems have been developed to support collaborative search, including SearchTogether [26] , Co-Sense [28] , Coagmento [32] , CollabSearch [37] , Querium [12] , and ResultsSpace [4] . These systems have been designed with the traditional search engine as the centerpiece component, but include additional features for collaborators to communicate, share information, and become aware of each other's search activities. The goal of these additional features is to allow collaborators to coordinate, learn from each other's search paths, avoid duplicating work, and to assist with collaborative sensemaking-becoming aware of collaborators' motivations, actions, and state of knowledge [20, 26] . Systems have also been designed to algorithmically alter the ranking of documents based on collaborators' activities, for example, by using documents shared between collaborators as a form of relevance feedback [30] .\nRELATED WORK: Studies have found that these specialized systems provide different benefits during collaborative search, for example, by improving the collaborative experience compared to non-integrated tools [26] , by raising the awareness of collaborators' activities [28] ; by supporting different strategies adopted by the group (e.g., agreeing on a few relevant items vs. being as exhaustive as possible) [4] ; and by reducing communication and coordination efforts [33] .\nRELATED WORK: While many different systems have been developed to support collaborative search, these systems have not enjoyed wide-spread use [14] . A survey by Morris [25] found that while collaborative search has become increasingly common, most people use a combination of everyday search and communication technologies to collaborate on search tasks. Morris concluded by suggesting that integrating lightweight search tools into existing communication channels may be a more promising approach than developing dedicated systems for collaborative search.\nRELATED WORK: Prior research has found that people often use social networks such as Facebook and Twitter to engage in asynchronous collaborative search, an activity referred to as social search [11, 27] . Efron and Winget [9] developed a taxonomy of questions posted on Twitter, and found that a large proportion request factual information that is likely to exist on the Web. This result suggests the possibility of developing search systems that can automatically respond to questions posted on social media and partly motivated the development of the SearchBuddies system [15] . SearchBuddies was designed to embed search results in response to questions posted on Facebook. The embedded search results appeared as a new post in the Facebook thread. A qualitative analysis of people's perceptions found interesting challenges and opportunities for \"socially-embedded search engines\". For example, users only reacted positively to the embedded search results when they were extremely relevant and non-obvious, or when they complemented another user's answer to the question. To our knowledge, no prior work has investigated how people perceive search systems that intervene in synchronous instant messaging conversations.\nRELATED WORK: Dynamic Help Systems and Interruptions: Prior research has investigated the reasons why people avoid systems that intervene to provide assistance. Users avoid help systems due to the cost of cognitively disengaging with the primary task, due to the fear of unproductive help-seeking, due to a failure to admit defeat, or because they are unaware of how the help system can provide support [8, 18] .\nRELATED WORK: An unwanted intervention can be viewed as an interruption. A large body of research has also focused on understanding how people respond to interruptions while engaged in a task (see Li et al. [22] for a review). Studies have found that interruptions can negatively affect task performance [2] , cognitive load [16] , and emotional state [1] . Research on interruptions has focused on three dimensions: the interruption protocol, timing, and relevance. Early work by McFarlane [23] investigated four interruption protocols: immediate, negotiated, mediated, and scheduled. Negotiated interruptions, which provide mechanisms for easily ignoring the interruption, were the most effective. A wide range of studies have focused on the timing of an interruption. Results consistently show that interruptions during periods of low mental workload are less disruptive. In this respect, studies have found that interruptions are less disruptive when they occur early in the task (before the user is deeply engaged) [7] and during sub-task transitions [1, 16, 17] . Finally, studies have found that interruptions that are more relevant to the primary task are less disruptive [7, 17] .\nRELATED WORK: Most research on interruptions has focused on interrupting individuals, rather than collaborators working on a common task. As one exception, Peters et al. [29] investigated interruptions aimed at one individual while collaborating with another. This study compared interruptions sent at random intervals versus interruptions sent by a human \"wizard\" monitoring the communication channel. The wizard's interruptions were less disruptive, suggesting that a system with access to the communication channel might be able to predict when to intervene.",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Aphasia is a communication disorder that afects the production and comprehension of speech as well as one's ability to read and write [43] . It often results from damage to the language center of the brain, which may be caused by stroke. People who have aphasia present along a spectrum and have varying abilities in terms of generating and comprehending speech and written language. Aphasia has been described as \"the most important potential consequence of stroke\" [56] as one's ability to communicate has a profound impact on their quality of life [16, 54, 56] . One's inability to communicate as a result of aphasia can lead to depression, social isolation and emotional distress [54, 56] . Despite the importance of designing AAC devices to support quality of life goals for people with aphasia, limited work has sought to understand their desired contexts of use and how technology may support people with aphasia in their quality of life goals.\nINTRODUCTION: Our work focuses on understanding how to design AACs for social and leisurely activities that might improve the quality of life of people with aphasia. In particular, this work is motivated by the difculties people with aphasia face while dining in unfamiliar restaurants. An aphasia.org survey of people with aphasia about dining in restaurants reported moderate to severe difculty levels by about 78% of respondents [44] . In addition, text-only printed menus have been named as a particular source of discomfort to people with aphasia especially after initially acquiring aphasia [23, 53] . Furthermore, studies about older adults and people with impairments name food choice as a means of expressing one's autonomy and independence [58] .\nINTRODUCTION: Using a high-tech AAC device (e.g., Lingraphica) is one way by which an individual's communication skills (production as well as reception) can be maximized for functional and efective communication [40] . Yet, these devices are often made up of large vocabularies that are difcult to search. They typically hold thousands of words [46, 47] that are organized into categories and subcategories. The users' core vocabulary, which might include their most accessed words is displayed on the initial interface of the device for easy access. Novel and less frequently used words are accessed by navigating through the diferent categories and subcategories on the device. To help address the issue of novel contexts, where words my not be loaded into the device or are buried deep in subcategories, prior work has considered context-aware AAC devices [36] . Context-aware techniques, however, may still miss important language and cues in the environment that would be helpful for communication (e.g., local signage and printed content).\nINTRODUCTION: This paper presents a three-year efort comprised of observations in both clinical and naturalistic settings and the design and exploration of three prototype applications for people with aphasia. We began by conducting 43 hours of observations of people with aphasia during therapy, which informed our understanding of aphasia as well as the dynamics of communication within the context of therapy. These sessions also helped us identify the challenges and limitations of current AAC devices, particularly around leisurely social activities. Based on our initial insights, we sought to understand the practices and experience of dining in restaurants for people with aphasia and their social contacts. We conducted contextual interviews with people with aphasia dining in restaurants and iteratively designed three novel applications to support independent meal ordering. The applications integrate photo captioning, OCR technology, and context-awareness to provide vocabulary in novel contexts and support meal ordering in unfamiliar restaurants. We tested PhotoSearch and MenuSpeak with speech-language therapists and people with aphasia in laboratory settings and evaluated OrderEat through a case study with one person with aphasia over four sessions of use, including in naturalistic restaurant settings.\nINTRODUCTION: Through the design and evaluation of these prototypes, we make three contributions to the accessibility and HCI literature. First, we extend knowledge around designing for leisurely activities for people with aphasia by focusing on the novel case of ordering meals in restaurants, a challenging but important social setting for communication. Second, we open up a discussion of accessibility in restaurants for neurodivergent people, particularly around the process of ordering and customizing a meal. Third, we contribute insights into designing AAC devices that integrate AI, such as image captioning, OCR technology, and geographic data. We refect on challenges of handling inaccuracies in real-time in order to support communication, and ways to present information generated in realtime in a comprehensible manner as well as how AI in AACs might hinder or support one's sense of autonomy.",
    "relatedWork": "RELATED WORK: We briefy review related work on the design and use of AACs, how AAC use and communication more broadly is shaped through social interaction, and prior advances in context-aware AAC technologies.\nAugmentative and Alternative Communication: Augmentative and alternative communication (AAC) encapsulates a broad set of strategies, techniques, and tools that support individuals with speech and/or language impairments in expressing their thoughts, desires, feelings, and ideas [40] . AACs include communication strategies such as gestures, prosody, pantomiming, facial expressions, residual speech and body language, low-tech communication tools that augment or replace speech (e.g., picture boards, photographs, pencil and paper), and high-tech communication devices [4] . Studies have also extended the defnition of AACs to include strategies and tools that enhance social participation and social networks with the broader goal to support improved quality of life [10, 48] . People who have acquired neurological conditions such as amyotrophic lateral sclerosis (ALS), traumatic brain injury (TBI), brainstem impairment, severe, chronic aphasia and apraxia of speech, primary progressive aphasia (PPA) and dementia often use AACs to augment or replace their speech.\nAugmentative and Alternative Communication: Experiencing aphasia often results in reduced social participation [56] , underscoring the importance of understanding the social contexts in which people use AACs and what kinds of support they want in those contexts. While some research has focused on designing AACs for leisurely activities [1, 25, 45, 47] , most research and commercial communication devices cater to functional needs. For instance, communication technologies for people with aphasia are able to support speech production in repetitive tasks like practicing speech during therapy. They are also efective at generating speech for everyday routine tasks, such as indicating that the person wants to brush their teeth or comb their hair. Yet, people with aphasia not only want to express their functional needs, they want to express their wants and opinions [61] . There is a need to feel like one is having an adult conversation versus simply training on words during therapy [34] . In addition, studies have described many users of high-tech AAC devices as partial and reluctant, as they may prefer to use other strategies in certain situations, such as with familiar conversation partners and for quick, easy, or routine tasks [8, 39] . Nevertheless, there are certain tasks and communication situations that require specifc vocabulary, are difcult due to the time pressure of a situation, and cause frustration when communication does not go well. For example, activities like dining in an unfamiliar restaurant, playing a board game, or discussing a sporting event are all important social activities but require specifc vocabulary that is usually unsupported by or difcult to fnd on AAC devices.\nSocial and Collaborative Use of AACs: Human communication is inherently social and co-constructed among multiple parties. Goodwin compellingly shows how people with aphasia act as competent speakers through the use of gesture, eye gaze, and coordination of interaction with one's social partners [26] [27] [28] . Related to this theorizing of communication as social interaction, researchers have sought to understand how conversation partners (caregivers, family members and other social contacts) contribute to communication involving people who use AAC devices. Importantly, however, Apler [3] warns against the trope of \"giving voice\" or that the devices give voice to a person rather than viewing technology as playing a part in socially situated and emergent communication. This interactive framing of communication has shaped the design and study of AAC devices. For example, Shin et al. designed a system to support caregivers in sharing information about AAC use by children with complex communication needs in order to better support them [55] . Similarly, Beukelman et al. focus on the ways in which communication partners can contribute to successful communication [8] . Roark et al. build on this by bringing conversation partners into the predictive system in order to increase the pace of communication with people who use AACs [51] . Fiannaca et al. draw on the social model [38] of communication and supported communication [34] for people with aphasia through a group ware system that addresses privacy and the pace of conversation for both communication partners [21] . Finally, Valencia et al. investigate constraints that afect the conversational agency of people who use AACs [57] . Their study leveraged practices of familiar and unfamiliar conversation partners to understand conversational constraints and make recommendations for design, which include designing solutions that preserve the AAC user's turn in conversation (e.g. through conversational robots or asking questions directly to the AAC user) and leveraging successful practices of close conversation partners. Building on this literature, the present study views the interactive and collaborative nature of communication as a crucial aspect of AAC design and use.\nAI for People with Aphasia: High-tech AAC devices are not always accepted and used by people with aphasia. Some reasons identifed in the literature include the inability of the device to support real-time communication in a timely manner and the form factor of the device [59] . Some studies have sought to increase the throughput of AAC devices and timely access to vocabulary through novel predictive algorithms and technologies [14, 36, 47] . One study explored predicting the most relevant next word from the word currently being accessed by the user [47] . Although Nikolova at al. reported moderate success, this method was unsuccessful in determining the frst word for initiating sentence composition. To address this limitation, some studies have explored one's context (e.g., geographic location) to anticipate relevant words prior to beginning a conversation [18, 31, 35, 36, 49] . Though understanding one's context can help to streamline the vocabulary of a device, the information provided by one's context might still be vague enough that the re-organized vocabulary is still too large to be searched by people with aphasia in a timely manner during conversation. For instance, conversational topics during a single meal can span up to 73 topics [6] . Context awareness has also been paired with machine learning and natural language processing models to predict relevant words for people with aphasia [12, 13, 15] ; however, these predictions are often approximations that require people with aphasia to make choices about the fnal word output in ways that are difcult for them to do because of their language impairment. Studies on designing AACs for people with aphasia have explored photo-capturing for archiving and preparing information for future communication [1, 2, 37] but not yet integrating AI (e.g., photo-captioning, OCR) to understand the content of photos (see also [9, 29, 32] ). Relatedly, MacLeod et al. have explored ways to communicate inaccuracies of AI-captioned photographs to people with vision impairments [42] . We know far less about how people with aphasia, who may have upper body mobility impairments along with difculty understanding text, are able to capture and discern the quality of their photos needed for AI systems to work. In this work, we draw on observations of people with aphasia interacting in clinical and naturalistic settings as well as a design exploration of three prototype systems that integrate AI for accessibility.",
    "rq": " ben: what do you think i should take?\n ben: can i get the mushrooms . . . get them out of them?"
}
{
    "intro": "INTRODUCTION: Examples of the first generation of autonomous vehicles are currently being tested on public roads [39] , and some predictions expect common use of autonomous vehicles around 2040 [17] . These systems have the potential for completely redefining what a \"driving\" experience will entail, moving away from the paradigm of having one driver in control with an analog wheel and pedal input, to a new model whereas an AI system is responsible for making most of the moment-by-moment driving decisions. Various concept vehicles have emerged suggesting new interfaces for control and collaboration with an increasing focus on shared mobility [11, 24] . This context brings focus to the user interfaces that can support collaboration, planning, and communication, which we use in our investigation reported in this paper.\nINTRODUCTION: Autonomous vehicles provide an emerging area for new research in fields such as safety, self-driving technology, trust, and mobility experiences. HCI researchers are starting to focus on new possibilities for in-vehicle applications [13, 25] . For this paper, we focus on how users can interact with autonomous vehicles for navigation. The development of autonomous vehicles is an incremental process, where vehicles are taking over more and more driving responsibility as they become increasingly sophisticated and capable. The stages of development are described by the five levels of automation, ranging from partial automation at level 1 to fully autonomous at level 5 [10] . At level 5 the vehicle will make moment by moment decisions in all contexts without the need for users interfering at any point, changing the role of the human driver. This makes the current conventions of e.g. steering wheels obsolete and brings up the obvious question of, what should take its place as a next driving interaction convention. An autonomous vehicle is a complex system, with many steps between the user input, and the car output, when it comes to driving. Dealing with this complexity involves identifying the desired granularity of control for interactions [40] , creating interfaces that provide the user with a desirable level of autonomy and feeling of control while riding in the vehicle [14] .\nINTRODUCTION: Vehicles without steering wheels are currently being designed and a number of such concept cars have been showcased by car manufacturers. To investigate the automotive industry's vision for interaction with future vehicles, we look at some recent examples. In 2017 Volkswagen unveiled Sedric -their first concept vehicle without a steering wheel [37] . Sedric is a publicly shared vehicle which users summon by pressing a button on a personal device referred to as OneButton. When in the vehicle the users interact with a digital voice assistant and view information on an OLED window screen. Honda also recently released a concept autonomous vehicle NeuV, in which users interact with an AI assistant Hana using either voice commands, or through a large touchscreen [9] . In 2018, General Motors released Cruise, their first concept for a personal autonomous vehicle [7] . Users interact with Cruise through a smartphone app to request rides, and through touch screens when in the vehicle. The Google subsidiary company Waymo, which is currently testing their autonomous taxi service also uses a smartphone app as the main interaction with the vehicle [39] .\nINTRODUCTION: The most common paradigm that emerges from these concept vehicles is the usage of touch screen and voice assistants as the assumed main modalities for interactions in future vehicles. However, we argue there is a different emerging modality that could play a role in future vehicles: shape-changing interfaces. The vision of shape-changing interfaces is the ability to \"transform into any shape or materiality relevant to the context of use\" [1] , and thereby accomplish or communicate input or output. One of the reasons shape-change is a compelling technology is that it caters to people's natural kinesthetic competencies and ways of interacting with the world through touch [38] . Shape-change has appeared in conceptual car designs by a number of leading brands in the automotive industry, for various design purposes. BMW has unveiled a concept car which has a shape-changing body and steering wheel [3] . Audi released its concept for a shape-changing steering wheel for the Audi James 2025 [11] . Also, in 2015, Ford filed a patent for a shape-changing interior that would allow for easy seating rearrangement [4] . In this paper, we further explore how shapechange can play a role in the main driving interactions of fully autonomous vehicles. The paper is organized as follows: Firstly, we present related research on controlling autonomous vehicles, shape change and voice assistants. Secondly, we present our design and prototype for a shape-changing autonomous driving controller. Finally, we present a user study comparing tangible controller to voice assistant when driving in social settings.",
    "relatedWork": "RELATED WORK: In order to investigate interactions within autonomous vehicles, we look at relevant research on the user experience of autonomous driving. Additionally, we look at examples of work on shape-change and voice interfaces to highlight relevant insights on the benefits and limits of both modalities.\nControlling Autonomous Vehicles: To create pleasant user experiences of future highly autonomous vehicles, passengers should be able to control the vehicles in a desirable way. This challenge has been addressed by NASA, which developed the H-metaphor, providing a mental model that the autonomous vehicle should act like a highly trained horse, able to sense the environment and drive on its own. This is while the user remains aware of what the vehicle is doing and are in control -like a rider feels what a horse is doing and maintain control through the reins [6] . Flemisch et. al. claims that implementing this metaphor in design can help make autonomous driving safe and natural. One example of a system for giving the passenger partial control over the vehicle is Actushape [28] . The system incorporates shape change as a modality for displaying what level of control the vehicle is calculating will be suitable for a given situation. Depending on the situation, different levels of control will be suitable. This is achieved through three wooden strips arranged in parallel. By shape-changing the three strips, the device can both provide information at a glance for the passengers in the vehicle while also functioning as an input device for gaining control over the vehicle. Their findings suggest that passengers enjoy having some range of control when in the vehicle, while at the same time being able to let the vehicle drive completely autonomous when desired. However, many fully autonomous cars today do not follow the H-metaphor and instead drive without the users being aware of what the vehicle is doing in its moment-by-moment decisions. This can have an implication on the user experience, and research has found that it can lead to an increased anxiety decreased sense of control for users [12] . Some of these effects might be mitigated by including more user feedback, to increase system transparency. For example, Koo et. al. found that voice alerts can help reduce anxiety in autonomous driving. Similarly, Ekman et. al found that providing users with feedback when the system senses objects around the vehicle can increased users' trust in the vehicle [5] . These results show how system transparency could be used as a way to create comfortable driving experiences.\nMultimodal interaction and surfaces: There are many examples of multimodal systems that support interaction and collaboration-some early examples of actuated tangibles are inspiring as they support social interaction inherently through the physical tokens that can be manipulated by co-present users [30] . There has been research exploring how tangibles can be designed to support interactions around a surface display with simple physical tokens [2] as well as more complex actuated and robotic tangibles [16, 31] . We take inspiration from these examples for highlighting the potential to merge the physical and digital worlds and examples of separating spatial or complex data to a surface while offering the tangible as a token to focus interaction and control.\nMultimodal interaction and surfaces: We also appreciate the related developments in the area of shapechanging interfaces because they are dynamic and offer the flexibility to adjust to a number of situations through adaptive affordances [1] . They can represent both fixed objects and objects which can be manipulated by the user. Research on shape-changing interfaces provides examples of how shape change can be utilized in a wide range of applications, creating rich tangible interactions. In an article by Van Oosterhout et al. they explore how physical ambiguous movement through shape-change can convey expressions and give meaning to an interface [38] . They create a novel wall thermostat capable of changing shape resulting in a change in volume by expanding and collapsing 32 arms while providing haptic force feedback when turned. Based off of two user studies exploring how these modalities affect experiences, they conclude that shape-change can be useful for providing affect and enhance the meaning assigned to the behavior of complex systems.\nMultimodal interaction and surfaces: With the concept Haptic Chameleon, researchers explored how a shape-changing hand-controlled device can convey information by altering its shape and feel [26] . They created a dial that can change shape to afford different functionalities for controlling video content. They argue that their shape-changing concept would be especially useful when mimicking real-world objects and selection devices, and when changing to shapes that represent agreed conventions, because these forms are likely to be the most clearly understood.\nMultimodal interaction and surfaces: Shape-changing devices also have the potential to be used for deictics-pointing and indicating through communicating a direction. Robot deictics is a field for exploring how robots can be augmented with the, otherwise, human quality of deictics. Saupp\u00e9 and Mutlu seek to explore how a humanoid robot can use deictics to help explain which specific objects in space are being referred to [35] . Specifically, they explore how deictics compare to strictly articulating. Their findings show that a range of deictic gestures provide consistent results in helping a robot to point out to specific areas or objects. Especially in a noisy setting, deictics show great promise, as articulated expressions were easily predominated here. As they explain in their paper, deictics can help robots, and in this case, shape-changing interfaces, to precisely refer and direct the users to real-world objects [35] . However, there are currently some challenges and limitations when designing shape-changing interfaces as described by Alexander et al. [1] .\nVoice Interaction: Speech is the fundamental modality for human communication and social interaction, and the human brain is highly attuned to speech [27] . Nass & Brave argue that because of this, speech is potentially much more powerful as a modality for interacting with machines than traditional graphic based interfaces, yet voice control has its limitations. Graphical interfaces are more suitable for presenting large sets of discrete values, map-based or spatial information. Voice interfaces have become increasingly popular in consumer products offering digital assistants with which users can converse [19] . Though these voice assistants have the possibility to provide a natural way of interacting with systems, consumer voice assistants of today are based on structured command and answer functionalities [19] . These functionalities do not yet resemble natural human-to-human conversation, characterized by fewer rule-based interactions taking place in contextual settings [32] . In their study of personal assistants deployed in users' homes over a month, Porcheron et al. investigated how voice assistants are being adopted into social interactions. They found that interacting with voice assistants is generally very different from conversation, and influences the social situation, e.g. by users employing social politics to govern the use, or by interactions prompting groups to be silent.",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": " that are required for recovering from an attentional disconnection: what objects changed?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Social touch is an essential non-verbal communication channel that is widely used for diferent purposes, from expressing less intimate feelings such as greetings (e.g., handshakes) to more intimate ones such as kissing or hugging [47] . However, social touch communications can be highly restricted under certain circumstances, for instance due to social distancing during a pandemic like COVID-19 [8] or for people with medical conditions [21, 39] . In these circumstances, even when people are physically in the same place (i.e., face to face), social touch breakdowns may still occur: they may not be able to directly touch each other like they used to. Moreover, the social touch breakdowns may not only have direct consequences on intimate relationships such as families or close friends, but also on semi-intimate relationships such as casual friends or colleagues. Research in other domains highlighted that long-term touch deprivations may severely impact health and well beings [13, 20] . As such, it is important to overcome these social touch breakdowns, especially under circumstances in which people might have to practice social distancing for a long time [8] .\nINTRODUCTION: Although research in HCI have proposed diferent technologies to mediate interpersonal touch communication, these touch devices were mainly designed for intimate relationships such as couples (e.g., touching cheeks [36] , kissing [40] ) and close friends or families (e.g., hugging [1, 3, 21] , tickling [15] ). Research in robotics have proposed robot arms to enable remote handshaking for semi-intimate relationships like colleagues [31] . However, robot-mediated touch devices often sufer from uncanny valley issues 1 [31] . Furthermore, these touch devices were mainly designed for and evaluated in the context of remote communications [17, 28, 31, 35, 48] . As such, their efectiveness and applicability when used during face-to-face communications remain unclear, especially since past research suggested that establishing face-to-face communications introduces diferent challenges compared to remote communications [6] .\nINTRODUCTION: Our work aims to complement and expand the design space of computer-mediated touch, to better support interpersonal touch communications that are: 1) not only for users with intimate but also semi-intimate relationships, and 2) not only used in remote but also face-to-face communications. Additionally, we aim to build a novel touch device that can generate realistic touch sensations.\nINTRODUCTION: To better inquire into the potential use case scenarios of touch devices in face-to-face communications, we frst conducted a survey with 136 participants (followed by in-depth interviews with 6 participants) to investigate social touch breakdowns due to social distancing during the COVID-19 pandemic. In particular, we were interested in the forms of social touch that were highly restricted with social distancing, the relationships that sufered from social touch breakdowns, and the challenges of re-establishing the alternative ways of touching others while social distancing. The survey revealed reduced social touch practices, in particular for greetings (e.g., handshakes) between people with semi-intimate relationships. Based on these insights, we built SansTouch, a multimodal hand sleeve used along with a smartphone to enable handto-hand touch interactions without directly touching. SansTouch 1 Uncanny valley is when the emotional response to a device would abruptly shift from empathy to revulsion as it approached, but failed to attain, a lifelike appearance [30] . combined three modalities that were recommended in the literature to enhance touch sensations: skin-like infation [31, 45] , heat conditioned to body temperature [3, 17, 31] , and visual feedback [15, 50, 53] . We then conducted a structured observational study with 12 participants to 1) evaluate user perceptions and the use of SansTouch's handshakes in a face-to-face communication for semiintimate relationships (i.e., colleagues); and 2) elicit user feedback regarding the generalizability aspects of SansTouch, including the use of SansTouch's hand-to-hand interactions for other types of relationships and remote communications.\nINTRODUCTION: Our paper makes the following contributions: First, we contribute empirical insights based on real stories from participants on social touch habit breakdowns, including handshakes were the most reduced form of social touch, the frequencies of touch were highly reduced in semi-intimate relationships like casual friends and colleagues, and participants were frustrated having to learn new forms of social touch. Second, we propose SansTouch, a novel wearable hand sleeve used along with a smartphone, as well as the design of hand-to-hand interactions with SansTouch. The participants in our structured observation study reported high similarities of handshaking with SansTouch to a real handshake, and they could easily adopt the mediated touch interactions in a face-to-face conversation with others with minimum learning eforts. Finally, we discuss the design factors that can better support mediated touch interactions in both face-to-face and remote communications as future directions, including the trade-ofs of combining diferent modalities, the level of synchronicity, and the social acceptability of the communication medium in diferent contexts of use.",
    "relatedWork": "RELATED WORK: As one of the major non-verbal channels in face-to-face communication, touch is widely used for conveying diferent information [47] , from social etiquette like greetings (e.g., handshakes, kissing cheeks) to functional touch such as notifcation, punishment, and massaging [13] . Touch also infuences human's physiology and psychology states as well as the social behavior [47] . For example, touch in intimate relationships (e.g., holding hands or hugs) could help lower pain, stress, and blood pressure [12, 19, 29] , while touch in semi-intimate relationships could help reducing heart rate and stress level [23, 49] . Touch deprivation might bring negative impacts on psychological state and well beings [13, 20, 27] . Past research have also shown a linear correlation between afection deprivation (the lack of afective touch) and depression, stress, and emotional awareness for adults [14] . Given the importance of maintaining interpersonal touch, HCI researchers have proposed diferent mediated touch communication tools. In this section, we focus on discussing past research that specifcally explored the role of touch devices in strengthening social connections and the design factors that are important to enhance touch sensations.\nComputer-mediated touch: 2.1.1 Robot-generated touch. In the feld of human-robot interaction, technologies are often designed to generate touch with similar purposes of human touch. The touch stimuli are often pre-recorded and replayed to users. The touch experience is afected by the visual appearance, the perceived intent, and the physical qualities, for example users prefer touching soft, warm devices [3] . Calmer [21] was specifcally designed to simulate maternal skin-to-skin holding for premature infants in incubators. Nakanishi et al. [31] coupled a robot hand and a video streaming to enable remote handshakes between two remote users. The robot hand was designed with a warm, skin-like material to imitate the sensation of touching a human hand. The handshake movement was robot-generated, although the replay was triggered by a user as soon as another user touched the robot hand. They highlighted the importance of synchronizing visual feedback and touch sensation. Although robot-generated touch was shown to strengthen remote social connections, it is often perceived as a \"back-up\" by users [21] . Furthermore, human-robot interaction often sufers from uncanny valley issues [31] .\nComputer-mediated touch: 2.1.2 Interpersonal mediated touch. Past research have also explored two-way remote touch communications. DiSalvo et al. [11] built a doll-like hugging device that works in pairs to enable remote hugs. The device provides visual and audio feedback when its paired device is being hugged; and vibrates when its counterpart is being stroked. As such, the user understands that the stimuli are initiated by the other user simultaneously. Similarly, the iXu teddy bear device [26] moves its arms when its paired device is being stroked, mimicking the strokes of the other user. As such, the touch experience is bi-directional and shared between both users: both experience touching and being touched at the same time. To increase the perception of shared experience, KUSUGURI [15] not only delivers a tickling stimuli on the user's palm, but also displays the other user's fnger as if it directly tickles the user's own palm.\nComputer-mediated touch: These mediated touch interactions were mainly designed for users with intimate relationships like couples or close friends, and mostly evaluated in the context of remote communications [17, 28, 31, 35, 48] . However, special circumstances like social distancing may restrict interpersonal touch communications even when users are co-located, for example when interacting with fragile patients in hospitals [21, 39] or during a pandemic [41] like the COVID-19 outbreak that became a global pandemic in early 2020 [2] . Existing research related to co-present mediated touch mainly focused on collaborative physical games [5] or augmenting direct touch experience in face-to-face communication [1, 51] . In this case, the users could still touch each other. To the best of our knowledge, few considered the situation in which the users are face to face, can communicate through visual and auditory channels, but cannot touch each other. Our work attempts to fll in this gap, by exploring the possibilities of enhancing the design of touch devices to better support face-to-face communications.\nDesigning mediated touch devices: 2.2.1 Communication medium. When designing a touch communication tool, it is critical to choose an appropriate communication medium. One option is using doll-like touch devices, as done in [11, 26] . O'Brien et al. [32] compared several doll-like touch devices to represent the other person's hand in remote touch communications. However, their participants showed unwillingness to use doll-like touch devices while they were in public, sometimes even hid them in their bag or left them at home. This suggests that doll-like devices might have a low social acceptance level.\nDesigning mediated touch devices: Another alternative is using wearable devices, for example tactile arm sleeves [53, 54] , gloves [42] , or jackets [10] ). These fabricbased wearable devices were compact but expressive, enabling users to exchange afective touch interactions while preserving the possibility of natural interactions with their environment [34, 38] . Another advantage of using wearable devices is the touch stimuli can be delivered directly onto users' skin as soon as the stimuli is triggered, enabling highly synchronized communication that is critical for face-to-face communications [6] .\nDesigning mediated touch devices: 2.2.2 Generating touch stimuli. Most touch devices used vibration motors to generate the touch stimuli. For example, Multi-Moji [50] used the built-in vibration motors in smartphones to enhance emojis with tactile feedback. TaSSt [24] and VisualTouch [53] used vibration motor arrays embedded on a tactile sleeve to generate tactile patterns on forearms. Similarly, Flex-N-Feel [42] embedded several vibration motors on a pair of gloves to support remote touch communication for couples. While vibration motors are easily available and relatively cheap, the touch sensation they can generate is quite limited and diferent from a real human touch [15] .\nDesigning mediated touch devices: Infatable actuators ofer more possibilities of generating realistic skin-like touch sensation, as it features a smaller diference in the mechanical impedance between the device and human [54] . For example, POKE [35, 37] used two infatable bubbles attached on each side of a smartphone. Users can poke the infatable bubbles as if poking the cheek of another person. Their longitudinal studies highlighted the diferent infation patterns that the each couple developed over time [35] . Infatable actuators have also been used to generate touch stimuli on arms [54] , wrists [52] , and torso [10] . Complementary but distinct from these devices, we proposed a novel touch device that embedded infatable actuators on a wearable hand sleeve to enable hand-to-hand touch interactions.\nDesigning mediated touch devices: Combining touch with other modalities could also expand the available range of emotional states and increase the touch perception [43, 50, 53] . For example, Wilson et al. [50] showed that combining visual and thermal modalities in touch interactions allowed users to convey a wider range of afective expressions. Tewell et al. [43] suggested that by adding a thermal array display (TAD) to text messaging systems, the emotional perception of the message could be enhanced. Adding an LED array that is activated along with a vibration motor array was shown to increase the touch perception [53] . Nevertheless, the combination of these modalities have not been evaluated in the context of face-to-face communications, thus its efectiveness for face-to-face communications remains unclear.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: From professional settings to casual interactions, speaking skills are vital in everyday life. Being able to effectively communicate ideas and thoughts are considered important skills that are valued in almost every profession. Given the importance of communication skills, many entities exist such as Toastmasters [61] and owntheroom! [51] that are intended to help individuals. While very effective, this approach eliminates a significant part of the population who are not comfortable seeking help yet love to practice and improve their communication skills.",
    "relatedWork": "RELATED WORK: Many efforts have been made to create tools for improving social or presentation skills. We find Cicero [5] , MACH [31] , Rhema [59] , ROC Speak [24] , Automanner [60] and Automated Social Skills Trainer [58] , Presentation Trainer [54] , Logue [19] , Tardis [2, 18, 25] to be the most recent examples. For example, Cicero [5, 12] simulates a virtual audience as part of live feedback to a speaker, Automanner [60] is an intelligent interface that makes users more aware of their body language by automatically extracting and displaying repeated patterns. MACH includes a reactive 3D character allowing anyone to practice job interviews and receive visual feedback at the end. While these systems adopt innovative ways to help users improve their communication skills, the evaluation of these systems does not reflect real world usage. We argue that, to create an unequivocally ubiquitous tool for improving communication skills, it is essential to consider the following aspects during evaluation.\nRELATED WORK: Naturalistic Environment: While lab experiments are valuable in providing insight on specific aspects, bringing participants into the lab, asking them to perform a task, and have them self-reflect on the experience become unavoidable confounding factors. Therefore, if possible, it is desirable to conduct an experiment that is outside of the lab so that the results can generalize to real world usage [43] . The environment should be reflective of the one participants will be using the tool in.\nRELATED WORK: Diverse Users: Most evaluation of the tools invites college student as participants from a university campus [31, 59, 60] . It is unknown whether the effect can be generalized to the general population.\nRELATED WORK: Robustness: When an experiment is conducted in a lab setting, we often control for devices, environment and many other factors. While controlled experiments are useful to understand an exact effect [5, 12, 31, 58, 59] , it is also important to deploy the technology in the wild and make it work across devices, sensors, and environment. It allows us to understand the real-world constraints and design the technology accordingly.\nRELATED WORK: Multiple Usage: any technologies are subject to the novelty effect where users tend to temporarily improve due to the introduction of new technology instead of the actual response to the technology. In some cases, evaluation is done in one session with very little downtime. As a result, it becomes difficult to assess if the effect and interest will remain after. Multiple usage with appropriate down time in between can test against these factors.\nRELATED WORK: External Evaluation: Self-reported data can be unreliable, especially when communication skills are being evaluated. For example, people with social anxiety generally rate their own performance being worse than when others evaluate them [14, 49] .\nRELATED WORK: An examination of the current state of the art systems and the evaluation procedures of those systems, are summarized in table 1. To satisfy the repetitive usage criterion, users must have used the tool more than twice. Cicero [5] x * x Rhema [59] * x Automated Skills Trainer [58] Automanner [60] * x MACH [31] x x Presentation Trainer [54] * x Logue [19] x x Tardis [2, 18, 25] x ROC Speak [24] x x x ROC Speak (Current Evaluation) x x x x x * tool was repeatedly used, but all done in one long session with very short downtime.\nRELATED WORK: Most of these systems are very innovative. However, they are computationally intensive, sensitive to environmental factors, or requires devices (Microsoft Kinect [2, 54, 60] and Google Glass [19, 59] ) that most people don't have access to. These limitations prevent them from being evaluated in a more realistic world setting. ROC Speak is a lightweight platform that is available for anyone that has access to a Chrome browser and a webcam. In our previous evaluation of ROC Speak [24] , we were able to address some of these evaluation issues by deploying our system online and allowing users to practice our tool in their own environment. In this paper, we deployed our system online for repetitive usage by employing a training regime for a 10-day period and have peers and a judge to evaluate their performance. To our knowledge, this may be the first deployment of a communication skill training system in a real-world setting that has proved to be effective in improving a users' communication skills.",
    "rq": "(1) how valuable is our treatment (combined automated and subjective feedback) interface compared to the control (subjective feedback only) interface?\n (2) is the system effective in facilitating the generation of subjective feedback and are those feedbacks useful?\n (3) how useful do people find the system?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Readily available motion sensing technologies allow the control of digital elements through interactions that use embodied movement in the real world [3, 15] . In recent years, attention has been drawn to using body motion to interact with Digital Musical Instruments (DMIs) within research communities such as NIME (New Interfaces for Musical Expression) [7] .\nINTRODUCTION: Developments in this area could provide opportunities to benefit movements towards accessible and adaptive systems, and provide a platform for supporting universal access to DMIs.\nINTRODUCTION: The focus of this paper is on research involvement in a music technology project, in which users with complex disabilities (conditions which affect both cognitive and motor abilities of an individual) use technology to assist them in music performance. The term 'facilitated performance' is used throughout to describe a situation in which a performer with disabilities is supported by another party in the act of musical performance.\nINTRODUCTION: The role of the facilitator is of high interest; particularly how their involvement may impact the direction and creative choices of the project such as the choice of technology and method for interaction. Questions pertaining to the role of the facilitator in supporting interactions with music technology in the performance setting will be discussed. A special focus will be placed on the facilitator's inclusion in the design of future HCI tools for this setting.\nINTRODUCTION: Outcomes of this research add to existing knowledge regarding the difficulties for users with complex disabilities when using technology in creative music activities [5, 6] .",
    "relatedWork": "RELATED WORK: Motion sensing is a rich area of study in research of DMIs, however there have been notable issues with the use of sensing technologies as a control mechanism for sound output. These include complications with real time processing (in terms of continuous analysis and discrete event handling) [4] , issues with triggering discrete sounds with precision [3] , as well as a differentiation between actual and desired states of interaction [1] . The general accessibility of such controls is also difficult to discern, as prior research in this field has mostly focused on the highly skilled electronic musician creating custom gestural interfaces for their own personal/professional use [5] .\nRELATED WORK: In rehabilitation, DMIs and computer applications have been recognized as technologies that can help achieve increased motivation and active participation for users with complex conditions affecting their mobility [2, 17] . Enabling performance with music specifically has proven to be an area that provides strong potential to motivate those with complex disabilities. Gilberston and Aldridge highlight that \"through performing, patients would not only move away from isolation but towards a more active and creative self. \" [8] Gesture controls have been the preferred and most successful method of interaction for users with complex disabilities to date in fields such as music therapy. In 2008 the 'Soundbeam' (a device that triggers music based on breaking a beam of light with movements) was the single most used electronic music technology device in music therapy [11] . Despite this, access to musical activity for users with complex disabilities is still low when compared to the general population [14] .\nRELATED WORK: The complexity and diversity of individual requirements makes establishing a universal piece of technology for disabled users very difficult in terms of Human Computer Interaction (HCI) [4] [5] . Thus, disabled users who cannot already play adapted versions of existing instruments are often limited to \"accesible\" interfaces such as the Soundbeam [5] .",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": " we explore the following research questions: how do students and instructors use clmc to support and engage in learning activities?"
}
{
    "intro": "INTRODUCTION: Tangibles have been used in educational settings as they have the potential to support the learning of abstract concepts [31, 64] . One area of education where tangibles can be utilized to make abstract concepts more accessible is data visualization comprehension. Questions about teaching data visualization have become a frequent topic within the visualization community [8, 21, 59 ] and education policy [62] . However, learning to explore and make sense of data visualizations is a challenging task [5] . Appropriate tools could support students with visualizing, analyzing, and interpreting complex visualizations of data.\nINTRODUCTION: So far, methods of interacting with data visualizations have mostly revolved around the use of pen and paper or Graphical User Interfaces (GUIs). These tools support developing an understanding by perceptually examining the representations. In contrast, tangible representations of data, Data Physicalizations [19] , can be used to create Tangible User Interfaces (TUIs) [18] . These allow users to manipulate data visualizations more directly. We argue that these tangible representations of data can facilitate a better vehicle for users to grasp the information held in data visualizations than perceptive methods, because they enable a more engaging, embodied way of data exploration.\nINTRODUCTION: Investigations on the applicability of data physicalizations in educational settings are still sparse [48] . Some studies have investigated how data sculptures can play a role in university courses [57] . Other studies have used tokens to construct visualizations within a workshop context [14] or investigated how people map data values to physical representations [17] . However, these forms of data physicalizations are static and thus lack the educational benefits that interactivity and feedback can provide.\nINTRODUCTION: To expand on previous research on the design of data physicalizations for learning, this paper presents an exploratory study through qualitative evaluation of four teams of students interpreting scatterplots augmented by data physicalization using a portable Collaborative Data analysis system, namely CoDa (Figure 1 ). This system realizes a tokenbased interactive tangible scatterplot using an interactive liquid-crystal display (LCD) based on a magnetic tracking system [26] . Through manipulation of the physical tokens on the platform, the users can interact with the visualized data directly; moreover, they can use the control panel on the sidebar to change the modes of visual augmentation for descriptive statistical analysis and comparison across multiple graphs.\nINTRODUCTION: This exploratory study was conducted to answer the research question; \"How do interactive data physicalizations support the collaborative interpretation of interactive data visualizations?\" In the study, 11 students in groups of 2 to 4, completed a data analysis task with personalized data. The qualitative results, which were analyzed from the dialogues between the students and their interactions with the system, show that the participants can jointly develop an understanding of the data through iterative interpretation processes with CoDa. We report on the students' process of data interpretation, how the interactions with the system aided their interpretations and shaped their collaborative behaviors, and the students' experiences of working with CoDa. And lastly, based on the lessons learned from the design, implementation, and evaluation of CoDa, we present implications for further educational applications of interactive data physicalizations.",
    "relatedWork": "RELATED WORK Graph comprehension: Studies in the field of graph comprehension focus on how perceptual and cognitive processes influence the understanding of graphs in an educational context. Graph comprehension is an iterative process. When one switches between the data, the representations, and the underlying mathematical model of visualization, they \"..go through a process of invention, noticing, and revision that helps them develop insight into the relation between representations and the quantities they represent [46, p. 138] . \" Shah and Hoeffner [47] have identified three underlying critical points of a learner's understanding of graphs: understanding the meaning of specific visual representations, understanding of the graph structure, and understanding of the context of the data.\nRELATED WORK Graph comprehension: Interactive models can increase the reasoning strategies that learners use [58] , furthermore interactivity can support learning through the construction of mental models and hypothesis testing [40] . The users of interactive visualizations can develop inferences, test those inferences, and revise their expectations to evaluate concepts. Control over a visualization allows the user to decide what to evaluate next, in line with their interests and preferences.\nRELATED WORK Graph comprehension: Interactive data visualizations might help students with the interpretation of data. However, Laina and Wilkerson [61] showed that not all middle-school students were able to link overlapping information that the students gathered from interactive data visualizations. There has been an increasing effort on the development of educational tools that support reasoning through interactive graphs [3, 30, 37, 43] . For example, Alper et al. [3] developed an app that aimed to boost elementary school students' visualization literacy through a concreteness fading approach. M\u00e9ndez et al. [30] investigated whether top-down, as seen in iVolVer [32] , or bottom-up construction approaches aided creativity and learning with InfoVis tools. However, these works investigate digital tools instead of physical manipulatives, which can provide more sensory engagement, accessibility, and group collaboration experiences for learning abstract concepts [64] .",
    "rq": ""
}
{
    "intro": "INTRODUCTION: According to the media richness theory, the richest communication medium is a face-to-face dialogue [1] . It is superior to video conferencing due to a number of factors, including depth perception and media naturalness [15] . However, collaborative virtual reality (VR) systems have the potential to deliver an even higher eectiveness in communication, when the perceived naturalness of the medium will eventually reach that of a face-to-face conversation. In an indistinguishable VR, virtual face-to-face communication can be enriched by computer aided tools for brainstorming, design or other specic tasks [2] . One method of improving virtual face-toface communication is to use visualization cues to convey what each user in a collaborative environment sees, freeing up other media channels like verbal or gestural communication.\nINTRODUCTION: There are several implementations of visualization cues, ranging from line-of-sight indicators to full displays of a user's actual image inside the headset (video mirroring). To nd the most appropriate type of passive visualization cue for VR collaboration scenarios, we implemented three distinct universally applicable methods and compared them in a controlled study regarding task completion time, precision and error rate. The implemented visualizations are Object Highlighting, View Cones, and Video Mirroring [3, 4, 22] .\nINTRODUCTION: Each of these perspective conveying methods is a direct or indirect interaction form between user & user or user & environment. There are many possible perceivable interaction forms as Manninen [18] denes them: \"Interaction forms are perceivable actions that act as manifestations of the user-user and user-environment interaction. These forms are used to convey the actions of the user to oneself and to others. They enable awareness of actions by oering mutually perceivable visualizations and auralizations within the environment. In addition, the feeling of presence and the level of psychological immersion are increased due to the communication, co-ordination and collaboration aspects these forms bring forward. \"",
    "relatedWork": "RELATED WORK: According to M. R. Endsley, situational awareness is the ability to understand a complex situation and predict its future states in order to make decisions [6] . A high level of situational awareness supports decision making, enabling an actor to identify one or more correct Session 3: VR + Games and Learning courses of action. In collaborative environments, being aware of other users' status and activities is crucial to coordinate activity and avoid interferences [7] .\nRELATED WORK: As collaborating team members develop shared mental models [11] , their situational awareness grows [6, 7] . This in turn allows them to coordinate more eciently [5] . Stemming from this situational awareness, the team's reliance on verbal communication is gradually reduced [17, 23] , shifting the mode of operation to an implicit coordination [8, 9, 21] . This in turn lets team members utilize the verbal communication channel for exchanging additional information, or shift their attention to other cues from their working environment. While it is relatively easy to maintain social and positional awareness in co-located collaborative environments, remote collaboration proved to require a greater mental eort in regards to these types of awareness [13] . Thus, groupware research has focused on interface techniques that facilitate communication and increase group awareness and awareness cues [12] .\nRELATED WORK: In this context, Wuertz et al. [25] developed a framework to classify awareness cues in multiplayer games based on an analysis of games, game mechanics and game interfaces. They describe the information each type of cue provides, how it is communicated, and how awareness cue design choices can impact the user experience. Additionally, prior works on Workspace Awareness [12] and Gamespace Awareness [20] identied four types of information that team members use when collaborating, which also applies to virtual reality collaborative environments. The Who -Presence and identity. The What -Status, task and social structure. The Where -Location, positioning, and gaze. And nally the How -Communicating the way important events occur. By emphasizing the gaze and enriching its expressiveness through highlighting or other rendering techniques, this type of awareness cue can be used to improve the eciency of collaboration [3, 18, 22] .\nRELATED WORK: Regarding the visualization of awareness cues, Dodds and Ruddle [3] presented a suite of techniques that aim to improve the quality and quantity of communication in large-scale collaborative virtual environments (CVEs). Their social awareness increasing method, called 'group graph', which is an explicit hierarchical grouping system, visualizes who is currently talking to whom. Additionally, they addressed the diculty of users to understand what other users are currently looking at, by rendering small view ports as planes in 3D space that displayed their respective user's point of view. The suite of techniques was evaluated using an urban planning scenario, highlighting that multiple view ports increased the maximum distance of eective communication between participants.\nRELATED WORK: W\u00f6ssner et al. [24] proposed a collaborative volume rendering application, that mirrors the point of view from one CVE user to another in a master/slave conguration. They compared the tight coupling of complete master control over the point of view to a more exible approach, where the slave could change their rendered orientation, but not position. They concluded that the loose coupling approach was rated most comfortable and that user enjoyment was signicantly higher in this scenario. Sonnenwald et al. [19] found that users saw potential benets in both independent and shared perspectives, depending on the task and form of collaboration at hand. Lee et al. [16] compared three dierent view-sharing techniques for mixed reality scenarios of local users receiving support from an external expert. The real world view port of the local user is transferred either as a at 2D plane, a 360 projection on a sphere or a 3D reconstruction of the real environment. They concluded that transferring 3D reconstructions of the real world allowed the expert to give the highest degree of freedom in perspective, resulting in shorter task completion time and higher social presence.\nRELATED WORK: Trapp et al. [22] described multiple approaches to highlighting objects of interest and potential elds of application. They divided the applicability of object highlighting into the categories User Selection Preview, Visualization of Computational Results and Navigation Aid. Here, User Selection Preview refers to rendering a single selected object, which is in the user's focus, in a dierent style. Visualization of Computational Results refers to highlighting, grouping and clustering results from database queries or other computational selection processes. Lastly, Navigation Aid refers to highlighting points of interest, routes, as well as navigation way points in order to guide the user's attention. Trapp Knowledge about the user's visual attention can also be used to facilitate mediated human-to-human interaction in VR environments. Duchowski et al. [4] demonstrated this by applying the eye movements of a user onto a virtual avatar, and additionally showed advantages of a visible line of sight for the communication of points of interest. Gamer and Hecht [10] further highlighted the importance of understanding other users' view cones. They investigated the inuence of several factors such as observer distance and head orientation on user interactions and their view cones. They concluded that the processing of gaze cues plays an important role in social interactions, and mutual gaze in particular is relevant for natural as well as video-mediated communications.\nRELATED WORK: Manninen [18] compiled and described the top level interaction form categories in CVEs. These are avatar appearance, kinesics, occulesics, facial expressions, environmental details and languagebased communication. Avatar Appearance contain all forms of indirect interaction that is emanating from e.g. the clothes, hair, physique or adornments of the avatar while Kinesics describes postures, body movement, head movement or gestures. Because eye movement, eye contact and line of sight are such important aspects of interaction, they have their own category: Occulesics. Finally, the environmental Details category contains all forms of interaction with objects or the environment itself.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: A pivotal point that can popularize Virtual Reality(VR) is natural communication; even in VR spaces, the facilitation of natural communication will not only provide unusual experiences such as space traveling and exploring space but also encourage the transformation of lifestyle and social structure. Natural communication in VR spaces would efects several positive efects to attain a sustainable society, such as improving work-life balance by eliminating movement such as for commuting and business trips and reducing energy consumption for passenger transportation. An essential factor for natural communication in VR spaces include methods that help express nonverbal behavior. However, these methods to express nonverbal behavior of intentions and feelings that includes expressions, hand gestures, and gaze are still in the rudimentary stages of development.\nINTRODUCTION: The need for remote communication has increased dramatically with the impact of the corona virus(COVID- 19) . The current mainstream for remote communication are video conference systems such as Zoom and Skype and usage of them is growing rapidly. However, they have not completely replaced face-to-face communication, and there are still some situations where face-to-face communication is preferred. A reason for preferring face-to-face interaction is that video conference system does not express nonverbal behavior compare to face-to-face communication. Therefore, it is difcult to apply the video conference system in a creative problem-solving discussion such as brainstorming.\nINTRODUCTION: Avatar communication in VR space would communicate nonverbal behavior better than video conference systems, even though current afordable standalone VR devices are restricted with regard to expressions, gestures, and gaze.\nINTRODUCTION: A video conference system such as Skype provides symbolized icons to express feelings and intent, as shown in Fig. 1 . In online games with numerous players, known as MMO(Massively Multiplayer Online), a function called \"Emote\" is provided to make players' character express symbolized emotions. Social VR platforms such as VRChat and Sansar also implements a function called Emote to provide participants' avatar certain gestures [17] . Although the symbolic gestures using Emote are useful to express intent in VR spaces, very little research has been done to study the efectiveness of an avatar's symbolized gestures in expressing participants' sentiments and intent.\nINTRODUCTION: This study examines brainstorming in VR space to determine the efectiveness of symbolic gestures through avatars for expressing the participants' behavior and sentiments under three experimental conditions: with and without avatar's symbolic gesture compared to face-to-face interaction communication.",
    "relatedWork": "RELATED WORK: Related work in the feld of computer-mediated communication (CMC) investigates the merits of diferent communication mediums, such as audio, video, and face-to-face, from the perspective of collaboration. Issacs and Tang [6] compared three diferent media-audio, video, and face-to-face-to understand how videos are efective in remote collaboration. They note that video communication is more efective compared to an audio-only communication, as it provides nonverbal information such as nodding and facial expressions. However, they also note that video communication is not as efective as face-to-face communication because nonverbal communication cues such as body positions and eye gaze are comparatively less. Therefore, to investigate the benefts of video communication over face-to-face communication, advanced studies have been conducted to improve communication and creativity by deforming visual information.\nRELATED WORK: To provide an example, consider two studies on improving video communication. One study increases number of answers in brainstorming by increasing artifcial smiles on the display using image processing [10] . The other study increases closeness by deforming a partner's face expression by mimicking the expression of a user [16] . These studies suggest that artifcial facial deformation can evoke positive psychological efect. Deforming facial expressions is efective not only in physical humans but also on avatars [11] . The study also suggested that not only the expressions but also the symbolic gestures by an avatar have a positive psychological efect in users [12] . As mentioned above, even in video communications with limited nonverbal communication cues, deforming nonverbal information improves the quality of communication. Riva et al. [13] suggested that a VR environment with a sense of presence tends to exploit emotion more than other mediums. Therefore, research results in improving video communication, which deform the appearance and behavior of a user's avatar, can be more efective in a VR environment.\nRELATED WORK: A recently developed VR technology provides nonverbal information that are richer compared to video communications. The VR technology can be useful as an interface to support group discussions. Recent research in VR and psychology has shown that a sense of ownership of an avatar's body afects self-perception and behavior [2, [7] [8] [9] 19] . For example, Banakou reports that an avatar that imitates Einstein improves the cognitive capacity of a user owing to a sense of ownership of the avatar's body [3] .\nRELATED WORK: Many of these previous studies have examined the efect of an avatar's appearance and other expressions on a single user's task performance and psychology.\nRELATED WORK: Previous studies investigating the efect of avatar expressions on the communication between multiple users in a VR space include research by Smith et al. [15] and Yoon et al. [20] . Smith et al. [15] investigated the efect of avatar expressions on social presence and communication through an experiment using two workers in a VR space under three conditions: face-to-face communication, and with and without avatars.\nRELATED WORK: Yoon et al. [20] explained that avatars can provide a rich collaboration experience for participants, even though the cartoon-style upper body has environmental restrictions and is subject to collaboration between participants.\nRELATED WORK: These previous studies have examined the efect of diferences in avatar appearance on communication.\nRELATED WORK: Tanenbaum et al. [17] compared the design of nonverbal information across 10 social VR platforms such as VRChat and Sansar, They described that an avatar's preset animation, called \"Emote, \" is the most common expressive nonverbal communication in virtual reality. Although the symbolic gestures made using Emote are useful to express intent in a VR space, little research has been done to study the efectiveness of an avatar's symbolized gestures in expressing participants' sentiments and intent. Therefore, our study aims to investigate the psychological efects of symbolic gestures expressed by avatars.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Occupational therapists (OTs) support people with physical impairments and disabilities (PwIDs) to maintain their functional independence in daily activities at home through patient-centered interventions (e.g., assistive technologies [ATs] ). Home care OTs assess the living conditions of PwIDs in their homes and recommend modifcations to create safe and supportive environments via appropriate ATs (e.g., grab bars, digital home assistance). However, the process of searching, selecting, and purchasing compatible ATs for their homes is complicated, necessitating several home visits to assess disability levels and the interior design of the home, as well as several iterations of AT interventions by the OTs [14] . In addition, OTs need to clearly communicate the home-modifcation plans to PwIDs. Current OT strategies for dealing with these issues are limited to providing a demonstration with randomly available ATs, paper printouts, and online images (e.g., from the North Coast Medical or Amazon websites). However, this process can lead to the purchase of ATs that do not ft PwIDs' home environments as OTs do not have access to the whole range of ATs available from medical-supply companies for demo purposes. Without such demos of the ATs and using only 2D images as resources, it is difcult for OTs to recommend the best AT available in the market and/or PwIDs to imagine or speculate the application of ATs in their home. The inefectiveness of the process frequently generates noncompliance with ill-ftting interventions, which may result in PwIDs abandoning the ATs. So, involving PwIDs in observing and interacting with ATs is both crucial in assessing the compatibility of ATs with the home environment and in reducing AT abandonment [18] .\nINTRODUCTION: According to the World Health Organization (WHO), more than one billion people globally need one or more assistive products, and more than two billion people will need at least one assistive product by 2030 [37] . Without ATs, PwIDs are often excluded and isolated, which leads to an increase of disease and disability in a person [25] . Although ATs are crucial to improving people's lives, getting the appropriate ATs is very difcult. Currently, only 10% of people in need have access to ATs due to their high costs and a lack of awareness, availability, and customization [37] . Furthermore, 29.3% of purchased ATs end up unused or abandoned due to a lack of proper ft or insufcient consultation with PwIDs [28, 30] . Choosing the right ATs is exclusively the responsibility of OTs as they require to provide consultation and recommendation on purchasing appropriate ATs in a reasonably accurate fashion.\nINTRODUCTION: In this study, we designed and evaluated an AR tablet-based platform to support OTs in home-modifcation processes and in making necessary adjustments to PwIDs homes, by demonstrating and assessing the ft, look, function, and compatibility of ATs in diferent environments. In particular, the AR tool superimposes 3D models of ATs onto the home environment to support envisioning the most appropriate home-modifcation scenarios for purchase, installation, and use of ATs. Our approach to the prototyping was iterative: based on our initial ideas on related research into AR in homes and semi-structured interviews, we created a preliminary AR prototype using Apple's ARKit 3 and Reality Composer. We then presented it to two OTs in a codesign session to solicit feedback on the necessary features of the tool and further developed the AR prototype according to their feedback.\nINTRODUCTION: To investigate the design potentials of the AR tool, we conducted a series of comprehensive user studies to evaluate the usefulness and usability of the AR tool with ten OTs. All studies were conducted in the OTs' work environments. The fndings from the user studies revealed that OTs perceived the utilization of the AR benefcial because it contributes to their home care practice by (1) providing visual clues (ft, size, function, and appearance of ATs) in the home, (2) supporting home-modifcation decision-making processes, (3) facilitating a holistic home-modifcation approach, and (4) involving all stakeholders (e.g., OT assistants) in the homemodifcation processes. Our study contributes to the HCI community by (1) investigating the design space for AR that supports OTs in home-modifcation processes, (2) implementing the tool's proof of concept through an iterative design process and evaluation, and (3) informing the AR design for home modifcations and home care ATs based on common themes and design recommendations.",
    "relatedWork": "RELATED WORK: Our research is informed by prior work on AT customization tools, AR application in home interventions, and home health care and rehabilitation.\nAR in Home Interventions: AR blends the digital and physical worlds in such a way that virtual objects are superimposed onto the surrounding environment to create a new real/virtual scenario [22] . Previous research into the use of AR in domestic environments demonstrated high potential for deploying AR systems in homes [13] , and several applications have already been explored or developed for improving the experience of living at home. For example, an augmented kitchen with overlaid projections on objects can facilitate an interesting, safe, and accessible cooking experience, or an augmented home window can display information or facilitate personal and family communication [8, 36] . Other studies on AR for homes have examined the customization of interior design, allowing users to model their homes by manipulating (e.g., inserting, moving, and/or removing) 3D virtual furniture through an augmented image of the domestic environment [27, 29] or enabling a couple to explore the furniture design of their living rooms collaboratively [34] .\nAR in Home Care and Rehabilitation: Prior works have begun to employ AR in rehabilitation, but little research has been done on the application of AR in home care and home-modifcation processes that are crucial in supporting safety and opportunities for independent living for PwIDs. Eforts have been made to design AR for rehabilitation to prevent falls by improving the balance efcacy of elderly people [23] or by practicing functional movement through gaming [35] . Djajadiningrat et al. illustrated the challenges of patients in unassisted care at home and how an AR health application facilitates blood testing [16] . Luo has implemented AR in post-stroke hand opening rehabilitation using a customized glove [24] . Other eforts in rehabilitation have explored virtual reality (VR) applications to support PwIDs with ATs and smart homes [32] or to describe predischarge home visit processes with a mock template of a home [6] . Although VR technologies are useful in visualizing simulated home environments, they don't support real-time assessment in real home environments; only Bianco et al. demonstrated the benefts of AR in home modifcations for fall prevention in the elderly [7] .\nAT Customizations: An AT is any item, piece of equipment, software, or product system that is used to increase, maintain, or improve the functional capabilities of PwIDs. ATs are often either acquired commercially of-theshelf or customized to ft specifc needs [4] . The AT customization is usually done by OTs after initial clinical assessment of PwIDs' abilities and modifcation of existing ATs through do-it-yourself (DIY) processes [19] . While digital fabrication and 3D printing technologies can be used to create durable and functional products [11, 17] and support OTs in making custom-designed ATs [10, 19, 33] , the lack of materials, time, and access to training restrict what OTs can create [33] . The lack of OT skills in design/manufacturing is critical and reported as an industry skill shortage by the U.S. Department of Commerce.\nAT Customizations: Eforts have been made to develop 3D modeling software, which facilitates designing/modifying parameterized models by nondesigners [3, 12] , and to study open-source online communities that facilitate rapid prototyping [9, 26] . While OTs can potentially use these technologies to design/modify existing ATs, limited adaptability of 3D models, complexity of interfaces, and access to 3D printing tools are just some of the shortcomings [33] . In addition, these technologies do not support current home care practice, which necessitates several OT homes visits, working with PwIDs in their homes [15] , or printing larger home care ATs. While we are aware of the importance of custom-designed ATs, our proposed AR tool advances current occupational therapy home-modifcation practices, facilitating home assessments to propose the most appropriate of-the-shelf AT solutions.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Due to the proliferation of live streaming platforms such as Twitch, many artists and designers have begun to adopt live streaming to showcase their creative processes [8, 28] . These creative live streams typically feature the streamer's full screen of their computer or workspace (Figure 1a ), a camera feed of their face, along with their narration while working [8, 28] . For creative live streamers and viewers, live streaming enables anyone to learn and be inspired by a streamer's creative process, engage with each other during real-time shared creative activities, and connect with other creative professionals around the world [8, 28] . However, streamer-viewer interactions on most live streaming platforms today are confined to chatting with text and emojis [2, 27, 28, 52] .\nINTRODUCTION: One way to better engage viewers in a live stream is to enable multi-modal communication through audio, video, and images [2, 19] . These rich modalities have been adopted in various genres of live streams including video gaming [27] , live events [19] , and learning [2, 18] , and have been shown to effectively improve viewer engagement with live streams. However, video and audio may be intrusive and physically and cognitively demanding for both streamers and viewers, while image has limited flexibility [2] . The present work aims to explore how sketches and text can be incorporated into creative live streams to enrich viewer-streamer interactions. Sketching, as a primitive form of human expression, has been shown to support visual thinking to help organize one's thoughts and improve one's ability to think and communicate [1, 14] . Sketching is also a flexible form of visual communication that is familiar to creative streamers and viewers [8] . Integrating sketches and text into a live streaming interface may be worthwhile because multi-modal messages can convey richer information and enable new forms of expression during creative live streams. For example, a sketch can depict illustrative ideas such as composition and patterns, while such information is hard to convey through text only. Although sketching may be a more natural method of viewer-streamer interaction within creative live streams, design challenges and affordances of such modalities are relatively unexplored within the live streaming literature.\nINTRODUCTION: Through a formative study with creative live streamers and an analysis of available visual interaction modalities for streamers to engage viewers, the present work found that text-only chat and emojis impose constraints on what viewers can convey and share with streamers and other viewers in real time. For example, when trying to ask specific questions about the content in a live stream, viewers could not accurately or conveniently make reference to objects or points of interest. When making a suggestion to streamers about a drawing, viewers can only describe it textually, rather than visually, with illustrative details and examples. Based on the results of the formative study and the analysis, we designed and built a system, StreamSketch, as a technological probe.\nINTRODUCTION: StreamSketch enables viewers to engage with streamers through multiple modalities of input (i.e., text and sketch), and aids streamers and viewers in making sense of this rich data. With StreamSketch, viewers can sketch and place text comments directly into the live stream video to convey their thoughts, suggest ideas, express opinions, and annotate the video (Figure 1a ). Sketches and texts can also be anchored both spatially and temporally. These messages appear in a chat window (Figure 1b ) as part of the chat conversation and are visible to the streamer and all of the viewers. Since the amount and complexity of multi-modal messages can be overwhelming, viewers can tag their text and drawings to facilitate filtering and clustering for increased visual clarity (Figure 1d ). To provide an overview of the multi-modal chat conversation, StreamSketch displays a Hybrid Cloud visualization below the live streaming video that visualizes keywords from viewer comments and relevant sketches in real time (Figure 1c ). To provide richer and more detailed contextual information to the streamer and their viewers, StreamSketch displays sketches and text alongside relevant contextual information (i.e., a snapshot of the video and position of sketches).\nINTRODUCTION: To better understand how the use of temporally and spatially anchored sketches and text facilitates new types of interactions and degrees of engagement between a streamer and their viewers, and how streamers and viewers navigate such rich, multi-modal data during live streams, we have conducted a user study with two professional creative streamers and their viewers (N=18) for six live streaming sessions that lasted up to an hour each. The findings demonstrated that viewers used sketches for interactions that were more relevant to art or self-expression and largely used text chat for socializing or non-art topics. However, the time required by viewers for ideation and sketching during the stream made sketching challenging for some. Both streamers and viewers also reported increased engagement while using StreamSketch.\nINTRODUCTION: This paper contributes:\nINTRODUCTION: \u2022 insights into the needs for supporting multi-modal interactions in creative live streams drawn from the literature, an analysis of existing tools, and formative study with streamers \u2022 introducing spatially and temporally anchored sketches with text and labels \u2022 the design of StreamSketch, a system which enables viewers to sketch over top of a live stream video while watching the live stream and supports the sense-making of multi-modal input from viewers \u2022 evaluations with real users (i.e., streamers and viewers) to test the usability and provide more insights into future design of multi-modal viewer input within live streaming 2 RELATED WORK",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": " 1) what challenges and barriers are chi 2020 paper chi 2020, april 25-30, 2020, honolulu, hi, usa there to inclusive play experiences for visually impaired and sighted children in mainstream schools?\n 2) how can we codesign inclusive play technologies with participants who have mixed visual abilities?\nwe started our investigation by addressing the first research question: what challenges and barriers are there to inclusive play experiences for visually impaired and sighted children in mainstream schools?\n we aimed to address the second research question: how can we co-design inclusive play technologies with participants who have mixed visual abilities?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Head-mounted displays (HMDs) provide the wearer with a highly immersive virtual reality (VR) experience, allowing one to feel present in a Virtual Environment (VE). While a high sense of presence is key in completely virtual experiences, there are several use cases where one wants to be immersed but also communicate or interact with people in the same physical space (e.g., lectures, technical demonstrations, public presentations, and supervised training.) These are asymmetric usecase scenarios: the immersed user has a more active role, and non-immersed users can be considered spectators with limited affordances. In such cases, giving the spectators complete freedom is not desired. While allowing all users to have their own HMD is an option, these non-immersed users may have tasks that require them to be able to see both the virtual and real-world environments without wearing a headset. These include taking notes, capturing images, operating other equipment that is part of the experience, or engaging in communication with other non-immersed users. In asymmetric use-case scenarios, we must provide an asymmetric option for sharing the virtual experience.\nINTRODUCTION: Display mirroring does not give the spectator any control over what they view. Allowing the spectator to shift their focus away from what the VR user is looking at could be very useful for demonstrations and collaborative scenarios -they would be able to focus on features of the virtual environment, while still being co-located with the immersed VR user to facilitate communication. It restricts the spectator to verbal communication with the HMD user, having no way to visually communicate or point out objects of interest, which is highly detrimental in interactive scenarios between both parties. Display mirroring can also have undesirable effects on the spectator's experience: rapid head movements and rotation from the HMD wearer are not pleasing to watch and may induce simulator sickness [25] . Additionally, a regular display does not have any of the immersive qualities of an HMD. This makes it difficult for spectators to understand what the HMD wearer is experiencing since they are not themselves immersed -a common problem in the VR industry when attempting to communicate the VR experience to people not wearing an HMD.\nINTRODUCTION: In this paper, we evaluate the impact of interaction tools (pointing and camera rotational control) for a VR spectator using a large-scale tiled video wall. We propose a spectator system approach that combines these two, allowing a non-immersed person using the video wall to follow the experience of an immersed HMD user, as well as look around and communicate with them. We performed a user study evaluating the impact of each individual tool on both task performance, user preference, and other aspects of collaboration.\nINTRODUCTION: Previous works on collaboration between immersed and non-immersed users were designed for constrained scenarios such as specially designed games [14, 15, 38] . Our approach targets generic asymmetric collaboration/presentation scenarios where users have different affordances and tasks, but need to be able to share their experience, sense of immersion, and communicate about the environment (through the use of pointing gestures).\nINTRODUCTION: In summary, our contributions are: (1) A novel approach for asymmetric collaboration between an HMD wearer and spectators, (2) a user study that analyses the collaboration between spectators and HMD wearers, and (3) a system implementation of our approach covering display arrangement, spectator controllers, user interface, and a motion filter for the virtual camera.\nINTRODUCTION: Our results show that users preferred the approach combining both tools. It positively impacted the spectator's perceived immersion, improved communication between users, and allowed them to explore the environment independently.",
    "relatedWork": "RELATED WORK: This section addresses the relevant research on interacting with large-scale displays. We introduce the concept of large-scale displays, their immersive properties, and their role in VR technology. We then address the related work on interaction with VR content for these types of displays. Finally, we discuss collaboration, present the concept of asymmetrical collaboration and highlight the significant problems that this paper addresses.\nLarge-Scale Displays: Large-scale displays are used because they elicit a stronger sense of spatial presence than smaller screens [49] and are often compared with HMDs due to their immersive qualities [28] . They allow multiple co-located people to be immersed in VR together. Many different forms of largescale displays have been developed, including highly immersive displays such as CAVEs [7] and large-scale domes [29] . Compared to HMDs, these screens offer users similar presence and spatial awareness levels [42] with reduced cybersickness side-effects [45] .\nLarge-Scale Displays: There are many potential input methods for interacting with large-scale displays. Prior studies have examined the use of mobile phones [2] , tablets [39] , gloves [13] , and gestures to select and manipulate 3D objects at a distance [32] . Input methods used alongside HMDs often employ a high level of motion tracking (up to six degrees of freedom (6DoF)) to capture the user's movement in as much detail as possible. This can increase the user's sense of presence in the VE [8] . Prior work have used motion controllers in non-HMD VR contexts to directly manipulate objects [4] and control the camera [51] . These studies also show that motion controllers need special consideration in implementation regarding the display type and the task being completed in the virtual environment.\nLarge-Scale Displays: In an HMD, the headset is used as the input device to control the camera [51] . There is a wide range of possible camera control methods for large-scale displays. Neng & Chambel [40] presented a dragging method for looking around in 360-videos, which has since been implemented in 360-video players on popular websites like Facebook and YouTube. Pavel et al. [43] presented a method for ensuring viewers can see important parts of a video. Users can press a button to be re-oriented in the direction of important content.\nLarge-Scale Displays: Previous works have investigated the use of pointing gestures as an interaction method [24] , and have shown that different pointing techniques are effective at accomplishing different tasks. Pointing metaphors [51] differ in the method used to calculate the direction of the ray. In the image-plane technique, the virtual ray is cast from the user's eye and through their hand. In the laser metaphor, the 6DoF motion controller's position is used to calculate the intersection with the screen like a virtual laser pointer. Both metaphors are similar in efficiency, but the laser metaphor is better when dealing with multi-user scenarios, as the image-plane technique only looks correct to the person holding the controller [48] . For this reason, we used a similar metaphor to represent pointing gestures using a large-scale wall display.\nCollaboration with Large-Scale Displays: Large-scale displays allow multiple people to interact and collaborate in a 3D space. Notable applications include design reviews in virtual environments [23] and virtual training scenarios [12] . Users can engage in a collaborative view of the VE, where multiple users can edit [52] , comment, and collaborate. The large-scale display can present a first-person view of a single user [46] or an outside view, where participants can be seen at the same time.\nCollaboration with Large-Scale Displays: The asymmetric nature of such systems has been recently explored in HMDs with immersed and non-immersed users [6, 16, 21, 34] . These systems use projections on the floor or tabletop [14, 15] to provide the non-immersed user with a view into the virtual world. These techniques improved the users' effectiveness at completing tasks and provided them with a sense of co-presence. Prior work has also enabled collaboration that converges users in VR and AR. A recent example is the Augmented Virtual Teleportation (AVT) system [44] , where users can remotely collaborate using a 360\u00b0camera to link two separate spaces, enabling a remote VR user to collaborate with a local AR user. A coupled view has also been proven effective in remote assistance tasks between VR and AR users [41] . Asymmetric systems have also been configured with AR HMDs in collaboration with conventional displays [18] .\nCollaboration with Large-Scale Displays: Asymmetric collaboration is also possible between different display surfaces, such as the one described by Kunert et al. [26] . This system enables collaboration between a stereoscopic wall and a tabletop surface, which allows people to share the same reference space by using pointing gestures [26] . An important aspect of this is the users' need to be aware of each other's actions in order to enable effective communication and collaboration. ShareVR [15] tries to solve this problem with the use of projected floors and a screen mounted on a 6DoF controller, which enables the non-immersed user to glimpse into the shared 3D world of the HMD user. ReverseCave [22] enables spectators to observe the HMD user's experience. Alternatively, other recent work [6, 16] allows HMD users to collaborate by placing touch screens that are mounted onto the HMD.\nCollaboration with Large-Scale Displays: However, none of these studies provide freedom of interaction as well as immersion to the non-HMD users. Collaboration with spectators has also been considered in augmented reality scenarios [19, 27] , where users wearing AR headsets are able to share virtual information via tabletops, vertical displays, and point-of-view video streams to collaborators. It has been shown that sharing and exploring 3D data through different display types and tracking methods has a positive effect on task performance and co-presence. However, no prior work addresses the specific scenario of multiple co-located people using a large display alongside an HMD wearer. In this scenario, users may need to perform tasks which that require them to have an overview of elements in both real and virtual environments and allow for richer non-verbal communication, which is something that an HMD alone may not always be able to provide.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Interpersonal touch is one of the most infuential nonverbal communications for human social behavior. One wellknown phenomenon, called the Midas touch efect, has been demonstrated by Crusco and Wetzel [7] , where servers in a restaurant were instructed either to touch or not to touch customers while returning their change. The researchers observed that the tipping rate was signifcantly higher in the with-touch condition than in the without-touch condition. A similar efect of touch on economically generous behavior has also been observed in another situation [17, 30, 35] . In addition, touch also modulates the tendency to comply with a request [18, 21, 31] , and help/collaborate with someone [19, 20, 36] . While there have been a variety of discussions over the efect of awareness of touch on behavior, Joule and Gu\u00e9guen demonstrated that participants who had noticed touch tended to comply with a request more than those who had not noticed [32] . In addition to the social behavior modulation efect, touch ofers various benefts and positive efects regarding nursing [5, 28, 56] , development of children [8, 11, 54] , etc (see [12, 14, 50] for an overview of interpersonal touch).\nINTRODUCTION: We are interested in quantitatively measuring interpersonal touch interactions in daily life to analyze and facilitate the efect on social interactions. Our challenge is not only to detect physical contact between two people but to also analyze contextual information from touches, such as when, where, who, and how a human touches.\nINTRODUCTION: Similar to the rapidly advancing smart wristbands that are capable of logging the users' personal activities such as the number of steps taken, we design the device to be Bluetooth-connectable to increase its applications fexibility. This allows human-human interaction designers such as social psychologists, social game developers, or therapists to monitor, log, analyze, and augment interpersonal touch interactions in daily life.\nINTRODUCTION: This paper presents EnhancedTouchX (ETX), a bracelettype interpersonal body area network (interpersonal BANmore specifcally, interpersonal hand area network) device that is capable of identifying the direction and gestures of an interpersonal touch (Fig. 1 ). Here, we defne the direction of a touch as one hand actively touching another that is passively being touched. Although we expect the direction and gestures to serve as key conditions in the research feld of interpersonal touch interaction, it is out of the scope of this paper to study how these two modalities afect social interaction.",
    "relatedWork": "RELATED WORK Touch Sensing: In searchers have developed techniques to measure bare skin to bare skin contact using sound/vibration processing techniques [27, 37, 58] , computer-vision/photo sensor techniques [26, 38, 57] , and capacitive sensing techniques [46, 59, 60] . These techniques are primarily used to identify the mode of the touch within a user (not with another person) to enrich inputs into a computer.\nRELATED WORK Touch Sensing: Conversely, we are aware of relatively few studies regarding the measurement of interpersonal touch interactions and their applications. There are systems that detect handshake when observing the similar oscillating profles of acceleration values obtained from a pair of a user's smart wristsbands [25, 55] . While the method is promising to detect a handshake, it is difcult to measure another mode of touch because it does not directly detect skin-to-skin contact.\nRELATED WORK Touch Sensing: Another method is to actively apply electric voltage to a human body [3, 34, 40] . Canat et al., for example, applied a swept frequency capacitive sensing method that is a capacitive sensing technique for detecting diferent types of touch patterns [46] to facilitate interpersonal touches between two players in designing a collaborative video game [3] . These sensing techniques are promising for measuring bare skinto-skin touches and in demonstrating the positive efects of interpersonal touches on social interactions. However, the systems require a wired-connection either to a large size of ground planes or to the earth for stable measurements, which would be a problem for certain wearable applications.\nRELATED WORK Touch Sensing: Zimmerman proposed the concept of BAN (also known as the personal area network, intrabody network, body channel communication, etc.) that uses a human body as a communication channel to expand the freedom of designing devices attached to the human body [61, 62] . In BAN, a modulated signal propagates between a transmitter and a receiver through the human body. The communication is only established the feld of human-computer interaction research, re-when the human body comes in contact with or in proximity of both the transmitter and the receiver. This can be used for touch/proximity sensing and for communication between the devices. BAN technology can be categorized into two types [47] : capacitive coupling (electric feld) type [2, 45, 61] and galvanic coupling (electric current) type [24, 33, 53] . The former does not require the user to directly attach the device (electrode) on the skin, because the electric feld induced around the surface of the human body is used as a communication channel. Conversely, the latter performs communication via electric currents in the human body, and the user has to attach the device/electrode on the skin.\nRELATED WORK Touch Sensing: BAN applications have been explored in the feld of humancomputer interactions involving user identifcation and localization. DiamondTouch is an interactive tabletop surface, where the touch location is determined independently for each user [9] . A set of transmitters is embedded in the tabletop and generates a location-dependent signal while a receiver installed in a chair receives the signal propagated through a user's body. Grosse-Puppendahl et al. demonstrated ubiquitous interaction facilitated by BAN technology [15] . In addition to a device ID, the acceleration values, for example, are sent, which expands the freedom of interaction design.\nRELATED WORK Touch Sensing: BAN technologies are also implemented into the bracelet device. Touching a peripheral device or a touch screen, a user is authenticated by a computer [29, 41] . Varga et al. developed TouchCom, a BAN platform, and explored the interactive infrastructure including bracelet devices [51, 52] . However, it can also detect proximity and should be difcult to discern contact and proximity because TouchCom employs capacitive coupling type BAN. In addition, it is a signifcant challenge to implement stable communication in wearable devices owing to the lack of common ground. One solution is to enlarge the size of the ground plane although it also enlarges the size of the device to enhance capacitive coupling between the device ground and the earth. Another solution is to use a sensor with exceedingly high input impedance to detect small displacement currents, although it requires a special component, such as [13, 48] . For an overview of capacitive sensing and the importance of ground connections, refer to Grosse-Puppendahl et al. 's review paper [16] .\nRELATED WORK Touch Sensing: Considering the measurement of interpersonal touch interactions, we employ galvanic coupling BAN technology because a channel is established only when two users touch each another.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Autism Spectrum Disorders (ASD) is mainly characterized by deficits in the development of socialization and communication skills, particularly impairments in social-emotional reciprocity and nonverbal communication such as gestures, eye contact, and facial expression [1] . Individuals with ASD have a range of cognitive and affective defects, resulting in individuals' difficulty in recognizing perceptions in themselves and others [2] . Although children with ASD may understand and recognize basic emotions, they still have difficulty in understanding more complex emotions in both themselves and others [3] . In addition, among the most characteristic early symptoms of ASD are atypical eye contact and joint attention, which profoundly impair the development of social-communication skills [4] . Several eye-tracking studies of young children with ASD illustrate an emerging consensus that detailed characterizations at the level of eye movements in response to fixating and tracking visual stimuli are important [5] . Children with ASD typically have behavioral difficulties that suggest problems with visual attention; it is unclear whether this attention deficit causes the other symptoms of ASD or is a consequence of the disorder [6] . Research [7] has indicated that children with ASD might be missing crucial nonverbal indicating behaviors, which likely causes them to have difficulty with recognizing or understanding nonverbal gestures and social cues, like facial expressions, that usually aid social interaction. Missing these cues generally has a negative effect on their social interaction skills and the flow of their communication [8] because people with ASD cannot interpret other people's facial expressions and emotional states, or understand the intentions and internal activities of others [9] . They also cannot respond with appropriate gestures, postures, or proximity [10] -a defect that researchers have called the ability to empathize: the skill to view things from other people's perspective and to understand the mental states of others [11] . Therefore, children with ASD, who normally pay more attention to inanimate objects than to faces, need to be taught specific verbal and nonverbal indicating behaviors involved in social interactions, and must learn to pay attention to the faces of people they meet and talk to, in order to understand social emotional behavior [12] [13] .",
    "relatedWork": "RELATED WORK: Blum-Dimaya et al. (2010) reported that using facial pictures and video training taught children with ASD to develop social communication skills and to focus on the specific visual representation and facial cues to judge others' emotions [14] . Some researchers believe that people with ASD have an impaired ability to understand complex emotional and social information from facial stimuli [15] . Facial expressions are a key determinate of nonverbal cues in social development and the ability to interact with others [16] . Moreover, understanding social situations requires paying attention to other people and to the subtle social cues they generate. However, people with ASD lack these abilities, especially an intuitive awareness and ability to judge the facially expressed emotions of others.\nCurrent Training Methods for Autism to Understand Emotional and Social Information: Video modeling (VM) has been used to teach pretend play to children with ASD [17] . VM can provide the video for children with ASD to mimic and pretend character's gesture and facial expressions [18] . For example, Axe, & Evans, (2012), using the VM strategy to train 3 participants with ASD to respond to eight facial expressions [19] . Probes consisted of showing an adult performing these facial expressions in a video, and they conducted generalization probes across people and settings. Training was showing a video of an adult modeling a response to each facial expression. They reported that two participants' correctly responded to all facial expressions across people and settings after viewing the video models one or two times. Thus, evidence shows that video-based interventions (VBIs) such as VM have been therapeutically effective for teaching functional, social, and behavioral skills to children with ASD [20] [21] . VM entails showing the participant a video segment demonstrating how to perform a task or behavior [22] . The participant is expected to learn by observing the instructional video segment and repeating the modeled behavior [21] [23] . VM has been effective for teaching a variety of social skills: (for example, social initiation [24] , social language in play situations [25] , social engagement [21] , and expressive behaviors [23] , such as intonation and facial expressions [24] .\nPrimary Problem with Current VM Training Strategy: Although VM are advantageous for promoting the motivation of children with ASDs to learn, the children still have difficulty dynamically adjusting the size of their attentional focus and switching the locus of their attention [25][26] [27] [28] [29] [30] , especially in patterns that include dynamic, repetitive, or social stimuli [6] . In addition, VM is not enough anymore in the interaction between user and learning content, because VM materials are generally too long and too difficult for children with ASD to handle; therefore, they always feel bored and helpless when watching a video on a tablet computer or TV. Consequently, they tend to stop watching long films because they have no interactive or linkage mechanisms and they lack visual-guideline feedback to help them construct and split them into different context scenarios [31] . Those problems might prevent VM from providing effective help for children with ASD, for example, a lack of progress, no reinforcement of attentional maintenance, lengthy and complex video content, or no prerequisites [32] . Thus, researchers need to create more effective visual strategies for VM, which requires a structural visual indicator and an appropriately limited visual area to help children with ASD increase focus their attention on the meaning and social value of perception judgement in specific social situations [33] .\nBenefits of Stop-Motion Video (SMV) as Visual Indicator Combined with Continuity Sketches: In this study, we freeze image targets in specific social cues to create SMV focuses on only few moments in a short video clip. The SMV strategy provides more effective visual strategies for VM used; it enables children with ASD to focus their attention on the specific elements in a video clip; Thus, it can be used to help them increase and drive their attention toward the meaning and emotional value of facial expressions in specific social situations. In intervention training phase, we provide the interactive game application for autism to play (we call this game \"Find Out\"); we use the SMV materials as a visual indicator and prompt the children to compare and observe the same images pattern occurring in the dynamic videos (DVs). The game design of Find Out can encourage children with Autism focus their attention to capture those social cues. Other scholars also have suggested that a relatively visual indicator which had effectively attracted and maintained the attention of children with ASD to nonverbal social cues and helped them better understand the facial expressions and emotions of the storybook characters [31] .\nBenefits of Stop-Motion Video (SMV) as Visual Indicator Combined with Continuity Sketches: In addition, the participants also completed a continuity sketch tasks to draw those social cues into a storyboard which allows the therapist to deduce patterns of judgment and awareness of the situation; Moreover, drawing task can help in self-expression, color recognition, coordination, focus, understanding boundaries etc. and a lot of other\u2026of self-development benefits [34] . In addition, having the child draw a picture of the scene can also provide a venue for structuring and enhancing play with children with ASD and gives them a sense of mastery, as well as increasing their pleasure and their motivation to play [35] . Moreover, drawing can provide real-time visual feedback and promote awareness to role and scenes' features visualization, which is related to ability to empathize, they can also reduce learners' cognitive loads and increase their interest in being trained [27] [36][37] . This process can enhance their attention with their perceptual and mind construction and also clarify their ideas of the whole scenario development. Simultaneously, the continuity sketches can guide children with ASD to express what they understand and feel in an interesting way.\nBenefits of Stop-Motion Video (SMV) as Visual Indicator Combined with Continuity Sketches: Although previous studies have examined a variety of instructional strategies using VBIs, such as VM, for teaching social-emotional awareness skills, few studies have evaluated the potential of multisensory teaching strategies using VM technology combined with hand sketches to facilitate social emotional ability in children with ASD. Moreover, previous studies state that to use VM to teach children with ASD is not suitable because these children's spontaneous visual attention deficit lead to their difficulty dynamically adjusting the size of their attentional focus on those social cues [25] , we need to create more effective visual strategies for VM, which requires adding a structural visual indicator combined with appropriately VM strategy to help them. Thus, in this study, we used SMV combined with continuity hand sketches (SMV-HS) of social cues into a storyboard training strategy to promote perception judgments and situational awareness in children with ASD. The continuity hand sketches can guide children with ASD to express what they understand and feel in an easy and interesting way. Furthermore, continuity sketches method can help integrate the senses, such as focus attention, understanding situation boundaries, kinesthetic learning experience, and watch scenes change. In addition, continuity sketches can facilitate sensory integration because hand drawing with the visual feedback can enhance their sensory awareness [38] , and especially they can pay their attention focus on the specific drawing part (e.g. facial expressions, body movement, it can simplify social stimuli, and help pay direct attention to the most relevant features and areas. Other research also suggests continuity sketches, drawing or through sand-play [39] can sustain children with ASD to collaborate and communicate in new ways; it can help children with ASD to learn how to observe and capture those social cues [40] .\nThe Goal of This Study: The present study extends and differentiates itself from previous studies that use VM strategies to promote children's social emotional awareness skills. In terms of the responses of children with ASD to train by SMV-HS strategy, it was presumed (1) SMV as visual indicator adding into the traditional VM can enhance the children with ASD to observed and describe the scenario's development and at the same time also improved judgment of perception of others; (2) SMV materials as visual indicator offered fixed and specific social cues of structured images for participants to focus, hand drawing and decoding the scenario story develop can increase the comprehension of understand situational awareness in the story; (3) the benefit of multisensory teaching method for children with ASD might can bring beneficial opportunities that be used to help children with ASD increase focus their attention on the meaning and social value of interaction behavior in specific social situations while they are drawing coloring specific part of scenarios figure. To date, the goal of this study was to create an SMV material for children with ASD that will catch their attention by signaling through body gesture movements and facial expressions. The participants will draw these social cues into a storyboard in which the therapist can be able to deduce patterns of judgment and awareness of the situation.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Urban communities arise as a result of a complex interplay between the material and the interpersonal: \"Social and physical environments do not exist independently of each other; any environment is the result of continuing interactions\" [13] . The social both drives and is driven by physical arrangements: \"space organisation is born social, constituting as well as representing society\" [32] . Accordingly, designers of public space now prioritize providing opportunities for collocated interaction: \"Mainstream urban design theory and practice are explicitly prosocial: the importance of socializing in outdoor public spaces is promoted\" [55] as \"the city of the twenty-first century is being reimagined as a site of connection\" [64:324] . A \"convivial turn\" in human geography and planning [48] , has highlighted the importance of semi-public spaces such as cafes and markets as 'sites of civic promise' [4] that provide opportunities for contact with unfamiliar others and a sense of togetherness and potentially \"incubate convivial multiculture\" [1] .\nINTRODUCTION: However, in both public and semi-public spaces, merely being collocated \"does not equate in any simple way to social encounter\" [63] . Even when there is a mutual willingness to interact, initiating or engaging in social interaction with a stranger is difficult for many individuals [15] . Successfully approaching strangers in public places necessitates considerable skills [38] [45] as individuals are generally reluctant to engage with unfamiliar others. While in some cultures or communities there are conventions of small talk or mingling, in some others people might lack the cultural acceptance or practices to do this. In addition to increasing the sense of community in neighborhoods, initiating a conversation with a stranger is considered a desirable goal in various other communities of collocated people, e.g. work places and schools (e.g., [2] ) and also innovation activities [53] .\nINTRODUCTION: Practitioners and researchers from many fields have proposed a wide variety of interventions [62] , gadgets [51] , installations [29] , objects [49] , and apparel [41] to support initiating interactions between co-located people. However, a systematically presented collection and analysis of existing social icebreaking design examples appears lacking. There seems to be little understanding of the design space as a whole, not to mention its key dimensions and variables. The lack of design theory easily results in various unconnected design cases lacking proper reflection. One symptom of lacking a holistic perspective is how similar design concepts appear to independently re-occur in different branches of culture and knowledge. In particular, technology-led research experimentation appears disconnected from DIY folk solutions and deliberately provocative concepts presented by artists and experimental designers.\nINTRODUCTION: As in any design practice, efforts to address this challenge would benefit from general patterns, heuristics, and other conceptual tools that help define and conceptualize the design space. In an attempt to bring together and systematize widely scattered design cases, we have been conducting a design space review examining diverse examples of both high and low-tech efforts for fostering new encounters between strangers. The contributions of this paper are largely conceptual and comprise the following three aspects: 1) we present five candidate patterns derived from our design space review, particularly focusing on signaling social intentions and impediments that prevent people coming closer and acting together. 2) We present an evaluation of these candidate patterns based on a series of focus groups and expert interviews. 3) We offer a synthesis of this evaluation through proposing three general design principles concerned with supporting the initiating of new encounters.",
    "relatedWork": "RELATED WORK: We argue for our work's relevance firstly regarding frameworks on designing for collocated interaction and then a need for further theoretical glue and practitioner-friendly design insights.\nSocial interaction design frameworks and principles: Martin Ludvigsen's design framework is helpful for articulating stages to build social collaboration [40] . He argues that in order to develop installations to bring strangers together, designers should think in terms of how people can be induced to progress through four different phases of co-located experience from an initial level of distributed attention upwards to increasingly higher levels of shared focus, dialogue and collective action. Ludvigsen contributes the concept of situational interaction mobility to express the degree that people can choose to move between these different levels of mutual attention [40] .\nSocial interaction design frameworks and principles: In what might be seen as an echoing of Hindmarsh's advice to developers of interactive multi-user museum exhibits to \"consider the relationship between action points and viewpoints\" [30] , Eva Hornecker et al. draws attention to the importance of designers providing \"minimal barriers\" in respect of \"entry points\" and \"access points\" to a system in order to induce sharing behaviors around, and through interfaces [32] .\nSocial interaction design frameworks and principles: Olsson et al.'s [50] recent systematic review of information technological solutions for enhancing collocated interaction proposes a hierarchy of design goals and approaches to be inspired by. They focus on conceptualizing enhancement in this context and indicate that it has taken various forms in prior design explorations: from inviting interaction through providing information that might spark social interaction to facilitating interaction by IT applications serving as icebreakers or tickets to talk, and encouraging interaction by motivating people to interact or engage in joint activities. While their review provides a solid conceptualization of a broad research and design space, its focus was mainly on categorizing prior ICT-based solutions, and could not reach strong conclusions on which kinds of designs work better than some others. In contrast, this paper focuses on physical forms of interaction design as interactive technologies are often considered counterproductive to conviviality. In order to offer insights for the designs of tomorrow, we inspect a mix of relatively low-tech and no-tech design examples. We would argue for the value of their relative simplicity as a stepping stone to understanding the ever-shifting complexities of designing for how people interrelate.\nAccessible and usable abstractions: Hornecker [34] and Mueller [46] among others, have called for human computer interaction guidelines and frameworks for whole body interaction to be presented in ways that are accessible for practicing designers. Furthermore, as designing to support social encounters can involve and impact upon many different kinds of stakeholders, we sought a non-academic format for ultimate dissemination of our design space review. Thus we turned to the notion of inspirational design patterns that emerged from design patterns (as explained previously in [42] and [43] ).\nAccessible and usable abstractions: Design patterns capture how recurring design problems are commonly addressed through generic, re-usable, and structured descriptions of typical solutions. In the words of the architect Alexander who originated the concept: \"Each pattern describes a problem\u2026and then describes the core of the solution\" [3] . Many researchers and practitioners within computer science and design have adapted and extended this idea, proposing, for instance design patterns and pattern languages for challenges faced in Human-Computer Interaction [17] and interface design [61] .\nAccessible and usable abstractions: Our work relates to Hespanhol and Dalsgaard's identification of how recurring social interaction patterns of media architecture lack cross-case analysis [28] . The patterns they present are very useful as a systematic review of how general social interactions can unfold around media installations. We focus on the narrower challenge of understanding strategies for supporting collocated interactions between strangers but take a wider scope in the cases that we examine. We are also influenced by the idea of Embodied Facilitation that highlights how group behavior can be influenced by different configurations of physical objects and space [34] .\nAccessible and usable abstractions: Design patterns and pattern languages are typically encapsulations of well-established design conventions [3] . The design knowledge captured in architectural and urban design patterns draw upon sometimes as much as hundreds or thousands of years of built environment traditions. For making accessible the insights gained via comparing diverse digital-physical practices, we prefer L\u00f6wgren's concept of inspirational design patterns [39] Inspirational design patterns attempt to fast-track insights from experimentation with new technologies into accessible formats that may help \"broaden the repertoire of the interaction design community\" [39] . We deliberately draw upon unproven, experimental and provocative designs. Although the \"solutions\" they suggest are often unwieldy, they can offer a vivid articulation of a problem and a fresh perspective on addressing it. The strong reactions they garner can also help unlock unarticulated needs, wishes or fears. Posing a variety of approaches is also hoped to stimulate new thinking by enabling comparisons between qualities of designs and open up for possible combining of different elements. Contextual factors are hugely important for the success of any social catalyst design [7] [27] . Our intention in developing inspirational design patterns is not to provide prescriptions but rather stimulus that a designer can draw upon in relation to their own particular design challenges, constraints and contexts.",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Understanding early childhood development has been a subject of increasing interest over the past several decades. Early childhood has been shown to be the most important time in development [9, 21, 28] . New methods and techniques have been used to study development in children with a variety of different educational philosophies [33, 34] . Our work presents a system to help educators and researchers develop a deeper understanding of early childhood learning in the Montessori classroom.\nThe Montessori Method: The Montessori Method is an educational approach that emphasizes independence and respect for a child's natural development process [19] . Montessori materials are a hallmark of the Montessori Method. These self-teaching tools encourage exploration of concepts in the areas of mathematics, language, sensorial development, and practical life, and allow children to direct their own learning with the light guidance of teachers and peers. In randomized controlled studies, children who attend Montessori schools have been shown to perform better in cognitive and socio-emotional measures than their peers in traditional schools [20] .\nThe Montessori Method: The classrooms are a mixed-age environment with children and one or two teachers. Unlike a traditional teacher, Montessori educators are primarily observers and facilitators for their students.\nThe Montessori Method: We interviewed a group of Montessori teachers and researchers in the early stage of our design and development process. Teachers were interviewed about their teaching methods and challenges in the classroom, and they revealed the use of manual observations in teaching as an important process.\nObservation: Observation is central to the success of Montessori education, helping teachers assist students in their growth [24] .\nObservation: There are three main types of observation [10] :\nObservation: \u2022 Individual: Teachers observe a student's progress with materials. For each material, they record the level of difficulty, engagement, and stage of progress to assess when a student is ready to be shown a new lesson. \u2022 Social: Teachers track clusters of students and study their evolution over time. They determine patterns of social behavior to study how children learn a new material together or assist others. \u2022 Classroom: Teachers record which materials or regions of the classroom are most often used and adjust the class design to encourage students to explore new or important concepts.\nObservation: It was apparent from our initial interview process (section 2.1) that given the different criteria for observation, it is challenging for two teachers to accurately assess a busy classroom. Besides observing, teachers introduce new materials to students, guide students who are having trouble, and intervene with students who are new to the Montessori environment (Figure 1 ). Teachers currently use manual note-taking to record their observations. From our interviews, we have found that this is time-consuming, prone to inaccuracies, and obscures large scale insights.\nObservation: Our work aids observation by introducing a proximity sensor network in the classroom.\nContributions: We present the following main contributions:\nContributions: \u2022 Sensors designed to instrument a classroom to capture proximity and motion data in an accurate, lowcost, and minimally-invasive manner. A carefully designed network event schedule scheme enables data collection from these sensors at a reasonably high sampling rate in a battery-preserving manner. \u2022 A smartphone application to control the data collection process and an interactive web application to visualize social, material, and classroom interactions. \u2022 Preliminary methods to reveal classroom dynamics from the proximity networks data that can inform researchers' and teachers' understanding of independence among students. \u2022 Technical and qualitative evaluation of the system to establish concrete use cases and to find out limitations of the system and future work possibilities in this area.\nContributions: With all the components together, the system allows the study of both social interaction and learning at scale. \nCONCLUSION: We have designed a system based on a dynamic range-based sensor network to study interactions between students, teachers, and lessons in early childhood environments. The sensors, visualizations, and data analysis are optimized for classrooms where children have freedom of movement and independence, such as Montessori schools. Teachers can use a smartphone application and web application to individualize their own teaching methods. We evaluated the usage of this system with a group of Montessori educators and determined three major use cases. According to our interviews, teachers suggested that parents can also benefit from such data to understand the progress of their children beyond traditional means.\nCONCLUSION: The deployment experience and data evaluations informed the limitations of our system too. Certain visualizations did not prove to be useful in some settings. Additionally, proximity does not always warrant an interaction with other children or materials. To tackle this issue, we plan to improve our sensing techniques to include microphone arrays (to infer who's speaking) and 3D hand gesture tracking over material trays using low cost electric field based proximity sensors embedded in the trays.\nCONCLUSION: Other than providing teachers important data and tools to learn about students' behavior and plan individualized curriculum, Sensei can provide valuable data and insights to early childhood development researchers. Large-scale quantitative insights can be found with longitudinal datasets collected over months or years. We hope to expand the capabilities of the system and deploy in more schools in the future.",
    "relatedWork": "RELATED WORK: Sensing educational interactions have been carried out in different forms mostly to understand student behavior at a higher age level than the range in which we are interested (2 -6) . RFID based student activity tracking [36] , or enabling multimodal learning analytics of tasks that require certain hand gestures by using video camera and depth sensing [5] are a few examples of innovative technology that help teachers/instructors and students in getting more quantitative information about learning interactions. We are not aware of any other system that tracks social interactions (or group activities) and lesson material interaction in an early childhood learning environment. Schools for young children tend to have more exploratory nature, and the freedom of movement and interactions in these classrooms suggest that our system should be equally applicable outside of the Montessori environment for early childhood. In this section, we state a few other sensing and analytics systems that use frameworks similar to our system, but in different application scenarios.\nWearable Electronics and Software for Contact Networks: The Sociometric Badge and Sociopatterns proximity sensor establish logging of proximity data between groups of people [3, 25] . The Sociometric Badge is a wearable badge that measures proximity during meetings and conferences. Bluetooth modules scan for similar modules in their proximity and use the RSSI signal strength of the data packets to establish a metric for proximity. Smartphones have also been used as proximity sensors in some studies. The authors of [32] used Bluetooth RSSI signal strength between phones to understand proximity and create a social interaction network of students in the Copenhagen Network Study.\nWearable Electronics and Software for Contact Networks: A challenge of using these sensors in an early childhood classroom setting is the power requirement and the consequent effect on the size of the circuitry. The batteries tend to be larger than coin cells, and using a big badge or smartphone in early education classrooms is discouraged by Montessori educators and teachers. Our sensors are small and discreet in nature, which does not disrupt the classroom experience.\nComputer Vision in the Classroom: A review of the literature on people tracking is well beyond the scope of this paper, we will only mention a few examples of the related work here. [4] demonstrated, among many, that it is possible to track people in a mildly cluttered scene when people are intersecting their tracks in a video. In the recent years, multilayer neural networks have become popular in recognizing and segmenting objects in a video. [1] provides a survey of several methods inspired by this technique. Depth sensing imaging devices like Kinect has been used widely to detect and track multiple human figures. Thermal imaging was used in [16] to improve human figure detection, and infrared motion tracking systems exist [37] that require the users to wear a tracker occasionally.\nComputer Vision in the Classroom: The main problem that we have faced with cameras in the classroom is privacy concerns from teachers and parents. If a parent wants his/her children to opt out from the intervention without any trace of data, it is hard to clean video data to meet that demand. Also, most computer vision algorithms still require good lighting conditions and are challenged by occlusion limits. It is difficult to track 15 -20 children of small heights in a cluttered classroom environment where tables and furniture act as major occlusion in the scene. Kinect and similar stereo based camera systems have a maximum range within which they are accurate (5m for Kinect) [14] , and Kinect can track only up to a finite number of human figures simultaneously. Thermal and infrared camera systems also require multiple cameras. Moreover, they are typically very expensive and are not suitable for a small school's budget. Our sensors are low-cost, uniquely identifiable, easily removable from the system, and do not require changing the classroom design to install a camera system.\nProximity Data Visualization: Visualization platforms for proximity data are limited to understanding social and organizational behavior. Data from Sociometric badges have been used to identify important social factors in organizational design and management. The authors of [25] used the badges to study and visualize longitudinal social interaction patterns over weeks. Sociopattern proximity sensors have been used in a high school [23, 31] to demonstrate the superiority of proximity data compared to observation diaries and friendship surveys. However, to our knowledge, no visualization framework exists that is accessible to a non-technical person in the context of proximity data.\nProximity Data Visualization: No system currently enables a teacher or a parent to understand changes in social behavior in an early childhood classroom and track lesson progress of children.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: The design of our urban environment cannot help but inform the quality of our urban experiences and the liveability of our cities. Good urban design can stimulate the senses in such a way as to encourage us to stay and enjoy our public spaces, essentially bringing society together. Unfortunately sound is not often considered in the design of our built environment, rendering our urban soundscape less than desirable. Recent developments in interaction design, and Sonic Interaction Design (SID) in particular, give us the means to integrate sonic feedback into our built environment and contribute to the composition of a more desirable acoustic environment [1] .\nINTRODUCTION: In this paper, we explore the integration of sonic interaction design in the public urban realm through a discussion of the London Soundmap; an interactive sound installation developed for the Transported by Design Festival held by Transport for London (TfL) in Regent Street, London, in July 2016. We use this installation as a case-study to explore the opportunities and constraints associated with integrating SID in the existing built environment, including the various administrative, environmental, design, technical and social challenges. First, we introduce the context of the event, including its various stakeholders and their objectives, before discussing how the Soundmap sought to address these aims. We then explain the various disciplines involved in its creation -from urban design to electronic engineering -before discussing the associated implementation challenges encountered. After presenting the results, we evaluate the outcomes generated by integrating sonic interaction design into the design of Regent Street.\nINTRODUCTION: initiative [3] . By pedestrianising one of the most heavilytrafficked streets in central London, the aim of this yearly festival is not only to entertain, but to explore the overall benefits of a traffic-free street, such as improving our urban experience, improving air quality and reducing noise levels [4] . See Figure 1 . Having designed two interactive bus stops for a similar event held in 2014 celebrating the Year of the Bus [5] , TfL commissioned [6] for another 'interactive and engaging' intervention. Thus in addition to promoting the important role design currently plays in its public transport system, they were also interested in exploring the benefits of more interactive approaches to design.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: In recent years, interactive crowdsourcing systems have been designed that allow end users to send requests directly to crowd workers, and to continuously collaborate with them in real time [32, 35] . In such systems, requesters can verbally communicate with workers to request various tasksranging from accessibility assistance to writing software-and researchers have developed a number of crowd-powered systems to enable this novel collaborative model [3, 7, 8, 38, 40, 45, 49] . This form of direct and natural interaction between a requester and crowd workers in real time minimizes the limitations of microtask crowdsourcing workflows, where a task needs to be broken down into context-free microtasks and the responses aggregated afterward [46] . Real-time collaboration between requesters and workers is useful in expanding the application of crowdsourcing to broader domains, allowing workers to help solve open-ended tasks whose steps are unknown in advance and may change during the process.\nINTRODUCTION: While this mode of end users directly collaborating with crowd workers unlocks new opportunities for crowdsourcing, it introduces new set of challenges for requesters beyond mere unfamiliarity with such collaboration. This is because the characteristics unique to crowdsourcing-ubiquity, scale, transiency, and anonymity-make using crowd-powered systems different from simply collaborating with other users via real-time groupware. Specifically, designing crowdsourcing systems that leverage direct interaction entails an inherently asymmetrical structure between a requester and crowd workers in multiple ways: scale, roles, expertise, communication channels, and user interfaces. Such differences and imbalances are embedded in interactive crowd-powered systems and pose new design challenges beyond those addressed in real-time groupware design [20] . However, the current body of research in real-time crowd-powered systems has focused on developing interactive crowdsourcing systems for specific domains, in which the system that enables workers to resolve certain types of tasks is the main contribution [3, 7, 8, 38, 40, 45, 49] . Furthermore, the validation process of such systems is often focused on the crowd's ability to complete the central task, holding the requester constant throughout each trial in order to control for variance among requesters [11, 38, 45] . As a result, we lack sufficient understanding of the challenges that requesters have as end users of interactive crowdsourcing systems in general. This work focuses on examining the requester side of crowdsourcing.\nINTRODUCTION: The goal of this paper is to better understand the challenges that are present when a requester directly collaborates and verbally communicates with crowd workers in real-time collaborative crowdsourcing systems, and to formulate design recommendations from the findings for future interactive crowdsourcing systems. In crowdsourcing systems, requesters are those who specify tasks for crowd workers. Crowd workers then choose and complete the tasks that are available through crowdsourcing platforms, such as Amazon Mechanical Turk [1] . Requesters generally have less control over who will do the tasks, and less direct interaction with the workers, than the traditional recruitment processes. This inherently transient nature of crowdsourcing poses a set of challenges that are not present in an existing real-time groupware. In addition, designing a single system for two disparate groups-requesters and workers-may require different approaches for each group, which can create unforeseen challenges. In particular, we are initially motivated by the following questions: Q1) How do requesters find and understand the benefits of real-time collaboration in crowdsourcing and engage with workers (5.1)? Q2) How do potential asymmetry of expertise, roles, or communication channels between a requester and crowd workers impact their communication and collaboration (5.2,5.3)? Q3) What particular challenges do requesters and workers face in collaboration (5.4,5.5)? Our findings can contribute to improving existing systems and informing the design of future interactive crowdsourcing and mixed-initiative systems (6) .\nINTRODUCTION: To answer these questions, we conducted a user study investigating how requesters collaborate and communicate with workers via an interactive crowdsourcing system. We chose to use an existing crowd-powered system, Apparition, which allows requesters to create sketch prototypes of graphical user interfaces with crowd workers [38, 45] . In this study, an end user (requester) is asked to verbally describe and draw a GUI sketch; crowd workers behind the scene listen to the description and view the sketch, and create a refined GUI in real time. While the authors of previous works focused on building and validating systems to enable crowd workers to effectively create an interactive graphical user interface prototype [38, 45] , in this study we focus on how various requesters use the system and collaborate with workers.\nINTRODUCTION: Our core contributions are valuable beyond the scope of crowd-powered design tools, derived from a qualitative analysis using questionnaires, interviews, and observations. The system and task in this study involve common components of real-time collaboration in crowdsourcing: a requester, crowd workers, an artifact (sketch), visual context (canvas) that is shared remotely, and communication channels (voice/chat). This structure is found in many crowdsourcing systems that involve real-time collaboration between a requester and crowd workers, allowing us to generalize our design recommendations. A summary of our findings is as follows:\nINTRODUCTION: \u2022 Requesters actively collaborate with workers and understand the benefits of working with them in real time (5.1). \u2022 Expert requesters may speak in a style that makes it difficult for crowd workers to comprehend their requests (i.e., be less descriptive, use more jargon) (5.2). \u2022 The asymmetry in communication modalities (speech versus text) causes confusion (5.3).\nINTRODUCTION: \u2022 Sharing the visual context was not enough to effectively coordinate collaboration (5.4 ).\nINTRODUCTION: \u2022 Speaking rates vary among individuals; rapid speech may result in a backlog of requests (5.5) .\nINTRODUCTION: The design recommendations drawn from the study contribute to the broader goal of enhancing the design of crowdsourcing systems that can facilitate effective real-time communication between requesters and crowd workers.",
    "relatedWork": "RELATED WORK: Crowdsourcing has been effective in resolving complex tasks that computers alone cannot automate. The most established approach to crowdsourcing involves breaking the task down into microtasks that non-expert crowd workers can solve independently [46] . This method is effective in settings where the problem solving process is known ahead of time and immediate feedback is not necessary, but has limitations in addressing \"complex\" tasks, where tasks cannot be as clearly and rigidly defined [33] . One way researchers have addressed this problem is by using real-time, continuous crowdsourcing to enable on-going interaction between a requester and crowd workers [35, 45] .\nReal-Time Collaboration in Crowdsourcing: Real-time crowdsourcing allows requesters to seek help on demand and get assistance over a much shorter timescale (seconds or minutes) [5, 39] . Researchers have explored tools and methods that make various tasks more convenient. One method to recruit crowd workers quickly is a retainer model, in which crowd workers are recruited in advance and routed when a task is available [5, 16] . VizWiz also demonstrated that a crowd could answer questions to assist the visually impaired in under one minute [7] . Chorus, an intelligent conversation assistant, leverages crowd workers' efforts by having them answer any question a requester asks in an instant messenger [27, 41] . EURECA leverages online crowds who collaborate to help robots robustly identify 3D point cloud segments corresponding to user-referenced objects in near real-time [17] . Bolt uses continuous real-time crowdsourcing guided by a AI agent's understanding of the world to enable super-human response speeds (milliseconds) to address even broader classes of interactive tasks [47] .\nReal-Time Collaboration in Crowdsourcing: Real-time collaboration between a requester and crowd workers broadens the applications of crowdsourcing, bringing crowdsourcing into the realm of interactive systems. For example, Legion enables interactive crowdsourcing, coordinating and aggregating crowd workers' responses in real time, and allows workers to resolve tasks that require continuous interaction (e.g., controlling graphical user interfaces in real time) [39] . Continuously interacting with crowd workers is effective when the target task is open-ended and a requester needs to explore various ideas and iterate on them. Such an approach has been used in creating early-stage sketches of graphical user interfaces and making them demonstrate interactive behaviors [38, 45] . Similarly, researchers have proposed an augmented whiteboard that allows a co-located team and remote crowd workers to work together in a brainstorming session [3] . Interactive crowdsourcing systems support end users by permitting natural interaction with the system via speech, and then crowd workers behind the scenes can solve complex requests. For example, some real-time crowdsourcing systems allow end users to leave requests for crowd workers in the form of voice recordings [7, 8, 40, 49] , or to directly and continuously communicate with workers via live audio streaming [3, 38, 45] .\nReal-Time Collaboration in Crowdsourcing: In practice, crowd workers typically work on a particular task for a short amount of time temporarily and can choose to quit a task and transit to another one frequently. This transient and temporary nature of crowdsourcing makes challenges of supporting real-time collaboration go beyond those addressed in real-time groupware design [20] . For example, it has been shown that mitigating the transient nature significantly can enhance the overall outcome of crowdsourcing: 1) co-located teamwork among crowd workers enhances their collaborative experience when compared to remote, independent crowdsourcing [26] , and 2) familiar crowd teams doubled the performance of ad-hoc (unfamiliar) teams [55] . However, much of the existing research works have focused on the feasibility of enabling real-time collaboration between a user and workers for domain-specific applications, overlooking the potential challenges of requesters working with crowd workers. Our goal in this work, given the challenging nature of crowdsourcing, is to better understand how a requester communicates and collaborates with crowd workers in the context of creation tasks through various real-time communication channels.\nSupporting Communication and Collaboration in Crowdsourcing: Facilitating communication between a requester and crowd workers is one way to improve the outcome of crowdsourcing. For example, the literature shows that giving workers feedback on a task outcome encouraged them to revise their solutions and yielded a better final outcome [11] . Similarly, Bigham et al. showed that requesters preferred continuous interaction with crowd workers [7] . Chorus:View demonstrates the same benefit of continuous communication with significant speedup in the completion time [40] . Some researchers developed computational systems that allow requesters to form a team of expert crowd workers to dynamically manage highly complex tasks such as animation creation [54, 61] . Similarly, the Huddler system assembles a stable team of familiar crowd workers to facilitate collaboration on complex tasks [55] . Salehi et al. further explored various communication mechanisms to study how workers can communicate asynchronously with a requester for continuous writing tasks [56] . Oftentimes, the validation of these systems used a controlled set of requesters for the purpose of ensuring request consistency. This experimental setup asks a single person (often an author) to play the role of the requester, having a fixed, scripted task [38, 45, [54] [55] [56] . While the thread of research confirmed that these systems are able to coordinate For the worker's interface, see [38, 45] . a crowd's effort, it did not fully reveal the potential challenges in communication and collaboration between requesters and crowd workers, nor does it allow us to measure the variation in how different requesters make requests.",
    "rq": " q1) how do requesters find and understand the benefits of real-time collaboration in crowdsourcing and engage with workers (5.1)?\n q2) how do potential asymmetry of expertise, roles, or communication channels between a requester and crowd workers impact their communication and collaboration (5.2,5.3)?\n q3) what particular challenges do requesters and workers face in collaboration (5.4,5.5)?"
}
{
    "intro": "INTRODUCTION: Collaboration often presents a variety of hurdles for teams. However, the benefits of collaboration versus isolation tend to outweigh these hurdles as teams can tackle larger, more complex tasks [5] . With an expanding global workforce, new organization structures require large portions of collaborative work be conducted within geographically distributed teams [33] . To bridge the gap between collocated and remote team members, teams use various channels of computer-mediated communication (CMC) [36] . These include applications such as instant messaging (e.g., Google Chat, Messenger) and teleconferencing and video-conferencing systems (e.g., Skype, Facetime). Mobile Robotic Telepresence (MRP) systems have recently received increased attention from commercialization efforts to offer affordable telepresence options for under $2500 (e.g., beam\u00ae, Double\u00ae).\nINTRODUCTION: In their simplest forms, MRP systems provide users the benefit of traditional video conferencing with added mobility and navigability within a geographically distant location. There is existing research regarding telepresence tools examining specific use cases (e.g., [8] [37] [24] ) and actual task performance (e.g., [28] [31] ). However, the literature is limited pertaining to the social-psychological implications of robotic systems in teams, whether focused on collaboration, decisionmaking, negotiation, etc. (e.g., [7] [29] [33] ). Ongoing research is needed to understand such dimensions of MRP systems.\nINTRODUCTION: Additionally, not all collaborative work is identical, and the structure of collaboration (i.e., the distribution of required information and resources) likely has significant effects on the team [32] . In distributed work, even via telepresence, this effect may be further compounded if the remote member is lacking information that is shared between others. For this reason, we explore how these differing task structures may influence MRP system collaboration. This paper studies the relationship between access to task information and the role of team members (collocated or remote) on collaboration perceptions and participation. We designed a collaborative task from which we collected video footage and survey data to compare differences in team member participation, team members' impressions of one another, and individual perceptions of the collaborative task. This research builds on prior work showing disparities between remote and local team members. For instance, collocated members tend to interact more with each other than with remote members [9] . Findings suggest an imbalance between collocated and remote participants, as remote pilots reported a more negative experience, participated less, and were perceived as less trustworthy by collaborators. The effect of task information access was also significant such that the most negative perceptions of and from RPs resulted when they lacked information shared between collocated members.\nINTRODUCTION: The following sections of this paper highlight the existing work related to MRP systems and further describe our current study. We detail the task and measures used to better understand collaborator experiences within a team employing a specific MRP system: the beam+\u00ae telepresence robot. Based on our findings, we offer practical use and design solutions for enhancing MRP systems, and we recommend directions for future research.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Real-time captioning provides vital spoken conversation access for many d/Deaf and hard of hearing (DHH) people. Both human-generated and automatic captions have received substantial attention from HCI and CSCW researchers, with a focus on captioning in classrooms or other formal environments (e.g., [13, 37, 43] ). While human transcription (e.g., CART) is most common in these settings, researchers have also examined automatic captioning, particularly for constrained environments like classroom lectures; commonly there is a single dominant speaker in these settings and the vocabulary used is more predictable [1, 37, 71] . In contrast, captioning for more informal small-group and one-on-one interactions has received less attention, despite the fact that human captioning's high cost ($60-$200 an hour in the US) and need for advance scheduling crea te significant barriers in t his context [73, 75] .\nINTRODUCTION: While automatic captioning is an increasingly viable alternative to human captioning [38, 61] , its accuracy varies from 9-37% word error rates across tools [38] . Unlike human captioners, automated techniques cannot convey non-speech context (i.e., visual references, emotion, emphasis) nor can it intervene to improve communication. Moreover, small-group conversation's interactive nature, flexible social dynamics, and high level of spontaneity further limit existing captioning services. Ultimately, captioning and other access tools (e.g., paper and pen, texting, notes apps) all come with limitations and do not fully support DHH people during small-group conversations [20] .\nINTRODUCTION: Despite the sociotechnical nature of small group captioning, most prior work has only examined technical considerations, such as how to convey uncertainty in automatic captioning through the use of simulated conversation in controlled experiments [7, 59, 60] . Seita et al. offer exceptions that explore how social interactions and behaviors impact captioning [65] [66] [67] . They first found that hearing people speak more loudly, clearly, quickly and with non-standard articulation when they are being captioned in small-group conversations [65] . In a preliminary [67] and follow up study [66] , Seita et al. had a hearing actor modulate their conversation behaviors in various ways as part of a controlled experiment (e.g., speech rate, voice intensity, eye contact), and measured what behavior variants DHH participants preferred (e.g., fast, medium, or slow speech), and which behaviors were most important. They provide quantitative evidence that hearing people's behaviors impact DHH people's experiences of one-on-one captioned and interpreted conversations. These findings motivate the need to more deeply understand DHH people's small group captioning experiences through a sociotechnical lens. In this paper we address the questions: What social, environmental, and technical factors impact the use and usefulness of captioning in small groups? What opportunities exist to design captions and caption d isplays in w ays that support more accessible group com munication p ractices?\nINTRODUCTION: To begin addressing these questions and to ground future small group captioning technologies in the needs and desires of DHH people, we conducted an interview and design probe study with 15 DHH participants. Each session began with an interview covering the participant's experiences with real-time captioning in small-group conversation and their perspectives on the role of hearing conversation partners in creating or obstructing accessibility. Participants then completed a design probe activity, building on methods outlined in [24, 26, 33, 51] . In this activity, we presented a series of potential future captioning features (e.g., displaying speech rate, flagging overlapping speakers, supporting error correction by hearing people) to provoke discussion around what new designs participants desire and how that technology could be integrated into small-group conversations.\nINTRODUCTION: Our findings highlight the myriad social (e.g., group norms, preferred communication modes), environmental (e.g., furniture configuration, online availability of a text chat), and technical (e.g., caption lag, built-in speaker identification) factors that shape real-time captioning, contributing an understanding of the context that surrounds captioned conversation. Particularly, we find that: (1) captioning's efficacy is highly determined by the group being captioned, (2) current captioning tools are often insufficient during interactive Social, Environmental, and Technical: Factors at Play in the Current Use and Future Design of Small-Group Captioning 434:3 conversation, and (3) while the lack of visual and spatial information online create barriers, features of video conferencing also provide new opportunities to increase access. Participants' responses to the design probe activity also highlight the potential to create more captioningfriendly environments, both online and in person, and suggest that providing conversation feedback and warnings to guide captioning-friendly group norms is a promising direction for future development. Based on these findings, we discuss the need to consider the intersection of social, environmental, and technical factors in captioning research, propose a reframing of captioning as a group technology, and put forth future design guidelines that center DHH peoples' needs.\nINTRODUCTION: More broadly, we contribute (1) an empirical account of DHH participants' experiences of small group captioning which highlights how social, environmental, and technical factors impact its use and efficacy, (2) an exploration of design opportunities to support small group captioned conversations and future design guidelines, (3) an understanding for both (1) and (2) of how online environments-a historically little-studied captioning context-shape captioning experiences and preferences, and (4) reflections on reframing captioning as a group technology.",
    "relatedWork": "RELATED WORK: To contextualize our study, we analyze current captioning methods, caption use and design, and provide a Deaf and disability studies framing.\nReal-Time Captioning Services: DHH people use a variety of real-time captioning technologies, each with their own tradeoffs. CART-human-generated verbatim captioning-is the most popular and claims to be at least 98% accurate for all words typed [74], but this includes after-the-fact corrections; the accuracy of live CART is lower [37] . Moreover, CART is expensive and must be scheduled in advance [73] . An alternative, C-Print, summarizes content within sentences and uses a shorthand style [76] , but is also costly (~$60/hour) and must be pre-scheduled [19] . Both CART and C-Print are frequently provided by in-person and remote transcriptionists. Crowdsourcing has been explored to allow non-experts to generate high-quality captions [29, 46] , but even the most developed system, Legion:Scribe, remains in private beta release [77] .\nReal-Time Captioning Services: While human transcription remains the legally protected standard for captioning in the US and around the world [55, 75] , automatic captioning using automatic speech recognition (ASR) is increasingly used for informal interactions and when accommodations are not otherwise available [30] . Tools such as Otter.ai and Google's Live Transcribe provide free or low-cost captioning but accuracy can be a concern: recent analyses found that Google's API outperforms other ASR, recording average word error rates around 9% [22, 38] . Additionally, ASR performance deteriorates in complex audio environments [69] and does not handle accents well, including Deaf accents-Glasser et al. [25] found that Microsoft's Translator Speech API's word error rate was 18% for hearing speakers and 78% for Deaf speakers. Unlike human transcription, ASR does not convey non-speech information such as laughter or consider high-level context, such as a child trying to say a new word. Furthermore, many within the Deaf community oppose using automatic captioning in place of human transcription, considering it to be insufficient access [16] . These services generate captions, but how people use captions and how to design effective captioning displays are additional research questions-and the focus of our study.\nCaption Design and Use: We review work on DHH people's experiences with captions, focusing on caption design and interactions between DHH and hearing people. Captions are an imperfect technology and prior work has documented challenges and potential solutions. A key concern when designing captioning systems is limiting visual attention split, and researchers have explored myriad display configurations to enhance DHH people's ability to read captions while attending to other aspects of conversation, including integrating captions into the environment, using head-mounted displays, and annotating captions [15, 33, 34, 42, 47, 52, 57, 59] . Additional design efforts seek to account for the fact that captioning flattens expressive elements of speech: adding punctuation to automatically generated captions can improve readability [28, 70] , placing captions near speakers in videos and displaying speech volume has been well-received by DHH viewers [31] , and both research prototypes and commercial tools use color to differentiate captioned speakers [27, 78] .\nCaption Design and Use: There are also known hurdles to caption comprehension that remain unaddressed. Jensema [35] identified that the ideal captioning speed is around 145 words per minute (wpm), with a drop off in comprehension after 170 wpm (typical human English speech rates are 120-160 wpm [62] ). Additionally, a key concern around ASR-generated captioning is high error rates, and many researchers have explored ways to communicate this uncertainty to caption viewers [5] [6] [7] [8] 59, 60, 65, 68] . We use the literature above to inform our design probes and to recommend designs that could address the challenges expressed by our participants.\nCaption Design and Use: To improve captioning design, HCI work has often introduced new technologies to classroom settings (e.g., [1, 13, 15, 19, 21, [39] [40] [41] [42] [43] 71] ). This classroom context differs from smallgroup conversations, which in contrast tend to be less structured, have multiple speakers rather than a primary lecturer, and are often not well-supported via formal accommodations (e.g., CART, interpreting). Several studies have explored small group captioning needs via simulated one-on-one conversations [6, 8, 59, 60] and while they provide insight into caption preferences (e.g., use 2 lines and common fonts [6] ), their non-interactive nature does not allow for understanding how captions influence small-group social dynamics. Other research has explored the viability of phone-based ASR combined with typed responses, having Deaf and hearing participants communicate in the lab [18] or field [49] , and their findings, though brief, have been positive. Further, some head-mounted displays for captioning have been evaluated in small-group conversation, showing that participants benefit from seeing captions in the same field o f view as th eir c onversation p artner(s) [34, 57] .\nCaption Design and Use: Compared to this existing body of captioning research, our interview and design probes treat captioning as a technology used by groups, which opens new questions about conversation participants' impact on captioning success and the potential for caption designs to shape individual and group behaviors. Accessibility research in other contexts has begun to explore such an approach, e.g., to help ASL interpreters and classroom instructors coordinate content [9] and to assist presenters in increasing non-visual accessibility [58] . Additionally, captioning tools to date have been predominantly studied in the context of in-person conversation, with the exception of Kushalnagar and Vogler's teleconference best practices [44] , while our study examines captioning both in person and with online video calls.\nCaption Design and Use: As mentioned in the Introduction, most relevant to our paper is work on understanding and designing to support interaction between DHH and hearing people during caption use. Seita et al. [65] studied automatically captioned small-group conversation between DHH and hearing people, finding that, in the presence of captions, hearing people altered speech characteristics, such as volume and rate, but the study did not report on DHH participants' experiences or the social impacts of using captioning, areas our research explores. Seita and Huenerfauth [67] also conducted a controlled experiment with 8 DHH participants to explore the impact of a hearing researcher modulating their speech in several ways (i.e., speech rate, volume, eye contact). At least some participants noticed each of the six behaviors, with open-ended comments suggesting that speech rate is particularly important but that all behaviors were relevant. In a 2021 follow up study [66] , Seita et al. repeated their methodology from [67] with 20 DHH participants in person, finding that modulations in intonation and enunciation statistically significantly impacted participants' satisfaction with the hearing person's behavior and that enunciation and eye contact where more important than intermittent pausing. They also ran this study with DHH participants online, using ASL interpretation rather than captioning to convey the hearing actor's meaning. Combined, these studies help to motivate our sociotechnical analysis of captioning use by beginning to show that hearing people adapt their speech in the presence of captions and that DHH people notice some of these adaptations. Building on that work, we conduct in-depth interviews on DHH people's small group captioning experiences and explore potential designs to guide group communication behaviors. Finally, we draw on qualitative work that contextualizes caption use. Kawas et al.'s [37] analysis of real-time captioning in the classroom identifies that most hurdles students face are fundamentally sociotechnical, requiring technological, social, environmental, and policy solutions, and we are inspired to explore small group captioning with a similar sensibility. Complementing our work is a study from Wang and Piper [72] , who interviewed and observed existing dyads of Deaf and hearing collaborators, focusing on interactions when accommodations are unavailable (i.e., not focused on captions). They found that, over time, these Deaf-hearing teams create accessible practices, including flexibly switching between spoken and written language, learning to prioritize shared visuals, and providing ad hoc, informal transcription and sign language interpretation. We explore their theory of accessibility as a co-created group practice among Deaf-hearing teams in the context of captioned conversations and with DHH participants who use a wider variety of communication styles (their D HH participants a ll both signed and voiced).\nDeaf and Disability Studies Perspectives: While the hearing world often thinks of deafness as an audiological diagnosis, many people identify as capital-D Deaf, signaling engagement with the Deaf community and Deaf culture. Deaf studies scholar Harlan Lane argues that Deaf identity is akin not to a disability but to an ethnicity, with its own linguistic, conversational, and cultural norms [45] . Deaf studies names audism as systemic discrimination on the basis of hearing ability, identifying the structural barriers that DHH people face as the fault of oppression from hearing people and institutions [2] . Because accessible technology developments for DHH people can all too easily perpetuate audism [23] , we ground our research in Deaf studies critique.\nDeaf and Disability Studies Perspectives: While respecting the contested cultural differences between deafness and disability, our approach to caption design is also impacted by disability studies thinking on accommodations. A key contribution of disability studies is the notion of models of disability, commonly contrasting the medical model, conceptualizing people as intrinsically disabled by abnormal bodies that need fixing, with the social model, framing disability as what happens when an ableist society does not meet the needs of people with impairments [56] . Mankoff et al. [50] highlight how moving away from the medical model in assistive technology design better supports disabled people. Following this model shift, Kasnitz [36] argues for \"community-based accommodation.\" This reconceptualization treats hearing and Deaf people in conversation as equally reliant on accommodations and reimagines who is responsible for arranging access. We also turn to interdependence, the move from treating disabled people as fundamentally dependent to viewing all people as inter-reliant [53] , which Bennett et al. [4] argue can lead to assistive technology that treats access as relational, challenges ability hierarchies, and highlights disabled people's competencies. Our study is guided by this body of work to focus on the social experience of captioning and to consider designs of future captioning systems that design for the g roup, not just DHH individuals. ",
    "rq": " in this paper we address the questions: what social, environmental, and technical factors impact the use and usefulness of captioning in small groups?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: We are experiencing the proliferation of augmented reality (AR) platforms, devices, and frameworks at a massive scale. Platforms like ARKit [23] and ARCore [22] provide AR on smartphones with only a single standard RGB camera, i.e., without motion or depth cameras that were previously required, and provide opportunities for creating novel applications. These advances have enabled motion tracking and environment sensing on common mobile devices, paving the way for more widespread adoption. However, this rapid growth also leads to a fragmentation of the ecosystem. While most existing research on AR has focused on hard technical problems with AR technologies, such as perception, annotation, visualization, and sensing [1, 2, 10, 24] , this fragmentation leads to a variety of new challenges for application developers.\nINTRODUCTION: Available frameworks differ in a wide range of characteristics, from system architecture, platform and hardware (in)dependence, to intended scope and provided features. Applications created with one framework typically only run on one kind of device, and porting to another framework to support more devices is cumbersome or prohibitively expensive. For example, applications that rely on marker tracking are developed entirely differently than ones that work with spatial mapping. It is thus becoming increasingly difficult for designers and developers to maintain an awareness of the available options, understand the strengths and weaknesses of different frameworks, and figure out ways to best combine available resources to satisfy application requirements.\nINTRODUCTION: In this paper, we present XD-AR, a cross-device framework that we designed to study the main issues that must be overcome to allow developers to create AR applications that serve the broad spectrum of hand-held, head-worn, and projective AR devices. XD-AR adds to the evolving research on cross-device frameworks [7, 15, 26, 27, 38] , but specifically focuses on the new class of rapidly evolving AR devices and platforms such as HoloLens and ARKit, as well as the Tango and ARCore projects, which have been recently merged. It enables developers to quickly create applications that allow HoloLens users to, e.g., place an object in a room, while other users can view and interact with that object through an AR-capable smartphone. XD-AR facilitates a wide range of novel cross-device applications using AR, thus enabling exploration of new interesting combinations of AR displays, especially for collaboration in multi-user settings.\nINTRODUCTION: To create XD-AR, we first studied existing frameworks to obtain a better understanding of the challenges and opportunities in cross-device AR development. Our study was informed by three sources: a) two motivating scenarios that are based on existing work and are challenging to implement because this would require integration of multiple existing frameworks; b) a survey of experienced AR designers, developers, and users to identify their needs and expectations; c) a technical review of the current ecosystem of AR frameworks to provide a better understanding of the conceptual and technical differences underlying existing AR systems.\nINTRODUCTION: Figure 1 gives an overview of the flow of the paper and how the different conceptual and engineering parts inform each other. We start by developing the two scenarios in the focus of our investigation and deriving a set of challenges in Section 2. The scenarios are also part of our survey with AR developers, designers, and users, from which we extract common themes in Section 3.\nINTRODUCTION: Next, in Section 4, we present our technical review, from which we derive a taxonomy of AR frameworks that captures the main dimensions of existing AR frameworks and is informed by both the scenarios and the survey. Based on these results, in Section 5, we distill five requirements for frameworks like XD-AR. In Section 6, we introduce the XD-AR framework and its various components to support cross-device development. This is followed by a description of our specific implementation of XD-AR in Section 7. Finally, in Section 8, we describe two proof-of-concept applications implementing the two scenarios using XD-AR.\nRELATED WORK: For this work we draw upon two primary bodies of research: augmented and mixed reality and cross-device interfaces.\nAugmented/Mixed Reality Research: AR has been researched for decades, with major survey papers covering the latest in the field extending from 1997 to 2015 [1, 2, 5, 6, 36, 39] . This research to date has necessarily been largely concerned with getting the core technologies to work together in a sufficiently robust way, so as to overcome the fundamental perceptual and human factors challenges [10, 20] . These are non-trivial issues, and involve dozens of open problems across multiple engineering disciplines.\nAugmented/Mixed Reality Research: The recent availability of high-fidelity see-through head-mounted displays, such as the HoloLens, coupled with a new push from smartphone makers to better support AR, means that for the first time we can begin to consider the new design and engineering challenges involved in making these systems work together. No currently identified work in augmented or mixed reality has addressed the challenges of how to engineer useful systems that will support a profusion of AR-enabled devices across a fractured ecosystem of vendors.\nAugmented/Mixed Reality Research: Projective AR is a unique category of AR approaches, as it converts the environment itself into a type of display. We make use of RoomAlive as a kind of representative example of what can be done with this technique, which allows network PCs to drive multiple projectors and Kinect devices to create an immersive room-filling AR experience [17] . In addition to entertainment uses, projective systems have been used to provide enhanced practical teaching methods by overlaying anatomy and live annotations over a student [14] . Projective AR has also been shown to have potential in mental health, such as being used to provide clinical treatment of phobias [4, 37] . To the best of our knowledge, there is no prior attempt at supporting AR interfaces that can be viewed using projective displays, such as RoomAlive, together with other types of AR devices.\nCross-Device Interfaces Research: XD-AR also adds to a long stream of research on cross-device interfaces. A number of studies have identified a strong need to facilitate tasks involving multiple devices [9, 28, 33] . Jokela et al. [16] provide a taxonomy of interactions and distinguish between resource lending, related parallel use, and unrelated parallel use. Display size, input capabilities, and level of multi-tasking support are the main factors in choosing which device to use in a specific situation. Despite these studies, support in current interfaces is still limited.\nCross-Device Interfaces Research: To support parallel use of multiple devices, a number of frameworks and tools have been proposed. For example, Conductor [13] demonstrates how to enable users to easily share information, chain tasks across devices, and manage sessions across multiple mobile devices. Panelrama [38] , XDStudio [26] , Weave [7] and WatchConnect [15] provide development and authoring tools to support specification and design of cross-device interfaces. In related research [25, 30] , researchers have also studied desired cross-device designs with end-users and identified patterns of parallel use of smartwatches, phones, and tablets. Finally, a number of projects have focused on infrastructure and approaches range from simple messaging [29] to dynamically migrating interface distribution engines [12] .\nCross-Device Interfaces Research: However, none of the prior work has attempted to support cross-device AR. The closest to our work are standardization movements, such as WebVR, which provide a layer of VR technology on top of existing web technologies. Similar efforts are planned around the idea of WebAR and Mozilla's WebXR. Open-source projects like AR.js and argon.js illustrate how to implement AR interfaces that can run on different mobile devices, but do not provide support for cross-device interactions to the extent provided by XD-AR.\nCONCLUSION: The proliferation of new AR devices and technologies confronts designers and developers with a fragmented ecosystem. Therefore, it is crucial to gain a better understanding of the challenges and opportunities in AR. In this paper, we presented two main contributions.\nCONCLUSION: First, we provided a taxonomy of AR frameworks that both supports developers in choosing the suitable technology for their needs, and allowed us to generate a comprehensive set of requirements for cross-device AR approaches.\nCONCLUSION: Second, we introduced XD-AR, a cross-device augmented reality framework that was designed to study the issues necessary to be solved to provide designers and developers with the means to create innovative applications for a wide variety of platforms and devices.\nCONCLUSION: To inform both the taxonomy and the novel framework, we a) identified two particularly challenging scenarios based on existing work; b) conducted a survey of AR designers, developers, and users; and c) analyzed the state of the art of AR frameworks.\nCONCLUSION: We were able to show the feasibility of our solution by implementing the two challenging scenarios as proof of concept applications on top of XD-AR. They integrate HoloLens, Tango, and RoomAlive, and would be easily extensible to novel technologies like ARKit and ARCore. These applications allow users to, among other things, place furniture in a room using HoloLens and view it on a Tango phone at the exact same position in real time. From this new foundation, we can move towards a future where cross-device AR designers and developers will be able to focus more on the usability and ease of development of the next generation of applications.",
    "relatedWork": "RELATED WORK: For this work we draw upon two primary bodies of research: augmented and mixed reality and cross-device interfaces.\nAugmented/Mixed Reality Research: AR has been researched for decades, with major survey papers covering the latest in the field extending from 1997 to 2015 [1, 2, 5, 6, 36, 39] . This research to date has necessarily been largely concerned with getting the core technologies to work together in a sufficiently robust way, so as to overcome the fundamental perceptual and human factors challenges [10, 20] . These are non-trivial issues, and involve dozens of open problems across multiple engineering disciplines.\nAugmented/Mixed Reality Research: The recent availability of high-fidelity see-through head-mounted displays, such as the HoloLens, coupled with a new push from smartphone makers to better support AR, means that for the first time we can begin to consider the new design and engineering challenges involved in making these systems work together. No currently identified work in augmented or mixed reality has addressed the challenges of how to engineer useful systems that will support a profusion of AR-enabled devices across a fractured ecosystem of vendors.\nAugmented/Mixed Reality Research: Projective AR is a unique category of AR approaches, as it converts the environment itself into a type of display. We make use of RoomAlive as a kind of representative example of what can be done with this technique, which allows network PCs to drive multiple projectors and Kinect devices to create an immersive room-filling AR experience [17] . In addition to entertainment uses, projective systems have been used to provide enhanced practical teaching methods by overlaying anatomy and live annotations over a student [14] . Projective AR has also been shown to have potential in mental health, such as being used to provide clinical treatment of phobias [4, 37] . To the best of our knowledge, there is no prior attempt at supporting AR interfaces that can be viewed using projective displays, such as RoomAlive, together with other types of AR devices.\nCross-Device Interfaces Research: XD-AR also adds to a long stream of research on cross-device interfaces. A number of studies have identified a strong need to facilitate tasks involving multiple devices [9, 28, 33] . Jokela et al. [16] provide a taxonomy of interactions and distinguish between resource lending, related parallel use, and unrelated parallel use. Display size, input capabilities, and level of multi-tasking support are the main factors in choosing which device to use in a specific situation. Despite these studies, support in current interfaces is still limited.\nCross-Device Interfaces Research: To support parallel use of multiple devices, a number of frameworks and tools have been proposed. For example, Conductor [13] demonstrates how to enable users to easily share information, chain tasks across devices, and manage sessions across multiple mobile devices. Panelrama [38] , XDStudio [26] , Weave [7] and WatchConnect [15] provide development and authoring tools to support specification and design of cross-device interfaces. In related research [25, 30] , researchers have also studied desired cross-device designs with end-users and identified patterns of parallel use of smartwatches, phones, and tablets. Finally, a number of projects have focused on infrastructure and approaches range from simple messaging [29] to dynamically migrating interface distribution engines [12] .\nCross-Device Interfaces Research: However, none of the prior work has attempted to support cross-device AR. The closest to our work are standardization movements, such as WebVR, which provide a layer of VR technology on top of existing web technologies. Similar efforts are planned around the idea of WebAR and Mozilla's WebXR. Open-source projects like AR.js and argon.js illustrate how to implement AR interfaces that can run on different mobile devices, but do not provide support for cross-device interactions to the extent provided by XD-AR.",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": " in order to answer the question: can the bollywood method reduce socio-technical challenges to collaboration for underprivileged indian students in an online learning environment?"
}
{
    "intro": "INTRODUCTION: When building relationships in a group, casual group conversation is important. During such casual conversations, group members disclose personal information, which causes feelings of intimacy among the group members to increase [12, 18] . However, the ability to successfully and strategically self-disclose in a casual conversation is a personal competency that many people do not possess. In general, it is difficult to engage everyone equally in a group in conversation. In a group conversation, each person takes part, but it is not uncommon for a few people to dominate a group conversation while the rest mostly remain silent and do not provide questions or comments. Other difficulties are related to the group members' sociocultural backgrounds. People from different cultures have distinct cognitive patterns and social orientations [9, 21] . These can affect how members ask questions, seek information, and socialize during a group conversation. Accordingly, due to the various possible combinations of participants, including diverse characteristics and sociocultural backgrounds, it is difficult but necessary to tailor conversation patterns and topics to one's conversation partners.\nINTRODUCTION: Especially in East Asian cultures, participants in casual group conversation seem to take into account a variety of social and cultural factors. These cultures are associated with a holistic, high-context cognitive pattern and with an interdependent and collectivist social orientation [39, 46, 47] . One of the distinctive characteristics of these cultures' group conversations is that they attend to relationships first-before the subject matter-by, for instance, using honorifics to indicate modesty and respect for others [2] . Regarding subject matter, talking about politics, religion, or sex with new acquaintances can be socially awkward [33, 34, 48] .\nINTRODUCTION: Researchers in the HCI community have noticed the potential value that technology-mediated solutions using mobile devices can have in group-conversation contexts [20, 31, 32, 37, 41] . However, most previous explorations of mobile systems to support group conversation have been in the context of decision-making and work environments, not casual group conversation. Thus, the sociocultural aspects of using such systems in casual, everyday situations have not been fully explored. Moreover, the use of mobile devices has been limited to either channels for content sharing or communication monitors. In both approaches, mobile systems have mainly been used to support users' existing behaviors and reflections. In other words, mobile systems have not been developed to recognize the diverse characteristics of group conversation or to actively intervene in such a conversation.\nINTRODUCTION: A skilled human facilitator can sometimes trigger spontaneous social interactions among group members and support natural self-disclosures that warm up a conversation, whether it is in a meeting, training class, team-building session, or another event. This has inspired us to explore the potential use of mobile devices to facilitate casual group conversations. In this paper, we do not aim to present a perfect mobile system. Rather, our aim is to learn about the potential role that mobile systems can play in casual group conversations. In exploring this role, we particularly aim to investigate (i) the impacts of a mobile system's passive and active conversation facilitation and (ii) the ways in which the sociocultural aspects that affect casual group conversations should be considered in the design of adaptive and proactive mobile systems.\nINTRODUCTION: For this research, we designed and developed Flower-Pop, a mobile system that supports casual, face-to-face group conversations. Flower-Pop consists of multiple mobile devices, a table, and a short-throw projector; it uses a custom-built software platform. This system provides facilitation modes that use visualization and photo sharing to both passively and actively interact in group conversations. This system provides passive facilitation modes such as a channel for content sharing and a communication monitor; its active facilitation modes include proactive conversation visualization and photo sharing to encourage inactive participants to share photos and select random people to speak. We evaluated the design and its impacts in a field study regarding the potential use of mobile devices as tools to facilitate group conversation. Based on interviews with study participants, we further discuss how to improve mobile systems so that they fit naturally in the casual conversation context. By understanding the potential sociocultural advantages and challenges of using such systems in casual group conversations, we will be able to develop a mobile system that smoothly supports and moderates conversations. Through this research, we hope to gain insight about how to design mobile systems that act as conversation facilitators to support interpersonal relationship development in a casual conversation context and about how to develop HCI technologies for this specific context.",
    "relatedWork": "RELATED WORK: Technologies for casual group conversation have been investigated for a variety of purposes, from allowing individuals to reshape their experiences of time and space [8] to helping them to adapt their social perspectives [17, 24, 43] . According to the previous studies, in casual settings, shared communication technologies (e.g., television and radio) are often welcomed [25] , but personal devices are viewed as creating tension among group members; the use of such devices is often managed through various social norms and restrictions [15, 16] . These findings mean there is a need for future work to explore the ways in which technologies can be meaningfully integrated into the social space of group conversations and thereby contribute to social interactions. In the same vein, our research project explores the ways in which mobile systems can be used in everyday conversational situations. In the following section, we present technology-mediated solutions that have been used in casual group conversations and review the sociocultural characteristics of such conversations.\nTechnology-mediated Solutions in Casual Group Conversation: Researchers in the HCI community have noticed the potential value of technology-mediated solutions in which mobile devices are used in group conversations [14] . In terms of functionality, significant developments have supported various group-conversation activities. The HCI literature contains many use cases for collocated mobile devices, including conversation visualization [13, 28, 29, 42] , media sharing [16, 38, 41] , and collocated searching tasks; these use cases often involve interaction with additional screens or multiple mobile devices [32] . These works demonstrate the benefits of using technology in collocated interactions and refute the negative view that mobile devices create social isolation.\nSociocultural Aspects in Casual Group Conversation: People from different cultures present distinct cognitive patterns and social orientations [9, 21] , which can affect how they ask questions, seek information, and socialize during a group conversations. Thus, it is important to understand the role of sociocultural factors when designing technological mechanisms to support casual group conversation. Our aim with this research is to learn about the potential role of mobile systems in casual group conversations and especially as conversation facilitators. It is necessary to understand the social and cultural factors that people take into account in group conversations. By understanding the potential sociocultural advantages and challenges of casual group conversations, it will be possible to develop a mobile system that smoothly supports and moderates conversations.\nSociocultural Aspects in Casual Group Conversation: In general, Western cultures are associated with an analytic, low-context cognitive pattern and with an individualist social orientation; by contrast, Eastern cultures are associated with a holistic, high-context cognitive pattern and with an interdependent and collectivist social orientation [39, 46, 47] . These differences are prominent in the group conversation pattern. Group conversations in Eastern cultures take into account a variety of social and cultural factors. Especially, one of the distinctive characteristics of group conversations in East Asian cultures is that participants attend to relationships before subject matter, using honorifics to indicate modesty and respect for others [2] . Among East Asian speakers, group conversations can spring from traditional values of propriety and deference. Propriety refers to respect for a hierarchical order in human relationships. Deference involves yielding to differing or opposing views. A group member who practices propriety and deference is passive and defensive in group conversations. In addition, East Asian speakers seek harmony in group conversations. In most aspects of interpersonal communication within these cultural areas, participants must take into account the other people's feelings and speak to avoid hurting those feelings. Most East Asian societies value avoiding confrontation and maintaining harmony. High context is another aspect of the East Asian style of group conversation, which is more affective and situation-oriented than the Western style. Nonlinguistic elements such as feelings and attitudes play an important role in interpersonal communication in the Eastern style. The Western instrumental style emphasizes ideas and thoughts, but the more affective East Asian communication style emphasizes feelings [45] .\nSociocultural Aspects in Casual Group Conversation: Some of the research on communication systems has implied the importance of sociocultural and environmental factors. For instance, research on technology use in the home and in public settings has shown that social relations among group members affect technology use. Hiniker et al. showed that specific social relationships (e.g., parent-child relationships) cause some of the differences in technology use, which implies that systems should be designed with groups' sociocultural backgrounds in mind [19] . In the same vein, O'Hara et al. showed that people care about host-guest and adult-child relationships when using technological supports during mealtime [41] . Because the previous research implies that sociocultural factors are important, it is necessary to explore specific sociocultural aspects of casual group conversations and to develop a system that is aware of such aspects to ensure smooth conversation facilitation. This study does not aim to compare cultures. However, to develop a system that is more adaptable to casual conversation situations, we aim to explore the factors that people care about in conversation (beyond simple conversational utterances). In particular, we aim to learn about the advantages and challenges of using mobiles devices as active facilitators in a culture that places particular emphasis on having both hierarchy and high context in group conversations.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: During the last years gaze-based input has found its way into the games domain. So called AAA games, like \"Assassins Creed Origins\" [24] or \"For Honor\" [25] , support eye tracking technology [23] that should provide players with a natural user interface. Furthermore, gaze-based interaction techniques should allow to act more e ciently within the world in comparison to other game input devices, such as gamepads, keyboards, etc. Typically, gaze is used as a partial (and in some cases complete) replacement for mouse and gamepad input or as a substitution for pointing devices in general. Via gaze, for instance, players can select game objects by looking at them, change the current camera view by moving their gaze around the screen, and may quickly navigate to a point of interest on a mini map [22] .\nINTRODUCTION: However, we deem that the aforementioned examples do not tap the full potential that gaze-based interactions o er. Velloso et al. [26] describe the design opportunities and areas of application by categorizing the di erent ways in which the eyes can be incorporated into games as a resource for future design. In their work the researchers compiled a list of game mechanics and elds of application for games supported with eye tracking technology, and derived a taxonomy by reviewing the State-of-the-Art of the HCI research literature and by analyzing various gaze-enabled game prototypes. Besides the usual approach of using gaze for explicit game input, Velloso et al. [26] describe the notion of social gaze, which o ers new layers of game-based interaction by adding a valuable channel of non-verbal communication between players. For instance, gaze and eye contact can regulate information ow among players [13] , or it may indicate shared interest on a speci c game entity [28] .\nINTRODUCTION: Although research in the area of social gaze in games appears to be very promising, only few research projects have been carried out (e.g., [2, 27, 29] ), leading to an under-explored research area with many unresolved issues. One of those challenges is the inclusion and augmentation of social gaze-based information in game se ings. Typically, the current gaze position is communicated to the other players by overlaying 2-dimensional visualizations (such as a dot, circle, heatmap, etc.) superimposed over the game view without including interaction mechanics that could amplify the social interaction (i.e. [17] ). Although there are rst research activities [14] that focus on asymmetric player-spectator constellations, the inclusion of socially relevant events and behavior in social gaze, such as, shared interest, turn-taking, etc., plays only a minor role in current research.\nINTRODUCTION: ese aspects could be especially relevant where players have to collaborate in on-line remote se ings to reach a certain in-game goal, which can be found in many games, such as World of Warcra [6] . We argue that an approach of gaze that supports and augments social aspects in collaborative se ings might enrich communicative acts, as it gives players the opportunity to use an additional nonverbal communication channel, o ers them means to coordinate their collaborative actions to avoid mis-or non-understanding, and gives them a feeling of \"being together\" with a another player in the game world (e.g., social presence [19] ).\nINTRODUCTION: Our work addresses the exploration of the potentials of a new approach of integrating social gaze in a collaborative se ing. In detail, we want to nd out if players experience a higher degree of social presence (co-presence, psycho-behavioral interactions) in a collaborative se ing in comparison to solutions with basic gaze functionalities or without any gaze features.\nINTRODUCTION: Summarizing this paper features the following contributions in the eld of social gaze in games:\nINTRODUCTION: \u2022 Introducing a novel approach of integrating gaze-based interactions in social se ings by augmenting social gazebased behaviors (e.g., joint a ention, mutual gaze, etc. ) via interaction mechanics and visualizations \u2022 Carrying out a comparative study consisting of 3 conditions: gaze (basic gaze features), social gaze (our novel approach that augments social haze-based acts), and no gaze (interaction without any gaze features) \u2022 Focusing on social presence in the context of social gazebased interactions in a collaborative rst person 3D game se ing rough our work, readers should gain insights on how social gaze could be integrated into collaborative se ings to foster social interactions in on-line se ings. Game and interface designers should receive information on how to design gaze-based interfaces that help players to coordinate their actions more e ciently and give them a feeling of being together in the game world. Furthermore, researchers should be informed about eye contact integration strategies and their e ects on the player experience.",
    "relatedWork": "RELATED WORK: Gameplay experiences are o en driven by the interplay and social interaction between players. In a face-to-face se ing, where another person is physically present, gaze and other bodily signals are important means of communication. Digital gameplay however, disembodies the player to some extent, pu ing the very player into the role of an avatar, potentially also limiting the communication via bodily signals.\nRELATED WORK: For instance, the work of Benford et al. [3] identify among others also embodiment design issues with availability and degree of presence and propose that they might be resolved explicitly through the use of an indicator, such as eyes, on their body. Moreover, the analyses from Bowers et al. [7] reveal systematic problems in collaborative virtual environments (CVEs) related to turn taking when the in-game characters have a considerably constrained way of displaying their a entiveness to others. ey suggest that for CVEs, where participants are interacting with one another, a direct support of actions of social interactional signi cance, such as glances, should be considered. us, the incorporation of gaze-based interaction as a means of interpersonal interaction seems viable, nevertheless, not very prominently used in games for interaction with another player in contemporary games.\nRELATED WORK: Social gaze in itself is a powerful medium for nonverbal communication in our daily conversations and in collaborative tasks [1] . Eye contact and gaze regulate information ow [13] , turn taking and mediate shared a ention on an area of interest [28] . Gazing at an object during an interaction can also establish a mutual understanding of what is being discussed. us, social gaze is recognized as a key component of interaction and is determined by the direction of the eyes, the orientation of the head, and the orientation of the body [9] . Furthermore, the awareness of gaze has positive inuences on collaboration, as people can apply this communication modality to manage their interaction and to pinpoint the possible interpretations of an entity [8] .\nRELATED WORK: Based on the importance of gaze in social se ings, it appears to be feasible to also include gaze-based interactions in the context of play and games [26] . In a social game se ing, the inherent contextual social a ordances as well as the interaction modalities, o en result in the notion of di erent roles (e.g., being an active player or a passive spectator).\nRELATED WORK: Lankes et al. [14] , for instance, propose an experimental se ing that investigates shared gaze integrations (constant gaze and eye contact) in games and their e ects on the social presence perceived by di erent roles (players and spectators) in a remote scenario. Results show, depending on the type of the gaze integration and the role, positive e ects of gaze towards an increased awareness and among participants.\nRELATED WORK: Eye contact and gaze is also considered to be connected to mutual a ention, in turn, facilitating a feeling of presence. Shahid et al. [21] explored the e ects of mutual gaze on the perceived social presence during video-mediated and co-present gameplay. Based on their gathered results, they argue for the inclusion of mutual gaze interaction as an integral part of playful cooperative se ings, as it leads to increased social interaction and mutual understanding.\nRELATED WORK: Gupta et al. [11] have dealt with the e ect of one-sided sharing of virtual gaze and pointing cues for remote collaboration. eir results show that these not only improved a given task performance, but also played a signi cant role in creating a sense of co-presence and creating deeper understanding between users.\nRELATED WORK: Maurer et al. [15] show how a shared gaze visualization changes in which way players work together and form collaborative strategies based on di erent combinations of gaze interaction and verbal communication. Study ndings highlight the positive in uence of a shared gaze visualization on team cohesion and involvement towards increased perceived social presence among cooperating team members.\nRELATED WORK: Maurer et al. [16] also inferred that the social se ing of a game itself, drastically in uences how the means of gaze-based communication is utilized by players. e meaning and usage of shared gaze interaction changes, for instance, when comparing competitive and cooperative se ings.\nRELATED WORK: Regarding the use of gaze in competitive games, Newn et al. [17] investigated if and how players can infer the intent of an opponent in a digital game based on di erent gaze visualization strategies. In the eld of cooperative gameplay, Newn et al. [18] researched gaze awareness in a tabletop game by projecting a player's gaze onto the gameboard.\nRELATED WORK: is line of research and related experiments show the impact of gaze-based non-verbal communication in remote gameplay, and how such communication channels can be a way to actively shape a social gameplay se ing. Gaze interaction and in particular embodied gaze representations facilitate social bondings between players, support feelings of presence and being together in the gameworld, ultimately supporting embodied sense-making in digital gameplay.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Presence in a virtual reality environment is the feeling of being physically present in a non-physical world. It is observed by a real user by perceiving his/her surrounding agents and their behavior in the virtual environment [14] [4] . Presence is sub-categorized into Spatial presence and Social presence [14] . We focus on social presence which can be judged by the sense of perception of being together with all the virtual humans.\nINTRODUCTION: Environment awareness is important for humans to interact in real life. There are many situations especially sport games like football, hockey etc. where humans interact and communicate with one another using objects. We focus on improving social presence for such scenario in a multi-party VR environment. In virtual environment, computer agents can perceive signals by computer process but to make them to be more human-like, corresponding attention behavior is required. Attention behavior has been a focus of research since a long time. Researchers in [12] , [9] and [11] implement gaze based model to make attention behavior more realistic. Researchers in [10] adopt a saliency map based visual attention model for the interactions between agent and the virtual scene. Most of these researches develop attention based simulations but do not focus on presence in VR.\nINTRODUCTION: Gaze is an important part in attention behavior and also regarded as a key aspect of non-verbal communication [2] [13] . In a virtual environment gazing can be directed to independent objects as well as other agents. Researchers in [8] propose an interactive approach, PedVR, which enables real users to interact with the virtual crowd in an immersive virtual environment. They develop a high dimensional trajectory by coupling 2D navigation with full body motion synthesis combined with gaze computation.\nINTRODUCTION: Other developments have indicated the effect of gaze in terms of interpretation of emotional expressions [1] [6] . Gaze is also increasingly being used by embodied conversational agents (ECA) [11] .\nINTRODUCTION: There have been various developments for incorporating interactions in VR. Recently, researchers [3] propose to import AI into virtual environment to make virtual avatar more smart. Researchers in [7] illustrate how communication improves realism in crowd simulation. They focus on communication between hundreds of computer agents but it is basically for crowd simulation and there ",
    "relatedWork": "",
    "rq": " q4. how much did the gazing behavior increase the feeling of being present in the same environment as the computer players?\n significantly q5. how much did the realistic motions improve your experience in vr?\n q7. how much did collision avoidance amongst players improve the feeling of being present in the same environment as the computer players?\n significantly q8. how much do you expect from computer players to pass you the ball?"
}
{
    "intro": "INTRODUCTION: The past couple of years have seen virtual reality reach the consumer market in a significant way. While a lot of attention has been paid to the high-end desktop PC-based systems, devices such as Samsung Gear VR and Google Daydream, which convert a high-end smartphone into a head-mounted display, are now a viable platform for more complex immersive virtual reality environments. Given that manufacturer's flagship smartphones tend to ship millions of units, such systems are bringing immersive VR to a broad and diverse audience. In particular, as devices become more popular, they may become a viable medium to support collaboration at a distance.\nINTRODUCTION: Gaming and social platforms have used non-immersive virtual environments for many years. Certain platforms such as World of Warcraft and Second Life have supported long-lived communities that have been studied from a variety of viewpoints. Certainly, very strong social relationships are formed and maintained on such platforms. However, such platforms have a certain barrier to entry. The form of technology, difficulty of use, or story-based settings have meant that whilst popular, they are not broadly used for social communication. Immersive virtual reality has new affordances over such systems. Whilst social interaction has been studied on immersive systems, this has typically been in a laboratory setting.\nINTRODUCTION: In this paper, we describe an exploratory in-the-wild study on the use of immersive virtual reality to support social groups. We are particularly interested in supporting existing groups: thereby we can investigate how groups might migrate to use immersive virtual reality as a collaborative medium as an alternative to voice, picture or text messaging. Participants in our study used social environments on the Samsung Gear VR over an extended period. Through diary studies, quantitative measures and interviews, we explored how they adapt to and use the systems.\nINTRODUCTION: The following section will cover the existing literature on how interpersonal relationships are created between people, and the way in which technology has previously supported this type of behaviour. We review current research into the effects avatars have on communication between users in virtual environments, and the way in which relationships can develop between users and avatars. This will provide a basis of contextual understanding that can then be applied to scenarios in VR, as well as act as a basis of comparison between the various technologies. Then, an in-the-wild exploratory study is conducted to gather data on how real users engage with their familiars remotely in VR. We use principles outlined in Grounded Theory [6, 15] to analyse the results and discuss how VR can be used as a platform to promote the development of interpersonal relationships.",
    "relatedWork": "RELATED WORK: A great deal of previous research into interpersonal relationships has focused on the importance nonverbal communication plays in human interaction [1, 5, 16] . Steve Duck [10] states that \"You cannot utter a word without also simultaneously indicating how you feel about the other person\" to highlight the significance paralanguage, facial expressions and posture play in the broader context of communication.\nRELATED WORK: Considering the vital role nonverbal communication plays in our everyday interactions, it is critical to consider how virtual environments support nonverbal acts. Collaborative virtual environments (CVEs) have been studied for many years. Such platforms typically represent individual users as an avatar inside a 3D virtual environment where the user can move around the virtual space. The avatar also acts as a mechanism to interact with the world: it may have arms or other means to effect grabbing and moving objects. Thus the avatar serves to represent the user's position, interest and activity [2] . However, even simple avatar representations convey subtle social dynamics.\nRELATED WORK: Fabri et al. [11] highlighted the affect nonverbal communication had on avatar-avatar interactions, noting that facial expressions and posture of avatars triggered emotional responses from players. A study by Moser et al. [20] builds on this and reports that the facial expressions of avatars in VR also have the potential to evoke emotion in users.\nRELATED WORK: Yee et al. [37] conducted an observational study in the online virtual world simulator Second Life (SL) to explore whether social norms surrounding nonverbal communication and interpersonal distance from the real world transferred into virtual contexts. The study established that social behaviors relating to personal distance were present in virtual environments: male-male dyads displayed larger interpersonal distances than female-female dyads, as well as less frequent eye contact. The study also provided evidence that the avatars of the virtual environment would adhere to standard turn taking protocol during conversations as well as noting shifts in user posture during periods of increased levels of intimacy. Further supporting their hypothesis of Equilibrium Theory; that the level of intimacy between dyadic groups is \"maintained by compensatory changes in gaze or interpersonal distance\" [1, 4, 37] .\nRELATED WORK: Similarly, Friedman et al. [12] looked at the social behavior of players in SL when pre-programmed bots would interact with player avatars. The experiment uncovered that when players engaged with the bot they would apply a level of proxemics appropriate to the context. However, contrary to the findings of Yee et al. [37] these did not necessarily mimic real-world behaviors, which the authors claimed was due to a hypothesized lack of presence in virtual environments such as SL.\nRELATED WORK: Presence is the concept that, to some extent, users of immersive virtual reality systems believe that they are in the virtual environment and thus tend to react to the environment in a manner similar to how they would in the real world [32] . Slater's analysis [33] distinguishes 'place presence', which is the illusion of being in a place caused by having the senses immersed in a first person view that is driven by the natural movements of the user, from 'plausibility illusion' which is the impression that what is happening in the virtual environment is happening around the user. A representation of the user as an avatar that the user can see is a key part of both place and plausibility illusion: it both evidences that you are part of the environment, and that events around you are plausible because your body is involved in them. Recent results have shown a fascinating relationship between the virtual body and sense of presence. These include having a type of body ownership over the body [34] and the virtual body having an important role in cognition through supporting gesture [35] . We can thus expect that the combination of immersion and embodiment has an important impact on social behavior.\nRELATED WORK: Collaborative immersive virtual reality systems date back to at least VPL's seminal Reality Built for Two system from the late 1980s. Otto et al. review the use of immersive systems for closely coupled collaboration [21] . They highlight that with immersive systems, users can naturally form different working configurations, where they can observe each other and shared objects. In particular, they highlight that in an immersive system, it is easy to observe where other users are attending, and thus gaze becomes an important resource. Dodds et al. have shown that the users' having animated avatars is an important resource for communication [8] . Our study will focus on a recent generation of Samsung Gear VR that did not come with hand controllers, but we will note examples of how users overcame this.\nRELATED WORK: Previous studies of immersive CVEs have tended to look at laboratory settings over short periods of time. An exception is Steed et al. [36] who studied the way dyadic groups of strangers and friends interacted and collaborated with one another using an immersive virtual reality CAVE. Participants who interacted with friends for several hours quickly establishing methods to perform the group tasks and engaged in small talk unrelated to the task. In contrast, a group of strangers coordinated poorly. Steed et al. speculated that a possible reason for this is an absence of social and interpersonal cues such as authentic facial expression and appearances. Our study is the first that we are aware of that has studied groups of users over an extended period.\nRELATED WORK: Gillath et al. [14] and Prendinger & Ishizuka [24] have shown that VR can be used as a tool for users to display empathy using virtual characters. Building on this, Riva et al. [25] noted that the degree of presence experienced by users in VR has a direct impact on the emotional states experienced.\nRELATED WORK: Typically, in SL and similar non-immersive CVEs, the user's avatar is on the screen and the user acts through a 3rd person perspective. This breaks the sensorimotor contingencies inherent to immersive virtual reality, and thus makes the relationship to the avatar unique. Schultze and Leahy [31] explore the avatar-self relationship and identify three key components: The Avatar, the Self and The Relationship between the two. This relationship is key in order to attain a sense of virtual presence. They then characterize this into two interrelated categories -(i) telepresence, the feeling that one is actually a part of the world they are experiencing and (ii) social presence, the sense of being in that space with others [4] . The idea of social presence, or co-presence is used widely in related research and is the focus of much of the research in the area (e.g. see [18] , [29] , and [30] ).\nRELATED WORK: Due to their wide availability, and popularity, non-immersive virtual environments such as SL have been studied extensively. Some users have engaged with such systems for years, and thus have formed strong social bonds with other users within the systems. Our study will focus on immersive CVs being introduced to existing social networks. The rationale of this is to understand whether existing social groups transfer successfully, without disadvantaging or discouraging members and thus changing the social group dynamics.\nRELATED WORK: Because the avatar is usually customizable, an interesting variety of norms and protocols have evolved around avatar representations. Galanxhi and Nah [13] suggest that avatars can be used as masks providing users with a layer of anonymity, creating an emotional distance between themselves and others. Prelinger [23] outlines aspects of the self that people use to identify others such as facial features, gender and physical characteristics. Bessi\u00e8re et al. [3] describes a more 'realistic' approach to avatars, one where the avatar symbolizes an idealistic representation of the communicator. In our study, users will be able to customize avatars and it may be that the immersive situation biases users towards one specific use of avatar representations.\nRELATED WORK: Ducheneaut et al. [9] and Kafai et al. [17] both discuss avatar representation trends from Massively Multiplayer Online Role-Playing Games (MMORPG) and other online virtual environments stating that avatar preferences tend to correspond to more representative portrayals. Mcarthur [19] elaborates on the character creation interface and the experience of creating an avatar, highlighting the importance of identity fidelity, that is, how accurately users are able to represent themselves through their avatars, and attempts to form a unified standard informed by user experience methodologies.\nRELATED WORK: There are several social VR demonstrators for current consumer systems. Our study involved users in Oculus Rooms 1 , AltspaceVR 2 and vTime 3 .",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Videoconference, the dominant medium for remote meetings, uses video cameras to provide remote participants with a 2D, screen-based visual connection. Embodied VR, an emerging alternative, uses motion tracking and VR headsets to place participants in a shared 3D environment. This immersive 3D experience is more similar to face-to-face interaction, although at a lower fidelity, and it is important to understand how it impacts the collaborative experience. This paper describes a study comparing people's behavioral patterns across the two media and over a set of four tasks spanning different elements of workplace meetings. It builds on a long standing interest in how the affordances of communication media support various tasks [14, 19, 61] . Understanding the behavior engendered by the different affordances has important ramifications for the design of remote collaboration systems. As but one timely example, recent work postulates the exhaustion people feel from videoconferencing, so called \"Zoom Fatigue\", may result in part from the behavioral pattern of people receiving too much gaze [1] .\nINTRODUCTION: The study employs state-of-the-art, embodied VR technology that includes body tracking, face tracking and finger tracking to drive the movement of semi-realistic avatars (Figure 1 , accompanying video), which provides a compelling interaction experience. Great care was taken to ensure that the videoconference (VC) and virtual reality (VR) conditions were as evenly matched as possible in the experiment, for example by employing a recommended videoconference framing that shows the upper body so that gestures read clearly, providing a shared mouse interface so that people could still point at shared artifacts in VC like they can in VR, and having all participants maintain a fixed, seated position in both conditions. Differences remain, however. The model based avatars have lower fidelity than video and do not fully reveal a person's identity (gender and ethnicity were matched). Conversely, the avatars allow people to be located in a shared 3D space, while videoconference remains screen based.\nINTRODUCTION: Participants worked in groups of three to complete a warm up and four experiment tasks that were designed to replicate different types of activities that might occur during meetings. An intellective task required them to come up with answers to questions where there was a correct answer. A decision making task required them to reach consensus when there was not a single correct answer. Two mixed-motive tasks required them to negotiate where each team member had different desires. The second mixed-motive task introduced a floor plan to visually ground the task and explore how this impacted nonverbal behavior. The experiment was run with Medium as a between subjects condition and Task as a within subjects condition. In other words, every participant was assigned to only one Medium, but completed all four Tasks. In total, 210 people participated in the study, 35 groups of three in each medium.\nINTRODUCTION: An analysis of performance and subjective measures (e.g. social presence) did not reveal notable differences between the two media. For instance, post task surveys derived from [10, 25, 45, 60, 61] showed high ratings without significant differences between media for the scales Satisfaction with Medium (mean 6.30 for VC and 6.41 for VR on seven-point Likert scales), Co-presence (mean of 6.48 for VC and 6.53 for VR) Mutual Understanding (mean of 6.12 in VC and 6.24 in VR) and Clear Communication of Affect (mean of 5.42 for VC and 5.37 for VR) This paper focuses on the marked differences in behavioral measures.\nINTRODUCTION: Problem Statement: This work seeks to understand the behavioral differences that arise from people's use of either embodied VR or videoconference as a medium for conducting work meetings and if these behavioral differences are based on the nature of the work task. As the current default remote meeting option, VC provides an important comparison point for evaluating the behaviors induced by embodied VR. It is important to understand the potential and impacts of embodied VR ahead of potential widespread adoption, and these are partially contained in the behavioral patterns the media encourages.\nINTRODUCTION: Contributions: As far as we are aware, this paper reports on the first large scale comparative study of behavioral patterns of triad interaction across videoconference and embodied VR. The study included 210 participants with diverse demographics. To illuminate the role played by the different technologies, the basic meeting configuration is kept as similar as possible between the two media. Different types of tasks are contrasted to explore if behavior changes as a function of task. Evaluated behavioral measures include conversational turns, gaze patterns and nonverbal behavior. The paper contributes clear evidence of marked behavioral differences across task and media. Media differences include that VC participants spent much more time looking at interlocutors, especially their faces, they provided more verbal and nonverbal backchannels, performed more self-adaptors, fewer deictic gestures, and in some cases, had longer conversational turns. A potential explanation for these behavior changes is that participants in videoconference exerted greater effort to maintain a social connection than participants in embodied VR. This suggests a differential in exertion required to use the two media that will likely impact users and warrants further in depth study.",
    "relatedWork": "Related Work: There is a large related literature that is briefly sampled here. Influential for this work is the study of Strauss and McGrath [61] that focused on how medium (online chat system or face-to-face) interacts with task type (idea generation, an intellective task, and a judgment task). Results showed very similar quality output across the two interfaces, but face-to-face was more efficient. In the judgment task, people were less productive for the mediated interface and responded more negatively to the medium and task.\nRelated Work: An early ethnographic study of videoconferencing [30] showed it had advantages over audio only in factors like showing understanding, expressing attitudes and nonverbal communication, but performed worse than face-to-face for peripheral cues, controlling the floor and pointing to objects, with the lack of correct eye contact seen as distancing. Dong and Fu [18] found videoconference more successful than audio or text for negotiation and attributed the difference to exchanging information in small pieces. Hauber et al. [27] found that spatial interfaces based on using multiple video screens to create a 3D environment positively influenced social presence and copresence measures in comparison to 2D, but the task measures favored the two dimensional interface. Other work shows a benefit of adding spatial video to an audio conference [29] . Nguyen et al. [41] compared videoconferencing systems that had a single camera for 2-3 remote participants with ones that had dedicated cameras and projectors directed for each participant. The non-directed video condition showed significantly less co-operative behavior than either directional video or face-to-face. Follow-up work showed greater empathy for an upper-body video framing than head-only [42] , so that wider framing was adopted in this study. Other work has explored video projections for two person remote interaction [48] . Schroeder [52] suggests that avatars could provide the spatial component missing in video, but they suggest a concern that the representation of the person may not be authentic. Wong and Gutwin [65] found that pointing in collaborative virtual environments benefited from being able to observe the prepatory arm motion, a direct connection between the gesturer and referent, and awareness of others views.\nRelated Work: Early avatar research used a mixed head mounted displays with participants at workstations, and found a positive relationship between presence and co-presence, with accord increasing with presence [56] . Dodds et al. [16] found that a gesturing avatar led to more words guessed correctly in a game scenario and more gestures than a static avatar. Bente et al. [9] conducted an early, large scale study of avatar representations in which pairs of participants selected job applicants using one of six interfaces: text, voice, video conferencing, low-fidelity avatar (cartoon-style) and high-fidelity avatar (3D character). Text performed worse than all conditions on perceived intimateness, copresence and emotionally based trust. Most dependent variables did not show a difference between video and avatar conditions. Notably, their avatars were displayed on 2D screens rather than the shared 3D environments used in this study. Steptoe et al. introduced one of the first avatar gaze tracking systems and provide preliminary evidence that it improves communication [59] . They later found that realistic eye movement increases participants ability to detect truth and deception [58] .\nRelated Work: A preliminary evaluation of the Holoportation AR avatar system suggests participants experienced spatial and social presence, and appreciated being able to control their point of view [46] .\nRelated Work: Pan and Steed showed that people asked for less advice from a key-frame animated, 2D projected avatar than a video or a robot expert, but would always prefer the more expert agent [47] . Smith and Neff [57] showed similar social presence and behavioral patterns for faceless, motion tracked avatars and face-to-face communication, but lower presence and a shift in communication patterns when avatars were not present in VR. Other work found no differences for avatar rendering style, but some differences for the amount of the body that was motion tracked [67] and a preference for full body avatar motion [28] . Jo et al. [31] found that video performed worse than avatars on a measure that included spatial and social presence questions.\nRelated Work: This study employs model-based avatars. A pre-rigged character model is used for each avatar, much like would be used in videogames, its movements driven by live tracking. An alternative avatar technology employs depth cameras or other optical techniques to create a point-cloud model of the person in real time, also known as 3D video (e.g.. [4, 22, 46] and hybrid approaches [32] ). 3D video has the potential advantage of better preserving the person's identity, but tends to suffer from visual artifacts such as tear out (holes in the mesh), pixelation and issues with occlusion, as well as requiring complicated capture setups. It is also difficult to place multiple avatars in a shared 3D environment with this technique without also rendering the head mounted displays on the avatars, which blocks the face and limits communication. This paper is not focused on the particular avatar technology and we will simply note that at this time, model-based approaches offer more consistent visual quality and easier immersion in 3D.\nRelated Work: The authors of impressive recent work suggest that it is the first to feature avatars with live tracking of the body, gaze and the lower portion of the face [50] and develop methods to augment avatar behavior beyond participants' actual motions. Our work tracks the same features, and also tracks hands and the full face, but our focus is on studying interaction patterns relative to a videoconference baseline, so we do not intentionally modify participant behavior. Other work has also explored the potential of adaptation, focusing on facial expressions in VR [26] . These studies point to the additional potential VR offers for modified or augmented social interaction.",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": "1. what is the effect of having a shared surface on user behavior, task performance and user experience?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: At the weekend, Sebastian decides to visit a local art gallery. While wandering through the exhibition, he realizes that some of the traditional interpretation panels that usually hang next to the artworks are replaced with iPads. He takes a closer look at an iPad and reads some reflections on the artwork. Next to the iPad he finds instructions on how to connect to it to add an interpretation of the artwork. Always interested in discussing art, he connects his smartphone to a dedicated network and is automatically connected to the iPad. He starts writing in the heat of the moment: 'This artwork reminds me of a tablecloth my grandmother used to have.' Then he decides to continue through the rest of the exhibition. At the exit he is informed that he can also track from outside the gallery how the interpretation evolves, and that he can edit the interpretation displayed on the iPad from anywhere and at any time. Later that evening, while relaxing on the couch and reflecting on the experiences of the day, he becomes curious whether the interpretation he added at the museum has changed. He turns on his desktop computer, types in the URL of the web-based platform he was given by the staff of the gallery, searches for the corresponding artwork, and realizes that his text has been replaced. Sebastian ",
    "relatedWork": "RELATED WORK: While traditional museums use official interpretations written by the curators, it is becoming more common for contemporary institutions to support visitors in engaging with artworks and determining their own way of making meaning [12] . However, research has repeatedly shown that using more technologies to engage citizens in art galleries does not necessarily lead to increased engagement [8] . Research has been focusing on engaging visitors when they are in situ at the museum, but has not investigated the advantages of inviting participation remote from the museum [12, 13, 14] . Fosh suggested moving towards technologies that enable access remote from the museum to further support visitors in making meaning [14] . Chalmers and Galani highlighted the lack of research that bridges between local and remote technologies in museums [15] . Al Zubaidi-Polli and Anderst-Kotsis [9] stated that supporting engagement and sustaining participation in collaborative coherent art interpretation writing requires an extension from in-situ to ex-situ participation. Our hybrid PIT focuses on enabling both in-situ and ex-situ participation and thereby supports collocated and remote usage.\nRELATED WORK: Our conceptual design supports real-time collaboration and multiple-device shareability, and thus resembles the approach by Klokmose et al. [16] [17] . They presented an environment for exploring shareable dynamic media and augmented web technology with real-time sharing. The only difference to their approach is that our hybrid focuses on collaborative real-time authoring, writing and sharing of information rather than on sharing various kinds of documents or media. The Lively Kernel offers an integrated web browser development environment to support run-time software malleability, but -unlike our approach, which supports both synchronous and asynchronous collaboration -it is limited to asynchronous collaboration [18] .\nRELATED WORK: In contrast to current approaches, our conceptual design supports five conditions; 1) collocated, 2) remote, 3) synchronous, and 4) asynchronous collaboration, and 5) hybrid communities, as it enables both online and offline interaction and communication. Notions of hybrid communities have traditionally been formulated around binary relationships, either as being distinct from offline social relations or acting as an additional but separate layer on top of offline social interactions. However, bridging the gap between offline and online participation, which several research groups have attempted (e.g. [1, 3] ), is extremely challenging. More recently, research has started to understand online and offline spheres as fundamentally interwoven with, and co-constitutive of, local, place-based communities (e.g., [3, 6, 20, 21] ). Jurgenson argued that the digital or online world can no longer be seen as existing in parallel with, and independent from, the 'real' or offline world; the activities of the digital community cannot be separated from physical, cultural, social, and historical aspects of local places [21] . Dourish and Bell wrote that \"[t]he technologically mediated world does not stand apart from the physical one within which it is embedded; rather, it provides a new set of ways for that physical world to be understood and appropriated\" [20 p. 132 ]. As we experienced in our two design experiments [2, 22] , offline and online communication channels are important when users face technical challenges or need to discuss the content of the collaborative writing process. Therefore, our conceptual design adopts this integrated perspective and enables both channels for interaction. Our hybrid PIT is a concrete endeavour to bridge online and offline; and in-situ and ex-situ participation.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Novel and more engaging technologies, together with the possibility to exploit emerging trends in information and communication technology (e.g., 5G), are shaping the tourism and hospitality sector [3, 15] . This scenario is actual today more than ever due to the critical situation we are experiencing because of the COVID-19 pandemic [6, 21] . The pandemic has compelled researchers and industry to explore better-suited technologies to reboot the tourist industry and regain consumer confidence [10, 24] . In this light, the tourism sector embraced immersive technologies, such as virtual and augmented reality (VR/AR), able to provide virtual representations of touristic places and to offer touristic innovative services and experiences, with the final aim to increase the likelihood to physically visit such sites in the future [6, 11, 14] . Indeed, the opportunity to explore such immersive technologies in the hospitality and tourism sector is not new. In fact, the recent widespread diffusion of mobile devices that enables access to multiple sources of information in a ubiquitous, continuously connected environment has changed the way we experience tourism-related services [8, 13] . In addition, the emerged concept of sharing economy is appropriating a collaborative and peer-to-peer market model, which prioritizes utilization and accessibility over ownership [5] . Tourism and hospitality have shown to be one of the pioneering sectors for its growth: residents can share their goods (such as homes, cars), as well as their expert local knowledge (e.g. locals being tour guides) [5] . This interest is aligned with the increasing travellers' demand for authentic, experientially oriented opportunities with more meaningful interactions with locals [20] .\nINTRODUCTION: In this rich context, we present ShareCities, an application that allows visitors and \"future\" tourists to exploit the 360\u00b0virtual representation of a resident's room, including peculiar details related to her/his life/interests/hobbies to foster playful immersive interactions between the visitor and the local community. Using ShareCities, local inhabitants can publish information about themselves, such as their profiles and preferences about what to see and do in their locale, while visitors can browse authentic and unmediated information provided by locals and use the platform to initiate dialogue with them. In particular, in this paper, we present the results of an experiment, engaging 19 users in the role of \"future\" tourist to investigate the possibility to use personalized 360\u00b0rooms to foster curiosity and affinity as the first step to initiate a conversation between tourists and locals, before the actual face to face meeting. The final goal is to design a system that explores novel interactions to create connections between locals and tourists to facilitate authentic travel experiences mediated by locals.\nINTRODUCTION: The rest of the paper is organized as follow. The following Section details background and related work in social interaction between tourists and locals, and participatory platform in tourism. Then, we present the ShareCities platform and functions. The paper continues presenting the study we carried out, the methodology and the results. Finally, we conclude with final remarks and future works.",
    "relatedWork": "BACKGROUND AND RELATED WORK: In this Section, the main studies and projects that inspired our approach will be presented.\nSocial Interaction between tourists and locals: The connection and the social interaction between residents and tourists are deeply investigated in the literature. In their research, Moyle et al. investigated the cultural exchange between locals and tourists in two different Australian islands [16] . Through 30 interviews with the community and some tourism stakeholders, they demonstrated that residents can have good reasons to connect with visitors (that can be merely economic or coming from the wish to provide quality experiences). Nevertheless, the interaction between the two parts may vary: it can be meaningful for both of them or even hostile, so it is crucial to inform and educate the local community about the advantages that may arise from the interaction. In [22] , the authors investigated the integration between the emotional solidarity of tourists and the destination loyalty in Capo Verde through surveys. In particular, they also analyzed how the emotional closeness of tourists with residents can influence their satisfaction and, finally, the destination loyalty. Also, Stylidis, in [25] , exploits the interaction between the two parts to investigate how and if it can positively influence the image of the destination and the intention to revisit or to recommend it. In particular, concerning the interaction from the tourist's perspective, the author considered the friendship with locals, the tips and recommendations about the places to visit or where to eat, the information about the city's lifestyle, and if the interaction has improved the perceived sense of safety. Moreover, many of the studies that analyze the relationships between travelers and locals were conducted exploiting some sharing economy providers, such as AirBnB 1 and Couchsurfing 2 , to deeper understand the intention of tourists to live more authentic experience [7, 9, 20] .\nSocial Interaction between tourists and locals: In this study, we took a step forward, investigating if a 360\u00b0VR technology and customized environments can facilitate the creation of social interactions between tourists and locals before the actual meeting.\nParticipatory platform in tourism: Many studies in the field of tourism concern mobile applications or, more generally, participatory platforms. For example, H\u00e1-Vita is an interactive participatory platform that aims to foster interaction between locals and visitors to finally create empathy between them and awareness about environmental issues and local biodiversity [18] . To achieve this purpose, some video interviews of community members were uploaded to the platform. Also, the YouTube platform can be considered part of the participatory culture, as stated in [2] . Here, the authors proved that the videos seen by users (and that eventually help them choose a destination) are not those of the official channels, but those uploaded by other users who have experienced similar experiences.In [12] , ParmoSense is presented: a participatory sensing platform that exploits gamification to be able to provide real-time guidance to foster tourists' satisfaction during sightseeing in Kyoto. Finally, in [23] , the authors developed a participatory mobile application to lure tourists to local destinations such as one of Japan's three great gardens. Through the application, a user, who can be a tourist or even a local, can upload a photo or see photos updated by others, and enjoy updated information about events and sights.\nParticipatory platform in tourism: Inspired by the presented approaches, we designed a novel participatory system that is not focused on the touristic experience per se but aims to allow locals to freely personalized a virtual environment to represent their personality and interests. At the same time, tourists can interact with the digital rooms, and eventually, initiate a dialogue with the host.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: GCED is built around three key principles:(1) respect for cultural, linguistic, and ethnic diversity, (2) solidarity, and (3) a shared sense of humanity, which distinguish it from other educational approaches.\nINTRODUCTION: To establish these principles in childhood, intercultural collaboration using machine translation for children has become more popular. KISSY (Kyoto International Summer School for Youth) is one of the workshops where children from different countries can engage in group activities, which is annually held by NPO Pangaea, a non-profit organization. This workshop groups the children in intercultural teams with one adult facilitator for each team to support them. Communication in a foreign language is more difficult than communication in a native language [7] . Therefore, a machine translation embedded tool is used to support communication among children speaking different languages. However, children speaking low-resource languages have difficulties in understanding the conversation and expressing their intended meaning with their own words because the translation quality of low-resource languages is relatively poor. As a result, some children are less talkative than others, because the children with low-resource languages cannot follow the conversation [4] . An existing study found that questions from the adult facilitator can support children group work; they examined a design task with guided children's dialogs [8] . Hence, we focus on the behavior of the adult facilitator, who is considered to be important in guiding the children's conversation in collaborative settings. On particular note, this paper pays particular attention to the type of facilitator speech and analyzed chat dialogs between a facilitator and children to clarify which utterances increased the children's responses. To this end, we collected conversation log data from an international workshop by children and linked the facilitator's utterances with children's utterances in adjacency pairing. We then annotated the paired data with tags defined on the speech-act verbs by Vanderveken [9] . We then identified the significantly effective utterances of the facilitator using \u03c7 2 tests and residual analysis. This paper first introduces related studies and research and explains the conversation data analyzed. After that, it defines tags, annotates the conversation data with the tags and performs statistical tests on the tagged data. Last, the analysis results are discussed, and some conclusions are drawn.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Today, we rely heavily on digital technology to connect with others. Furthermore, with the global COVID-19 pandemic diminishing in-person social contact, technology-mediated communication is more prominent than ever before [35] . However, digital communication is well-known to be challenging due to limited access to important nonverbal cues, such as our body movements and facial expressions [32, 60, 63] .\nINTRODUCTION: An emerging area of research in HCI has explored a novel social cue for improving the way we interact over technology: our biosignals. Biosignals, such as heart rate and skin conductance, are well known to change according to our physical and emotional responses, and can be revealed in everyday interactions using wearable sensor technologies. For example, applications like Pulsoid or Onbeat 1 explore this possibility through livestreams of heart rate during gameplay or exercise. Researchers have shown that expressive biosignals, or biosignals displayed as a social cue, have the potential to facilitate communication as a means to recognize and express our emotions and physical being [21, 23, 26, 40-43, 50, 55, 57] . However, researchers have not yet described the role that biosignals play in communication. Biosignals are personal and private data that require careful design and consideration [20, 22, 26, 41] . In particular, as cues that are sensed and recommended by systems, they present a new form of AI-mediated communication that could shape our interactions in unintended ways [25] . Thus, it is crucial that we understand the value and consequences of integrating them into our existing means of communicating.\nINTRODUCTION: In the present work, we expand on expressive biosignals literature by demonstrating the efects of shifting from communication without biosignals to communication with biosignals. We designed, developed, and deployed Signifcant Otter, an Apple Watch and iPhone app that enables romantic couples to send heart rate-driven otter animations as messages to each other. By setting adaptive thresholds for each person based on their past heart rate and motion data, Signifcant Otter intelligently suggests animations that match their current emotional and physical state. To explore the design of expressive biosignals as AI-mediated communication, we incorporate AI-recommended sets of shareable sensed states. In a one-month within-subjects feld study, we investigate how couples' behaviors and perceptions are afected when shifting from a sensing OFF version of the app, with no biosignals sensed, to a sensing ON version, with biosignals sensed. We present qualitative results from interviews during the study and discuss opportunities and challenges for biosignals in communication.\nINTRODUCTION: The core contributions of this work are: (1) Signifcant Otter 2 , a novel smartwatch and phone app that promotes communication and connection between romantic partners through animated avatars recommended based on heart rate; (2) an empirical study with 20 couples who used Signifcant Otter with sensing OFF and ON that demonstrates the value of biosignals as a lightweight and authentic social cue; (3) design implications and future directions for expressive biosignals research, including suggestions for integration into social platforms as a form of AI-mediated communication.",
    "relatedWork": "",
    "rq": " rq1: how does shifting from sensing off to sensing on afect the stages of communication (sending, understanding, responding) between couples?\n rq2: how does shifting from sensing off to sensing on afect social connection between couples?"
}
{
    "intro": "INTRODUCTION: The science of recording human eye movements has its origins in the nineteenth century [Wade 2007 ] and is, by now, touching many areas of research. Nowadays, sophisticated tools and programs for generating eye movement data are available (see, for example, [Farnsworth 2019] ). The use of eye tracking devices within research is commonly applied in cognitive sciences and has contributed to different discoveries [Matlin and Farmer 2017; Tanenhaus 2007] . Eye tracking data has, however, proven useful in many more scientific areas, covering topics such as dyslexic reading behavior [Ragozin and Kunze 2019; Robertson and Gallant 2019] or the impact of scrolling past Facebook messages [Chou et al. 2020] .\nINTRODUCTION: The focus of this work is on eye movement data, for which it is important to create appropriate visualizations to support analyses of the data in order to obtain qualitative information related to the research questions and tasks at hand. Eye tracking systems often already provide visualization options, but these are usually limited. Eye movement data sets can quickly become large, in turn, often resulting in overplotting and visual clutter [Rosenholtz et al. 2005] .\nINTRODUCTION: As key contributions we provide a web-based approach combining several existing and well-known visualization and interaction techniques [Yi et al. 2007] . Another novel aspect is the fact that not only separate visualization techniques can be chosen but also the same one several times to show different data aspects, particularly for comparison tasks. Further goals are to not require installation, helping users to get started, making the tool easy-to-use, and offering different visualization techniques necessary to enable researchers analyze different aspects of their eye movement data. For building the user interface, we used React [Walke 2013 ], a flexible componentbased JavaScript framework, together with Material-UI [Nguyen 2013 ]. The tool is available at https://dbl.p4ul.nl/.\nINTRODUCTION: Used individually, visualization techniques are typically limited in value (e.g., heatmaps do not show sequential or temporal information [Bojko 2009; Burch 2016b] ; gaze plots are susceptible to visual clutter [Goldberg and Helfman 2010; Rosenholtz et al. 2005] ). Our tool addresses this issue by providing four visualization techniques that cover several data dimensions in eye movement data such as space, time, and participants, as well as additional attached data attributes: a scanpath visualization [Goldberg and Helfman 2010], a 2D density plot [Burch 2016a ], a bee swarm [Blascheck et al. 2017] , and a scarf plot [Yang and Wacharamanotham 2018] . We illustrate the usefulness of the interactive visualization tool by applying it to eye movement data from a formerly conducted eye tracking study investigating the readability and route finding tasks in public transport maps [Netzel et al. 2017 ]. Moreover, we conducted a user study to investigate if the interactive web-based visualization tool is understandable and usable to solve typical eye movement data tasks.\nCONCLUSIONS AND FUTURE WORK: This paper described a web-accessible visualization tool for eye movement data. It offers four different visualization techniques: a 2D density plot, a scanpath visualization (or gaze plot), a bee swarm with animation, and a scarf plot. It offers several interaction techniques, linked views, and a simple platform where users can upload and store data sets, share visualizations, and access public data sets. Results of a preliminary online study showed to be promising. While the tool currently offers four visualizations, others could be easily integrated into our tool in the future. Including further visualizations is likely to increase opportunities of pattern or trend discovery. One could also think of allowing user-made plug-ins, making the tool extensible with more interactions and visualizations. Similarly, based on user feedback, ease of use could be further investigated and optimized. Also, more options to reconfigure the visualization techniques can potentially be added.",
    "relatedWork": "RELATED WORK: Exploring eye movement data in a combination of interactively linked visualization techniques has been studied before. For example, Bakardzhiev et al. [Bakardzhiev et al. 2020 ] describe another web-based solution but in their tool there is neither an option to inspect different data aspects with the same visualization type next to each other nor a way to share the found insights with others. Moreover, only a limited set of interactions is supported and they do not evaluate their tool to illustrate the usability aspects. The EyeClouds tool [Burch et al. 2019b] , on the other hand, integrates several fixed but interactive views, making it less flexible and hard to exchange and drag-around views on users' demands, but positively, they provide a perspective on image-based clustering [Burch et al. 2020 ] that is not implemented in our tool. The VERP Explorer [Demiralp et al. 2015 ] combines views based on standard visualization techniques with a specific focus on recurrence plots in a very specific domain, hence, it does not allow to explore eye movement data in a more conventional way. Another specific visualization tool focuses on the combination of standard eye movement data visualizations [Menges et al. 2020] , enriched by more advanced ones such as attention flows or 3D scanpath representations. Many more tools can be found that focus on very specific application domains such as fixation distances [Burch et al. 2019a] , parallel scan paths [Raschke et al. 2014] , or video visual analytics [Kurzhals et al. 2014 [Kurzhals et al. , 2016] ] but they typically do not provide the standard easy-to-understand visualizations, nor do they allow to easily exchange the views, and to share the found insights with others.\nRELATED WORK: In general, many visualization techniques for supporting the analysis of eye movement data have been developed in the past. Blascheck et al. [Blascheck et al. 2017] describe two main categories of visualization techniques: point-based and AOI-based methods. Point-based visualizations are useful to provide a more detailed unaggregated view on the data while AOI-based methods are useful for including semantic information of the stimuli: Users are able to select areas of interest within a stimulus, after which this information can be integrated in the visualization. An example of such an AOI-based visualization is the scarf plot [Yang and Wacharamanotham 2018] which is also integrated in our tool. Also a scanpath representation in the form of a gaze plot [Goldberg and Helfman 2010] is supported. Although gaze plots provide a lot of information concerning gaze trajectories, they can be prone to visual clutter [Rosenholtz et al. 2005 ]. To address this issue, we implemented a dispersion-based algorithm to cluster the data points into more general fixation points [Salvucci and Goldberg 2000] . A novelty in our tool allows users to interactively change the dispersion value to a minimum of zero which removes visual clutter while the original scanpath is still preserved.\nRELATED WORK: Another popular visualization technique from the large repertoire is the heatmap or sometimes also called visual attention map [Bojko 2009; Burch 2016b] . A heatmap only provides an overview about the hot spots of visual attention, neither showing temporal aspects nor individual people's visual attention. It typically shows an aggregated form of the recorded eye movement data and focuses on space. Nonetheless, some approaches [Burch 2016b ] also try to incorporate the temporal aspect in the final representation. Although heatmaps aggregate the recorded data a lot, they definitely provide a good overview, however, at the cost of hiding the stimulus in the background if a heatmap becomes very dense. Hence, one view is typically not sufficient, requiring a combination of several visualization techniques in a multiple coordinated view [Roberts 2003 ]. Moreover, to make the occlusion of the visual stimulus by the heatmap less problematic we use contour lines instead of explicit On the right, a scarf plot for the same stimulus is displayed. The sliders are positioned to the left at default values (20 for the bandwidth and 15 for the threshold). The time slider for the scarf plot is placed at 6.7 seconds. All users are selected in both views. The sliders at the bottom of each view allow to change parameters of the visualizations. density fields to encode the hot spots of visual attention on a stimulus. In addition, a bee swarm visualization can provide an animation of the fixation points [Li and Sayed 2012] to examine the temporal change, exploiting the Gestalt law of common fate [Wertheimer and Riezler 1944] . This animation-based concept is not useful for comparing data over time or regions and has some cognitive drawbacks [Tversky et al. 2002] . It, however, allows to quickly identify hot spots and their movement over time. As such it can be seen as a kind of rapid serial visual presentation mode [Spence and Witkowski 2013] to get a time-varying overview about the visual attention behavior.",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "RELATED WORK: Our work builds on advances in three main areas: (1) eyes-free mobile text input; (2) context-aware mobile communication; and\nRELATED WORK: (3) use of context in recommender systems.\nEyes-Free Mobile Text Input: Navigating mobile keyboards using touchscreen devices has been widely adopted by blind users. Technologies such as Apple's Voice-Over [28] allow users to touch the screen and get feedback via voice. While voiceover interaction makes mobile keyboards accessible, it requires users to continuously move the finger on the screen to find and select the intended letter. Researchers introduced better typing methods by rearranging the alphabets of mobile keypads. BrailleTap [22] and NavTap [23] , for example, eliminate the need to memorize the letter position by grouping 3 or 4 letters as keys.\nEyes-Free Mobile Text Input: Other technologies combine multi-touch input with audio feedback to enable fast typing. No Look Notes [10] is a multi-touch text entry that arranges letters in pie menus. Users can tap the finger on the screen to switch to the desired pie menu. These approaches enhance the accessibility of typing, but are still screen-centric, thus requiring the blind to hold out a phone at all times to interact with a visual display. Speech input is now broadly available and enables voice dictation for texting [6] . When used outside, however, besides causing issues when background noise is present [5] , speech input also introduces barriers to effective use. For example, it is challenging for the blind to review and edit the dictated text on the screen [6] . Social and privacy boundaries can also be easily broken by voice input, and this is problematic especially when the blind prefers not to draw unwanted attention in public spaces [16, 42] . Wearable devices including armbands, hand-and finger-worn devices open additional input channels that can potentially bypass the constant reliance on a reference screen. NailO [27] , for example, is a nailmounted wearable surface that detects various swipe gestures to input emoticon or punctuation without the need to use the screen. However, users still have to rely on the mobile screen due to the difficulty of typing using only the nail surface. Recent work has shown the use of the Myo [37] , a hand-worn band that recognizes and uses as input the arm's muscle movement, to control entirely auditory keyboards for the blind without the need for a reference screen [34] . The approach still requires performing quite ample hand gestures that are not as discreet as users would like, and may unnecessarily fatigue the arm.\nContext-Aware Mobile Communication: Research in ubicomp has studied the potential of leveraging contextual dimensions, such as location and time, to facilitate human communication through text messages. Location is a common type of contextual cue deployed in ubicomp applications [3] . LAMMS [9] , for example, demonstrates the notion of location-based text messaging, enabling people to communicate locally with each other and receive messages related to that area. Time is another dimension often used when sending text messages. LATTE [39] , for example, is an email-based system that incorporates both time and location to dynamically expand emails with the corresponding temporal and spatial information. Other forms of context-based mobile messaging systems allow users to define the context while sending a text. Jones and Neil [25] introduced a contextual constraints model, in which the user can place constraints on whom to send the message to (i.e. personal dimension), where the message has to be sent (i.e. spatial dimension), and when the message has to be sent (i.e. temporal dimension). Contextual information has also been used to provide feedback about the environment while an individual is on the go and holding the phone for typing [36] . Many people experience walking and typing at the same time they often fail to perform both tasks perfectly [35] . One method for providing feedback about the user's ambient environment is through audio [14, 32] . These systems provide real-time feedback to users allowing them to focus on typing while reducing errors by increasing awareness about the environment. However, the method can create difficulties for BVI users due to receiving auditory information from both typing and their ambient environment.\nUse of Context in Recommender Systems: Context-aware recommender systems use the concept of 'context' to recommend items that are both relevant to the user's preferences and their specific context such as time and location [4, 7] . Magitti [8] , for example, is a context-aware mobile recommender system that automatically detects a user's activity based on the user's context and behavior to recommend personal and timely leisure activities associated with a local environment. SoCo [30] is a social network aided recommender system that combines location and time with social information learned from friends with similar tastes to provide highly accurate recommendations. Bouidghaghen et al. [11] introduce a situation-aware recommender system to alleviate the problem of information overload. The system retrieves contextual information related to the user's location and time and provides a mobile user with personalized search results. However, context-aware recommender systems that rely on past experiences (i.e. exploitation) cannot model user's interest evolution because the learned rules will only reflect past user behavior; for this reason, a fraction of recommendations are selected at random or using a heuristic to obtain information about the user and discover better recommendations (i.e. exploration). In [29] authors use a bandit algorithm to exploit well-established advertisements for short-term capitalization as well as exploring less-known cases to find out potential advertisements that can be recommended in the future. Bouneffouf et al. [13] introduce a hybrid-\u03f5-greedy algorithm that takes into account the context to deliver recommendations that are bound to the user's current situation and interests. The algorithm combines the user's situation based on time, location, and social ontologies with an \u03f5-greedy algorithm and content-based filtering techniques.",
    "rq": ""
}
{
    "intro": "INTRODUCTION AND BACKGROUND: Several decades have passed since the HCI community saw its first explorations of digital gesture communication [3, 4, [8] [9] [10] 20] . These tangible, networked objects support expressive interpersonal communication over distances, evoking \"a visceral feeling of intimacy\" that \"conveys a direct sense of [the other's] corporeality\" [9] . For example, touching a picture illuminates its counterpart, indicating you're thinking about that person [4] . Since then, many more projects have been added in a steady stream. With the current constraints on in-person socializing due to COVID-19, there is both renewed interest in this research direction [11] and a more urgent case to be made for imagining new and intimate ways of connecting people across distances. This moment also provides the opportunity to reflect on the history of gesture communication devices, and identify areas that are still largely unexplored.\nINTRODUCTION AND BACKGROUND: Digital gesture communication devices build on ideas from a number of research areas in ubiquitous computing and the Internet of Things: information appliances [10] , awareness technologies [9] , ambient devices [17, 24] , social objects [14] , etc. Most gesture devices send a simple, often tangible, signal from one user to another remote user of an identical device, with the aim of creating an intimate and felt connection between pairs of people [8, 9, 11, 19] . Though there is a wide variety of such projects, we find that few acknowledge cultural diversity in their designs, as they assume a Western context.\nINTRODUCTION AND BACKGROUND: Other research in design and HCI has called attention to such cultural biases inherent in the design of technology products, namely that their assumed users are Americans living in a stereotypical and idealized home [1, 6, 15] . As Lucy Suchman observed in the 1990s, Silicon Valley has been regarded as the \"here\" where the future is defined for the rest of the world [21] . This US-centric perspective can be seen as a colonizing move that is harmful to the development and the identity of \"non-centric\" perspectives [12] . Recent work by Sultana and Ahmed illustrates how a postcolonial view of spirituality and local beliefs can enrich design and HCI [22] . Subsequent research has demonstrated that looking outside these assumptions provides enriching perspectives that can help \"create spaces and technologies that people will want to use, not just admire from a distance\" [1] . One approach to both close this distance and discover desirable alternatives has been to engage users in the design and creation of their devices. Audrey Desjardins' Alternative Avenues for IoT demonstrates how taking users' desires, unique environments, and identities as a starting point for IoT devices can open up new considerations of the values and beliefs embodied in our devices, and create more meaningful and positive relationships between humans and technology [6] . Further engaging users through a DIY approach, the Yo-Yo Machines project from the Interaction Research Studio at Goldsmiths empowers users to build their own gesture devices, personalize the materials and signals of their communication, and take pride in creating a tool for themselves [11] . This project also points to the seemingly universal desire in the pandemic to send tangible messages to people from whom we are now isolated.\nINTRODUCTION AND BACKGROUND: Building on the history of gesture communication devices as well as recent critical reflections on technology, this short paper contributes this field in two ways: first we suggest an opportunity for culturally-informed communications that has largely not been explored and second we demonstrate this approach through material speculation [23] that yielded an interactive artifact, SneezeLove. This is next described. When the \"nose\" on one device is touched, the other connected device produces a \"sneeze\" to signify that someone is missing you.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Drones provide a platform for aerial telepresence, remote collaborative work, and social interactions that was not available until recently. Drones are increasingly used to support humanitarian crises and events that involve dangerous or costly tasks. During the coronavirus pandemic, drones were used in applications such as lockdown patrols, thermal scan, medical goods and blood samples delivery, and aerial disinfection [5, 27, 33, 45] . In the Notre Dame fre incident, groups of frefghters along with local and global authorities and professionals connected to a feet of drones to collect essential data in real-time to track and evaluate the fre and fnd best ways to tame it [23] . In other hazardous and high-risk missions, groups of people such as police personnel [21] , healthcare professionals [5] , and scientists [25, 38] , collectively evaluate the situation, review the data from the scene, and make real-time decisions when needed. In these scenarios, drones enabled skilled practitioners to focus on the problem without risking their lives, reduced risks of human contact, increased the delivery speed, facilitated access to hard-to-reach areas, and freed up front-line responders' time.\nINTRODUCTION: Despite the recent growth in using drone technology as a platform for telepresence [2, 4, 11, 18, 22, 35, 44, 46] , existing drone technologies do not support remote collaboration and social interaction between multiple remote users. In addition, these technologies do not give the remote user the freedom to control and manage the drone without relying on an onsite drone user [11, 35] .\nINTRODUCTION: We undertook this project to understand how a drone-base telepresence system might be able to support collaborative tasks such as search and rescue and other forms of aerial survey. These types of tasks beneft from having many eyes and many difering views in a timely manner. As well, there might be value to responders' decision-making and situational awareness. Using this motivation, we prototyped Squadrone, an aerial drone-base telepresence system that enables multiple remote and local users to connect and perform mid-air collaborative work. Squadrone is a complex confguration of diferent technologies consisting of a quadcopter drone, a 360degree camera, programmed game controllers, VR head mounted displays, and a novel web-based platform to manage interactions and resources (hardware and software). (Figure 1 ).\nINTRODUCTION: We designed and developed a functional prototype of Squadrone over the course of 23 months, from November 2018 to September 2020. We took a Human-Centered Design approach and evaluated Squadrone through three design iterations (DIs). We make four main contributions in this paper: (1) we present the design of a drone telepresence platform that supports synchronous remote collaboration using a novel shared 360-degree display, (2) we enable remote users to pass the control of the drone to other remote users while collaborating on a task, (3) we present how remote collaboration might impact the sense of \"being there\", and (4) we make an intellectual contribution by refecting on an existing telepresence design framework by Rae et. al [31] and extend it to address the design challenges we identifed through this work.\nINTRODUCTION: We describe the Squadrone system design and implementation, our three design iterations, report our fndings, and discuss the challenges of using a drone as a shared telepresence communication and collaboration platform. We frame our fndings and structure the results based on Rae et al. 's framework for designing for telepresence [31] .\nINTRODUCTION: We found that drones have a great potential as a platform for remote collaboration. In addition, collaboration over a shared aerial 360-degree visual space enhanced social presence for multiple remote users. The participants in our study talked about their sense of \"fying there together\" with the other members of their team. We also found that the participants developed a shared mental model to be able to point and reference objects in the environment and give directions to the drone pilot. As a result, the participants developed a better situational awareness as a group, which could lead to a more efective search strategy. Furthermore, we found that when users have agency in controlling and navigating the drone, they are more engaged in performing the task and collaborating with each Mehrnaz Sabet et al.\nINTRODUCTION: other as opposed to automated fights. Finally, we found that the ability to collaborate and communicate with the other members of the team combined with the 360-degree aerial views were key factors that contributed to the participants' sense of \"being there\".\nINTRODUCTION: Squadrone resolves some of the limitations in current telepresence systems such as limited feld of view and mobility, and it enables the remote users to perform task more independently. As the commercial drone market expands [34] , a wide variety of commercial applications have emerged. Many of these commercial applications require or could beneft from interaction and collaboration between multiple users. As more collaborative operations use drones, it is important for the design community to understand how specifc design choices may infuence collaborative activities and mid-air interactions for achieving a specifc goal.\nCONCLUSIONS: This study shows that remote aerial systems are promising in their potential to promote collaboration between multiple remote team members. Participants' collective opinion of the viability and utility of Squadrone was positive, indicating the experience was infuential enough to overcome the initial fear of fying the drone for the frst time. Participants considered immersion created by shared 360-degree aerial views as a key component of their telepresence experience. In addition, participants put a high premium on programmed game controllers capturing their actions accurately and with a high degree of precision. Participants considered Squadrone capabilities and its future potential to be signifcant in how we collaborate and work. They called out perceived benefts that Squadrone could aford their life, such as helping onboard new teammates, participate in collaborative trainings, monitoring crops, explorations and recreating real-world scenarios for sensitive tasks such as search and rescue or fghting wildfre. Participants also found that activities such as joining a fight with others, engaging in conversations mid-air, and independently immersing themselves in 360-degree viewpoints provided meaningful aspects to the telepresence experience and whet their appetite for what future evolutions could bring. Our fndings also suggest that defning and communicating team members' roles and allocation of resources may positively afect remote collaboration and overall user experience. Further research can shed light on how aerial telepresence systems can enhance remote collaboration.",
    "relatedWork": "BACKGROUND AND RELATED WORK: Squadrone builds on several areas of research, including studies on various aspects of telepresence, systems that support telepresence, challenges of designing drone systems, and remote collaborative work. We categorize these studies as Drones, Telepresence and the existing telepresence systems, Drone controllers, Collaborative work using Drones, and Rae et. al. 's telepresence design framework:\nDrones: Hasanalian and Abdelkef [7] provide a comprehensive classifcation of fying drones, their defnitions, capabilities, and limitations. According to their classifcation, small drones, generally known as, Unmanned Air Vehicles (UAV), are robots that fy in confned spaces. Squadrone uses a DJI Phantom 4 Pro, which is a rotary-wing quadcopter civilian outdoor drone. It is categorized as a Micro Air Vehicle (MAV) which include drones smaller than 100 cm in length and weight lower than 2 kg. Our drone is 290 mm wide, excluding propellers, and weighs 1388 grams.\nTelepresence: There is no universal agreement about the meanings and scope of telepresence and its diferent dimensions [1, 6, 10, 17, 20, 24, 26] . Marvin Minsky is often credited with the frst defnition of telepresence as, \"presence in a remote environment through technology\" [28] . Minsky anticipated that improved technology would allow us to work from \"thousands of miles away\", enable us to export specialized skills, provide safer and more efcient ways of completing dangerous and hazardous tasks, reduce transportation costs and commuting time, among many other benefts.\nTelepresence: Yet, despite many technical advances, today's telepresence devices are limited in what they can accomplish. Minsky envisioned \"technologies where remote users' experiences would be so similar to actually being there that there would be no noticeable diference\" [28] . Much of the work on telepresence focuses on making the technologies as good as being there. This is an important goal. However, years ago, Hollan and Stornetta [10] pointed out that telepresence should also strive to provide experiences that go 'beyond being there' and work more like a running shoe to enhance our performance and less like a crutch that makes up for something lost. The Squadrone platform seeks to provide telepresence services to enhance collaborative tasks.\nExisting Telepresence Systems: Several studies have worked on drones as a platform for telepresence. Bae [2] introduces a drone platform called \"avatar drone\", which is \"an anthropomorphic life-size telepresence drone with a movable head and arms\". Bae indicates that this life-size drone has the potential to \"transform drones as a new type of embodied medium, the social extension of human in the sky\".\nExisting Telepresence Systems: Zhang et al. [46] designed and developed LightBee, a telepresence drone featuring a self-levitating light feld display. LightBee displays a 3D representation of a remote user's head and enables the user to control the drone by moving his/her head. Using this feature, local users can experience their own perspective of the remote user's head. LightBee currently supports only indoor conversations due to hardware limitations.\nExisting Telepresence Systems: In 2016, Google announced a telepresence drone with the goal of addressing the mobility issue with mobile robots [22] . A major limitation of mobile robotic telepresence systems is climbing stairs and navigating through unusual foor plans and rural areas such as farms where accessible infrastructure does not exist. However, Google has not developed their telepresence drone yet. Squadrone extends these technologies by enabling multiple remote users to connect to a feet of drones indoors and outdoors.\nExisting Telepresence Systems: In addition to using drones as a platform for telepresence, Mobile Robotic Telepresence (MRP) systems enable remote users to \"physically move and interact within the same space as local participants\" [3, 17] . MPRs are interesting because they can increase remote users' sense of presence and enable them to interact with local participants as if they were having a \"face-to-face interaction\" with regards to \"turn-taking and interruption patterns\" [30] . However, MRPs are limited in their range of view that causes lack of situational awareness for the remote user. Compared to drones, MRPs have limited mobility and navigation possibilities. Also, existing MRPs only support one remote user when used in a collaborative setting [39] . Squadrone, provides 360-degree view and various navigation options while connecting with multiple other remote users for synchronous collaboration. In addition, remote users are less limited by the physical obstacles in the environment such as climbing the stairs or moving on an uneven ground.\nExisting Drone Controller Systems: There are various types of controllers and UI systems to operate drones. Dronely is a visual block programming language for controlling drones, which supports Parrot drones. Dronely enables users to create a program visually by placing specifc blocks like \"take of\" and other actions for the control of the drone [41] .\nExisting Drone Controller Systems: Landau and van Delden developed an application to control drones onsite using voice commands. Their voice recognition system works on the onsite mobile phone that is connected to the drone [19] .\nExisting Drone Controller Systems: Flying Head [9] maps the users' body and head movements to the remote drone to change the drone's orientation and camera directions. The authors report that participants preferred this novel way of controlling drones in contrast to using game controllers and users had shorter task completion times using Flying Head. However, Flying Head had limitations in outdoor fights because it could not locate the drone due to sunlight and disturbances in the air. In addition, they could not map user movements to change the drones' altitude so they combined their method with other controlling tools. Furthermore, the authors state that \"in some telepresence exercises, the operator and the robot will not move at equal scales, in which case the system should be able to perform distance scaling. \" In Squadrone, users can change their view by moving their heads, without disturbing other users' views.\nExisting Drone Controller Systems: In Birdly, users can control drones with their own bodies. The authors use a fight simulator to track hand movements and provide haptic feedback. The authors indicate that using a fxed-wing drone that works with user commands can make users feel more like fying. Birdly users found it easier and faster to learn and work with Birdly compared to radio controllers [4] . In another study, Nagi et al. enable drone operators to use hand gestures via video cameras to interact with fying drones. [29] Several studies use augmented reality technology to design and develop controllers. For example, EXGbuds is a wearable assistive device that helps disabled people interact with the environment. The authors use a VR headset and a camera mounted to a drone or a robot to create a telepresence experience in which users can control the drone by moving their eyes [44] . In exTouch, the authors leverage augmented reality technology and touchscreen interactions to design a spatially aware method to control the object [13] . The authors suggest that their method can be used for drones too. This approach requires the drone operators to see the drone's body in their feld of view to be able to touch and interact with it which is not suitable for Squadrone because the remote users do not see the drone's body to be able to drag or move it. A few studies have worked on the challenges of drone teleoperation and used game controllers as a tool to control the drone. For example, Smolyanskiy et al. [37] present a stereoscopic frst-person view system for drone navigation. In their work, the drone operator uses a head-mounted display and a remote controller (RC) or an Xbox controller to control the drone. Smolyanskiy et al. indicate that drone operators that used VR goggles with frst-person views from the ground were able to maneuver the drone more efectively and estimate the height and position of the drone more accurately.\nExisting Drone Controller Systems: Furthermore, Walker et al. 's work [43] highlights the challenges of teleoperating the robotic system, especially for novice users, and leverages the advances in augmented reality to design novel teleoperation interfaces. In Walker et al.'s work, remote drone operators wear augmented reality head-mounted displays to see a virtual representation of the drone and use Xbox controllers to navigate them. Moreover, Hedayati et al. [8] works on designing drone teleoperation interfaces using augmented reality technology. They also use Xbox controllers to control the drones remotely.\nExisting Drone Controller Systems: In Squadrone, we build on these studies and utilize game controllers as a familiar physical interface for many people. We extend prior work by programming game controllers for specifc tasks such as passing the control of the drone to others during the fight, maneuvering the drone, and a push-to-talk communication button. In addition, we enable the remote drone operators to see a 360-degree birds eye view which extends the prior work on feld of view. \nCollaborative Experience using Telepresence Drones: Jones et al. [11] designed a video conferencing system in which a remote user connects to a semi-autonomous drone via a desktop to explore the environment from the drone's perspective. A local user controls the drone on a smartphone and together with the remote user performs collaborative navigation and search task. The main diference between Squadrone and Jones et al. system is that Squadrone supports multiple remote users while Jone's et al. system supports one-to-one collaborations. Teledrone is another system that uses video conferencing with a drone for sharing outdoor activities between pairs of people over distance. In Teledrone, the remote user connects to a video call through a tablet attached to the drone and the drone follows a friend or family member during an outdoor activity. However, this system is designed to support one remote user and one local user. In addition, Teledrone is not designed for collaboration, it follows the local user and cannot be navigated by the remote user independently [35] . Squadrone supports at least two diferent forms of collaborative activity. The frst is a collaboration on a focused task, and the second is collaboration around the control of the drone itself.\nCollaborative Experience using Telepresence Drones: Prior studies show that in remote collaborative work, remote users can be at social and communicative disadvantage [3, 39] . While the new MRP systems seek to mitigate these disadvantages by providing local embodiment, these studies show that remote users contribute less and have poor collaboration experiences. Also, with current systems for telepresence, \"remote participants are often at risk of becoming isolates\", which afects their participation level [3] . In addition, remote users struggle to fnd a balance between communication, navigation, and task engagement [39] . These studies suggest that stronger social connections can directly impact collaboration quality compared to more advanced embodied technologies. We designed Squadrone with these challenges in mind and attempted to reduce the social and communicative disadvantages of remote users and enhance their collaboration experience.\nA prior Framework for Telepresence: In our Discussion section, we use Rae et al. 's [31] seven dimensions for designing for telepresence. This framework provides a structure to communicate the research fndings with the telepresence design community and determine potentially generalizable fndings. We list the seven dimensions in Table 1 and briefy describe them to familiarize the reader with this framework. These descriptions come directly from Rae et al. We refect on Rae et al. 's telepresence design framework and extend it to address the design challenges we identifed in this work.",
    "rq": " (q2) to what extent were there times during the experience when the environment that you were exploring was the reality for you?\n (q6) how much did the collaborative experience and communications with other participants during the experience helped you in building and having a better awareness of the remote environment?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": "1. what is the difference between using omniglobe in the first-person mode, and the third-person mode in terms of user interaction and performance?\n2. how could the face window feature be used to support collaboration between the two sides?"
}
{
    "intro": "INTRODUCTION: Most research on the shortcomings of real-time 'hybrid' collaboration, in which some members are collocated and others are remote, tends to focus the glaring asymmetries of access in the moment. For a range of intersecting socio-technical reasons, those who are locally present are more likely to be included, while those who are remote are more likely to be marginalised [78, 79] . Much of this has to do with the disparity between remote and local participants' abilities to 'belong' to the space: to assert their presence in the space [79] , and to make use of presence as a collaboration resource, to deictically reference (e.g., point) [24, 28, 68] , and to be aware of others' actions and presences (i.e., workspace awareness [32] ). Remote participants need the same level of autonomy in a physical space as local participants in order to engage fully in certain types of meetings (such as brainstorming), and when the physical space is integral to the activity (such as a site manager's inspection of a work site for safety hazards or a choreographer's feedback on dance rehearsals in a theatre). Broadening out from a task focus, 'belonging' is also characterised by social affiliation with one's colleagues, in the moment [60] and over time [14, 65] , both directly and ambiently.\nINTRODUCTION: Remote workers fundamentally lack five of the building block affordances for physical autonomy and affiliation available to local workers: mobility, embodiment, expressiveness, awareness, and presence. A number of newer technologies exist to enable partial fulfillment of these needs -Mobile Robotic Presence (MRP), Virtual Reality (VR), Augmented Reality (AR), 360\u00b0video -but none of these fulfill all. In this paper we report on a study of an experimental Extended-Reality (XR) Telepresence technology probe [37] that combines these existing technologies in an attempt to fulfill these needs of autonomy and affiliation.\nINTRODUCTION: The probe, named VROOM (Virtual Robot Overlay for Online Meetings) [41] , puts belonging at the centre of the experience. Instead of twinned endpoints, VROOM provides an experience contextualised to each endpoint (See Figure 1 ):\nINTRODUCTION: \u2022 For a person in the local activity space (Figure 1 , left) wearing a HoloLens, an AR interface shows a life-size avatar of the remote user overlaid on a Beam telepresence robot using tracker markers. The Beam is also equipped with an extra 360\u00b0camera. \u2022 For the remote user (Figure 1 , right), a head-mounted VR interface presents an immersive 360\u00b0view of the local space. The remote user also has VR controllers, which allow for both piloting and gestural expression. The remote user has separate mobile and visual autonomy in the local activity space.\nINTRODUCTION: The remote user's speech, head pose, and hand movements, are applied to the avatar, along with canned blink, idle, and walking animations. The local user sees the entire avatar body in a third-person view. The remote user sees the avatar from the shoulders down in a first-person view. Local users can thus identify the remote user as a specific person, while the remote user has an identifiable embodiment of self, and both can take advantage of naturalistic verbal and gestural communication modalities. Together, these features enable a remote user to be recognised as an autonomous entity with which others can affiliate.\nINTRODUCTION: Masahiro Mori coined the term \"Uncanny Valley\" [59] to refer to the sharp dip in the continuous increase in affinity for entities with human likenesses. An industrial robot provokes limited affinity, a toy robot more affinity, but at some point, human likenesses such as human limb prosthetics provoke a strong sense of revulsion. AR and VR avatars can provoke similar reactions.\nINTRODUCTION: Given our emphasis on 'belonging' as comprised of autonomy and affinity, we argue that there is an 'Uncanny Valley of Telepresence' in which there a sharp dip in the continuous increase of both local and remote users' senses of belonging as a telepresence experience becomes more like physical presence. The more the sense of belonging is provided, the higher the expectations, but also the more amplified the differences to physical presence become. Remote users expect more of their abilities, and local users expect remote users to have more abilities. Thus, when even simple problems occur (as they are bound to), people become more frustrated as their higher expectations are not met.\nINTRODUCTION: However, the Uncanny Valley is not a cliff. Mori included both the sharp dip and its equally steep recovery. On the recovery side of the function, Mori placed Banraku puppets just below a healthy human. Part of traditional Japanese theatre, Banraku puppets are less realistic than prosthetic limbs, but the gestalt of the theatre experience contextually enables audience members to focus on emotional performance. Thus, we argue that overcoming the 'Uncanny Valley of Telepresence' will involve finding the gestalt of what is most accountable about the comfort of belonging with others.\nINTRODUCTION: To explore these issues, we ran an exploratory study in which pairs of participants (one local, one remote) played two games (one involving gathering, the other guessing words) in two conditions: once using VROOM and once using standard Mobile Robotic Presence (MRP). Given that the MRP system was a mature commercial product and VROOM was a hack with significant limitations, the goal was not a simple evaluation of the success of one system compared to the other. Rather, the goal was to understand how users account for the systems' different technological approaches to 'belonging' to a space, with special reference to finding the treacherous slopes and potential handholds in the 'Uncanny Valley of Telepresence'.\nINTRODUCTION: Our findings illustrate that XR Telepresence has potential to support dynamic collaborative activities in which embodiment, gesturing, mobility, awareness, and non-verbal expressions in a physical location are important. However, there are also challenges to be addressed, specifically around proprioception, the mixing of a physical robot body with a virtual human avatar, uncertainties of one's partner's views and capabilities, fidelity of expressions, and the appearance of the avatar. Furthermore, the more immersive an XR Telepresence system is, the more amplified technical issues such as latency, video quality, and control become, largely because of the higher expectations of both remote and local users. We conclude that XR Telepresence should focus on providing the comforts of belonging, even if done in unnatural ways, rather than focusing on pure imitation. Aligning with Hollan and Stornetta's [36] proposal that telepresence should seek 'beyond being there', we argue that the goal of XR Telepresence should be to help remote users belong there.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Videos rely heavily on visual cues to convey information and thus are often not accessible to people with visual impairments. Audio descriptions, verbal commentaries of visual information in videos are critically used to increase access [17, 22, 39, 48] and improve the accessibility of instructional, educational, and entertainment videos [12, 15, 35] . Providing audio descriptions is increasingly essential as online video consumption rises by 20 -40% during the COVID-19 pandemic in 2020 [1, 13] . However, providing high-quality audio descriptions remains challenging as hiring professional audio describers is both time-consuming and costly [52] .\nINTRODUCTION: Technical solutions minimizing the time and cost have mainly focused on user interfaces that streamline the authoring process or automate part of it [10, 25, 26, 42, 52] . Notably, Kobayashi et al. suggested that involving novices in the process could be a reasonable, cost-effective alternative [26] though no cost estimates or in-depth quality characteristics were reported. We build on this prior work to explore more fine-grained questions like \"is it really cost-effective to involve novices in audio description authoring?\" and \"what are the dimensions of audio description qualities and how \"good\" are novice-created audio descriptions along those dimensions?\"\nINTRODUCTION: Motivated by the benefits of online collaborative authoring and peer assessment in improving work quality [27, 34] , we investigate how sighted novices can author high-quality audio descriptions through collaboration with a sighted or a blind reviewer. To this end, we designed and developed ViScene, an interactive web-based application, illustrated in Fig. 1 . Using ViScene, an author writes the scene descriptions (SDs)-textual descriptions of video scenes-and ViScene converts them into audio descriptions through text-tospeech (TTS) [2] . Descriptions can be written in scene segments where dialogues are absent (Fig. 1b ). A reviewer can access the authored SD segments and provide feedback on quality in the form of open-ended comments at a scene-level (Fig. 1c ), which the author can address subsequently. The interface also visualizes SD succinctness (Fig. 1d ) based on the length of the generated audio descriptions.\nINTRODUCTION: We used ViScene and three types of videos to explore whether novice authors and reviewers can collaboratively create highquality SDs cost-effectively. We conducted a mixed-design study with 60 sighted participants who used ViScene to write SDs. The participants acted as authors and split into three groups: withoutfeedback (baseline), sighted-feedback, and blind-feedback. All participants authored SDs on the same videos over two sessions. A sighted or blind reviewer provided feedback between the sessions to those in the sighted-feedback and blind-feedback group. The reviewers provided multifaceted feedback on the quality of SDs using a SD quality codebook that we developed through an extensive review of existing guidelines. Sighted and blind evaluators assessed the quality of the final SDs. Our analysis suggests that novice authors Descriptive, Objective, Referable, and Clear SDs with the feedback. Although our collaborative authoring approach took about an hour to author SDs for a one-minute video, we show that the authoring cost is still cheaper than hiring a professional (i.e., US$0.85 -US$9.65 per video minute). In summary, this work contributes:\nINTRODUCTION: \u2022 The design and development of ViScene, a web-based collaborative SD authoring system. \u2022 A concise codebook grounded in experts' guidelines that characterizes the quality of SDs. The codebook enables reviewing and evaluation of SDs and can be used in settings beyond collaborative authoring. \u2022 Empirical results with 60 participants demonstrating the feasibility, value and limitations of mixed-ability (i.e., sighted and blind people) collaborative SD authoring. \u2022 Design recommendations for future SD co-authoring interfaces elicited through the evaluation of ViScene.",
    "relatedWork": "RELATED WORK: We introduce prior efforts related to video accessibility with a focus on audio descriptions and technologies that support their authoring.\nVideo Accessibility and Audio Description: Internationally, anti-discrimination related regulations mandate the provision of videos with audio descriptions [30] . In the U.S., for example, CVAA Title 2 [14] requires major broadcast and cable networks to make online videos accessible. Similarly, Section 504 and Section 508 of the Rehabilitation Act (which refers to WCAG [9] ) mandate public entities as well as federally funded organizations to make videos that they publish to be accessible [9, 14] . See [30] for a comprehensive survey of recent audio description-related legislations around the globe. Although the current regulations do not widely enforce non-public entities to make video contents accessible, video streaming platformers like Netflix make a significant effort in making their contents accessible [35, 40, 41] .\nVideo Accessibility and Audio Description: Decades of work have gone to making guidelines for authoring high-quality audio descriptions [39] . Perhaps one of the most comprehensive sets of guidelines had been offered by the Described and Captioned Media Program [15] . While its focus is on educational video content, the guideline discusses a broad range of topics on how to make videos accessible with audio descriptions. Its content often overlaps with other guidelines like the Audio Description Coalition's guidelines, which focus on cultural video content [12] . We have reviewed these guidelines and other existing ones [17, 22, 35, 48] to create a concise codebook that has captured the key qualities of audio descriptions, as described in Section 4.2. In our study, the codebook is used both by the minimally trained reviewers who provide feedback to novice authors as well as the evaluators who assess the quality of the generated SDs.\nVideo Accessibility and Audio Description: Despite these legislative, corporate, and grassroots efforts to make comprehensive guidelines, the number of inaccessible videos seem to be increasing as more content is being uploaded to online video platforms. Guidelines like WCAG 2.1 encourage making videos accessible [9] , but there are hundreds of hours' worth of video content uploaded every minute to YouTube [11, 20] with the majority of them lacking audio descriptions. Unfortunately, we are not aware of any prior work that quantitatively analyzed the prevalence of videos without audio descriptions. But, it is clear that many videos remain inaccessible for people with visual impairments. This could be partially attributed to the cost and availability of professional audio description service [38, 52] . One estimates the cost to be \"$12 per video minute to $75 per video minute\" [45] and the turnaround time takes days to weeks [45] . This makes it hard for casual video creators to make videos accessible via paid services. Thus, much work is needed to make the audio description authoring process more cost-effective and labor efficient, while maintaining quality.\nTechnologies to Support Audio Description Authoring: Prior work has proposed technologies to support authoring of audio descriptions [7, 19, 25, 31, 50] . Kobayashi et al. created a script editor that allows novice authors to edit the audio description and modify synthetic speech parameters easily [25] . Branje and Fels also reported that it is feasible for minimally trained amateur SD authors to generate overall medium to good quality audio descriptions [7] . The 3PlayMedia's post-production tool allows its users to author textual descriptions of scenes and convert them into audio description through TTS [31] . Tools such as Able Player [43] can deliver the audio descriptions created using the HTML5's <track> tag; Web content publishers can add audio descriptions in textual VTT format [46] . The textual information can then be read out by a screen reader or TTS [18, 26] . Although prior work has offered easy access to tools that allow users to provide audio descriptions, the rationale behind what is considered as a \"good\" quality SD was not elaborated. Kobayashi et al. examined the quality of novice-created audio descriptions [26] , but their main focus was examining overall quality, not looking deeper despite how the quality of audio descriptions is multifaceted. Solely looking into the overall quality prevents us from understanding fine-grained audio description qualities. We believe these qualities are useful for SD authoring training, feedback, and automating SD generation. Recent work has also investigated ways to combine manual audio description authoring with automation [10, 28, 42, 49, 52] . For example, Yuksel et al. created a workflow where the computer vision (CV) and natural language processing (NLP) model generates the initial description and volunteers post-edit [51, 52] . Their analysis focused on the time efficiency of generating the audio description and usability of the technology. Their quality assessment was centered around overall and topic understanding qualities. Campos et al. designed a way to automatically scripting audio descriptions using the pre-existing video scripts (i.e., the blueprint of chronological rundown of the scenes) and subtitles [10] . Wang et al. [2021] attempted to fully automate the generation of audio descriptions, but the result still lacked some useful information for blind audiences (e.g., the character actions, gender, places) [49] . Pavel et al.'s Rescribe automated the editing and revision processes to fit the content into limited space for audio descriptions using dynamic programming [42] . However, the focus was on providing succinct descriptions and audio description's other quality facets remained under-explored (e.g., descriptiveness, objectiveness). Overall, the effects of online human-human and mixed-ability collaborations in SD authoring have not been previously explored.\nTechnologies to Support Audio Description Authoring: In this paper, we design and develop ViScene that supports the sighted novice to author SDs with the commentary feedback from either a sighted or a blind reviewer. The work is motivated by prior research demonstrating the benefit of collaborative writing and peer evaluation in improving one's writing [4, 21, 24, 27] . The mixedability collaboration in our work is a novel framing in the context of audio descriptions and is inspired from the prior studies of how blind and sighted people, when working in pairs, can co-create an accessible household [5] and workspace [6] .",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Nowadays, virtual meetings are an essential tool for collaborative work across the entire globe or for those working from home. Compared to the more established video conferencing, virtual reality (VR) meetings do not use webcams to share video information of the meeting but instead personify attendees in the form of avatars in VR. The intensive usage of video conferencing software during the COVID-19 pandemic showed that such systems can cause privacy issues, while VR meetings lead to an increase in involvement, spatial presence, and experienced realism compared to video conference systems [42] . Besides all obvious advantages, we observe that attendees often multi-task during virtual meetings 1 , which makes it harder to moderate them and to ensure productivity of the conversation.\nINTRODUCTION: Several information can be expressed through body language, such as emotion [12] as well as how carefully a person is listening to a conversation or how willing they are to communicate [5] . Thus, we easily understand whether a person is listening or doing something in parallel through body language in physical meetings [41] . Avatars however do not let us read the conversation involvement of meeting attendees very well as nonverbal signals given through mimic, gesture, and body posture are missing.\nINTRODUCTION: Inspired by status signs used in video conference systems, such as Skype, to inform about the availability of a person to be contacted, we propose to provide the VR meeting attendees with information about other attendees' conversation status, attention, and engagement. Referring to our ability to \"read\" body language, we propose using avatars' body language to show if they are highly involved in the conversation, multi-tasking, or even not listening for some time.\nINTRODUCTION: We expect that avatars acting in a way that represents the cognitive involvement of users in VR meetings can increase empathy between virtual meeting attendees, which -as a consequencewould ease work with remote collaborators and ensure communication productivity.\nINTRODUCTION: We aim at extending previous research that investigated emotion estimated through the pose of avatars [9, 26, 28] , effects of the body language of avatars in virtual meetings [45] , and possibilities of influencing a conversation through body language [39] through proposing avatars' body language as non-verbal information about the conversation readiness and involvement in VR meetings.\nINTRODUCTION: While standing up and talking on the phone to somebody else would be impolite in a physical meeting, we possibly could use such behaviors in a virtual meeting to indicate a parallel task. As no social rules for such behavioral meeting status exist, we aim to explore if and how a set of avatar behaviors would be interpreted in VR meetings. Hence, we created a simulation of a VR meeting with avatars showing typical as well as untypical behavior, such as carefully listening, checking their phone, and having a nap.\nINTRODUCTION: In a user study, we found that the body language of an avatar can indeed represent the communication status of their corresponding user and provide design recommendations about what body language including behaviors are appropriate in VR meetings and which are not. With this work, we contribute to the field of VR collaboration as well as avatar research. We hope to inspire future work as this paper serves as a proof-of-concept, and more work is needed to develop better VR meeting systems.",
    "relatedWork": "RELATED WORK: Two research areas are particularly related to this work: (1) visualizations of the communication status in virtual meeting systems and (2) body posture and emotion expression of avatars.\nCommunication Status: A large body of research on virtual meetings explores why it is important to know about the status of the other participants during a meeting [33, 43] , or advantages and disadvantages of virtual meetings, like the visibility of the audience [32] . Other works show how the virtual communication status can be received or symbolized [18, 23, 29, 37] .\nCommunication Status: McCarthy et al. showed that it is essential for every virtual conference attendee to be informed about the status of the conference and about the status of every other attendee [33] . Otherwise, synchronization problems and communication problems can occur. Stimmel patented such a workplace intercommunication tool that could communicate remote co-workers' status through different devices [43] . The system enables users to determine the status of others by showing text blocks in an extra window when choosing a user.\nCommunication Status: Leacock et al. explored how to have conversations in multiple virtual environments at the same time and proposed to display the users' attendance status by having a representation in form of a 2D or 3D avatar in the room where the user is present [29] . Unlike this work, the avatars gave no different communication status cues, but only showed which users were present in which room.\nCommunication Status: Cobb et al. investigated online status indicators (OSIs) using apps that were commonly known by the participants. The OSIs were graphical symbols like dots or written text. They found that users often do not correctly understand OSIs and that OSIs lack desired privacy preferences [13] . Nardi et al. found by investigating early Instant Messaging (IM) tools that the knowledge about the availability of co-workers is important for the users. The availability was shown through the co-workers' names in a list [34] . Greenberg used bold graphics of people working at a computer for symbolizing that the user was at their place. When the graphic fades out the user was inactive and no graphic indicated that the user was logged out [20] .\nCommunication Status: In opposite to graphical symbols representing the remote user, DeGuzman et al. used a physical artifact to symbolize the status of users' availability as well as activity in an IM [15] . A plastic ball changed its size depending on the online status of a user mapping large size to online and small to offline. Moreover, the ball contracted when the user was writing.\nCommunication Status: Further research used automatic communication detection of the users' availability status [7, 22, 24] . Begole et al, for example, detected a person's activity through sensors and used graphical symbols displayed in front of users' names to communicate their available status [7] . A yellow diamond was here used to show possible unavailability and a red triangle for probable unavailability.\nCommunication Status: The readiness to communicate or the level of communication engagement can also be given through non-verbal cues, such as gestures and gaze. Schneider and Pea, for example, showed that mutual gaze perception improves the quality of collaborative learning [38] . Collaborative learning groups of two members used an eye tracking system that showed each participants' gaze to the other user as small dots on the screen. The collaboration and learning gain was higher than for groups without gaze perception. Bai et al. found that remote collaboration can be enhanced by combining gaze and gesture cues. In the experiment, the remote expert and the local worker shared the same view which was augmented to the local worker with gaze and gestures of the remote expert [4] . This led to a higher feeling of co-presence and the local worker being able to see what the remote expert exactly refers to.\nBody Posture and Emotion Expression: Body language in a narrow sense means how we communicate just by the move of our body, but can also include hand gestures, facial expressions, or how we use our voice [16] . Body postures and emotion expression can indicate how much somebody is engaged in a conversation or willing to talk as they are important nonverbal cues [35] .\nBody Posture and Emotion Expression: Body language has often been researched for collaborative virtual environments [8, 46] . Benford et al. defined an initial list of parameters for collaborative virtual environments (e.g., availability, gesture, facial expression, etc.) [8] . Especially for conversations, they pointed out facial expressions and gestures as important by being a strong external representation of emotions. Tromp and Snowdon defined detailed facial, gestural expression as well as a natural movement and activity as requirements for having a good virtual body language [46] .\nBody Posture and Emotion Expression: Jian-xia et al. showed that the body language of an instructor in video lectures has a significant impact on the learning effect compared to an instructor with no body language [25] . Also, body expressions and body postures are used to perceive and recognize the affective status of a human-like avatar [9, [26] [27] [28] . Negative affects as well as positive affects are dimensions to describe emotional states of mind [49] .\nBody Posture and Emotion Expression: Berthouze et al. showed that the recognition of emotions is a requirement for rich social interaction and that body language is important in affective communication [9] . Kleinsmith and Berthouze eveloped a system that enables affective posture detection using sequences of static postures from motion capture data [27] . The system recognized the affective states nearly as well as observers who were presented the affective expressions as animations, which included both, form and temporal information. This demonstrates that even a single pose can show a good affective expression or user status.\nBody Posture and Emotion Expression: Kleinsmith and Berthouze also showed that body expressions are important in nonverbal communication and in perceiving somebody's affective status [28] . Bosch et al. researched on automatic detection of affective states because they assume the detection of affective states is a key component for intelligent educational learning interfaces [10] . They wanted the system to detect the states of students in a normal class situation by their facial expressions. They found that automatic detection of the affective states is at least possible for some affective states (e.g., boredom, confusion, delight, engagement, and frustration).\nBody Posture and Emotion Expression: A large body of researchers have been working on the relation between emotion and body postures [1, 6, 14, 40] . Andr\u00e9 et al. showed the importance of affection for life-like characters as that affect can enhance the credibility of a virtual character and create a more natural conversation manner [1] . In addition, an affect can be used to change or modify a character's behavior and create emotions in social interactions. Beck et al. confirmed body language as a good medium for robots to display emotions [6] . In a study they found that emotional body language showed by an agent or a human is interpreted quite similar. Using 176 computergenerated mannequin figures, Coulson showed that some emotions were recognized with high concordance (anger, disgust, fear, happiness, sadness, and surprise) while others were hardly recognised (disgust) [14] . In addition, anatomical variables and viewing angle influenced participants' responses. Si and McDaniel found that body language plays a more important role than facial expressions when representing the attitude of non-humanoid robots [40] .\nSummary: Previous work on the communication status in virtual systems show that visualizing if a user is available and actively involved enriches virtual communication. Previous work on body language and body posture recognition shows that emotions can be understood using images as well as animation. While the emotion (being recognizable through body language recognition) is an indicator on how willing a person is to communicate and how carefully they are following a conversation, previous work did not combine body language recognition and communication status visualization. Thus, this work aims at exploring the usage of avatars' body language to increase the understanding of participants in VR meetings by displaying their communication status and willingness.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: English is officially recognized in India as a means of public communication as it plays an important role in academia, government affairs, maintaining unity in the country etc. [8] . This has led to an integration of English in daily communication in the Indian context [9] . However, only 10.6% of Indians speak some English according to the 2011 Census report [6] . Therefore, several NGOs have been working in this area to help impart spoken English language skills to students from Indian regional language-medium schools to help them access higher education opportunities, career opportunities and foreign opportunities.\nINTRODUCTION: For this project, we collaborated with an NGO which works at the intersection of education and employability with schools in the Maharashtra state. The NGO conducts a program called Junior Rural Fellowship (JRF) which focuses on teaching class 12 students spoken English language skills and soft skills (communication, teamwork etc.) such that they can get jobs or study further in English medium colleges. The program has been conducted physically by the NGO trainers in government schools but at present, it is being conducted virtually due to the pandemic. Continuing this program virtually would provide the opportunity of scaling it to a greater number of schools and including different interested students such as first year college students, in the program. The online classes are being conducted using the same reference textbook and activities designed for the delivery of the program in the physical context. An opportunity to design a digital solution focusing on the needs of this area was found.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: The rapid advance of computers has seen more and more data manipulation disappear into circuits and software. Traditional data visualisation has, at best, given us a window into this electronic realm. Immersive display technologies, such as augmented and virtual reality, offer to bring images of data back out of the computer and \"into the world around us\". Making it possible to touch and feel these representations of data is a difficult design challenge, yet it is important to address if we are to properly use our full human sensory abilities to explore data.\nINTRODUCTION: Quantitative data visualisation revolves around the concept of mapping quantitative data dimensions to a spatial axis. In this paper we introduce the MADE-Axis, a Modular Actuated Device to Embody an Axis. The lightweight, hand-sized, cordless device features actuated sliders, a rotary encoder and button for spatio-data coordinated interaction [12] . Past research has demonstrated benefits of tangible, mixed-reality systems to enable new pathways in our understanding of complex data. As sophisticated as some of these past systems have been they are yet to find widespread application outside of research labs and challenges remain to create devices for data manipulation that are practical.\nINTRODUCTION: Compared to past devices for interacting with data visualisations (Sec. 2), we contribute a novel device with a set of interaction affordances that are particularly supportive of the practical interactive dynamics of visual analysis (Sec. 4). In particular, the MADE-Axes are: (1) easy and affordable enough to build that many units may be constructed and used in powerful combination (Sec. 3); (2) flexible enough to support a variety of use cases (Sec. 5); and (3) robust, small and light enough to support hand-held use in mixed-reality scenarios, where ( 4) it has a particular strength as an embodied affordance for visualisation control, which can be physically shared in collaborative scenarios, as demonstrated in a qualitative user study (Sec. 6).",
    "relatedWork": "RELATED WORK: Our work is directly related to immersive analytics research [51] . We build on multidimensional data visualisation authoring and exploration by leveraging tangible interaction, actuation and composability.\nTangible interaction in visualisation: Tangible interaction aims at leveraging and mimicking natural human manipulation of objects in their 3D environment [20, 33] . Past research demonstrates that tangible interaction is useful to provide fast and precise 3D manipulations [3] , to foster collaboration [52, 53] , and to provide engaging immersive experiences [3, 70] . Such results make tangibility an interesting candidate for visualisation research projects and tangible interfaces have been one of the most studied interaction paradigms for interaction with 3D spatial data in visualisation [5] .\nTangible interaction in visualisation: A pioneering example by Hinckley et al. [28] used tracked props for neurosurgeons to explore the internal structure of their dataset by manipulating cutting planes. Following this example, many research projects have created tangible props for a specific purpose. For example, Schkolne et al. [57] used custom tangible devices, such as a gun, to explore and manipulate DNA datasets in VR. Jackson et al. [35] used a paper roll to create a tangible prop to select thin fiber structures. Gomez et al. [24] combined two tracked devices: a pen-like probe to brush in a 3D volume and a cube to manipulate the data, while De Haan et al. [16] combined a pen-probe and a transparent acrylic plane to help select regions of interest in 3D data. Issartel et al. [34] created a 'cuboctahedron' to manipulate fluid dynamics data with 6DoF in handheld AR, providing additional props (e.g., a stylus) to manipulate a cutting plane or place seed points in 3D.\nTangible interaction in visualisation: Other researchers have tried to use pre-existing devices (some with built-in tracking solutions) to increase their versatility. Cassinelly and Matasoshi [8] used a tracked screen to facilitate exploration and annotation of medical data. Song et al. [60] and Lopez et al. [48] used a similar approach, combining a smartphone/tablet with a large vertical display. Besan\u00e7on et al. leveraged the built-in tracking of the Google Tango tablet combined with its tactile screen to propose data and cutting plane manipulation or seeding point placement [2] as well as 3D extrusions of 2D lasso shapes for 3D selection of spatial data [4] . Spindler et al. [61, 62] made use of tracked props to support a variety of visualisation tasks ranging from augmenting an existing visual representation with colours to providing different levels of abstraction.\nTangible interaction in visualisation: On the spectrum between specialised and generic devices, the work of Cordeil et al. [11] stands out. Their work highlighted a novel approach to controller design with the concept of spatiodata coordination [13] . Where most specialised controllers use novel mechanisms to achieve a specific goal (i.e., simulating weight or momentum), their Embodied Axes explored the application of a standard, common component -the actuated slider. Paired sliders were mounted on three orthogonal axes, to create a volumetric range selector. In this paper we generalise the Embodied Axes concept from a specialised 3D interaction device, to universal controllers -not only for spatial data with three orthogonal dimensions, but to any number and spatial or non-spatial configuration of quantitative and categorical data dimensions. While Cordeil's Embodied Axes system had three table-mounted axes fixed orthogonally to provide a 3D interaction space, we explore the value of decoupled (wireless and modular) axes controls that can be handheld or placed in various combined configurations to cater to a variety of applications.\nActuated visualisations and devices: Haptic feedback has been explored in visualisation using specific commercial devices (e.g., PHAN-ToM [49, 50, 66, 67] ) or lab prototypes (e.g., [47] ). While most physical visualisations are inert, actuation can make them dynamic and interactive (see e.g., [36] ). Actuated systems can represent physical \"pixels\" representing binary values or a range of different values. Many of these systems rely on arrays of motorised bars [22, 45, 54, 64] , while some provide a more continuous control of the final shape [21, 55] . The physical pixels of these systems can be mapped to data values in order to allow physical visualisation to represent non static data [22, 44, 64] . The primary focus of these systems is to provide some control over a specific topology or geometry. Yet, only a handful of these systems facilitate interaction (see e.g., [64] ). It must be said that these systems are also typically expensive to manufacture, large and unwieldy.\nActuated visualisations and devices: To provide a more versatile control of geometry and interaction techniques, Le Goc et al. [40, 41] introduced Zooids: small robots that can rearrange themselves on a table in order to provide several visual representations and adapt to the data. In addition to the versatile output they produce, they can also be used as flexible controllers to interact with the data with filtering, picking, and dimension selections. Our work directly takes inspiration from these, but rather than aiming to provide actuation per data point, we target an axis as an embodiment of an entire data dimension and as the primary affordance for interaction (inspired by the work by Lischke et al. [47] and Cordeil et al. [14] ).\nComposability in visualisation: The concept of interactive composition of visualisation elements to allow users to create rich displays has become fairly standard for data visualisation software [37] . Classen and Van Wijk [10] demonstrated the use of axes (linear representations of data dimensions) as the elements of composition. Several systems have drawn inspiration from this work since then [25, 68] and have all been designed for 2D, desktop-style non immersive and non embodied setups.\nComposability in visualisation: In contrast, our work focuses on data visualisation composability with embodied interaction in mind [17] -the MADE-Axes are designed to embody a data dimension with a device that can be manipulated and positioned in the user's space to compose visualisations. Previous work has investigated how embodied interaction helps users organise and compose their workspace with tangible representations of data (e.g., air traffic controllers compose paper strips [46, 56] representing flight data). Huron et al. [30, 31] explored the use of simple building blocks to build data physicalisations.\nComposability in visualisation: We also see MADE-Axes as composable building blocks for interacting with data. One inspiration for this work is parameter bars by Ullmer et al. [65] . They combined sliders and a display into a device that could filter attributes along a data dimension. Parameter bars were combined to interact with multiple dimensions: when the bars are adjacent a Boolean \"AND\" operation is applied, and when they are spatially separated a Boolean \"OR\" operation is applied. However, the combination of these devices was through attaching them to a slotted rail rather than free-form position (as we explore in this work). Further, they did not explore the possibilities of actuation of the sliders. With ImAxes, Cordeil et al. [14] explored the construction of multivariate visualisations with data axes as building blocks in virtual reality, but using standard VR controllers. Batch et al. [1] further studied how data scientists leverage the 3D space to compose, organise and explore their visualisations with ImAxes. Strongly related to these approaches, Khadka et al. [38] proposed the use of discs to represent slices of a dataset that can be spatially rearranged and even worn by users.\nComposability in visualisation: MADE-Axes physicalises the virtual axes composition explored in ImAxes [14] . and the composable tokens explored by parameter bars [65] , but with greater possibility for tracked control, actuated haptics, and application to a greater variety of visualisation types and Use Cases including mixed-reality (Sec. 5).\nImmersive Analytics: Immersive Analytics aims at combining several research fields to better support data analysis with immersive technologies [19, 51] . As an emerging field, it currently faces a number of key research challenges [19] . We focus here on two of these key challenges. The first, \"Supporting transitions around immersive environments\" [19] , pertains to the inherent need of analysts to combine several working environments, such as a desktop station (for its powerful computing power and variety of software tools) and an immersive context (for its ease of use). To address this challenge, past research has looked into using classical desktop input devices (mouse and keyboard) or touch devices in immersive settings [6, 23, 26, 69] . In contrast, with MADE-Axes we create a device for immersive setting that also present some affordances for classical desktop workstations with its two sliders.\nImmersive Analytics: The second challenge pertains to \"supporting behaviour with collaborators in immersive settings\" [19] . Collaboration scenarii are rarely explored in augmented reality [59] . In most cases, they involve a singular shared visualization between all users [43] , while some approaches relied on mobile devices with touch screens in immersive context to allow for collaborative analysis [7, 9, 29] . In our work, however, we propose to use multiple MADE-Axes devices: they can be used by several users independently, combined and collectively manipulated, and their inherent tangible properties afford for more natural collaboration patterns [52, 53] between users.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Broadening access to learning experiences about artificial intelligence (AI) is increasingly important as AI becomes more integrated into our everyday lives. Individuals who have little knowledge of AI or how it works are engaging with an increasing number of commercially available AI devices and technologies. Growing concerns about AI's role in misinformation [3, 37] , data privacy breaches [43] , and bias/discrimination [7] suggest that technology users need new skills to be able to engage with AI critically and thoughtfully. This skillset has been referred to in the literature as AI literacy (i.e. \"a set of competencies that enables individuals to critically evaluate AI technologies; communicate and collaborate effectively with AI; and use AI as a tool online, at home, and in the workplace\" [34] ).\nINTRODUCTION: There has been a recent surge in AI learning interventions for individuals without a computing background, with a focus on K-12 audiences (c.f. [34, 53] ). However, few efforts to date have focused on museums as venues for AI education, even though museums and science centers have historically played an important role in public science education [45] . We are investigating how to provide learning experiences in museums that can foster public AI literacy, both by developing novel AI education exhibits and by adapting existing AI research projects into educational experiences through which learners have the opportunity to interact with authentic cutting-edge research in the field.\nINTRODUCTION: Prior work suggests that certain design features-collaboration, creativity, and embodied interaction-can help facilitate effective learning experiences in museums. Embodied interaction is an intuitive way to engage with exhibits and can aid in concretizing abstract concepts [23-25, 31, 41, 44] . We explore designs that utilize full-body interaction, tangible user interfaces, and spatial metaphors. Most visitors come to museums in groups [21] , making collaboration an important part of the museum experience that contributes to learning and motivation [13, 25, 26] . We define collaboration in this paper as encompassing both shared dialogue and working together to achieve a shared goal. Finally, creative interactions have been shown to contribute to prolonged engagement at exhibits and can lead to personally-relevant meaning-making [4, 18, 26] . In this paper, we use the term creativity to refer to designs that encourage learners to generate personally creative (i.e. P-creative, or novel to the individual [5] ) ideas by expressing themselves through activities like dance or generating novel artifacts and combinations of ideas. Research suggests that embodied interaction, collaboration, and creativity may also be effective at facilitating learning about computing [10, 15, 20, 35, 46, 54] .\nINTRODUCTION: Our hypothesis is that interactions with embodied, collaborative, and/or creative AI learning activities in informal learning spaces lead to interest development in AI and improved understanding of AI. In this paper, we test this hypothesis by exploring two core research questions: 1) How can embodiment, collaboration, and creativity be used in museum exhibits to encourage interest development in and learning about AI? and 2) What design features contribute to engagement with activities that increase interest in and improve understanding of AI in informal learning spaces? To address the first question, we designed three AI literacy exhibits-Knowledge Net, Creature Features, and LuminAI-that each incorporate collaboration, creativity, and/or embodied interaction to varying degrees. We explore how to develop AI literacy exhibits \"from scratch\" with Knowledge Net and Creature Features. The third exhibit (LuminAI) explores how to adapt and augment an existing AI research project to facilitate an educational experience about/with authentic AI technology. To investigate the second research question, we conducted remote user studies with 14 family groups (38 participants) with two study sessions. The first session of users engaged with an early iteration of our prototypes, and the second session interacted with a later iteration of the same prototypes. We present results from these studies, focusing primarily on an analysis of participant surveys (supplemented with qualitative observations when relevant). Our analysis assesses the degree to which the exhibits supported collaboration, creativity, and embodied interaction, and explores the relationship of these design features to AI learning and interest development.",
    "relatedWork": "RELATED WORK: There is a growing body of research investigating how to design AIrelated learning experiences for novice audiences. Researchers are developing curricula for both K-12 audiences [2, 48, 50] and non-CS majors at universities [6, 19, 46] . Others are developing courses, interactive online tools, and programming platforms that can engage novice audiences in learning about AI (e.g. [1, 15, 30, 54] ). The exhibit designs presented in this paper are grounded in two recently published frameworks related to AI literacy. The first framework presents five \"big ideas\" that define areas of AI that are important for K-12 audiences to understand: 1) perception; 2) representation and reasoning; 3) learning; 4) natural interaction; and 5) societal impact [48] . The second framework is a set of AI literacy competencies and design considerations we developed based on a review of AI education literature [34] . The competencies are high-level ideas about AI intended for novice audiences, and the design considerations are intended to guide the development of AI literacy learning interventions. We used both of these frameworks to guide the design of the prototypes presented in this paper.\nRELATED WORK: In the remainder of this section, we review several AI education projects that emphasize our key design considerationscollaboration, creativity, and embodied interaction. There are numerous existing platforms that are designed to engage learners in creative programming activities involving AI. Cognimates is an add-on for the Scratch programming environment that allows learners to incorporate AI technologies like image or voice recognition in their Scratch programs [14] . Similar tools exist for other coding platforms (e.g. [1, 28, 51] ), allowing learners to incorporate AI in their creative multimedia projects. Others have developed activities to engage learners in creatively imagining alternative AI futures-like an AI ethics activity that engages middle school students in redesigning YouTube [2] . A recent paper outlined a set of design principles for introducing co-creative AI research projects in public spaces-while not explicitly focused on AI education, we draw on several of these principles in our work [33] . Research also suggests that having learners enact embodied simulations of algorithms (either on their own or by programming an embodied AI device [15, 49] ) can help them to concretize abstract concepts [15, 46] . Other platforms engage learners in building machine learning (ML) models of physical gestures like dance or sports moves [8, 54] . There are fewer existing projects that are focused on collaboration. However, recent papers suggest that facilitating social dialogue, particularly between adults and children, is important in AI learning contexts [16, 34, 50] . AI plugins on platforms like Scratch also facilitate social learning by allowing learners to share their work with a wide audience and \"remix\" others' projects [40] .",
    "rq": " 1) how can embodiment, collaboration, and creativity be used in museum exhibits to encourage interest development in and learning about ai?\n and 2) what design features contribute to engagement with activities that increase interest in and improve understanding of ai in informal learning spaces?\n usability: are the exhibits usable?"
}
{
    "intro": "INTRODUCTION: Multimedia resources are important elements to use in order to augment our communication capacity and achieve better audience attention. Video technologies may present a great potential to support and positively impact learning and education [21, 33] . For traditional open education, MOOCs (Massive Open Online Courses), video-based learning content is one of the most critical materials for distance or online learners. Additionally, videos are an essential teaching resource in the context of a flipped classroom, within a blended learning model, where we have online combined with physical classes.\nINTRODUCTION: The use of annotations is an important learning and reviewing strategy. Traditionally, after studying a text, the student writes down her/his thoughts and highlights important content summary for invoking their memory or offering hints for the future. This annotations-based approach helps the student by improving the understanding during the learning process.\nINTRODUCTION: Video annotation tools are essential tools for stimulating user creativity in order to add and share information. The interfaces for video annotation are often based on a traditional WIMP interaction [24, 40] , which is not a problem when working alone on a video. However, presenting video in a classroom context requires the lecturer to be standing next to the device desk all the time to control the video playback or save comments and reflection about the current scene. Still, usually, the lecturer needs to have some mobility in the classroom between the computer, the projection, and the students to keep good communication. Additionally, during the talk, natural behaviour from the lecturer is essential to get the audience's interest. Unnecessary movements are required to manage the video playback, creating an unnatural pause that can decrease the audience's focus.\nINTRODUCTION: Researchers have wanted to talk to computers almost since the first one was invented [17, 22] . In the last few years, smart assistants with speech interfaces became very popular; Apple's Siri [45] , Amazon's Alexa [2] , and Microsoft's Cortana [14] enable users to accomplish complex operations efficiently, including sending text messages, obtaining navigation information, booking a taxi, among other similar tasks, toward pervasive and ubiquitous environments. Speech interfaces can render interactions much friendlier since they enable users to state their objectives without learning the new software interface. However, video annotation is a complex task to be accomplished only through speech. Previous work has shown that visual tasks are best accomplished by combining speech and manual input (e.g., touch, mouse, keyboard) [12, 18, 48] .\nINTRODUCTION: Previously, we conducted several test sessions where users could be observed working with a previous version of our video annotation tool. We identified a mix consisting of actions, symbols, sketches, and text used during those sessions. Later, we concluded that such communication presents the potential to benefit from interaction based on both speech and manual input. We intend to improve user interaction in video annotation tasks while using our proposed interface. Our research uses a set of design ideas for video annotation collected during previous observations of annotation practices.\nINTRODUCTION: We implemented MotionNotes, a web video annotation tool that enables users to express their desired actions using voice recognition and natural language. This tool was developed in the context of dance annotation analysis, yet different application environments are taking advantage of our video annotation features. A user can ask the player to pause the video in a specific frame, saying \"Pause the video\" and then say, \"I want to add a text annotation\". Motion-Notes adds a textbox on top of the video and collects the user's speech. Finally, the speech is converted to text and displayed on the screen inside the textbox.\nINTRODUCTION: The natural language module was developed to be applied to the video player and annotation domain; this module can map user sentences onto software operations. The system records the user voice with an HTML 5 audio API that already exists in all modern web browsers. Then, the audio is segmented in multiple audio files and then processed and mapped to predefined intents. MotionNotes processes simple instructions in the browser while more complex commands are sent and handled on the server-side. This strategy was followed since we observed while developing the MotionNotes speech module that a two-level interpretation process provides superior results. First, a more domain-specific method and, then, if success is not achieved, it will try another sub-module with a more extensive vocabulary.\nINTRODUCTION: We performed tests with 27 people to understand the real benefit of using a speech module in conjunction with the manual interface in video annotation tasks. Afterward, the tool was also tested in a classroom context, achieving positive feedback. This use case appeared after we informally interviewed several teachers, whose experience told them that our solution could work very well in certain classes. Therefore, after conducting the needs assessment, we intended to investigate if this could be the case, intending to learn how it could work properly in a classroom context towards a ubiquitous computing environment.\nINTRODUCTION: This paper presents the following main contributions:\nINTRODUCTION: \u2022 The Web-based application that combines speech with manual interaction for video playback control and digital annotations.\nINTRODUCTION: \u2022 A user study based on the manual and speech interface, reflecting the challenges and benefits behind this approach. \u2022 The knowledge gathered with the use case of the classroom context, where the video annotation software was used with the speech module.\nINTRODUCTION: The paper is structured as follows. We start by analyzing the related work in Section 2, followed by the MotionNotes description, its speech integration architecture, technology, and features in Sections 3, 4, and 5. Afterward, section 6 presents the user study and the results achieved. Finally, we make our conclusions and present future work in sections 7 and 8.",
    "relatedWork": "RELATED WORK: The use of digital content in lectures increases the audience's interest and concentration by using multimedia elements and design customization applied to the respective topic [47, 50] . Presentation software usually requires the presenter to be next to the terminal while running the application and interacting with it to control the presentation flow. This problem was mitigated by introducing wireless remote controllers, giving the presenter better flexibility regarding his position in the presentation room. Researchers have explored other solutions to address this problem, making it possible to find literature regarding approaches to control presentations with human gestures [36] and voice commands [11] .\nRELATED WORK: Regarding practices more focused on video content to create debate and brainstorming, it is not easy to find previous studies flexible enough to give us position freedom around a room while controlling the playback. For instance, pre-recorded videos as a teaching technique are becoming very popular across the education area [13] . Videos can optimize face-to-face time, enabling more collaborative activities that provide greater opportunities for students to interact and consequentially foster creativity. Additionally, video has become more interactive, with software enabling timestamped annotation features, where users can make comments, marks, add ideas for the future, and share them with colleagues [26, 41] . Annotations are often used in various tasks daily. A classic scenario is the one in which students add notes in the classroom, which motivated some research on how video annotation could impact learning activities [19, 35] .\nRELATED WORK: In a context of a presentation room where the presenter needs to be moving and interacting with the audience, the interaction with the video player can benefit from multimodal interaction. People interact with their surroundings using multimodal channels. The Human-computer Interaction is performed using these capabilities, providing users, as much as possible, the most natural and productive experience to complete tasks [51] . Speech and natural language interfaces have been studied in multiple application fields, including automated web chatbots [37] , smartphones [4] , home media systems [27] , image editing [25] , car interfaces [29] , industrial applications [3] , among others. However, interaction through natural language interfaces for video players and annotation practices are still rare.\nRELATED WORK: Our system supports multiple command types, including video player operations, multiple annotation types, and speech to text. Previous studies on interfaces with speech recognition and natural language understanding demonstrated good outcomes, especially when the voice was combined with other modalities [34] .\nRELATED WORK: Video annotation is a valuable resource in different application areas, which motivated the development of several tools: Elan [54] is one of the best-known annotation tools, with applications in many areas such as language documentation, sign language, and gesture research. Choreographer's Notebook [44] was explicitly designed to be used on Choreography workflow, enabling digital-ink and text annotations. BalOnSe [38] is a web implementation that allows users to annotate classical ballet videos with a hierarchical domain-specific vocabulary and provides an archival system for dance videos. Cabral et al. [5, 6] presented Creation-Tool, a project optimized for tablets and pen-based interaction to create different types of annotations, while Ribeiro et al. [39] have expanded this methodology into 3D visualizations. VideoTraces [49] allows users to capture the video and annotate it by talking or gesturing, and it also enables users to evaluate recorded dance performances and share those evaluations with peers for discussion. Commercial video annotation applications such as Wipster [53] , Camtasia [8] , Frame.io [15] , and Vimeo Pro Review [52] have simplified annotating and sharing videos for users. However, none of them supports speech commands for interacting with the applications.\nRELATED WORK: Researchers have studied how to integrate speech and natural language interfaces into video players. Singh et al. [43] created a Voice-Controlled Media Player capable of recognizing seven different speech commands. Chang et al. [10] developed research that led to voice-based navigation for How-To Videos and presented a set of design recommendations for this kind of interaction. Remap is a multimodal search interface that helps users find video assistance via speech, improving users' focus on their primary task [16] . Interaction in the Internet of Things field can also be enhanced by using multimodal and speech interfaces, as the Minuet system demonstrated [23] . Still, none of these systems supports annotation practices through voice modules.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Caused by demographic change, the shortage of qualified medical personnel with sufficient practical experience will increase. When emergency service personnel in the field or later the doctor in the hospital have to make vital decisions in difficult situations, direct consultation with experts may not be possible in the future [8] . One strategy to compensate for this lack of experienced specialists and emergency physicians is improved medical training combined with an efficient teleconsultation system that allows access to all available experts, even if they are not on site. Since current practice communication and interaction with external experts is limited to voice calls or, in rarer cases, video conferencing, such consulting physicians receive only insufficient patient information and, as a result, can only assess the current situation to a limited extent. Especially in the course of pre-hospital emergency response, novel teleconsultation systems could integrate additional expertise. Such a system should enable the rescue personnel on-site to correctly carry out the diagnosis and initial treatment in complicated cases and give the command a comprehensive overview of the current situation [10] . Advances in Virtual Reality (VR) and Augmented Reality (AR), combined with a fast and robust internet connection, allow novel teleconsultation solutions whereby users can virtually \"immerse\" themselves in a remote environment, move freely within it and communicate verbally and visually with other persons [9, 21] . Augmented reality, in particular, has considerable potential to provide new learning experiences and construct new understanding based on the user's interactions with virtual objects. Although there are studies that suggest that there is significant potential for augmented reality in healthcare, including medical education and surgical subspecialties, there has been no review of the potential uses of augmented reality in the context of emergency medicine. [16] .\nINTRODUCTION: In this work, we investigate whether our 3D teleconsultation system is superior to a 2D video-mediated consultation in teaching the correct placement of electrodes for an electrocardiogram (ECG) from the perspective of the consultee in AR. There is little work on a direct comparison between video-based and 3D teleconsultation in a medical use-case. We capture the environment of the patient with multiple depth cameras and stream the data for reconstruction to the remote expert. The remote expert can fully see the patient inside the 3D reconstruction and can convey the position of the electrodes directly to the patient through non-verbal communication such as pointing gestures using a shared avatar representation and freehand drawn annotations.\nContribution: We investigate both i) video-mediated consultation and our ii) 3D telepresence system for the preclinical use-case. The latter method utilizes both Virtual and Augmented Reality in order to create an asymmetric 3D teleconsultation experience. We describe our approach and compare both methods in a collaborative task of placing electrodes onto a mannequin for an ECG during an emergency situation. We report on the findings on the AR side while keeping remote expert consultation equal among all participants. The 3D telepresence system allows participants to place slightly more accurate electrodes. In terms of task load ratings, both methods were similar. Both conditions showed acceptable usability scores; however, the usability of the video-mediated consultation was significantly better. The 3D teleconsultation system and its comparison with the current state provide valuable insights and demonstrate high potential in improving medical emergency responses.",
    "relatedWork": "RELATED WORK: Koncz et al. [18] published a paper about their work on objectives and requirements as well as the practical implementation and the potential indications for a telemedical emergency system in the preclinical situation. Their methods included discussion of scientific facts and expert recommendations, specifically from experiences of the tele-emergency physician (Telenotarzt) project in an urban environment (City of Aachen). In addition, reference is made to a second pilot project in a rural region (Straubing, Bavaria). They found out that the overall concept of the telemedical emergency physician meets the changing challenges in the preclinical healthcare system and opens up new possibilities for patient care.\nRELATED WORK: Systems employing collaborative telepresence have been developed using a variety of approaches. Avatars are often deployed to create a sence of presence and to mediate non-verbal communication [11, 31, 32] . Such collaborative digital environments have been presented in literature and can take place in a virtual environment ( [19, 28, 30, 35] ), in augmented environments ([4, 24, 25] ), or in an asymmetric combination of virtual and augmented reality ( [5, 17, 20, 23, 26, 27, 36, 41] ). Immediate environment can be captured and represented by a point cloud using real-time reconstruction [21, 37, 38] . Several work focus on task-based consultation in combination of hand-drawn shared 3D annotations: Oda et al. [23] introduce the metaphor of virtual replicas for the guidance of placing a physical object at the consultee's environment. A virtual model of the object is required, which allow the remote user to create links between virtual model and the goal position inside the physical world to indicate spatial relationship. An asymmetric teleconsultation allows a remote user to digitally immerse and connect to a local environment while the local users can see the remote user as a digital representation using AR alongside their normal environment. Weibel et al. [39] present an asymmetric teleconsultation system in which a reconstructed point cloud of a local site that can be annotated is streamed to a VR remote user. No work has yet compared conventional video-conference based consultation versus an asymmetric 3D teleconsultation system in a tasks-centered use-case.\nRELATED WORK: Previous work has been published for evaluating video-conference mediated telemedicine [1, 2, 14, 29] and direct comparison between video-conferencing and face-to-face meetings [12, 13] . Further, Nguyen et al. [22] conclude based on their study that body language and non-verbal cues contribute significantly a face-to-face like experience in telepresence.\nRELATED WORK: Budde et al. [7] conducted an interview with emergency doctors and paramedics using a similar tele-emergency system . Based on the received insights a requirement analysis for an AR-based teleconsultation system has been evaluated. Hereby, the subjects mentioned that offering new camera perspectives and the ability to choose the perspective individually may be considered.\nRELATED WORK: This work aim to fill the gap on comparing video mediated consultation with an asymmetric 3D teleconsultation system and report on the impact on quality of a task in a preclinical context.\nHypotheses: Based on our literature review and projecting the findings on our proposed method, we assume that H1: The display of annotations as auxiliary lines during the AR-condition in the view of local paramedics increases the precision of ECG electrodes. Further, we assume that H2: Due to the increased complexity of the consultation, the system usability score of video-mediated consultation will be higher. Lastly, our last hypothesis claims that H3: The task load of the 3D teleconsultation systems from the perspective of the local user will be lower since in-situ visualization of annotations reduces the mental, physical and temporal demand in finding the correct anatomical location.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Affinity diagramming (also known as the KJ method [36] ) is a spatial clustering technique where analysts manually move around and group individual data items based on their similarity or relevance to a shared topic [6] . It has been applied for a wide variety of tasks in different domains, such as Human-Computer Interaction (HCI), anthropology, and management [6, 43, 57, 60] . It is generally applied for three kinds of purpose [32] : to elicit diverse input, such as in brainstorming or project planning; to organize data into known categories; and to analyze data, where people exploit spatial organizations to help them make sense of unstructured and seemingly fuzzy qualitative materials, such as interview transcripts. This work focuses on this last category.\nINTRODUCTION: Many activities are involved in building such understanding from collections of data: analysts need to closely examine original materials; make highlights or annotations while reading; extract related pieces of information to put them in space; experiment with different kinds of arrangements Fig. 1 . An example analysis using paper, where the user has customised the environment to support various analytic activities and representations in qualitative sensemaking. Source: Sarah Scarsbrook. 1 to explore conceptual alternatives; they might also build higher level diagrams to abstract the emerged concepts, etc. Fig. 1 shows a real analysis environment customised to support various activities and representations with paper tools 1 . The overall sensemaking process is highly iterative [52] , where analysts switch back and forth between different analysis phases and views, such as to refine their mental models, collect additional evidences, or test alternative hypotheses [42] . The analysis artifacts could also hold various connections. For example, a participant's quote in a sticky note might make little sense without its context in a transcript. Current tools, however, tend to treat the analysis activities and artifacts as discrete pieces. Transitioning between these phases involves too much friction, effectively linearizing this iterative process. For example, refining codes in the source data might break the existing layout in any corresponding affinity diagrams.\nINTRODUCTION: Qualitative data analysis is characterized as a dynamic, intuitive, and creative work [4] . Many additional factors may come into play: whether the analyst is working alone or collaborating with others; what kinds of analytic methods they prefer; whether they are working in the same location or from remote work sites; the tasks they are engaged in; or available devices. When working on a common analysis project, different collaborators might adopt different tools, methods, and data representations, and they might tackle the analysis from different perspectives [41, 61] . Even a single user often needs to adopt different conceptual methods or transition between different contexts, such as when appropriating different devices in the office and from home, etc.\nINTRODUCTION: Current tools focus on specific aspects of affinity diagramming, but do not address the holistic process of qualitative data analysis. Choosing one tool often means compromising the other factors stated above. For example, many continue to prefer the traditional paper approach due to its ability to be spread out in available space to extend the \"space to think\" [2] . Yet, it is difficult to share remotely, search the context of a quote snippet, or manage different diagrams [32, 35] . Digital affinity diagramming tools, such as Affinity+ [14] or Miro 2 , focus on the clustering tasks without considering other analytic phases such as coding. Qualitative data analysis tools, such as MaxQDA 3 or NVivo 4 , support diagramming analysis in conjunction with coding practice, but are often desktop-based and limited to a single user, single device.\nINTRODUCTION: In this paper, we present a vision and proof-of-concept system ADQDA (Affinity Diagramming for Qualitative Data Analysis), a cross device, collaborative affinity diagramming tool for a fluid, holistic sensemaking process. In our vision, users may seamlessly transition between different analytic phases or adopt various methods and representations, all while preserving consistent analysis artifacts. Users may work alone or adopt different collaboration styles depending their situated needs. They may any appropriate available digital devices as they fit their tasks at hand. 1 https://theartsjournal.net/2019/03/28/the-coding-cave/, accessed 10 Sep 2021. 2 https://miro.com/ 3 https://www.maxqda.com/ 4 https://www.qsrinternational.com/nvivo-qualitative-data-analysis-software/home Proc. ACM Hum.-Comput. Interact., Vol. 5, No. ISS, Article 489. Publication date: November 2021.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Rarely is there a celebration without a cake. Apart from being an edible art, a customized cake is often a ceremonial symbol [11, 16] , which is special and personal, and closely associated with social relations and emotions [28] . Customized cake services enable clients to collaboratively personalize their cake in shape, color and favor with pastry chefs [63] . However, the customization process is not easy for both clients and chefs, which usually starts in a face-to-face meeting. Most of the follow-up communications are through text messages with the aid of reference cake pictures, which is insufcient for them to fully communicate their creative thoughts and to have a clear image of the fnal design [56, 78] . Cake customization requires professional skills. Based on 2D reference pictures and texts, it is not only difcult for clients to express the ideal decorations they want [40, 78] , but also challenging for pastry chefs to immediately visualize and show the size and decorations of the cake to the clients [56] . Fig. 2 illustrates such difculties. VR technology is developing at an unprecedented speed, which can simulate users' physical presence in a virtual environment, allow them to move around, and to interact with virtual objects. With the shifting focus from isolated experiences to a social medium, social VR has attracted a large stream of research exploring its potential for creating innovative communication approaches, supporting remote experience sharing and collaboration in diverse scientifc, artistic, informational and educational domains [18, 55] .\nINTRODUCTION: As a new remote communication medium [2] , Social VR is distinguished from video conferencing tools by their capacity to portray 3D spatial information [68] , to exploit users' natural behaviors, and to immerse users in the virtual world [13, 32] .\nINTRODUCTION: We posit that social VR is a promising medium to support clients to remotely co-design customized cakes with pastry chefs. Social VR allows pastry chefs and clients who are physically separated to co-present in a shared virtual space, and to assist their cake co-design by providing intuitive virtual interaction techniques and real-time 3D visualizations of virtual cakes. Both clients and chefs can instantly see the real-size 3D cake visualizations as their codesign results.\nINTRODUCTION: In this paper, we aim to address three research questions:\nINTRODUCTION: \u2022 RQ1: What is the current communication process of cake customization between clients and pastry chefs? \u2022 RQ2: What are the design requirements for a social VR cake co-design tool (CakeVR)? \u2022 RQ3: To what extent the design and implementation of the CakeVR prototype meet the design requirements? Exploring cake co-design as a new social VR use case, this paper made two main contributions. First, it specifes the requirements for designing a social VR tool (CakeVR) to support remote co-design activities (Section 3). Second, it implemented the CakeVR prototype and had it evaluated by experts (i.e., pastry chefs and experienced cake customization clients) using the cognitive walkthrough method [54] (Section 4 and Section 5). The evaluation results show that CakeVR supports idea generation and decision making by allowing users to intuitively manipulate sizes, select favors, design the decorations, and check whether the design fts the celebration theme (Section 6). We also discussed how CakeVR can be adapted to other domains, and its potential to transform product design communication through remote interactive and immersive co-design (Section 7).\nPrototype fdelity and evaluation methods: The goal of the expert evaluation is to invite expert users (i.e., pastry Prototype fdelity level is defned by visual refnement, breadth and chefs and clients) who are experienced in customization cakes, to depth of functionality, richness of interaction, and data model [37] . use the CakeVR prototype following a pre-defned sequence of tasks, Vermeeren et al. [70] defned that prototype fdelity comprises fully and to check whether the design requirements are addressed in the functional, functional, early proof-of-concept, and non-functional. prototype.\nPrototype fdelity and evaluation methods: According to NASA technology readiness level (TRL) [35] , our CakeVR prototype falls between TRL-3 and 4, indicating that a proof-of-concept model is constructed for a laboratory test. Our CakeVR prototype can be best seen as a mid-fdelity prototype, as it is not a paper prototype, however, it lacks some functionality.\nPrototype fdelity and evaluation methods: It has covered the breadth of functionality (i.e., implemented all identifed design requirements), but needs visual design refnement (e.g., cake fdelity), depth of functionality (e.g., precise operation), richness of interaction (e.g., facial expressions), and richness of data model (e.g., automatically calculating cake prices). For this stage, it is recommended to involve domain experts to evaluate the prototype, since their domain-specifc experiences can help identify infeasible and missing functions or interactions [70] .",
    "relatedWork": "RELATED WORK: The section presents related work about the co-design method, cake customization practices and social VR technology.\nCustomers as co-designers: Co-design invites customers, who are not professionally trained as designers, to actively participate in the design process and the fnal decision making of a product development [22] , which is proved to be powerful in product innovation [57] . Co-design helps efciently identify customers' needs [34] , and enables the design outcome to achieve higher user satisfaction. Customers are transformed into codesigners, who bring their tacit and contextual knowledge to help shape design solutions [6] . Meanwhile, designers are also changed from doing design themselves to building creative environments that inspire customers to co-design with them [19, 58, 67] . Trischler et al. [67] found that the concept generated by the design team with customers achieved a much higher score in user satisfaction and novelty.\nCustomers as co-designers: Nevertheless, co-design is confronted with challenges regarding social interaction between designers and customers. The frst challenge is the trade-of between designer's authority and customer's autonomy [15] . Dahl and Moreau [14] found that autonomy (i.e., being independent, fexible and creative) and competence (i.e., having design skills or having facilitation in design) are two motivations for customers to participate in co-design. Customers often lack professional design skills to fully express design ideas [48] . However, too much guidance by designers may decline customer's feeling of autonomy. Therefore, a careful balance between designer and customers needs to be considered to ensure successful co-design experience.\nCustomers as co-designers: The second challenge is to enable efective remote co-design, because it is not always possible to have all the co-designers physically collocated [50] . However, Sanders and Stappers [58] suggest that face-to-face interaction can support co-design to achieve the best results, because it involves complex nonverbal and tacit information exchanges (e.g., gestures [39] , and aesthetic feelings [57] ) that are difcult to replicate in remote communication. Thus, a new medium is needed to resemble face-to-face co-design activities, and to facilitate the competence and autonomy of co-designers.\nCake customization: In the late 1980s, the emerging mass customization allowed customers to confgure a limited combination of features on top of a basic product to meet their individual needs [24, 47] . However, in mass customization, customers do not participate creatively in the product development, and it is often pricey if customers want to go further into personal choices [46] . In a recent wave of personal customization [24, 73] , customers are invited to bring their creativity and values into the product development, and to modify the design solutions with the assistance of designers. Co-design is a helpful method to bring the customization service to a personal level [46] .\nCake customization: Cakes are usually served on celebrations, such as weddings and birthdays. A well-decorated customized cake is always cherished as a ceremonial symbol and perceived as an edible art [16] . The demand for customized cakes has never decreased . When designing a cake, a pastry chef does not only take aesthetic and gustatory elements into account, but also considers the social meaning, the event context, and emotions related to this cake [11] . With the development of 3D food printing technologies, people can personalize their cakes in shape, color, favor, texture and even nutrition [63] . Miyatake et al. [40] presented a 3D whipped cream printing technique for decorating cakes using a robot arm, which can dispense and stack the cream accurately according to users' drawings. Karyappa and Hashimoto [30] described an approach to accurately perform 3D printing of chocolate-based materials. In addition, many web-based or mobile applications are developed to assist product customization and provide instant 3D rendering of the customization results on screens [41] . Some bakeries provide such web-based cake customization tools. For example, Havi Proprel [49] and Baking IT [26] both ofer cake confgurators that provide step-by-step guidance for clients to personalize all the components of a cake (e.g., shape, flling, decoration, text and size). The customization outcome is immediately rendered and presented to customers on screens, with an automatically calculated price. However, these solutions can neither visualize the real size of the cake, nor allow customers to immersively see whether the cake design fts the celebration theme. As an emerging medium for remote communication, VR technologies have the potential to provide immersive product visualizations, real-time interactions, which can elevate the customers' customization experience in an entertaining and engaging environment [1] .\nSocial VR for remote communication and co-design: Social VR is promising in approximating face-to-face interactions [12, 20, 32, 60] , and has the potential to better support remote communication in terms of social presence [42] ), rich non-verbal communications [71] ), and immersive realistic interactions. On popular social VR platforms such as Facebook Horizon 1 and Al-terSpaceVR 2 , the facial expressions, voice, eye direction and body gestures of a user are captured and mapped to the virtual avatar of that user in real time. Social VR has been applied in diverse domains in supporting remote multi-user collaborations. McGill et al. [38] found that, despite technical limitations of consumer VR devices (e.g., limited feld-of-view), users signifcantly preferred the embodied social VR as a means of remote communication over the video-based communication. Similarly, Wang et al. [72] developed ReliveInVR, which allows multiple users to immersively relive the experience together. Reliving the experience in social VR is preferred by users to be more immersive and higher in social presence than watching the experience on screens.\nSocial VR for remote communication and co-design: In the design domain, Social VR has been used in supporting design collaborations. For instance, it provides a platform for car designers to show the 1:1 scale design details to communicate with managers who are not intimately involved in the design process [77] . Compared to desktop design tools (i.e., AutoCAD, Solidwork), VR enables immersive 3D modelling [61] , and intuitive gesture-based interactions [21] . For example, VR supports 3D mid-air sketching and real-scale modeling that enables a precise understanding of the scale, proportion and spatial impacts of a design [17, 61] . Tilt Brush 3 is a well known commercial tool for 3D sketching in VR . In addition, VR enables a realistic view of the design concepts [52] . It can ofer articulated visual cues (e.g., material texture) in highquality interactive 3D graphics or photo-realistic visualizations [44] , which enables immediate evaluation and discussion of the design outcome [77] . Moreover, the simulated virtual space can envision possible scenarios for designers to experience how this product makes a diference in the user environment [36] . Given the advantages of VR technologies, we are specially interested in exploring a remote co-design use case using social VR.",
    "rq": " rq1: what is the current communication process of cake customization between clients and pastry chefs?\n rq2: what are the design requirements for a social vr cake co-design tool (cakevr)?\n rq3: to what extent the design and implementation of the cakevr prototype meet the design requirements?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": "1. how the proposed features address the issue of the invisible hemisphere or improve the usability of the system in a collaborative task?\n2. how the proposed features affect the users' understanding of the remote environment?"
}
{
    "intro": "",
    "relatedWork": "RELATED WORK: This work builds on previous research on (interpersonal) communication and the work on eHMIs.\nInterpersonal Communication: Interpersonal communication is a widely studied field since 1949 [61] . In interactional models, the position as sender or receiver of messages [59] is switched, e.g., in the Sender-Message-Channel-Receiver (SMCR) Model of Communication [7] . Further factors can be included [5] , for example, social or cultural context. DeVito's interactive model [15] incorporates the factors context, feedback, and noise. As Richardson [55] put it: listening and feedback are essential for successful communication. Interpersonal communication was already used as a basis for the AV-pedestrian communication design In the field of eHMIs, Colley and Rukzio built their design space on this model and included these factors (e.g., affective communication) [11] . As feedback is essential for successful interpersonal communication, we investigated the effect of such communication in AV -pedestrian communication.\nExternal Communication of AVs: For the substitution of driver-pedestrian communication, several modalities such as displays [23] , LED strips [23, 42] , movement patterns [72] , projections [2] auditory or tactile cues [44] and combinations [44] or enhancement of the infrastructure [62] were proposed. The first works showed the usefulness of such communication in simple scenarios of one vehicle approaching one pedestrian. For example, L\u00f6cken et al. [41] compared six eHMI concepts. The Smart Road (based on work by Umbrellium [45] ) was rated best. Also, there are still ongoing discussions about the need for external communication of AVs [48] . Nonetheless, Dey et al. [19] showed the support eHMIs can give in communicating the intent of AVs. Still, when communicated intent and driving behavior contradicted each other, participants relied more on the behavior than on the eHMI [19] . Currently, work on eHMIs tend towards unresolved questions such as overtrust [33] or scalability [14] . For example, Dey et al. [17] evaluated an eHMI additionally showing distancedependent information indicating when the AV will stop. This improved pedestrians' comprehension of the AV's intention and increased willingness to cross. Additional work is undergone with accessibility in mind. For example, Colley et al. [12] included people with vision impairments in the design process. They found that additional auditory communication is necessary and can lead to improved communication quality also with seeing pedestrians.\nExternal Communication of AVs: While most work until now has focused on pragmatic communication, research has often included and now starts to focus more on hedonic (i.e., pleasurable, non-goal-oriented) communication (see [37, 58] ). Some hedonic aspects of this communication were already addressed: politeness and respect [38] and traffic prosociality [58] . Sadeghian et al. raise the question \"how 'mindful' and 'prosocial' automated vehicles (AV) behave and communicate\" [58, p. 205 ]. Sadeghian et al. [58] evaluated the prosociality of the previously suggested on-road light projections. They found that such communication is trusted and perceived as clear, adequate, and effective. This is in line with previous work [3, 10, 12, 17, 41, 44] . They showed that communication is situation-and role-dependent. Being in the role of the pedestrian, communication was received worse than from the driver. Additionally, in scenarios where pedestrians are more vulnerable (e.g., unsigned crossings), communication was perceived better. Lanzer et al. [38] compared a Chinese with a German sample regarding different communication strategies concerning politeness for AVs. They employed these strategies in two scenarios: at a crosswalk and while the pedestrian was blocking the way on the street. The polite strategy lead to higher acceptance in both samples but only the Chinese sample complied more often to the polite request. In both studies (and in the broader research field on eHMIs), communication is still one-directional, i.e., from the AV to the pedestrian. Therefore, we present a study on bidirectional communication [38, 58] .\nAnalysis of the Communication Evaluation in eHMI Research: As we wanted to measure the effects of feedback communication, we first determined currently employed measurements for eHMIs and AVs in general. Therefore, we queried the proceedings of the five most cited Human-Computer Interaction venues, according to Google scholar [26] . Due to their focus on (future) mobility, we also retrieved publications from the Conference on Automotive User Interfaces and Interactive Vehicular Applications (AutoUI) [4] and the International Conference on Human-Computer Interaction with Mobile Devices and Services (MobileHCI) [46] . We considered publications from the last ten years (01/2010 -10/2020). The first and second authors carried out the literature search and categorization. Our exclusion criteria were: (1) eHMIs on AVs must be the main focus of the work, (2) the publication must contain some form of user study regarding eHMIs, and (3) either system (i.e., the AV) or communication must be evaluated. The search query for each conference or venue in the respective digital library was: \"query\": AllField:(\"external communication\" OR \"eHMI\" OR \"eHMIs\") \"filter\": Conference Collections: [Conference / Venue]). First, the title and the abstract were screened to determine the research focus of the paper. Then, the full text was analyzed. Both authors evaluated all publications. Disagreements among the authors were resolved via discussion. We found and screened 45 publications and excluded 30 of them. 18 of the 30 exclusions were based on the focus not being on AVs and eHMIs and 12 were excluded as they did not present a user study evaluating AV or communication (e.g., [11] ). The remaining 15 papers were analyzed regarding the evaluated measurements on the communication and the AV. Our literature search led to the following results:\nAnalysis of the Communication Evaluation in eHMI Research: We found that either only the communication (8 times) or communication and the AV (6 times) were investigated. For the AV, measurements were included that are only partly related to the system. For example, trustworthiness (e.g., in [13] ) is a characteristic of a system but trust (e.g., in [10, 13, 21, 33, 35, 41] ) is not. Trust is a relationship [30] , which is dependent on potentially a variety of characteristics including situation [34] or personal disposition [36] . As measured by Lanzer et al. [37] , compliance and acceptance also describe a relationship towards the AV. Communication was evaluated using numerous aspects. There seems to be no consensus on appropriate measurements. However, pragmatic (including efficiency, clarity, etc.) and hedonic (including aesthetics, novelty, pleasantness, etc.) aspects of communication were often measured. This analysis showed that mostly only the evaluation is examined. Therefore, we also focused on these aspects in our subsequent experiment.",
    "rq": " we focus on the research question: what impact do the independent variables feedback communication of the av, presence of (other) pedestrians, and scenario have on a pedestrian in terms of (1) mental workload, (2) perceived safety, (3) trust, (4) communication comfort, the (5) av\\'s perceived intelligence, and (6) the perceived intention of developers?"
}
{
    "intro": "INTRODUCTION: Understanding how team processes underlie team performance is key for the design of organizational strategies as well as the development of new technologies for making groups more effective. The formation of teams to solve complex problems is salient in Science and Engineering, with over 90 percent of all publications now written by multiple authors [17] . In addition, the past decades have seen the rise of large and open communities of contributors, collaborating and competing to solve problems in ways that traditional organizations are ill structured to manage [9, 16, 18] . Such collaborations introduce unique challenges, from communication to coordination, which, if left unaddressed, can jeopardize the success of the projects.\nINTRODUCTION: Prompted by the prevalence of the phenomenon, a series of studies have explored how team composition [12, 27, 30] , organisation [1, 15] or dynamics [22] determine the performance and survival of teams, usually relying on conceptual models [1] or proxies from scientific co-authorship data [31] to quantify team impact and resilience. For example, team composition and its relation to team success has been measured in collaborative coding in Github [15] , in the artistic setup of Broadway musicals [12] or in private organizations [20, 28] . Complementary to such data-driven and modeling approaches, sociological approaches have provided indepth qualitative insights from case studies, for example through the inspection of laboratory notebooks as anthropological artefacts, revealing the multiple factors that underlie the process through which a group of individuals work together [29] .\nINTRODUCTION: Yet, we are still lacking the ability to obtain fine-grained, largescale in situ qualitative and quantitative insights on micro-level team processes. The problem comes in part from the fact that human activities, human dynamics, inter/intra collaborations and team organization [3, 8, 10, 19] require the ability to reach individuals to access such information. Data gleaned through ex-post surveys or interviews are typically small in both sample size and in cross-sectional or short time scales-limiting the ability to provide generalizable insights to understand the processes and dynamic patterns that underlie team work and performance.\nINTRODUCTION: Poised to tackle this problem, the use of smartphones and wearables has been on the rise to derive fine-grained, controlled insights on social interactions. For example, Radio-frequency identification (RFID) badges have been used to map temporal face-to-face group interaction dynamics [2] , with numerous real-world applications in human and animal groups. Such an approach yields quantitative insights but requires local human guidance and supervision to ensure a proper usage of the RFID badges, creating a bottleneck for scalability. Smartphone apps on iOS and Android [5] have also been used to derive interaction networks using Bluetooth scanning. Yet, recent legislation on the use of Bluetooth for contact-tracing studies in iOS has limited such attempts to authorized governmental applications. Moreover, such studies focus on face-to-face interactions, limiting the full understanding of social interactions at play. Beyond physical interactions, passively obtained personal data from social platforms and mobile phone data allows for the collection of large interaction datasets [10] , generating insights on friendship networks with a statistical power previously unattainable. However, such data is obtained in a non-controlled manner, and the resulting interactions remain hard to qualitatively assess.\nINTRODUCTION: To enhance such quantitative insights, recent studies in health and psychiatry [14, 24] have leveraged digital mobile applications that use active methods of experience sampling, including explicit self-reports that may range from occasional and detailed survey instruments to more frequent, brief and in-the-moment questionnaires that are referred to as \"ecological momentary assessment\" (EMA). EMA offers a number of major benefits over traditional survey instruments including the reduction of retrospective bias, realtime tracking of dynamic processes, simultaneous integration of multi-level data, characterization of context-specific relationships, inclusion of interactive feedback, and enhanced generalizability of results [25] .\nINTRODUCTION: Such active methods leveraging self-report rely on a regular user engagement. Traditional models reward engagement through monetary prizes, making them hard to scale or be replicated in absence of funding. To tackle this issue, the Open Humans platform has implemented a \"Quantified-self\" participatory system where study participants share their data in an anonymized manner with researchers, who in exchange provide digital notebooks for participants to investigate their own data along with general statistics over the entire cohort [11] . Such an approach has proved to promote regular, long-term engagement in symptom self-report studies during the COVID19 pandemic (\"Quantified Flu\" project [26] ), demonstrating the benefits of including participants in the loop of the experimental research study.\nINTRODUCTION: Building on these insights, we present a new web and mobile data collection platform for team network reconstruction called CoSo (Collaborative Sonar). CoSo focuses on a group in the loop intrinsic motivator (Fig. 1 ) to collect data on activities performed by a team through a mobile app, and present summary statistics and visualizations of the collected data on a companion web dashboard. In the following, we describe the features of the platform, showcase its use on a case study, and discuss the perspectives offered by promoting group-level metacognition and collective introspection. ",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Drones are steadily becoming ubiquitous [47] and are used in a variety of professional environments such as in the police forces and military [14, 18] , in search and rescue [4] , and firefighting [38, 56] . They represent a novel type of mobile technology that professionals bring with them and activate when needed. Recently, we are seeing a technological shift with increasing levels of autonomy [4, 10] ; with drones that can now perform tasks such as indoor navigation, surveying, filming, or mapping [20, 21, 28, 35, 40] autonomously using embedded sensors. This shift towards autonomy creates new interaction paradigms where drones become fully mobile devices that can interact with people at any time and location. In firefighting, prior works envisioned that drones will, over time, support various missions including detecting, monitoring, and extinguishing fires [22, 36, 38] . Indeed, drone technology offers unique opportunities to firefighters by bringing bird's eye view situational information, in real-time and at a low cost, which can reduce the risk to human life and properties. We imagine that, in the future, drones will not only support firefighters from afar, but also join them in their missions, helping them navigate inside a building on fire, or even guiding them through smoke. Yet, this vision will require people and drones to work closely and safely together.\nINTRODUCTION: Nonetheless, the integration of drones in human environments is not trivial, and neither is the introduction of technologies in emergency response. Firefighting presents incredibly complex challenges in terms of the \"physical risk, psychological state, and operating conditions\" [23] . Operating conditions of firefighting missions, which are referred to as \"harsh environments\" and can typically include low visibility, noise, open fires, smoke, and extreme temperatures, which furthermore require firefighters to wear heavy protective equipment.\nINTRODUCTION: In this work, we propose to investigate, first, how drones are being integrated into firefighting teams; and second, to elicit gestures that would enable collocated interaction between firefighters and future autonomous drones. The field of collocated Human-Drone Interaction (HDI) is rapidly growing, and prior works have investigated various interaction techniques. Our analysis of the literature shows why gestures are best suited in this context. Yet, while gestures have been investigated for collocated HDI with lay users [7, 12, 30, 32, 34] , the unique constraints of the firefighting environment require additional work to fully understand which gestures can and should be used in their professional context. It is then primordial to further examine gestural interaction for collocated HDI with firefighters, who are best suited to comprehend the constraints of their environments.\nINTRODUCTION: In this work, we take a holistic approach to understanding current and future practices for drones in firefighting. We conducted a first study where we interviewed the leading R&D firefighting officers in Israel, which allowed us to identify the current and future usage of drones, as well as the unique constraints and specific needs of firefighters. Based on our findings and our literature review, we opted for a gestural interaction strategy. We then conducted a second study with operational firefighters (N=9) which consisted of a focus group followed by a gesture elicitation study. The results led to a set of tasks and matching user-defined gestures.\nINTRODUCTION: Our results contribute a novel perspective and understanding of how drones can be integrated into firefighting, as well as a set of gestures for specific tasks where drones are needed in firefighting. Our findings further inform future researchers and practitioners on human and socio-technical factors to consider when integrating drones in emergency response.\nmonths: Prior academic with PhD in wildfire fighting they can monitor what is happening in real-time. The drone allows the firefighters to better prepare for the situation, support them in conducting their missions, with the potential to reduce the risk to people's lives. The experts described a recent example of a forest fire near a village. After airplanes finished their part of fire extinguishing, a ground unit kept fighting the fire with the assistance of a drone that was then launched to detect active fire spots. They described another example, where a drone was used to take pictures and videos of a wildfire before the firefighting airplane arrived. The National Fire and Rescue Authority is working at expanding the number of drones and the tasks drones can fulfill. One of the experts mentioned a drill conducted as part of the R&D effort where they tried using a drone to extinguish a fire. The test was conducted with a large agriculture drone carrying water as part of a 5 kg payload. Unfortunately, this attempt was unsuccessful as the drone sprayed water too slowly and the wind generated from the propellers made the fire stronger instead. This shows the firefighting brigade's need in having novel technologies that can support them in their work.\nmonths: In another drill, a drone was used to enable the commander on the ground to monitor the fire as well as his team on a crane by a tall building. The drone operator stood beside the commander and the drone companion app on a smartphone showed the video stream. This shows that a drone can be used as part of command and control, helping troops on the ground by increasing their situational awareness.",
    "relatedWork": "RELATED WORK: In the following section, we will review the related literature on the use of ground and aerial robots in firefighting, on existing collocated human-drone interaction techniques, and prior gesture elicitation studies with drones.\nGround and Aerial Robots in Firefighting: Firefighters typically operate in environments characterized by high temperatures, smoke, and low visibility, which are referred to as \"harsh environments\". These have been defined as environments that are \"hazardous to agents (human or robot etc.) within it [...] characterized by high levels of radiation, high explosive risk, extreme temperatures or pressures, and lack of oxygen. \" [54] . Additional characteristics include \"unknown, unstructured, dynamic, cluttered, hazardous and/or limited in resources (such as the availability of communications, GPS and visibility)\" [53] . As such, these environments present unique challenges for Human-Robot Interaction (HRI) and for integrating mobile technologies more widely (e.g., sensing next to open fire sources or dealing with dynamic obstacles and faulty sensors [6] ).\nGround and Aerial Robots in Firefighting: The HCI community has been investigating technologies to support firefighters [11, 24] . The literature suggests using ground and aerial robots to aid firefighters in various missions, such as in search and rescue [29] , detecting and extinguishing fires [27, 46] , mapping, and detecting gas in both indoor and outdoor environments [19, 40] . Yet, prior work investigated the potential of ground and aerial robots for firefighting missions [42] and revealed that drones were hardly used to firefighting missions; and further showed that firefighters found the drones' relevance for their purpose to be rather low. The researchers mentioned their findings may be explained by the limited range and operating time of drones at the time (2016), and by the additional resources required to control drones. Since then, promising works have shown that drones can benefit firefighting missions by providing visual access to low visibility areas, increasing safety to firefighters during emergency missions, optimizing the data gathering processes, time-saving, and reducing human error in inspections of a zone [38] . Further work investigated the use of drones in emergency situations [26] by interviewing firefighters and 9-1-1 callers, and showing that both user groups felt positive about drones. Agrawal et al. [2] then proposed an interface for search and rescue drones that provide added situational awareness to firefighters. Finally, earlier this year, Rold\u00e1n-G\u00f3mez et al. [39] interviewed professional firefighters and concluded that drones have the potential to improve firefighters' efficiency and safety in forest firefighting, providing a bright outlook for the future of drones in firefighting.\nGround and Aerial Robots in Firefighting: Yet, in all of these works, the drone is considered as a remote control technology. Since prior work has shown that as systems increase in autonomy, workload decreases [17] ; we suggest that autonomous drones could further support firefighters in their mission.\nGround and Aerial Robots in Firefighting: In particular, we focus on situations when firefighters and drones are collocated. To help us understand which type of interaction is best suited, we analyze prior work on collocated HDI.\nCollocated Human-Drone Interaction: Various interaction techniques have been proposed in the literature for collocated HDI such as using a remote control [3] , braincomputer interface [25] , wearable technology [15] on the user, speech using a ground station for the recognition side [9] , or by combining multiple modalities such as speech, body position, and hand gestures [45] . Other techniques proposed direct interaction with sensors embedded on the drone by detecting the user's face poses and hand gestures [7, 30, 32, 34] or touching the drone [1, 16] .\nCollocated Human-Drone Interaction: Based on the review of existing collocated interaction techniques and input modalities, we chose to focus on gestures as a promising approach for a drone to interact with firefighters. Our reasoning is multi-fold: 1. Gestures have been described as the most natural interaction technique in collocated HDI [7, 12, 33] , where prior research showed that participants preferred direct communication with gesture and voice over other techniques, and with high agreement on gestures; 2. Drones usually carry a camera, so no additional equipment would be needed for sensing, and gestures are easier to recognize than face poses when wearing a helmet. This will also enable the drone to interact with any firefighter and not only with the person holding the token of control (e.g., wearable device); 3. Remote controls are incompatible because they limit the operators' ability to perform other tasks [24] , in addition to being harder to use and less intuitive than gestures; 4. Other interaction techniques, such as voice, may not work with sensors embedded on the drone, and may fail in this environment, for example, because of the noise of the scene and the mask that firefighters wear.\nCollocated Human-Drone Interaction: While gestural interaction may be less precise than remote control [47] , we envision that semi and fully autonomous drones will not require the precise control required to maneuvre a drone, but instead a more general understanding of high-level commands. To design appropriate gestures in HDI, prior works used a gesture elicitation methodology [7, 12, 34, 43] , which we further elaborate on in the next subsection.\nGesture Elicitation with Drones: The gesture elicitation methodology is increasingly being used by designers and researchers to identify appropriate user-defined gesture sets for mid-air gestures across various applications [50] . Villarreal-Narvaez et al. [49] surveyed 216 gesture elicitation studies including from the fields of Human-Robot and Human-Drone Interaction (HRI/HDI). Prior works that dealt with gesture elicitation in drones first explored how lay users naturally interact with them and focused on low-level commands (e.g., stop, land, follow, or fly lower) [7] . The methodology was replicated in China [12] and showed that, while participants used different gestures, there was a high agreement in the choice of gestures [12] . Additional works investigated further gestures through elicitation for drone navigation [34] and to interact when running [43] . All of these works have been conducted with lay users for the everyday environment, with a focus on low-level commands.\nGesture Elicitation with Drones: While future autonomous drones will require less precise control, it is still unclear how to convey high-level commands to a drone, using mid-air gestures. In addition, the firefighting environment is a highly constrained and harsh environment, with the usersfirefighters-who are professionally trained personnel. We propose to run a gesture elicitation study to identify the appropriate gestures for firefighters to interact with drones in collocated situations. We will emphasize the work on high-level commands specifically designed for users in harsh and highly constrained professional environments.\nGesture Elicitation with Drones: To understand the current and future uses of drones in firefighting, the constraints of the environment, and how firefighters could interact with drones, we conducted two in-situ user studies. In the first study, we conducted a need-finding interview with the leading experts in the country responsible for the use of drones (N=3). This study aimed to understand their point of view as decision-makers and to identify scenarios of use and their future vision for drones. In a second user study, we conducted a gesture elicitation study with active firefighters (N=9) at their fire station.\nmonths: Only drone operator in the firefighting forces.\nmonths: Prior equivalent position in the police forces Head of operational research and development 6 months Previous experience working with drones and information systems in the army Intelligence officer of the firefighting forces in charge of wildfire firefighting theoretical models",
    "rq": ""
}
{
    "intro": "INTRODUCTION: The COVID-19 pandemic clearly shows how strong remote meetings have become a common feature of everyday cooperative tasks in many industrial and commercial contexts. How best to support such remote meetings has become a well-established and important discourse in Human-Computer Interaction [16, 25, 27, 33, 52, 62] . The development of tools to support remote collaboration generally depends upon the creation of some kind of artificial environment within which certain activities can be pursued. However, as Luff et al. [28] demonstrated ages ago, the relationship between participants' actions and these kinds of artificial environments is prone to fracture. This can render even straightforward activities problematic: \"Despite the opportunities afforded by the system, the materials reveal the difficulties participants face in accomplishing object focused actions, and the ways in which they attempt to establish and sustain common frames of reference; the system fractures the environment(s) of action and inadvertently undermines the participants' ability to produce, interpret and co-ordinate their actions in collaboration with each other\" [28] . Although Luff et al. 's work [28] was published over 15 years ago, the problem of providing a commonly shared and immersive environment without fractured ecologies still persists. Video conferences via Skype or Zoom support only certain aspects of face-to-face communication and lack support for dynamic and interactive activities beyond the screen. Yet practical collaboration during meetings often demands rich environments, resourced with a variety of physical artifacts, especially annotatable ones such as whiteboards or tabletops [18] . Some approaches have focused on setting up complex structures equipped with multiple screens and cameras that aim to offer an improved immersive experience and shared environment [10, 43] . However, the material complexity of such environments makes them largely inapplicable to everyday work from home or for ad hoc activities, even though telework and multi-site working is now a typical aspect of everyday cooperative work. The more artifacts the activities demand for their realization, the more troublesome this issue becomes.\nINTRODUCTION: To step around this, prior research has focused upon using Virtual Reality (VR) to provide shared, digital meeting rooms [35, 56] or upon using Augmented Reality (AR) to support remote collaboration [37] . While these approaches can offer sophisticated functionality, they fall short in foster a unfractured relationship between actions and the relevant environment. As these technologies have matured, however, new possibilities have started to open up for the support of distributed meetings [19, 49, 61] . A key innovation has been a shift towards the use of Mixed Reality (MR) to support remote collaboration, with AR and VR being used conjointly [41] . However, significant issues still remain with regard to supporting communication in MR environments that demand some form of compromise from the local and remote participants of a meeting.\nINTRODUCTION: In this paper, rather than retrofit functionality to support problems as they occur, we focused on the work practices performed during actual meetings, to see how virtual artifacts might be designed to minimize the likelihood of fractured ecologies even arising. To do this, we examined how to bring together multiple in situ AR users and multiple remote VR users in such a way that they all have a sense of sharing a single environment. Key to this is the construction of an environment that provides familiar shared cues and artifacts that can be interacted with conjointly without any loss of cooperative coherence. To accomplish this, we followed a user-centered design approach over a period of six months to get detailed insights into cooperative remote settings and the relationship between participants' actions and their environment. Based on the findings from this initial study, we derived a technical concept for supporting remote cooperation, which was implemented as the fully functional application called BeamLite.\nINTRODUCTION: BeamLite aims to create an immersive experience during remote meetings with asymmetric settings where multiple participants are collocated in the same physical location and multiple participants attending a meeting from distributed remote locations can feel like they are working together in a real room that is well-known in common. Within this room the participants can collaborate as they would in situ, drawing upon mutually available artifacts. The shared room and artifacts as well as the technology itself can provide better awareness of the participants' actions compared to traditional tools like Skype or Zoom. Furthermore, the shared and interactive artifacts make the meeting more inclusive for the remote participants in situations where physical artifacts like whiteboards are being used. Clearly, the designed system, built on a University budget, was not as sophisticated as some of the remote collaboration products now available on the market. However, the purpose was not to provide a polished solution but rather to create an environment addressed to issues associated with fractured ecologies to see: a) how to use new technologies to establish a system fostering unfractured environments in a principled way; and b) by implementing such a system, to obtain insights regarding what still needs to be done to fulfil the ambiguous aim of unfractured relationships within remote meetings.\nINTRODUCTION: Once implemented, BeamLite was qualitatively evaluated by observing meetings where it was used to tackle complex design tasks, followed by interviews with the participants. The results of this evaluation form the core of this paper and are used to demonstrate how the use of a familiar environment and specific virtual artifacts can reduce the fractured relationship between users' actions and their digital environment. In particular, it was found that each of the key concerns raised by Luff et al. [28] were, to varying degrees, addressed by providing this form of support. Thus, it enabled:\nINTRODUCTION: \u2022 orientation to a shared local environment;\nINTRODUCTION: \u2022 the use of local methods for animating and transforming features of the environment; \u2022 the provision of centers of coordination; \u2022 all of the participants' conduct (remote or otherwise) to be naturally intelligible; \u2022 and the contingent organization of artifacts to support specific courses of action.\nINTRODUCTION: Insights were also uncovered regarding the broader orientation of the participants towards the fact they were occupying a shared local environment. These immediate insights will also be drawn upon to examine their implications for the future design of MR-based support for remote collaboration. ",
    "relatedWork": "",
    "rq": " we thus address the following core research question: how can mixed reality foster unfractured remote collaboration?\n this can be decomposed into the following considerations: how can mr provide for the mutual referencing of -and orientation to -features of the shared local environment?\n reports of that kind reinforce the question: how can the flexibility of face-to face collaboration be supported in remote meetings?"
}
{
    "intro": "INTRODUCTION: Telepresence is a method of telecommunication where user immersively connects to the remote entity, typically a surrogate robot [22] . The robot's perception is transmitted to the user, and the user's motion controls the robot which enables the user to feel as if the user were in the remote place as a robot. This concept eliminates the limit concerning physical location, and can be used for various remote applications such as remote inspection and manipulation. person in the remote workspace enabling better mutual communication. For instance, a worker in the field can be remotely supported by an expert by sharing the context. In this case, the human is the surrogate for the other human.\nINTRODUCTION: In both cases, current telepresence systems mainly supplies the view from the surrogate(robot/human)'s perspective, which we also called the first person view. However, there are several shortcomings when the first person view is the only viewing method.\nINTRODUCTION: First, it causes motion sickness. This is particularly noticeable in the human-human telepresence. When the user in the workspace moves their head to looks around, the first person view tends to be very shaky and is not very comfortable for the remote user to watch. This problem can be solved by using the head mounted omnidirectional (360-degree) camera, and applying image stabilization [23, 29] .\nINTRODUCTION: The second problem is spatial awareness. In a real co-located collaboration, we can easily grasp positional relationship between humans, nearby objects, and the surrounding environment. For instance, when people are collaboratively doing the maintanace of the machine, they can easily understand the situation by walking around. If they do this through telepresence collaboration, on the other hand, the remote user can only understand the situation from the viewpoint of the surrogate (worker), and the remote user is not able to walk around to gain better understanding. If there are two or more surrogates(robots, humans, drones, etc.) in the workplace, it should be very convenient to \"jump around\" these viewpoints to understand the situation. However, if the system suddenly switches the viewpoint from one viewpoint to another, the user would lose their spatial context.\nINTRODUCTION: To address these issues, this paper proposes a method that en- ables seamless transitions between first person view and third person view, and the seamless transitions between multiple viewpoints in the workspace (Figure 1 ). To achieve this concept, multiple depth cameras (Kinect 2 sensors) are used to capture and reconstruct three dimensional data of the workspace and person. Once the vision is channeled to the third person view, user can freely change their viewpoint to look at the entire scence and can also go back to the previous first person view or any other first person/robot view.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Humans use touch to communicate friendliness, affection, support, playfulness, and more [32] . Just as technologies like videochat allow users to communicate by transmitting video and audio, Mediated Social Touch (MST) is a technological paradigm that focuses on allowing people to transmit touch [6] . Mediated Social Touch is a relatively nascent area of investigation, with few ideas taken beyond the conceptual design In this paper, we propose and investigate a novel technology for MST haptic actuation. Our primary contribution is in exploring a novel technical approach of using Shape Memory Alloys (SMA) for constriction and heat actuation for MST in a collaborative context. We demonstrate the feasibility of this approach by implementing the SqueezeBands prototype, which augments touch gestures directed at a videochat partner with haptic actuation (see Fig. 1 ). We describe the development process and implementation of this system with sufficient detail to support other researchers in adapting SMA for future MST technologies. Our secondary contribution is a user study of this system, undertaken to address the following research questions: \uf0b7 RQ1: How does the addition of SqueezeBands affect social presence, task load, and touch gestures attempted in a collaborative task? \uf0b7 RQ2: How does the emotional salience of the collaborative task influence SqueezeBands' effect on social presence, task load, and touch gestures attempted?\nINTRODUCTION: We begin by situating our contributions in terms of the previous work conducted in this area. We discuss the SqueezeBands system, including its design and implementation, to support others in using SMA in future investigation. We describe the setting, participants, and procedure of our empirical investigation, which included 57 pairs of participants from the Minnesota State Fair. We report our findings, which show this system's capacity for encouraging touch interaction and its benefits in reducing mental and physical task load in high emotion tasks. We conclude with a discussion of our findings that examines the broader opportunities and challenges for Shape Memory Alloy actuation in MST.",
    "relatedWork": "",
    "rq": " rq1: how does the addition of squeezebands affect social presence, task load, and touch gestures attempted in a collaborative task?\n rq2: how does the emotional salience of the collaborative task influence squeezebands' effect on social presence, task load, and touch gestures attempted?\n rq2: how does the emotional salience of the collaborative task influence squeezebands' effect on social presence, task load, and touch gestures attempted?"
}
{
    "intro": "INTRODUCTION: \"In the coming age, Computer Graphics will become an integral part of our language\" -Ken Perlin [41] Augmented and mixed reality technologies enable user experiences that leverage virtual elements to alter, enhance, and extend our perception of the real world. One simple but powerful form of augmentation is blending animated graphical elements like illustrations, icons, and text with live action footage of performers (Figure 1 ). Historically, this technique has been used as a special effect for various types of content, including scientific documentaries, instructional material, and music videos. While augmented graphics are typically added as a post-process after the primary footage has been captured, some examples involve performers manipulating graphics in real-time, as in weather forecasts or more recently, social media apps with video overlays.\nINTRODUCTION: The expressiveness of augmented videos stems primarily from the range of possible interactions between human performers and animated graphics. To better understand this interaction space, we have analyzed a diverse set of examples and found that augmented graphics are triggered and controlled by a broad spectrum of different gestures and body poses. For instance, graphics are sometimes \"attached\" to parts of an actor or the scene to highlight, decorate, and enhance the video footage. One specific example of this technique is adding digital clothing or accessories to a character. In other cases, actors directly drive the appearance or motion of graphics by performing a specific pose or gesture, like sweeping their hand to change the scale of an overlaid data visualization or manipulating a virtual, animated object.\nINTRODUCTION: While the richness of the interaction space makes it possible to produce a wide range of augmented effects, many of these effects are hard to achieve in practice. Producing real-time augmented videos, like weather forecasts, usually involves specialized professional tools that require a production team to prepare and trigger graphics manually in response to the performer [21, 40] . Video filters on social media apps offer an easier way to achieve real-time effects, but they support a very constrained set of gestures and graphics that limits the expressiveness and potential applications. An alternative is to add augmented effects via video postprocessing, using tools like After Effects. However, this approach is restricted to users with the time and expertise to execute advanced video editing and compositing work. Moreover, post-processing is obviously not suitable for real-time augmented videos.\nINTRODUCTION: In light of these challenges, we present an authoring tool that helps novice user produce real-time augmented video presentations with a wide range of gestures, body poses, and graphical effects. Our system provides an offline setup mode where users first map gestural actions to shape and appearance attributes of graphical objects via a direct manipulation interface. Then, in the interactive performance mode, the system interprets user actions in real-time, generates the corresponding animated graphics based on the authored mappings, and overlays the graphics onto the video footage. While our method is designed primarily to support real-time scenarios, the proposed approach also facilitates the creation of traditional videos; in this setting, the live performance mode can be used to generate an initial version that users can refine with additional post-processing if necessary.\nINTRODUCTION: Our main high-level contribution is in the design of the direct manipulation mapping interface, which provides a flexible, customizable way to associate hand gestures and body postures with graphical effects. We leverage existing hand gesture taxonomies in HCI to represent common gesture categories [13] as well as static body postures as design elements. Using a relational graph structure [30] , the user then maps the variety of gestures and postures to graphical actions \u2212 triggering, direct manipulation, indirect parameter tuning, and deformation of the graphical elements. By providing a useful set of predefined and composable primitives, our approach enables users to author a wide range of augmented effects without requiring any explicit programming.\nINTRODUCTION: We demonstrate the expressiveness of our system by creating several different styles of augmented video examples, including animated stories, scientific lectures, and explanatory videos. In addition, we have conducted design sessions with a diverse set of users to gain insights about the capabilities and limitations of our tool. We have received positive (albeit preliminary) feedback on the usability, benefits, and potential applications of our approach. The main contributions of this paper include:\nINTRODUCTION: \u2022 A direct manipulation interface for authoring how input bodies map to output graphical effects. \u2022 An interactive performance interface that applies these mappings in real-time. \u2022 A categorization of gestures and postures based on their different capability and suitability for various mapping scenarios.",
    "relatedWork": "RELATED WORK Gestures and HCI: Myron Krueger's Videoplace [35] is one of the earliest explorations demonstrating a virtual environment that responds to dynamic human gestures for interactions (e.g., painting, pointing, selection). In order to overcome the limitations of WIMP (windows, icons, menus, pointers) interactions, HCI researchers have explored a variety of novel interfaces to leverage the qualities of mid-air gestures for sign-language [44] , retrieving and manipulating imaginary objects in 3D [45, 50] , and interacting with multi-touch screens or AR (augmented reality) tabletops [16, 32, 46, 48, 49] .\nRELATED WORK Gestures and HCI: In this paper, we consider how to leverage the communicative aspects of gestures to enhance real-time human-tohuman communication through dynamic graphics. In this vein, prior systems in HCI and graphics, such as Charade [15] , ChalkTalk AR/VR [42] , and live multimedia presentation [39] , use dynamic hand gestures to control a computer-aided presentation to communicate with the audience. Inspired by this existing work, we propose a direct manipulation interface that enables user to define their own mappings from input actions (gestures and postures) to output graphical effects without programming. Given the idiosyncratic nature of gestures, this flexibility facilitates diverse usage scenarios across many domains. Moreover, our representation and flexible UI accommodates a wide range of input actions (gestures, postures) and output graphical effects, thus enabling an expressive range of interactions with graphical elements.",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "Introduction: Many work situations require people to collaborate together from remote locations. However sometimes the task can be complicated, so a conventional teleconferencing system cannot provide adequate support. For example, if people are trying to collaborate on a task in the real world which requires good spatial understanding. One approach that has proven successful on spatial tasks is to enhance remote collaboration using Mixed Reality. Mixed Reality (MR) remote collaboration allows a remote expert to use visual cues such as virtual hand or pointer to provide nonverbal communication to the local worker. This offers an additional communication channel between the users to help simplify the task instruction and collaboration.\nIntroduction: Apart from sharing visual cues, MR remote collaboration can also enhance the user experience by allowing the remote user to wear a Virtual Reality (VR) Head Mounted Display (HMD) and be immersed into a virtual version of the local user's surroundings. Similarly, the local user can also use an Augmented Reality (AR) HMD to view 3D visual cues overlaid on their real surroundings. With these AR and VR interfaces, both users can better understand the context of the shared environment or visual objects improving the collaboration [Piumsomboon et al., 2017] .\nIntroduction: MR remote collaboration systems can also present the shared environment in different ways. Using a head worn video camera is one of the traditional ways of sharing the environment, however it has some shortcomings, such as limited field of view. Sharing a 360-video view is one way to overcome this limitation, allowing the remote user to have an independent view from the local user [Kasahara and Rekimoto, 2015] , [Lee et al., 2018] . There are also other methods such as reconstructing a 3D scene to allow the remote user to be immersed into the shared environment [Gauglitz et al., 2012] , which we will discuss in the next section.\nIntroduction: It is clear that different configurations can carry different benefits, but there has been little research on how to combine them together. In this paper, we present a novel technique for MR remote collaboration (see Figure 1 ) that combines sharing 360 views and 3D scenes by embedding 360 panorama views into a 3D reconstructed scene. This provides a new solution for remote collaboration where multiple type of tasks are involved. For example, a remote mechanic expert guiding a local worker to assemble an engine on a workbench where the remote mechanic expert also needs to guide the local worker to pick the correct engine parts within the workshop environment.\nIntroduction: Compared to our prior work that allows users to switch between different viewing modes [Teo et al., 2019] , this paper focuses on providing (1) a novel MR remote collaboration technique that embeds 360 panorama videos into a 3D reconstructed scene, (2) it is a preliminary evaluation of the features of having a 3D scene with 360 panorama inserts, and (3) it provides design implications for MR remote collaboration systems that merge 360 and 3D views.\nIntroduction: In the remainder of this paper we first review related work (section 2), then provide a detailed outline on our prototype design (section 3). Next, we describe user study we designed in section 4. Then, we provide the result of our findings (section 5) together with discussion on our analysis (section 6). Finally, we summarize our work and our future plan (section 7).\nIntroduction: 2 Related Work",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "RELATED WORK 2.1 Mobile Video Collaboration: e exibility o ered by the mobile devices to easily switch camera feeds and change device orientation enable their use for collaborative physical tasks. O'Hara [2006] conducted a diary study. In their sample 28% of video calls involved showing things in the environment to talk about and 22% were for performing functional tasks (e.g. planning events or seeking guidance). Similarly, Brubaker et al. [2012] note a growing trend in using video to support joint activities (e.g. seeking guidance to accomplish physical tasks) and experiences (e.g. giving a tour of a at). Jones et al. [2015] studied how people collaborate using mobile video and found that a serious shortcoming of commercial mobile video conferencing services is the lack of support for remote gesturing, which is known to be important to e ciently use video as a collaborative activity space. Previous work has shown the growing trend in mobile video telephony to use \"video-as-data\", instead of the conventional \"talking heads\" [Nardi et al., 1993] . ese new applications of mobile video require gesturing mechanisms e.g. to point out interesting details in the environment, or to e ectively communicate procedural instructions in a physical task.\nRELATED WORK 2.1 Mobile Video Collaboration: 2.2 Gaze sharing in collaboration: Does the level of awareness matter?\nRELATED WORK 2.1 Mobile Video Collaboration: ere have been numerous studies on gaze awareness in collaboration, in tasks involving visual search [Brennan et al., 2008] , programming [Stein and Brennan, 2004] , trip-planning [Qvarfordt et al., 2005] and puzzle-solving [Velichkovsky, 1995] .\nRELATED WORK 2.1 Mobile Video Collaboration: e most common approach to provide gaze awareness in collaboration is to present the gaze of the partner as an abstract visual element, such as a dot, ring or icon of the eye, overlaid on the shared visual space (notable exceptions are Trosterer et al. [2015] and D'Angelo et al [2017] ).\nRELATED WORK 2.1 Mobile Video Collaboration: e previous studies on shared gaze can be classi ed, based on the level of awareness the producer of the gaze has on the gaze sharing. Qvarfordt et al. [2005] studied value of naturally occurring eye movement in a collaborative trip-planning task and found that gaze, even if not explicitly produced with the intention to communicate can aid deictic referencing, aid topic switching and help reduce ambiguity in communication. Similarly, Stein et al. [2004] found that eye gaze produced instrumentally (as opposed to intentionally), can help problem solving in a programming task. Liu et al. [2011] noted that naturally occurring gaze can help to e ciently achieve referential grounding. In all these studies, the producer of gaze was not aware that the partner would see their gaze point and thus did not use gaze as an explicit communication channel. us, the gaze point re ects their natural gaze behaviour.\nRELATED WORK 2.1 Mobile Video Collaboration: In contrast, other studies used a setup where the collaborator is aware that their gaze is being shared, and thus they use their gaze more explicitly to communicate. However, some studies showed the collaborators their own gaze point, providing accurate awareness of the point that is transferred and others did not show own gaze point to the collaborator. Akkil et al. [2016] and Higuch et al. [2016] studied a set-up where gaze of the remote instructor was physically projected to the task space of the partner. us, the instructor saw the physical projection of their own gaze point on the video captured by the situated camera, giving the instructor direct feedback of their own eye movements and accuracy of gaze tracking. Others have studied shared gaze in a collocated scenario, where both the collaborators are in front of the same display, enabling the collaborator to see their own gaze. Zhang et al. [2017] studied collocated visual search on a large screen, Maurer et al. studied gaze gaze sharing between a collocated game spectator and gamer [2015] , and between passenger and driver in a driving simulator [2014] . Similar set-up was also used by Duchowski et al. [2004] in collaborative virtual environments.\nRELATED WORK 2.1 Mobile Video Collaboration: Similarly there are a number of studies involving shared gaze used in a set-up where the collaborators are aware of shared gaze and saw their partner's gaze, but did not see their own gaze point. Examples include, Brennan et al. [2008] in a collaborative visual search task, D' Angelo et al. [2016] and Muller et al. [2013] in puzzlesolving tasks, Lankes et al. [2017] during online game viewing and Maurer et al. [2016] during online cooperative gaming. Interestingly, Maurer et al. [2016] note that their participants commented they would have liked to see their own gaze point, along with the partner's gaze. In contrast, D'Angelo [2016] note that showing the own gaze pointer may not be a good idea, since it \"can produce a feedback loop that causes people to follow their own cursor\", when gaze tracking is not accurate.\nRELATED WORK 2.1 Mobile Video Collaboration: In summary, previous studies on shared gaze have used three di erent con gurations of gaze sharing, and found value for all three in the collaboration. is brings us to the question, are they all equally e ective? A comparative evaluation between the three setups would give us novel insights into the utility of each of the con gurations. is was the focus of our work.\nGaze Awareness in Collaborative Physical: Tasks Akkil et al.[2016] noted that gaze overlaying in egocentric videos improves accuracy of interpreting hand-pointing gestures. Similarly, Gupta et al.[2016] found that in collaborations using head-mounted cameras, gaze sharing improves collaboration performance in a stationary LEGO building task. Other studies explored physically projecting gaze to the task space in a circuit assembly [Akkil et al., 2016] and block arrangement task [Higuch et al., 2016] . ey found that gaze sharing made referring objects easier and also improved the feeling of presence between collaborators. We recently conducted a study involving an object arrangement task using a similar experimental setup, involving stationary cameras and physical projection of gaze, but we compared shared gaze with a shared mouse for remote gesturing [Akkil and Isokoski, 2018] . We found that shared gaze improved collaboration compared to having no gesturing mechanism at all, However mouse outperformed gaze in both objective and subjective measures. ere was no di erence between shared gaze and shared mouse cursor in tasks that required only pointing. However, when the task required conveying procedural instructions (e.g. \"turn the object like this or orient it like this\"), mouse was the be er of the two remote gesturing mechanisms. In summary, previous studies on shared gaze in collaborative physical tasks involving stationary tasks have shown that while gaze is useful, it may not be as useful as the mouse. e focus of this study is on mobile video collaboration. Mobility of the task and additional complexity due to the hand-held device may in uence how the remote gesturing is perceived and used by the instructor and the worker, and perhaps even on the e ectiveness of gaze and mouse-based remote gesturing.",
    "rq": " rq3: how does shared gaze compare against a shared mouse pointer in a mobile collaborative physical task?\nrq3: how does shared gaze compare against a shared mouse pointer?"
}
{
    "intro": "INTRODUCTION: In the past decade, interactive tabletops have been successfully used in multiple domains, including healthcare, education, entertainment, and cultural exhibitions [2, 18, 34, 36] . These devices show significant advantages in supporting multi-user interaction, namely due to their large shared surface. Also, tabletop geometry has been shown to encourage equitable participation among users as well as simultaneous individual interactions with the digital content [32, 33] . Previous research highlights several benefits of using interactive tabletops: improving collaborative learning [37, 55] , supporting reflection-type conversations [32] , enhancing social interaction [18] , fostering creativity as well as engagement [10, 17] . However, people with visual impairments can struggle to engage with such large surfaces [23, 39] . We argue that not being able to use interactive tabletops, for example, due to visual impairments, and consequently participate in these group activities, can be a vehicle of social exclusion.\nINTRODUCTION: Despite being an inherently visual technology, touchscreen devices can be used by blind users. Screen readers, such as Apple's VoiceOver [3] or Android's Talkback [20] , allow users to explore and control the device by providing audio feedback for touch actions. However, when considering the spatial awareness of interface elements, these accessibility services are mostly designed for smaller form-factors, rather than large collaborative surfaces where the ability to locate items and establish relationships between them is more challenging. Examples of applications for such devices include exploring maps [39] and anatomic models in educational settings, or mind maps in brainstorming sessions [53] . In these, the ability to locate artifacts without losing spatial awareness and to relate them is relevant. While there is a large body of work on touchscreen accessibility, research has been restricted to single-user interaction [15, 21, 23, 39] .\nINTRODUCTION: This raises the question of how accessibility services can support nonvisual collaborative activities on large touchscreen devices. Particularly, how can we inform users about the actions of others? This notion of monitoring the activity of others, which provides context for one's activities is defined as workspace awareness [14, 28] . Workspace awareness is a crucial aspect of collaboration [28] and often relies on visual display techniques [30] . However, little is known about using audio for tabletop multi-user interaction, particularly on how to provide workspace awareness and support blind people in joint activities.\nINTRODUCTION: We present a study that contributes to the understanding of auditory design in enabling nonvisual collaboration in large touchscreen surfaces. We developed a nonvisual tabletop prototype, shown in action in Figure 1 , with three feedback modes. We examined workspace awareness in colocated collaborative activities to answer: is auditory feedback effective in supporting nonvisual collaboration in large touchscreens? How does the amount of auditory information delivered influence patterns of awareness information exchange? How do blind users leverage audio feedback to engage with one another in co-located collaborative tasks? How is task performance in large touchscreens affected by the amount of auditory information displayed?\nINTRODUCTION: The main contributions of the paper are: first, the design of multi-user auditory feedback for nonvisual tabletop collaboration; second, analysis of the effects of auditory feedback on blind users' workspace awareness and task performance when using interactive tabletops; third, we describe emergent nonverbal collaboration behaviors when blind users engage in joint activities; and finally, we propose a set of implications for the design of multi-user auditory displays.",
    "relatedWork": "RELATED WORK: The related work reviewed in this section is two-fold: first, we discuss research on touchscreen accessibility for visually impaired users, including mobile devices and large surfaces; second, we examine previous attempts to create collaborative systems that leverage audio output as a feedback modality.\nTouchscreen Accessibility: Mobile touchscreen devices have built-in screen readers that allow blind people to use their devices. They enable users to explore the screen by dragging their fingers and having the interface elements (e.g., buttons, labels, images) read aloud as they touch them (i.e., Explore by Touch). Alternatively, users can perform directional swipes to cycle through interface elements sequentially. A double-tap or split-tap selects the last heard element [38] . Over the last decade, numerous projects aimed at improving touchscreen interfaces, including understanding the performance of gestures [41] ,\nTouchscreen Accessibility: proposing new text input techniques [6, 24, 51, 52] , and leveraging Braille knowledge [4, 58] . Other solutions took advantage of concurrent speech feedback [23] , augmented touchscreens with physical interfaces [40, 60, 62] or haptic feedback [19, 46] . Touchscreen devices have also been combined with computer vision-based solutions to create novel assistive technologies [26, 27] .\nTouchscreen Accessibility: For more examples, Grussenmeyer and Folmer [21] provide an extensive survey of accessible touchscreen technologies.\nTouchscreen Accessibility: Regarding large touch surfaces, Guerreiro et al. [25] investigated the exploration strategies employed by blind users when two-hand interaction is supported in a tabletop. Bardot et al. [5] also studied two-hand exploration with bi-lateral audio and vibrotactile feedback using a smartwatch on each wrist. Kane et al. [39] proposed three input techniques that enhanced performance with these devices by removing the need to perform absolute spatial explorations. These techniques were evaluated in an interactive map application. Indeed, this is the most frequent application associated with large touch surfaces for visually impaired people; several projects relied on large tactile maps augmented with auditory feedback and projections [1] . A detailed review was conducted by Brock et al. [7] . More recently, Ducasse et al. proposed the use of tangibles over interactive tabletops to ease blind users' spatial explorations [15] . The authors proposed a new type of tangible that uses a retractable reel to render digital links into physical. Another existing alternative for making large interactive surfaces accessible to the blind are large tactile displays [59] , which can help preserve users' spatial memory. However, these do not scale properly for multiple users.\nTouchscreen Accessibility: Overall, although efforts have been made to make large touch surfaces accessible to blind users, the potential to use these devices in collaborative activities with blind people has remained mostly unexplored.\nCollaboration and Audio: According to Gutwin and Greenberg [28] , it is important to consider workspace awareness when designing systems to support collaborative work over a shared workspace. This includes providing knowledge about where the other users are and what actions they are performing. Naturally, this involves monitoring both the shared space and others, as well as displaying one's activities to the group [14] . Audio has been shown to support awareness in groupware systems when combined with graphics to overcome limitations associated with visual displays [30, 31] . More recently, it has been shown that it can be used as a sole means for supporting nonvisual collaboration between sighted users in accessing and editing shared hierarchical menu structures [48] .\nCollaboration and Audio: One question that arises in auditory collaborative systems is the effect of the different means of delivering audio feedback on collaborative work. Hancock et al. [31] assigned a different timbre to each user, showing increased group awareness but at the cost of decreased individual performances. Others [54] investigated the use of spatialized speech feedback, showing that users leverage the spatial position of their peers' speech to maintain awareness of their interactions with the shared space. Morris et al. [50] compared the use of headphones and shared speakers to deliver auditory feedback to a group of tabletop sighted users. They found that collaboration strategies changed when users wore headphones and that using headphones does not impede group communication. Although numerous projects have studied the effect of auditory designs on collaborative work, these studies focus on combining it with graphical interfaces.\nCollaboration and Audio: In a seminal work, Winberg and Bowers [61] showed that visually impaired users could interweave their active manipulations of collaborative interfaces with talk; however, missing content changes from peers could result in disengagement from the task. McGookin and Brewster [47] investigated users collaborating to create graphs with an audio-haptic tool. The use of shared audio enabled awareness; however, the amount and type of audio needed seemed dependent on the strategy adopted (divide and conquer vs. turn-taking). Buzzi et al. [9] examined the accessibility challenges of an online text editing tool, highlighting the need for \u0142accessible awareness\u017e where the status and actions of users should be visible. On a similar topic, Das et al. [13] conducted interviews with 20 visually impaired professionals that use collaborative writing tools. They highlight the need for better auditory designs and the potential of using spatial audio, concurrent feedback, and multiple voices to enhance the visibility of collaborative actions. Kunz et al. [42] proposed to make pointing gestures above tabletop devices accessible to blind users as nonvisual cues are crucial in collaborative work. More recently, P\u00f6lzer et al. [53] presented a system to support blind people to participate in co-located brainstorm sessions with mix-ability teams by detecting pointing gestures and transcoding visual information to audio feedback. More broadly, Luque et al. [45] proposed a framework to analyse collaborative settings involving people with disabilities in order to identify potential challenges and coping mechanism. All these studies emphasize the importance of designing for workspace awareness in collaborative systems and the challenges of doing so through auditory feedback.",
    "rq": " we examined workspace awareness in colocated collaborative activities to answer: is auditory feedback effective in supporting nonvisual collaboration in large touchscreens?\n(1) is auditory feedback effective in supporting nonvisual collaboration in large touchscreens?\n(2) how is task performance in large touchscreens affected by the amount of auditory information displayed?\n (3) how does the amount of auditory information delivered influence patterns of awareness information exchange?\n (4) how do blind users leverage audio feedback to engage with one another in co-located collaborative tasks?\n answering the research questions 1. is auditory feedback effective in supporting nonvisual collaboration in large touchscreens?\n2. how is task performance in large touchscreens affected by the amount of auditory information displayed?\n3. how does the amount of auditory information delivered influence patterns of awareness information exchange?\n4. how do blind users leverage audio feedback to engage with one another in co-located collaborative tasks?"
}
{
    "intro": "INTRODUCTION: Compared to co-located collaboration, remote collaboration is more difficult as collaborators are not in the same place [18] . Technology for remote collaboration has been studied for decades to reduce this difficulty [15] by improving user interfaces for better sharing the local task space [45] and supporting better communication between collaborators [10] . In this paper, we mainly focus on the user interface for better communication between collaborators.\nINTRODUCTION: In a typical remote collaboration study with a real-world physical task (e.g. fixing a car or making a Lego model), there are two main participants: a local user in the task space and a remote collaborator [10, [26] [27] [28] . While the local user directly performs activities in the local task space, the remote collaborator could use technology (such as video conferencing) to see a view of the task space and use visual cues to aid communication. Typical visual communication cues are pointers [29, 35, 42] , sketches [12, 13, [26] [27] [28] , and hands [1, 20, 32, 44] , and many researchers have explored the effect of these cues individually in remote collaboration studies.\nINTRODUCTION: However, to the best of our knowledge, there is no previous study investigating the effect of combining the common visual cues (i.e. pointer, sketching and hand gestures) together in Mixed Reality (MR) remote collaboration. In this paper, we explore and compare four different combinations: 1) hand only, 2) hand + pointer, 3) hand + sketch, and 4) hand + pointer + sketch -a 2 x 2 factorial design study where the hand gesture cue is the baseline condition with two independent variables (Pointer and Sketch) with each variable having two levels (On or Off).\nINTRODUCTION: This research makes the following novel contributions:\nINTRODUCTION: (1) Design and implementation of a prototype MR system supporting both local and remote collaborators wearing a head mounted display (HMD) and using combination of visual cues including the pointer, sketch, and virtual hand. (2) Comparing four combinations of the typical visual communication cues (virtual pointer, sketch, and hand representation) with three physical tasks (Lego blocks, Tangram puzzle, and Origami) in MR remote collaboration. (3) Measuring the effect of the combinations of visual cues on the co-presence and required mental effort",
    "relatedWork": "RELATED WORK: Many researchers have investigated the use of visual communication cues in remote collaboration on spatial tasks. In this section, we describe previous studies according to the type of visual cues (pointer, sketch, and hand gesture) used.\nRELATED WORK: The Pointer Cue Using a pointer is a quick and precise way to indicate an object of interest. For example, Gesture-Cam [35] used a pointer in a desktop remote collaboration setup. It allowed a remote user to control a laser pointer in the local task space, so the local user knew which object the remote user was referring to. Sakata et al. [42] put the laser pointer and camera on the local user's shoulder, so the remote user could have an independent view of the task space and point with the laser pointer. Fussell et al. [10] overlaid a virtual pointer on a shared live video with a desktop monitor, and Kim et al. extended it with a tablet interface [29] . Later, Kim et al. [27] compared the pointer and sketch cues and found that the pointer was more precise and quicker to provide a simple pointing information for positioning and selecting objects.\nRELATED WORK: Prior work showed using a pointer can increase users' co-presence by encouraging a remote user to be active in collaboration [27] . Moreover, pointer cues also have been used for displaying collaborators' gaze in the shared task space, so the other user could know where the collaborator was looking at and increase the level of co-presence. Gupta et al. [16] developed a system sharing the local user's gaze pointer and remote user's mouse pointer and found both pointer cues increased participants' feeling of co-presence. Higuchi et al. [19] compared using hand gestures alone and hand gesture + remote user's gaze pointer together and found that adding the gaze pointer helped participants to better understand each other. Lee et al. [39] investigated the effect of sharing both local and remote users' gaze pointers and found that the gaze pointer increased both users\u00e2\u0102\u0179 feeling of co-presence and the level of awareness of where the users were looking at.\nRELATED WORK: The Sketch Cue Sketch cues have been shown to be effective for presenting spatial information for manipulating objects [25] . Fussell et al. [10] and Kim et al. [29, 30] compared pointer and sketch cues and found that participants completed tasks quicker using the sketch cue than with the pointer cue. One important feature of the sketch cue is permanence, with the sketches remaining where they were drawn [25, 27] showing lines or object shapes, while information from the pointer cue is ephemeral.\nRELATED WORK: Early systems often had a fixed camera view to avoid sketches losing the reference frame (e.g. if the view changes, a sketch of an arrow may no longer point at the same object). For example, Tang et al. [47] developed 'Videodraw' sharing a task space with a top-down fixed camera and projecting it on the other end. Ishii et al. [22] developed 'ClearBoard' sharing a synchronized screen view with sketches. Fussell et al. [10] used a fixed camera on a tripod with sketches overlaid on the video.\nRELATED WORK: Using Augmented Reality (AR) tracking techniques, sketches can remain where they are drawn, even when the view changes. For example, Kato and Billinghurst [24] stabilized the user sketches with marker-based tracking that determined the real-world position of the remotely added sketches. Fakourfar et al. [6] used marker-based tracking with a tablet, while Kim et al. [25, 27, 28] and Gauglitz et al. [13] employed SLAM (Simultaneous Localization and Mapping) tracking to stabilize and fix the user sketches to the real-world space.\nRELATED WORK: In previous studies, there were two interesting results about sketch cues. First, participants preferred sketches to be automatically erased after a while when it is drawn because old sketches cluttered the view of the scene as they became no longer relevant [10, 29] . Second, the sketch cue increased the feeling of co-presence between local and remote users as it clearly showed remote user's explanation [27] .\nRELATED WORK: The Hand Gesture Cue Hand gestures can express diverse information [31] such as pointing, shapes, and even social cues such as appreciation (e.g. thumbs up) [14, 31] and emotion (e.g. making a heart shape with the hand gesture) [8] . Goldin-meadow [14] mentioned that hand gestures are often used with speech to provide spatial information such as pointing at an object or guiding listeners to focus on a specific area or an object. Researchers found that using hand gesture cues in remote collaboration helped to complete the task faster than without it [32, 33] as it supported easy explanation of object manipulation with pointing and hand shapes, and by showing the required hand movement. The hand gesture cue also increases the users' co-presence [21, 32, 34] as it showed the other user's hand motions.\nRELATED WORK: Tasks Visual communication cues may have different effects according to the task [25] [26] [27] . Prior user studies with multiple tasks [6, 19] showed different effects of using visual communication cues. For example, for Origami task which requires 2D information on how to fold a paper, a sketch cue was a good way to show a folding line [6] . However, it was difficult to use sketches to describe how to insert an object at a certain depth in a car repair task [48] .\nRELATED WORK: The most relevant work is Chen's study [4] which compared a combination of hand gestures and sketch cues to a video only condition (no additional visual cues) and Higuch's study [19] that compared a hand gesture condition to a hand gesture + gaze pointer condition. However, there are several differences between their studies and ours. First, we investigate the combinations of three typical visual communication cues while they explored the effect of combining two visual cues. Second, in Chen's system [4] , the hand capture area and display monitor were not in line with the remote user's perspective, while they are in our system. Third, Higuch et al. used a desktop monitor for remote user's display and studied a user gaze pointer, but we use a HMD and a hand pointer. Most recently, Teo et al. [48] used the three cues in a live 360 video-based MR remote collaboration system. However, their study did not compare the sketch and pointer cues in different combinations, and also did not look into social aspects, such as co-presence.\nRELATED WORK: Overall, the three visual communication cues have different strengths in sharing spatial information and all three cues have been shown to improve the user's feeling of co-presence. However, no prior work further investigated different combinations of these cues. Based on the findings from prior work, we suggest the following five hypotheses:\nRELATED WORK: \u2022 H1 Using more visual communication cues in combination results in faster task completion time. \u2022 H2 Using more visual communication cues in combination results in better usability. \u2022 H3 Having more visual communication cues in combination leads users to have higher feeling of co-presence. \u2022 H4 Using more visual communication cues in combination reduces the mental effort for remote collaboration. \u2022 H5 The effect of using the combinations of three visual communication cues (hand, sketch, and pointer) would show different trends according to the type of task.\nRELATED WORK: Apart from the visual cues, several researchers have also studied providing an independent view to the remote user [9, 11, 13, 21, 23, 26, 38, 41, 46] and different types of hand gesture cues (virtual hand, 2D hands, and 3D hands) [21, 33] . Moreover, Kim et al. [26] compared two collaboration styles: mutual and remote expert collaborations. In a mutual collaboration, both the local and remote users do not have a solution for the task, so they need to discuss ideas together to find a solution [26] [27] [28] 39] . In a remote expert collaboration, a remote expert has the solution for the task and provides instruction to the local worker [10, 13, 20, 21] . In this paper, our system uses dependent view configuration which is known for its benefit of shared focus in remote expert style of collaboration.",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Today's computing power, the availability of fast data networks, and the spread of wireless connectivity to even remote areas allows us to virtually connect people wherever they may be through technologies such as Zoom, FaceTime, and Skype. Further technological advancements could make this experience even more realistic by giving users the impression that they really are spatially present (the sense of \"being there\" [4] ) in another place, or socially present (feeling that their communication is unmediated [18] ) and co-present (mutually attentive and entrained [5] ) with their remote communication partner.\nINTRODUCTION: Many solutions have been proposed that attempt to give users this feeling of presence by either surrounding them with sensors or tethering them to stationary systems, but with how pervasive mobile phones have become it seems unlikely that users would revert to a desktop computer as their primary telecommunication device. This has led to mobile devices being integrated into several advanced telepresence prototypes as simple portable displays powered by stationary hardware [23, 26] , though this nullifies any advantages in mobility and spontaneity that mobile phones can provide [2] .\nINTRODUCTION: We present Mobileportation, a prototypical system that provides an immersive nomadic telepresence experience on purely mobile devices. We use a mobile phone with an in-built RGBD sensor to incrementally create a 3D reconstruction of the local user's environment as they move around it, which the remote user can freely explore in 6DoF (Degrees of Freedom) in an exocentric view (from outside of the local user's position). An attached 360 \u2022 camera also captures a live feed of the space encompassing the local user, allowing for a higher resolution egocentric view of the environment (from within the local user's position) revisionby simply walking to their current position. Each user's face is also captured and spatially displayed in the shared environment on a virtual avatar, allowing Mobileportation to provide the face-to-face communication users expect from such systems while also giving them complete freedom to move around the shared environment independently.\nINTRODUCTION: We show that despite the technical limitations of mobile devices this experience can be achieved in real time and with minimal latency over a cellular network, allowing for ad-hoc immersive communication and exploration of remote environments without requiring any additional hardware, and provide technical details on how this was accomplished. We also present the results of a user study showing that despite the reduced visual fidelity of its incrementally constructed environments, Mobileportation is similar to video-only 360 \u2022 mobile videoconferencing systems in its ability to invoke a sense of presence between its users and within the virtual space while providing a significantly more social experience with more freedom for its users. Finally, we explore the possible innovations that this hardware platform could provide in the future, showing how this application can be extended to support advanced features such as view-dependent rendering, full-body capture and display, and 3D hand gestures once mobile phones become sufficiently powerful.",
    "relatedWork": "RELATED WORK: Many approaches have been proposed that attempt to bring distant people into a user's local environment. Tang et al. [28] explored how a 360 \u2022 camera could be used to enhance standard videoconferencing by allowing a remote user to obtain independent views within a shared environment. Such views have been shown to significantly increase users' spatial awareness of the environment [14] and spatial presence within it [21] as well as allow them to guide the topic of conversation rather than have it dictated to them. While rotational independence can thus be achieved, this only shifts the asymmetry of interaction as the remote user still has no control over their position within the space.\nRELATED WORK: The solution to this asymmetry is often to create a three-dimensional reconstruction of the users or their environment and combine the two in an exocentrically viewed space. Holoportation [22] attempted to virtually bring the remote user into the shared space by volumetrically scanning and displaying them within the local user's Hololens display through several custom RGB-D camera arrays placed about the space. This volumetric reconstruction also required expensive and powerful desktop computers, severely limiting the system's user base and potential use cases by raising the cost of entry.\nRELATED WORK: Many systems use this outside-in method of depth sensor placement to create these reconstructions [1, 10, 19 ] as it allows the full space to be visible once communication starts, however this environment and the users within it are constrained to the limited area covered by the cameras. To remove this constraint, technologies such as KinectFusion [13] or photogrammetry [24] can be used, which combine multiple captures from a moving Kinect sensor or RGB camera into a coherent 3D mesh to create larger interaction spaces. This requires a lengthy pre-construction stage before conversation can begin, and any updates to the environment will not be reflected in the reconstruction, making this method unsuitable for dynamic spaces where the task object or topic of conversation is likely to be moving. Some recent work explored how light fields could be used in place of a mesh for object-focused interaction [20] , and while this offers advantages in visual quality even on mobile devices, it is unlikely that this can be extended to room-scale environments.\nRELATED WORK: The alternative is to stream the RGB-D data to the remote user as it is captured in real time, dynamically reconstructing the environment as the local user walks around it. Stotko et al. use this method in SLAMCast [27] , which combines real-time data captured from a handheld Kinect sensor to allow the remote user freeform exploration of unconstrained environments. The size of this environment is still limited given that the Kinect must be tethered to power and a stationary computer; the authors considered using a mobile phone as their capture device, though this was not further explored. Creating the environment in this way also means that the remote user must either wait until all areas have been captured or be satisfied with an incomplete model.\nRELATED WORK: The ideal system would combine the benefits of these various depth capture methods, allowing free and immediate movement within an arbitrarily sized environment while also allowing it to be shown and updated in real time. Komiyama et al. proposed such a system with JackIn Space [15] , which reconstructed a small area to be shared by placing several RGBD cameras around it. Users could explore this limited space, or could transition to an egocentric view of it captured by a fisheye camera attached to the local user's head, allowing them to view the space's wider context. This transition was appreciated by users, though the computer-mediated mechanism to do so was not as well regarded.\nRELATED WORK: Teo et al. [29] proposed hand gestures as a more natural way to transition between these ego-and exocentric views. Users found an object search task faster and significantly easier when in an egocentric view, with social presence and the ability to gauge their partner's focus rated significantly higher in that mode. However, the exocentric view was overall preferred by users, while the ability to freely transition between the two was significantly preferred over either view alone.\nRELATED WORK: To the best of our knowledge such an experience has yet to be achieved using purely mobile hardware, which would make it immediately available to most of the population and as convenient and effortless as mobile video calling is today. The inherent portability of these devices would mean that true exocentric views within arbitrarily sized environments could be attained without being constrained by tracking spaces or cables, but despite this the few existing purely mobile approaches [21, 28, 31] only allow egocentric viewing. In this work we close this gap by providing a prototypical solution that provides this real-time exocentric and egocentric exploration with spatially-rendered avatars on consumer-grade hardware.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Exploratory Data Analysis (EDA) is the process of performing initial inquiries on a dataset, with the goal of discovering interesting patterns, spotting anomalies, testing hypotheses and checking assumptions [Tukey 1977] . In order to make sense of data, data scientists combine many graphical and numerical techniques, drawing elements from statistical inference, clustering analysis, dimensionality reduction, and sensitivity analysis. In this sense, there is no standard workflow for performing EDA, which, in its current form, vastly varies based on the type of data and on the assumptions that an analyst wants to assess. Scripting frameworks (e.g. R, Matlab) and data visualization tools have been developed to assist data scientists during the EDA process, but are mostly desktop-based and do not provide support for collaborative analysis. Over time, researchers have repeatedly tried to propose immersive technologies (such as CAVE systems [Cruz-Neira et al. 1993 , 1992; DeFanti et al. 2011; Manjrekar et al. 2014] , powerwalls [Marai et al. 2016; Marrinan et al. 2014] , and virtual and augmented reality headsets) as a possible solution for specific data science tasks, defining a research area known as Immersive Analytics [Chandler et al. 2015; Marriott et al. 2018 ]. The technological improvements and hardware commoditization of head-mounted-displays (HMDs) of the past few years have reignited interest in applying these devices to domains such as EDA, and a few companies have even proposed some first VR-based commercial solutions [3da tics; vir AIVR]. Despite many successful research demos and various marketing campaigns showcasing the advantages of full immersion and natural interaction with 3D data, no standalone AR/VR software tool-to the best of the authors' knowledge-has been able to contend with the complexity of collaborative EDA at large and therefore compare favorably with existing desktop-based software tools.\nINTRODUCTION: In this work, we take a step back and ask ourselves which level of immersion a fully-fledged exploratory data analysis session would actually benefit from. To delve into this, we leveraged a Dataspace [Cavallo et al. 2019] hybrid reality system, a reconfigurable roomscale environment that builds on the legacy of CAVE2 and other smart workspace concepts [Farrell et al. 2016; Jones et al. 2014; Lischke 2018; Prante et al. 2004; Raskar et al. 1998 ], and is characterized by unique flexible data immersion capabilities. In particular, we introduce Immersive Insights, a hybrid analytics Dataspace application for collaborative exploratory data analysis. Immersive Insights attempts to cover a broad range of EDA tasks (including statistical analysis, dimensionality reduction and clustering) in the context of tabular data analysis, freely taking inspiration from the state-ofthe-art, desktop-based clustering tool Clustrophile 2 [Cavallo and Demiralp 2019] .\nINTRODUCTION: We evaluated Immersive Insights through a two-part user study involving twelve data scientists. In the first part, participants performed a set of EDA tasks using Immersive Insights in different modalities (the Dataspace environment only, AR integration, AR standalone, VR standalone), allowing us to time their performance at different levels of immersion. The second part of the study compares Immersive Insights with its desktop-based counterpart Clustrophile 2, attempting to quantify the eventual performance improvement achieved through immersive technologies during a more complex EDA session.\nINTRODUCTION: In the following, we first review the literature on data visualization for EDA, from standard desktop-based tools to immersive environments and AR/VR headsets. Then we briefly describe the Dataspace environment, and articulate the rationale underlying our Immersive Insights implementation. We then describe our user study and its results, concluding with a few considerations regarding the integration of new technologies into EDA and the future of collaborative data analysis.",
    "relatedWork": "RELATED WORK: We have built our work upon existing software, and the extant literature on EDA, specifically those tools and texts concerning dimensionality reduction and clustering analysis with statistical support. We first consider how state-of-the-art desktop-based tools are currently being used for EDA, and then discuss how researchers have previously attempted to employ immersive environments such as CAVEs, tiled-display walls and head mounted displays (HMDs) for visual data exploration and analysis.\nTools for Exploratory Data Analysis: 2.1.1 Visualizing High-dimensional Data. Dimensionality reduction (DR) is the process of reducing total features (variables, dimensions) in the data, for purposes of preprocessing or visualization. For example, algorithms such as PCA [Tipping and Bishop 1999] are often used to reduce total dimensions to two, so as to visualize high-dimensional data on a 2D plane (e.g. in the form of a scatterplot) and therefore interpret similarities among data points and possible structures. Due to the lack of a clear mapping between the axes generated by DR techniques and the original data dimensions, earlier work in data analysis has proposed various tools and techniques to guide users in exploring low-dimensional projections of data [Engel et al. 2012; Liu et al. 2017 ]. Methods such as rotation and isolation [Fisherkeller et al. 1974] enable the user to interactively rotate multivariate data, and statistical information can be used to structure possible visualizations of the data [Asimov 1985; Cavallo and Demiralp 2019; Demiralp et al. 2017; Seo and Shneiderman 2004; Vartak et al. 2015; Wills and Wilkinson 2008; Wongsuphasawat et al. 2016] . Since low-dimensional projections are generally lossy representations of high-dimensional data relations, researchers have introduced visual methods to convey and correct dimensionality reduction errors [Aupetit 2007; Chuang et al. 2012; Lespinats and Aupetit 2011; Stahnke et al. 2016] . Similarly, enhanced biplots [Coimbra et al. 2016; Gabriel 1971] and prolines [Cavallo and Demiralp 2018; Faust et al. 2019 ] have been introduced to visualize the contribution of data features to the DR plane. Researchers have also used direct manipulation to interactively modify data through DR visualizations [Jeong et al. 2009; Orban et al. 2019 ] and out-of-sample extrapolation [Bengio et al. 2004; Van Der Maaten et al. 2009] . A recent what-if-analysis example is represented by the forward and backward projection interaction introduced by Praxis [Cavallo and Demiralp 2018] .\nTools for Exploratory Data Analysis: 2.1.2 Identifying Structures in the Data. Clustering is the task of grouping sets of objects so that members of the same group (\"cluster\") are more similar to each other than to those in other groups, according to some specific distance measure. Combined with statistical analysis, clustering is often used in EDA to discover and characterize salient structures in a dataset [Jain et al. 1999; Xu and Tian 2015] . To improve user understanding of clustering results across domains, early interactive systems introduced the use of coordinated visualizations with drill-down/up capabilities [Seo and Shneiderman 2002] and the visual comparison of different clustering results [Cao et al. 2011; Lex et al. 2010; L'Yi et al. 2015; Pilhofer et al. 2012; Seo and Shneiderman 2002] . To contextualize the various assignments generated by clustering algorithms, tools such as ClustVis [Metsalu and Vilo 2015] , Clustrophile [Demiralp 2016] and ClusterVision [Kwon et al. 2018 ] coordinate visualizations of discrete clusterings with scatterplot visualizations of dimensionality reductions. Correlation and ANOVA-based significance analyses are seamlessly integrated in the clustering process in Clustrophile 2 [Cavallo and Demiralp 2019] . Immersive Insights takes inspiration from Clustrophile 2's combination of data projections, clustering results, distributional information and significance testing, and complements them with aggregate views to facilitate collaborative analysis.\nImmersive Data Visualization: 2.2.1 Virtual Reality Theaters and Tiled Display Walls. Due to the continuous growth in size and complexity of data digitally collected and stored, researchers have tried to find ways to make sense of these datasets through the development of new visualization instruments. While desktop-based applications can provide overview information through summarization, abstraction, and focus-plus-context techniques, the necessity of fitting vast quantities of data on a single display is often detrimental to multi-scale data exploration [Ni et al. 2006] . By surrounding users with visuals through \"immersion, \" physical data visualization systems have been effective in allowing users to explore 3D spatial data (such as molecules, astrophysical phenomena, and geoscience datasets) and high-dimensional data in general [McIntire and Liggett 2014] . CAVE (CAVE Automatic Virtual Environment), introduced in 1992, was a cube measuring 10 feet on each side, and utilized a set of projectors to allow a small number of researchers to experience stereoscopic 3D graphics on five of its sides. Leveraging improvements in LCD technologies made in the early 2000s, tiled display walls (also known as \"powerwalls\") gradually arose as a viable alternative to CAVE systems, offering superior image quality and resolution with relatively low maintenance. Despite renouncing stereoscopic 3D rendering, display walls still enabled the visualization of large datasets, and provided both detail and context. This shift opened up new possibilities for collaborative data analysis, as demonstrated by the use of SAGE [Renambot et al. 2004 ] (and later SAGE2 [Marrinan et al. 2014] ) in the EVL Cybercommons room [Krumbholz et al. 2005; Marai et al. 2016 ]. In recent years, researchers have explored a number of ways to further improve on these two technologies [DeFanti et al. 2011; Manjrekar et al. 2014] , by sensibly increasing resolution [Papadopoulos et al. 2015b ], providing more flexibility in screen configuration [Ponto et al. 2015] , integrating mobile devices [Horak et al. 2018; Krum et al. 2014] , and exploring interaction with artificial agents [Farrell et al. 2016; Venkataraman et al. 2016] . CAVE2 [Febretti et al. 2013] , a system composed of 72 cylindricallypositioned displays, aimed at combining the effectiveness of CAVE systems in visualizing 3D datasets with the capabilities of more recent ultra-high-resolution environments, which were a better fit for 2D data visualization. By combining the SAGE [Renambot et al. 2004] tiled display system and OmegaLib [Febretti et al. 2014] virtual reality middleware, CAVE2 enabled researchers to seamlessly interact with large collections of 2D and 3D data, providing the first full implementation of a Hybrid Reality Environment (HRE) [Febretti et al. 2013 ].\nImmersive Data Visualization: 2.2.2 HMD-based Immersive Analytics. While CAVE was originally introduced as an alternative to existing bulky desk-based head-mounted displays (HMDs) [Sutherland 1965 ], recent technological advances have drastically improved the resolution, field of view, form factor and availability of virtual and augmented reality headsets, which have once again become tractable tools for data visualization [Chandler et al. 2015; Marriott et al. 2018 ]. In fact, HMD-based Immersive Analytics [Cliquet et al. 2017 ] provide an effective solution to the high cost, complex maintenance, and scalability limits of immersive environments [Papadopoulos et al. 2015a ]. Millais et al. [Millais et al. 2018 ] demonstrate the advantages of using immersion for data exploration in virtual reality, while Butscher et al. [Butscher et al. 2018 ] examine how immersive technologies can facilitate collaborative analysis to better detect clusters, trends and outliers. However, as outlined by McIntire et al. [McIntire and Liggett 2014] , the use of stereoscopic displays alone for information visualization still has limitations. While HMDs may be convenient in performing tasks associated with spatial or multidimensional data, they can fall short in displaying statistical and abstract information, which is instead more successfully handled by 2D visualizations [Bach et al. 2016; Marriott et al. 2018] . Dataspace [Cavallo et al. 2019 ] attempts to bring together the advantages of all the workspaces outlined above, combining highresolution displays with augmented reality headsets, plus a central interactive table and integrated AI-based cognitive functionalities. Using Dataspace, Immersive Insights enables collaborative analysis of spatial datasets by allowing users to rapidly move between highresolution statistical information (displayed on 2D screens) and 3D representations of high-dimensional data (visualized in AR).",
    "rq": ""
}
{
    "intro": "INTRODUCTION: According to the Centers for Disease Control (CDC), there are nearly 34.86 million Americans living with limited upper body abilities that affect daily activity [19] . Many of these affected people live in rural areas with limited access to medical centers for the musculoskeletal examination. Tele-medicine applications would be able to provide better accessibility to healthcare for these patients. Current state-of-the-art tele-health applications only allow audiovisual communication [4, 16] . Some applications go beyond traditional video conferencing by using 3D Tele-Immersion (3DTI) [7, 12, 20] . 3DTI enables collaborative augmented virtuality, allowing geographically distributed users to interact with each other in a virtual world, using their \"live\" 3D reconstructed virtual models. This extra 3D information allows the doctor to visualize and diagnose the patients better [13, 29] . In the absence of physical feedback, for musculoskeletal examinations, a physician is forced to employ a clinician to physically examine a remote patient, and the doctor arrives at a evaluation based on the clinician's feedback and the audiovisual dialogue with the patient. For accurate evaluation, the doctor and clinician need to have a good rapport and understanding. This leads to the lack of availability of quality healthcare for remote individuals.\nINTRODUCTION: For musculoskeletal examinations, a doctor typically focuses on three important aspects of evaluation: (a) the muscle strength of the patient, (b) the range of motion for different movements, and (c) the direct oral feedback of the patient. Using a pair of haptic devices in 3DTI for musculoskeletal examination, both the patient and the doctor can feel each other's actions and reactions. One straightforward way is to use haptic avatar to interact with 3D virtual world and finish some special tasks [25] . Another way to employ the haptic devices is to mirror the haptic devices in both sites, i.e., one haptic device is moved, the other will follow the same movement due to the spring force [29, 30] . Achieving these seamless visuo-haptic interactions over the Internet between the patient and doctor is extremely challenging for the following reasons:\nINTRODUCTION: a. Just-Noticeable Difference (JND): In terms of human perception, the JND of force change is 7% -10% for the movement. If user is moving very slowly, it is difficult for the other to distinguish the force change during the movement. This scenario needs force enhancement for the doctor to feel even small/slow movements of the patient. The setup of H-TIME system with the doctor at one site and the patient at another. Each site shows virtual scene on 3D TV screen, and each user can the feel force feedback using haptic device when the other makes movement.\nINTRODUCTION: b. Delay Compensation: Haptic transparency is a term used to describe how a remote interaction using a haptic device compares to local interaction. Since tactile feedback of human is much more sensitive than the visual and auditory ones, the haptic transparency is more sensitive to the network conditions such as delay, packet loss, and bandwidth. Internet latency in communicating the haptic movement data will hurt the haptic transparency, therefore the incurred delay needs to be compensated. Furthermore, delay compensation is required to avoid artifact of force feedback that can potentially influence the evaluation.\nINTRODUCTION: c. Visual-Motor Coordination:. Network delay affects different modalities in different ways. Haptic data has a very high update rate (in the range of 1000 to 4000 samples per second). \"Live\" 3D mesh capture and rendering is typically around 30 Hz. So, there will be disparity between the haptic feedback and the visual rendering of upper limb.\nProposed Approach: In this paper, we introduce Haptic-enabled Tele-Immersive Musculoskeletal Examination (H-TIME) system (Figure 1 ) that enables bi-directional force feedback and motion, facilitating remote evaluation of problems associated with joints and muscles. The proposed system provides a smooth force feedback based on motion exhibited on either side, even when the doctor and the patient are geographically separated over the Internet. Our Contributions: To solve the afore mentioned challenges involving haptics data, we have proposed the following novel approaches:\nProposed Approach: \u2022 A new force enhancement algorithm is proposed to enhance the force rendering and overcome the JND limitation. \u2022 An improved force compensation method is proposed to compensate the influence of delay for force rendering.\nProposed Approach: \u2022 To minimize the disparity between the visual and the haptic data, we have used the haptic interface point (HIP) for prediction of the visual rendering. \u2022 Due to the large volumes of data processed and transmitted over the internet, 3DTI systems have low frame rates and significant delays. The proposed H-TIME system implements efficient algorithms in parallel utilizing the GPU, and distributing the load on each side to efficiently process, compress, decompress, transmit, and render 3DTI data in real time at high frame rates.\nProposed Approach: Using H-TIME: The high frame rate, low latency performance of H-TIME allows doctors to evaluate patients having problems with their upper limbs, specifically the shoulder and elbow regions. The system was tested with the doctor and patient located in different cities, connected over the internet. The doctor could adapt in-person evaluation for remote evaluation using H-TIME. The whole setup is shown in Figure 1 . Both doctor and patient are able to feel each other's movement, since both haptic devices are \"linked\" to each other by the spring-damper. One motion example (elbow flexion / extension) is shown on the top row of Figure 1 , left to right, the real-world motion examination and the mapped haptic motions. 15 patients, with shoulder ailments, were recruited, to be evaluated using the developed H-TIME system. The trial protocol had two independent cases: (i) patients were evaluated in-person by a doctor;\nProposed Approach: (ii) and then, evaluated using H-TIME by another doctor. A high degree of correlation was found between the in-person evaluation and the evaluation of the patients using H-TIME. All the users -the doctors and the patients -expressed a very favorable opinion about the system.",
    "relatedWork": "RELATED WORK: Tele-Medicine systems [4, 16] has been shown to have some effective clinical outcomes in recent years. It enables remote therapy and save lot of time and provide more flexibility for patients.\nRELATED WORK: Researchers [1, 2] have reviewed the effectiveness and safety of tele-rehabilitation of motor function and multiple sclerosis. Furthermore, some researchers applied Tele-Immersion to incorporate virtual reality/ augmented reality techniques, which provides a 3D immersive virtual environment to help the tele-medicine procedure [7, 12, 20] . In recent years, tactile feedback has been exploited in tele-medicine systems, which explore the new potentials for the medical therapy and diagnosis [29, 30] . 3DTI systems allow geographically distributed users to interact with each other by displaying their \"live\" 3D reconstructed models in the virtual world [13] . To allow such interaction on each site, multiple calibrated RGB-D, or stereo cameras are used to capture the user, reconstruct a 3D model, and transmit it to other sites, every frame. Some 3DTI systems use overlapping 3D meshes from individual cameras to render the user [3, 5, 17, 22, 28] . In setups having multiple sites connected over the internet, the data is transmitted to and rendered on displays at the other sites. There are many techniques [5, 17, 22, 28] developed specifically for the efficient transmission of 3DTI data.\nRELATED WORK: With the development of the end-effector haptic devices, many inexpensive and small enough haptics have been used in physical therapy [21, 25, 33] . There are numerous challenges for hapticenabled networked applications towards the quality of experience [8] . As a control system, stability is one major issue, and another important challenge is haptic transparency [6] . Haptic transparency is a parameter that describe how similar a remote interaction using haptic device is with respect to the local interaction [9, 14] . The tradeoff between stability and transparency has been investigated in the tele-operation applications [18, 26] . In multi-user networked applications, haptic transparency is defined as the consistency for both visual rendering and tactile feedback. Some researchers [15, 23] have investigated the haptic transparency in client / server architecture, and proposed an adaptive stiffness reducing method to compensate the force error. Later, this method is extended for interaction with 3D deformable objects [24] . Advanced control algorithms are also applied for haptic transparency in the haptic-enabled networked applications [30] .",
    "rq": ""
}
{
    "intro": "INTRODUCTION: During the last years, we have seen many advancements in computer technology and interfacing in both music and digital media, and a plethora of attempts have been made in order to create virtual AM'18, September 12-14, 2018, Wrexham, United Kingdom User Interfaces (UI) for sound generation and the control of musical expression. In addition, the rapid development and availability of low cost sensing technologies has also created a wide interest in developing virtual reality (VR) applications. Such advances in sensing technology, such as the Microsoft Kinect sensor and others, combined with the improvement of gesture recognition algorithms, have enabled the development of gesturally controlled musical instruments, which increase the users' degree of freedom to move and to express their emotions, or musicality, without actually contacting a physical object. As also defined in [9] , a virtual music instrument is something analogous to a physical musical instrument, a gestural interface, that could however provide for much greater freedom in the mapping of movement to sound.\nINTRODUCTION: In this work, we implement a web-based real-time application for gesture-based interaction with virtual instruments using a Microsoft Kinect v2 sensor. The user is able to musically \"air control\" virtual instruments, such as a guitar and an upright bass, without any physical contact. For the web-based visualization, see Fig. 1 , we employ virtual 3D avatars for increased control and better manipulation of the virtual instruments as well as facilitation during the generation of music by the user.\nINTRODUCTION: Camera-based motion tracking has been widely used in many applications including music human-computer interaction (HCI). In this case, cameras or infrared sensors are coupled with computer vision and machine learning algorithms to estimate a subject's pose, position and specific motion. Such approaches are advantageous since not only they do not constrain the user's movements but they also support full body motion tracking. Even though some drawbacks can be found in the literature, as for instance the placement of the cameras that could limit the interaction workspace to the camera's field of view, the tracking errors when subjects are occluded or the limited frame rates, they are still considered as affordable solutions that are unobtrusive [4] , constituting a good solution to implement high precision motion tracking.\nINTRODUCTION: Many works have used gesture-based interaction, using Kinect cameras for motion sensing, for similar applications. For instance, in [12] an augmented piano performance system was created, using gestures and a small projected area above a keyboard to create new extended techniques for manipulating the sounds of the piano. The \"Third Room\" [6] is also a Kinect-based hybrid 3D space for music composition and performance, where users are placed in a virtual environment to interact with new instruments for musical expression. In [11] gesture recognition and the Kinect's skeleton tracking functionality are used to track the position of the mallets for the performance of percussion instruments, while in [7] the same technology is used to experiment with three virtual musical instruments, a drum, a guitar and the spider king. All three instruments virtually set the relevant sensing input areas, i.e., strings of the guitar or cymbals of the drum, and the user can control the instrument through those virtual inputs. A gesture-based virtual interaction for a drumkit is also explored in [10] . This real-time system detects user-generated drum-hitting gestures incorporating different machine learning solutions, based on the analysis of velocity and acceleration or Wiener filters, in order to compensate for various latency effects. In [5] , head tracking from depth images was performed for motion analysis of musical ensembles, while in [3] a real-time gestural interface was presented for musical expression, combining images and sounds specific to percussionists. In addition to the Kinect sensor, hand glove controllers were also used to sense accelerations, kicks, inclinations, orientation movements and relative angles of the hands.\nINTRODUCTION: Other approaches, using other sensing technologies than cameras, include wrist mounts that couple the Leap Motion optical sensor with inertial measurement units to combine the benefits of wearable and camera-based motion tracking [1] , while in [8] user input from data gloves was used for the gestural control of four virtual instruments: a xylophone, a gestural FM Synthesizer, a membrane and an air guitar. Motion capture devices can be another alternative for sensing motion; for instance, as used in [2] to track, interpret and sonify the performer's movements and gestures in relation to a virtual object.\nINTRODUCTION: This work aims at developing an application, where the user will engage in innovative interactive music activities with advanced multimodal interfaces, using only a Kinect sensor, which actually gives the ability to the user to move freely in the physical space, unconstrained and without any other sensors attached to his body. Additionally, the proposed application is web-based, making it widely accessible to everyone; that means that anyone with a Kinect sensor could virtually play music even at home. The gestures are intuitive, including \"motion templates\" that have some referential similarity to the gestures that a musician does when performing the specific instruments, thus no extensive training or effort to memorize them is required.\nINTRODUCTION: The remainder of this paper is organized as follows: Section 2 presents a general overview of our application and elaborates on the gesture-based interaction regarding the different modes as well as the specifications of each virtual instrument. Section 3 outlines the methods and technologies used for the implementation of the webbased real-time application and the visualization of the 3D virtual environment, using only raw skeleton data from the Kinect sensor. In Sec. 4 , we analyze the results of the qualitative human evaluation, showing that the application gained really positive feedback, and finally in Sec. 5 we present our conclusions and thoughts for future work.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": " (1) what are the major underlying reasons that can lead to missed early screening, late-or misdiagnosis?\n (2) what is the potential of the co-design method in the local context for developing a user-friendly and a validated assessment tool?\n (3) how acceptable would a valid digital tool be, for screening children at an early age in the pakistani context?"
}
{
    "intro": "INTRODUCTION: Mixed Reality (MR) Remote Collaboration systems allow physically dislocated users to be virtually co-located. A common scenario is the remote expert (visitor) -local novice (visitee) scenario -e.g., calling in an expert to help with car repair, or investigation of a crime scene [26] . In these scenarios, the remote visitor needs access to the environment of the visitee, but not the other way around. Therefore, collaborative telepresence systems are often asymmetrical [31] . The visitor needs to engage with and feel present in the visitee's environment, while the visitee needs to understand instructions from the visitor to perform the task at hand. For the purposes of our work, we decided to use names for these two roles that reflect this asymmetry better than visitee and visitor. Users who are physically present at the site that is being visited (the on-site location), we call On-Site Operator (OSO). Users not physically present in this environment, but only virtually from a remote location, we call Remote Operator (RO).\nINTRODUCTION: Our aim is to provide a modular toolkit for building such systems. We used a technology driven method to develop a toolkit called OpenIMPRESS. It works with emerging off-the-shelf mixed reality hardware that is available to anyone with a small to medium budget. The system should afford collaborative interactions between physically dislocated users, and support various scenarios. To this end, we informed the design of our system with findings from existing MR systems for remote collaboration.\nINTRODUCTION: Our toolkit is characterized by three main principles that determine the experience of and interaction between users. First, we allow the RO to move freely in the on-site environment. Second, the RO engages with the on-site environment in a natural way through an immersive, mixed reality interface that includes gesture tracking. Third, the RO's presence and actions are visible to the OSO through a multimodal projection of the RO and their gestures into the on-site environment using augmented reality technology. This allows communication between RO and OSO, which is further enriched through the possibility to place persistent virtual annotation content in a shared digital layer over the environment.\nINTRODUCTION: In this paper, we describe the system and the rationale behind it. In an initial validation of our system we investigated whether we implemented these core aspects to good effect, looking at the contribution of the three main design features (free viewpoint, immersive presentation, and remote avatar representation of the visitor) to the overall experience on performance and user experience.\nINTRODUCTION: Different implementations of those design aspects have been evaluated in studies before, often with the focus on only one of the three design aspects, with the remaining two aspects only partially or not at all implemented. Our user study thus serves to both validate our design and allow for some comparative insights and support for the designs in related work. Specifically, we find that immersive VR display as opposed to a conventional desktop and setup affects the experience in terms of performance, usability and (social) presence related metrics when used in a collaborative task with our system. Making use of free navigation in the virtual representation of a remote scene did increase presence, usability and performance significantly.\nINTRODUCTION: What is more, as part of the system, we describe a novel, fast and easy procedure for setup and on-the-fly (re-)configuration of capture sites (i.e., the on-site locations).\nINTRODUCTION: OpenIMPRESS can be extended with content and logic to build task-specific implementations of MR remote support systems, and is already used in ongoing research projects in this way. At the end of the paper, we briefly discuss the Augmented Reality CPR system (ARC), our envisaged augmented reality cardiopulmonary resuscitation support system (CPR) that helps bystanders perform better CPR through assistance by a virtually telepresent dispatcher.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: As robotics and computers became more and more present in daily activities, efforts were made in some countries during the 70's and 80's to introduce their use in school, the earliest of which was done with the Logo Turtles based on the work of Papert. Despite these institutional advances, these efforts were eventually dropped, and robots disappeared from schools for about two decades ( [9] ). The most prominent reason explaining this failure was that robots were expensive and unreliable, causing disinterest in educational practitioners.\nINTRODUCTION: One other common reason for explaining this disinterest in robotics and computer science is explained by what Gander et al. call the teacher availability deadlock : \"As long as Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. , creating winds and pushing the grasped robots. The tablet displays the map and allows the learners to guess the positions of the pressure points by drag-and-drop (all found correctly in the above instance). Directions of winds can also be displayed (seen in this instance) if the group is observed to be stuck. informatics is not in the curriculum, there is little incentive to educate teachers in the subject; as long as there are no teachers, there is little incentive to introduce the subject.\" Since Papert, many attempts were made to introduce robots in education (with subjects well beyond programming or robotics such as language) but mainly stayed limited to extra-curricular activities. The reasons for this particular limitation can be found in several studies on school teachers' opinions on robotics (such as [38, 19, 26, 17] ): Besides viewing robots as expensive, resource demanding and a potential source of distraction, teachers want the technology to adapt to their practice and not the opposite. For better acceptance, such technologies should primarily target helping them achieve their duty more efficiently.\nINTRODUCTION: In the Cellulo project, our focus is to attempt to fit the current curriculum while minimizing the financial investments for schools and maximizing the usability of the robots by teachers. Our robotic platform, designed with these constraints, aims to be versatile, ubiquitous and practical. Our robots are low-cost and carry few but powerful affordances such as planar haptic and tangible interaction. They operate on printed paper sheets and are aided by consumer-grade mobile tablets; such elements are either fully (paper) or potentially (tablet) ubiquitous in the classroom. The combination of these elements results in a platform that can be easily deployed, orchestrated and maintained. The design of learning with our platform is activity-driven, where each activity is intended to teach a singular, well-defined concept found within the curriculum. A major hypothesis in our project is that many such concepts can be mapped to efficient activities with our platform so that its usage as part of daily learning life in the classroom can be justified.\nINTRODUCTION: In this article, we present the first complete learning activity with Cellulo, called Windfield (seen in Figure 1 ), that aims to teach the atmospheric formation mechanism of wind (introduced as part of scientific thinking and climate knowledge in schools in the country where this study was conducted) to early middle school children. Our contributions also include the design and results of an experimental user study conducted with 24 children in the form of a real hour-long lesson. Therefore, this study marks the first step towards definitively showing the educational value of the Cellulo platform. The rest of the article is as follows: After discussing the related work in Section 2, we describe in Section 3 the robotic platform used for the Windfield experiment. Then, we present the user study on Windfield in Section 4. Section 5 presents the results and discussion on our findings. Finally, we give our outlook and future plans for Cellulo in Section 6.",
    "relatedWork": "RELATED WORK: Due to the historical evolution of educational robots, teaching programming and robotics preceded all other subjects. Later, other closely-related Science, Technology, Engineering and Mathematics (STEM) subjects were brought into focus. Educational robots conceived to teach these subjects were classically designed as facilitators in learning the subject through their related inherent qualities such as programmability and sensor/actuator hardware; in other words, they played the tool role identified by [26] . [38] calls this approach \"robots for education\", and points out that the method can potentially be used to teach subjects of any kind, while also acknowledging its clear historical focus on STEM. [5, 16] provide reviews of studies that target such topics and the devices used for this purpose; individual programmable mobile robots (often with a differential drive for locomotion) and programmable robotic construction kits seem to be the two major choices for the tool approach. Within this approach, subjects of focus include engineering ( [34, 25] ), mathematics and problem solving ( [37, 12] ), robotics and programming ([36, 2, 40, 28, 33] ), physics ( [43] ) and STEM in general ( [4] ). Our work aims to venture beyond this mostly classical approach to the tool role where the robot's inherent qualities are directly mapped to the learning scenario (e.g. program the existing hardware or build necessary hardware from existing components to solve a task) towards an approach where the robot's apparent qualities are mapped to the learning scenario, e.g. mapping the interaction paradigms that the robot offers to a key quantity whose behavior must be understood within a curriculum topic. We hypothesize that this approach will offer more versatility and subject coverage than what is currently found in the literature.\nRELATED WORK: Later on, social robots that play the role of tutors that provide help to learners or peers that stimulate learning (identified again by [26] ) were introduced. These robots are often designed to exploit social aspects of interaction, and are unsurprisingly often found in the form of humanoids (such as Nao). Although these robots are applied to the teaching of a larger variety of subjects, our approach differs from these in the sense that our robots do not possess inherent or apparent intelligence or social qualities; their behaviors are designed to be practical and only seek to spark social interaction in the form of collaboration among the learners. Therefore, this class of studies is not considered here.\nRELATED WORK: From another perspective, robots that enable collaborative learning may have greater potential of adoption into the classroom. Studies that are concerned with collaborative learning aim to measure social interactions among the learners and group dynamics. Such studies are relatively new and less numerous in the domain of robots for education; examples are [24] (teaching geometry) and [7, 20, 44, 15] (teaching programming and robotics). We aim to not only improve the currently lacking subject coverage, but also equip our platform with natural collaborative aspects by designing low cost, replaceable robots that operate in large numbers. If desired, multiple learners can interact simultaneously with a collaborative activity via multiple shared robots or one/multiple personally assigned robot(s). The embedded hardware on each robot can then be used to calculate several complex metrics related to collaboration, as described in Section 4.4.\nRELATED WORK: Haptics is conceptually designated to be one of the main modalities of interaction within our robot-enabled learning activities, including the one discussed in this paper that utilizes a fairly large collaborative workspace that presents a significant challenge for haptics. This challenge was partly addressed by Mobile Haptic Interfaces (such as [27, 3, 11, 30] that are human-sized platforms and [1, 35] that are relatively smaller desktop robots) that were not used in educational or collaborative computer-human interaction studies so far.\nRELATED WORK: Although studies that are concerned with the use of haptics in learning are limited to traditional grounded mechanisms (often off-the-shelf devices), they still present significant motivations for us to pursue haptics as a main interaction modality with our mobile robots: [22] gives a review that exposes the potential of haptics in improving motivation and attention, in kinesthetic and embodied learning, and in the learning of invisible phenomena. Moreover, a number of successful studies that focus on teaching diverse subjects across various levels of education can be found, such as biology ([23, 14, 6] ), physics ([42, 10, 18, 21] ), geometry ( [41] ) and handwriting ( [8] ). Nearly half of these studies show improvement in learning with the addition of haptic feedback, reinforcing our motivation to integrate meaningful haptics in our activities. A marks the paper playground containing printed graphics and the optical microdot pattern allowing the robots to localize globally with high accuracy and framerate. B marks the tablet running the activity application software and coordinating the robots connected to it via Bluetooth. C mark the mobile robots, each able to self-localize on the paper (with instant recovery from kidnapping), move omnidirectionally along their three DOFs (shown on one robot with (x, y, \u03b8)) and give independent haptic feedback along the same DOFs when grasped.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Collaborative writing is an increasingly integral part of professional and academic work. Experts in business, engineering, and law report that they write together more often and more complex than ever before [18, 48] . As such, a rich set of web-based collaborative writing systems has been developed, including tools such as Google Docs and Overleaf that enable new ways of writing together. In particular, synchronous writing, which refers to two or more people editing a document simultaneously, has emerged as a widely adopted practice. In contrast to the early 2000s, when synchronous editing was not well-received [24] , a recent study focusing on undergraduate students showed that 95% of student teams used synchronous writing in their collaboratively-written class assignments [41] . The popularity of collaborative synchronous writing creates challenges and opportunities for researchers as well as the developers of such editors.\nINTRODUCTION: In this paper, we focus on the collaborative synchronous writing experience in remote settings. It is crucial to consider the increasing level of distributed work as the number of employees working primarily from a location outside of their place of employment has tripled over the past 30 years [31] . The significance of remote collaboration was reflected in a survey we conducted on collaborative academic writing, where 73% of the respondents had participated in remote collaborative writing at least a few times in the past year.\nINTRODUCTION: Our work draws inspiration from research that has shown the benefits of writing together at a distance. For example, a study showed that distributed groups who used an internetbased collaborative writing tool that enabled synchronous editing produced lengthier documents of higher quality than groups that used traditional word processors [30] .\nINTRODUCTION: However, it is challenging to maintain group awareness and mutual understanding while writing together in a remote setting. In addition to the overall complexity of the collaborative writing process, the physical distance between co-authors creates further challenges. Contrary to co-located, remote computer-supported cooperative work tools often limit implicit references such as deixis or gaze direction [37] . Therefore, specialized collaborative writing platforms need to provide enhanced coordination, group awareness, and collaborative writing activity support [30] . One possibility to improve these aspects is to build awareness tools that improve the comprehensibility of discussion about the document as well as allow efficient use of references [4] . In previous research, this has been achieved with the use of dual eye tracking technology, however, the context has been limited to collaborative game-playing [23, 35, 36] and pair programming [10, 37] . No prior studies have combined research in synchronous collaborative writing and gaze visualization.\nINTRODUCTION: The goal of this research is to study the potential usefulness of gaze sharing during the process of collaborative writing. In addition, we examine the effect of gaze sharing on the level and quality of collaboration by conducting a study on dyads (i.e. pairs). The study focuses on academic writing as its constraints on time-efficient delivery and length of text are likely to promote synchronous collaborative writing.\nINTRODUCTION: We developed Eye-Write, a novel tool that incorporates gaze awareness functionality, to investigate the effect of gaze sharing on collaboration. Eye-Write extends the open-source collaborative text editor Firepad. Based on dual eye tracking technology, it visualizes the gaze location of each co-author and overlays it in real time on each of the active viewports of the mutually-edited text document.",
    "relatedWork": "RELATED WORK: Collaborative writing is an iterative and social process that involves a team focused on a common objective that negotiates, coordinates, and communicates during the creation of a common document [29] . Writing together has been shown to promote learning and encourage initiative, creativity, and critical thinking [26] . Recently, many scholars have become interested in synchronous remote collaborative writing, for example, in academic settings [46] . Collaboration at distance has also become a center of interest for researchers in the field of eye tracking. With the use of dual eye tracking technology, the shared gaze awareness has shown potential as a means to facilitate communication in remote problem solving [8, 10] . In this section, we explore literature on collaborative writing and eye tracking.\n-Collaboration: Participants perform separate tasks that have a high degree of interdependence, but work is still done by individual members.",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": "rq2: how do changes to the game's social collaborative environment change player behaviour?\nrq2: how do changes to the game's social collaborative environment change player behaviour?\nrq2: how do changes to the game's social collaborative environment change player behaviour?\n this leads us to the question: how do we design 'behind the screen' for behaviour carried out in front of the screen?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Virtual Reality (VR) technology has been the subject of intense academic interest since the late 1960s, but it is only recently that VR has gained traction as a mass market consumer technology. This resurgence has been accompanied by a renewed interest in the potential for VR to have a positive impact on the lives of older adults. A growing body of research has examined how the immersive properties of VR might be used to assist older adults in areas such as recovery after a major illness [32, 67] , exercise [48, 67] , responding to the impacts of dementia [30, 49] , and as a tool for encouraging reminiscence [8] . However, to date, the potential for VR to be used as a social communication platform (social VR) by older adults has yet to be explored. Social VR has the potential to introduce new opportunities for social participation among older adults in virtual space, going beyond the opportunities enabled by existing technologies such as video calls or online social networks. Indeed, recent research has indicated that older adults are interested in the potential for 3D virtual environments to support social experiences [e.g. 65] , but no research has yet explored older adults' preferences within the context of immersive social VR technologies.\nINTRODUCTION: In this paper we address this gap by conducting a study involving 25 older adults aged 70-81, who participated in three workshops that allowed them to experience core aspects of social VR. In the first workshop they designed embodied avatars -digital self-representations that are controlled via natural body movements and gestures [53] . In the second and third workshops, these avatars were used in two different social VR prototypes, where participants interacted with each other via voice while controlling their avatars. We use the workshops to investigate older adults' acceptance of VR as a tool for social communication and the ways in which our participants saw social VR as potentially beneficial for the lives of older adults. These workshops also allow us to examine, for the first time, older adults' views about the design and use of embodied avatars in a social virtual world.\nINTRODUCTION: Participants were enthusiastic about the potential of social VR as a tool for enriching their own lives and the lives of other older adults, particularly those who are socially isolated. Our findings highlight the importance older adults place on avatar anthropomorphism -the embodied avatars' ability to speak move and act in \"ways that might be expected of humans\" [53, p.47] . Our study also sheds light on the importance placed by older adults on the way body movements are translated onto embodied avatars in the virtual environment and the impact that tracking errors can play on perpetuating negative ageing stereotypes. To emphasise the importance of this finding, we contribute an extension to an existing framework and argue that these Translational factors should be combined with User and Platform factors when assessing the quality of social interactions experienced by older adults in social VR [65] . We conclude with guidelines to support the design of future social VR applications that are more suited to the needs of older adults.\nINTRODUCTION: Interrogating Social Virtual Reality as a Communication Medium for Older Adults 149:3",
    "relatedWork": "RELATED WORK 2.1 Older Adults, ICTs and Social Participation: There has been a long standing focus within the HCI and CSCW communities on the role that technology can play in responding to the needs of older adults. Research suggests that older people are very accepting of digital technologies, provided that the technologies in question are highly usable and have demonstrable value for enhancing people's lives [e.g. 78] . It has also been recognised that technologies should be designed to accommodate the physical and cognitive changes that arise as people grow older, such as by designing more intuitive user interfaces [25, 35, 52] .\nRELATED WORK 2.1 Older Adults, ICTs and Social Participation: Other research has sought to address the broader challenge of supporting older adults in living healthier lives and ageing 'in place' [36, 40, 56] . A key element of this research has been the design and evaluation of technologies that seek to address social isolation and/or encourage social participation [6] , given that social disconnectedness is a risk factor that is known to impact older adults' health and wellbeing [20, 24] . Prior work has shown how communication technologies can address social disconnectedness by, for example, mediating conversation between grandparents and their grandchildren [26] or by enabling intergenerational play over distance [29, 73] .\nRELATED WORK 2.1 Older Adults, ICTs and Social Participation: A related body of work has examined older adults' use of social media. A number of issues are particularly important to older users, including concerns about privacy and anonymity [13, 31, 47] and a desire for interfaces that are easy-to-use [18, 41] . Although the use of online communication platforms by older adults is increasing, research suggests that those over the age of 70 are often reluctant to embrace online communication platforms because of the way in which these systems are designed to promote lightweight interactions [31, 41] . In considering future goals for technologies that address older adults' reluctance to embrace social media, Hope and colleagues have shown that older adults value \"deeper, well thought out, carefully crafted social communications\" and modalities that \"enable depth of thought, reflection, and personalization\" [31, p.3911] . Other research has similarly demonstrated the importance of these so called 'heavyweight' communications to older adults [31, 41] , emphasising the need to consider how technologies can allow older adults to remain in touch with other people in ways that are more meaningful and acceptable to them [26, 41, 52] .\nRELATED WORK 2.1 Older Adults, ICTs and Social Participation: To this end, Brewer and Piper [11] conducted in-depth interviews with 20 older adult bloggers and developed a number of recommendations for designing for late-life development. Chief amongst these was the need to support \"multiple, changing identities in older adulthood\" by designing online systems that \"support complex and multi-faceted identities of older adults\" [11, p.9] . Their work is reflective of a broader movement within HCI that emphasises the capabilities and heterogeneity of older adults over deficits and pejorative ageing stereotypes [2, 3, 74] .\nRELATED WORK 2.1 Older Adults, ICTs and Social Participation: HCI research with older adults embraces new and emerging technology as a means of encouraging social participation. Touch-screen interfaces, in particular, have been applied across a range of domains, often in combination with bespoke user interfaces that employ a simplified design to enable the sharing of digital artefacts [6, 7, 52, 75] . Exergames have also received some attention in the literature [16, 21, 71] . Though research has focused predominantly on the impacts of exergames on physical health, their use of natural body gestures to control embodied avatars in games has been noted to have positive social effects [16, 71] . This suggests opportunities for social participation that are quite different to previous modalities. Social VR is an example of an emerging consumer technology that has potential as a communication medium. By incorporating immersive experiences and interaction via natural gestures, social VR could provide older adults with new opportunities to engage in meaningful social interactions by meeting other users in virtual space [7] . The use of embodied avatars in social VR likewise presents opportunities to address Brewer and Piper's call for communication platforms that support self-expression and embrace complex, multifaceted older adult identities [11] . Avatars can also address older adults' concerns for privacy and anonymity by allowing them to choose how they wish to be viewed by others [37] . However, despite authors acknowledging that social VR \"will vastly expand\" social media opportunities [59, p.128] , there has been no research evaluating immersive social VR as a communication medium for older adults. Our work addresses this gap by exploring the emergent needs and concerns of older adults in order to inform the design of future social VR technologies with this population.\nVirtual Reality and Older Adults: In contrast to the lack of research examining social VR for older adults, there has been considerable interest in the application of VR to specific challenges facing older people, for instance as a tool to assist with rehabilitation [32, 48, 60, 67] . Key areas of evaluation relate to the use of VR and exergames as tools to improve range of movement [48] , and the application of VR to specific conditions such as Parkinson's disease [32] . There has also been interest in the application of VR for individuals living with dementia [30, 43, 49] . For example, Hodge et al. [30] , report on a series of design workshops that explored the potential to create virtual reality environments for use by people with dementia and their carers. Though their work was not geared towards designing social VR experiences, they found that participants \"saw virtual reality as an opportunity to be together in meaningful ways with their loved ones\" by enabling social interaction within the confines of the virtual environment [30, p.8] . These studies illustrate that VR technology has the potential to be acceptable to older adults, but they have not explored the ways in which VR systems might be designed to support greater social participation.\nVirtual Reality and Older Adults: In the wider literature on virtual environments, research has described how online games such as World of Warcraft and open-ended worlds such as Second Life can allow people to socialise online [12, 22, 50, 58] . Second Life in particular has been studied in terms of its potential to enhance the lives of older adults by enabling healthcare delivery [19] and online education [61] . Other studies suggest that older adults are interested in using Second Life because it allows them to keep in touch with friends and family [58, 63, 64] and because it affords opportunities for building new friendships [54] while offering close control over self-disclosure of physical appearance [66] . Older adults also see potential for environments like Second Life to be used for collaborative activities such as online shopping [65] and to address social isolation among those who are housebound [19] . However, work in this space to date has only considered desktop-based systems and has not explored older adults' use of fully immersive VR systems that involve head-mounted displays and other advanced technologies. Our work addresses this gap by exploring the extent to which older adults are accepting of immersive social VR, and explores how they envision this medium being used for social interaction with other adult users.\nOlder Adults and Avatars: The vast majority of online virtual environments employ the use of avatars as a way of representing users within digital space. An avatar is \"a digital representation of a human user that facilitates interaction with other users, entities, or the environment\" [53, p.34] . Creating an avatar is a common first step in many virtual worlds [44] and avatar design plays a central role in the development of users' identities within these spaces [46] . While users often create avatars that reflect their own appearance and identity [72] , avatars also allow users to explore alternate and imagined identities [9] . However, research has noted that users' avatar design choices are shaped and constrained by the limitations of the software afforded to them [45] and that users typically create avatars that are targeted towards the context in which the avatar is anticipated to be used [44] .\nOlder Adults and Avatars: In terms of older adults, avatar-based research has predominantly focused on the evaluation of static avatar designs or avatars used in the context of gaming [15, 17] , as opposed to customdesigned 3D avatars intended for use in social VR. For example, one exploratory study by Cheong et al. [17] introduced a range of custom-designed static avatars which were simply printed on paper and shown to 24 adults over the age of 55. More recently, researchers have sought to understand how older adults' avatar designs reflect aspects of their personality and stereotypical views of older adults [14, 15] . Reed and Fitzpatrick [58] studied the avatars designed by older adults in Second Life, finding that their participants were concerned about self-presentation and did not wish to appear 'older' than they really were. Carrasco et al. [15] conducted interviews with ten older adult gamers between the ages of 65 to 95 years. Their findings indicated older adult gamers \"designed their player self-representations to project aspects of their lost (former) sel [ves] and to embrace their present older selves\" [p.441]. Despite the advantages of avatars in aiding communication [53] and the clear implication that avatars can be used to support approaches to design that embrace \"multiple, changing identities in older adulthood\" [11, p.9] , no studies have attempted to understand older adults' design motivations and use of avatars in social VR situations. Our work addresses this by studying what motivated older adults when they designed their avatars and their reactions to the use of these avatars in two social VR environments.",
    "rq": " facilitator: how do you think we could give them more vitality?"
}
{
    "intro": "INTRODUCTION: With the emergence of digital communication, the way we communicate has changed dramatically. The expressive way of communicating information in combination with emotions and feelings using mimics evolved to a rather puristic way of exchanging information. Digital communication helps simplifying communication by abstracting from parts such as tone, gestures, mimicry, or body language which normally facilitates face-to-face communication. However, conveying emotions in chats has become more difficult since it requires the conscious selection of emojis. Despite the emerge of emojis or GIFs, pervasive sensing technology has put a lot of effort into allowing to implicitly assessing user's emotions. Accordingly, recent work in human-computer interaction (HCI) has often tapped into new ways of sharing and communicating emotions in digital communication platforms through facial expressions [19] or body worn sensors [24, 33] . Through this sensing advancement, emotions can be sensed continuously and ubiquitously. However, there has been little research on the long-term effects of emotion representation in chats in the wild and their deployments for a period of four weeks. While prior work often focus on the evaluation of a specific system or a single emotion representation, our research explores rather the design aspect in connection with the user experience and the consequences for their communication. In this work, we investigate potential effects, risks and preferences on how emotions are favorably displayed in chats by representing emotions in four different visualization modes in an in-the-wild study being deployed for four weeks. As an exemplary chat application, we took the commonly used WhatsApp Web and enriched the text-based communication with current user emotions by building the Chrome browser extension that detected facial expressions via the web cam. Our extension injected Session 8: Mobile and Wearable Interaction HTML code into WhatsApp Web to display the emotions of users by employing four different visualizations. We report on the design process and a long-term user study with 28 participants using all the visualizations for four weeks, particularly one week each. Based on 112 data sets, we discuss how emotion representation in chats is perceived by users when being deployed for four weeks in a natively used text messenger. From this genuine in-the-wild collected feedback, we infer four design recommendations holding insightful implications for designers and developers when augmenting chat applications with emotions.\nLIMITATIONS AND FUTURE WORK: Although, 16 participants said happiness and surprise were recognized well, AffEXT's reliability was perceived only moderately. Using facial expressions for emotion detection in general yields some inherent drawbacks. Tantamount to that users do not always express their emotions through facial expressions or conversely could fake an emotion, one facial expression might involve more than one affect. For this, the classification being processed via the Microsoft Emotion API [48] cannot be regarded as ultimately accurate, although it is well suited to detect emotions that are well expressed by the user (e.g., a broad grin) and considered to be one of the latest and most powerful APIs. This can be observed from the underlying recognition process, being that each emotion is assigned a value between 1 and 100. Only those emotions that exceed 50 are classified, while the others are neglected.\nCONCLUSION: In this work we investigated potential effects, risks and preferences on how emotions are displayed in chats by comparing four different designs. For this, we designed and built the plug-in browser extension and collected qualitative feedback from 28 users using it for four weeks in the wild. The results revealed that the openness to share emotions was affected by associated privacy concerns, predominantly on data storage. Further, we observed that the person with whom affects were exchanged mattered and even more importantly, the kind of emotion. In this context, the aspect of responsiveness was perceived to partly impose social pressure. Through the user feedback, we found that participants preferred to retrace emotions of oneself and over entire conversations, preferably using colored chat bubbles. Moreover, they preferred having fine grained control with whom emotions are shared by using contact filters for sharing and receiving emotions. Regarding the type of emotions, participants stated they would preferably see positive affects. We derived four design recommendations stimulating novel approaches to improve the augmentation of emotion representation in chats from user, designer, and developer perspectives.",
    "relatedWork": "RELATED WORK Emotions: Emotional responses are a part of humans' everyday lives. Due to the diversity of emotions, there exist multiple definitions. Kleiniginna and Kleiniginna [29] categorized 92 definitions and concluded that it is best to give a broad overview referring to the numerous ways in which emotions can be explained. Ekman and Oster [14] were among the first who discussed the notion of a universal set of emotions. According to Plutchik [46] a set of basic emotions, namely happiness, surprise, acceptance, anticipation, sadness, disgust, anger, and fear is sufficient to express more emotional response also varying in their intensity. Considering intensity, Russell [52] provides another approach; he described emotions in a two dimensional Circumplex model of affect containing arousal on the y-axis and valence on the x-axis. Based on this work, there has been a lot of research focused on emotions and exploring emotionally motivated behavior in HCI. In particular, Rosalind Picard, as one of the pioneers, shaped the term affective computing [45] . Emphasizing the importance of emotion recognition to progress in practical applications and wearable computers, prior research focused on the development of an user adaptive system sensing emotions [20, 27, 28, 37, 58] . Simultaneously, Umair et al. [56, 64] presented a wrist display for visualizing affective data. The AFFECTECH training network exceeds the approach to capture emotions based on physiological sensors and rather focuses on understanding and training emotions [50] .",
    "rq": ""
}
{
    "intro": "Introduction: This paper explores how adding various visual cues can affect the performance of a 360 (spherical) panorama-based Mixed Reality (MR) remote collaboration application. MR remote collaboration involves using Augmented Reality (AR) and Virtual Reality (VR) in a system to enable users from two different places to collaborate with one another. In 360 panorama-based MR remote collaboration, a local user can share his/her surroundings with a remote user, using a 360 panorama camera [Lee et al., 2018] . The remote user uses VR hardware to see the shared 360 environment and to have the impression of being immersed in the local user's environment. In addition, the remote user can communicate by showing virtual hands or other visual cues into the local user's AR glasses. Such remote MR remote collaboration can be very useful in various industry use cases, such as manufacturing, facility maintenance, and construction, where a remote expert assists a worker in the field.\nIntroduction: Many features can be introduced in a MR remote collaboration, but one primary goal is to help remote people to feel socially present with each other. Mixing AR and VR allows one person to be immersed into another's space, and using 360 panorama imagery further enriches this experience. There are many ways of using MR in remote collaboration. Exploring these possibilities is the primary motivation of our research.\nIntroduction: Seeing a 360 virtual environment from a first person perspective in VR can further enhance Social Presence with a remote user. This creates an experience such that the remote user looks through the camera on the local user for sharing the environment. Hence, it allows both users to work together as if they are next to each other. Not only that, it also enables both users to view and work independently in the same environment [Lee et al., 2018] further promote the collaborative performance.\nIntroduction: It is also important for the remote user to communicate with the local user. Previous research has shown that this can be done in several ways, such as using hand gestures [Kawasaki et al., 2010] , pointing [Piumsomboon et al., 2018] and drawing [Teo et al., 2018] on the view of local user, or having a virtual replica such as a human avatar with the local user in AR. Among these, we are interested to identify novel methods that not only deliver communication information but also improve the feeling of being socially present. In this paper, our focus is on exploring the potential of supporting visual cues in a 360 panorama-based MR remote collaboration to promote social presence. We did this by building a prototype system using a similar configuration to a prior work [Lee et al., 2018] , providing us with an environment to perform our experiment. The main feature of our prototype is to allow the remote user to communicate nonverbally with the local user using a variety of different visual cues in a 360 panorama-based MR interface (Figure 1 ). This paper focuses on providing (1) the first paper that investigates on benefits of using different visual cues in a 360 panorama based MR remote collaboration application, (2) evaluates the impact of visual cues on Social Presence and Spatial Presence in both asymmetric and symmetric remote collaboration tasks using a 360 panoramabased MR remote collaboration interface, (3) provides suggestions for designing visual cues that improve Social Presence in a 360 panorama-based MR remote collaboration system, and also (4) provides observations of user preference and behaviour in an MR remote collaboration application using gesture, pointing and drawing as visual cues.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Interactive software development in the 1970s and 1980s involved traditional software engineering processes, with limited interdisciplinary collaboration between developers and graphic designers. By the 1990s, the advent of full color, high-resolution displays enabled high-quality interactive graphics, with a corresponding need for professional designers. Today, graphic designers, interaction designers, and user experience specialists are routinely part of the teams creating interactive software, together with software developers. However, integrating designers' and developers' 19:2 Fig. 1 . An example of a custom interaction in the Paper mobile app from FiftyThree. In the first screen, the user has selected the scissor tool and draws a circular area with one finger. In the second screen, the user drags this area to move the content to a new position and taps outside the circular area with another finger to create a copy of the selected area. In the third screen, the user drags the selected area to reveal the copied shape.\nINTRODUCTION: Addressing the gap between designers and developers, and more precisely the gap between the tools that they use to create interactive systems thus remains an open research question. In this work, we identify problems faced by designers and developers as they collaborate, with a particular emphasis on the representation, communication, and interpretation of custom interactions. Our goal is to inform the design of new collaborative prototyping tools that reduce these problems, without forcing professionals to abandon their preferred representations. We want to facilitate the transition from design to implementation, as well as the transition from implementation back to the design when changes occur. More specifically, we address the following research questions:\nINTRODUCTION: -What are the most common and critical problems that impair designer-developer collaboration when creating interactive software that involves custom interactions? -How can prototyping tools help mitigate these problems?\nINTRODUCTION: We start by describing the motivation and methodology of the project, and review relevant related work. We then describe three studies that we conducted to better understand designerdeveloper practices, leading to a set of design principles to reduce the breakdowns they encounter. We introduce Enact, a prototyping tool for touch-based mobile interaction based on these principles, and report on two studies that we conducted to assess Enact.\nSTUDY FIVE: COMPARING ENACT WITH TRADITIONAL TOOLS: Based on the feedback from the previous study, we created the version of Enact described in Section 8. We replaced the graphical rules with the interactive state machine diagram and added the augmented code editor to provide more control.\nSTUDY FIVE: COMPARING ENACT WITH TRADITIONAL TOOLS: We then conducted a second study with this new version in order to better understand how Enact affects designer-developer collaboration. The goal was to observe and compare the strategies used by designer-developer pairs to represent, communicate, and implement interactions with their own tools and with Enact. We focused on the issues that arise from the inability to successfully represent the interaction itself, rather than those that arise from generating design ideas. We therefore used the same strategy as in Study Three and provided interaction examples from existing applications that participants had to describe and implement. For ecological validity, we organized the study in three phases that reflect the common collaboration patterns we observed involving a design hand-off: communication of the initial design (designer only), initial implementation (developer only), and side-by-side collaboration (designer and developer, co-located).\nMethod: 10.1.1 Participants. We recruited 12 participants (6 women and 6 men, ages [23] [24] [25] [26] [27] [28] [29] [30] [31] [32] [33] [34] [35] : six professional developers (P1 dv to P6 dv ) paired with six professional designers (P1 ds to P6 ds ), who create websites, mobile applications or interactive installations. Their experience in collaborating across disciplines ranges from 0 to 7 years. P1 dv and P6 dv reported no significant collaboration experience as they were just starting their front-end developer careers. 10.1.2 Apparatus. Participants used their own setup, i.e., laptop, mouse, and pre-installed software, to work with their Traditional tools. We provided two LG Nexus 5X running Android 8.0 and an Apple iPhone 6S running iOS 10 as the target mobile devices. We helped participants to connect these devices to their development environments before starting the study. We run Enact in a Macbook Pro 13-inch with a 2,7 GHz Intel Core i5 processor and 16GB of memory. The target Each pair first creates an interactive prototype of an existing interaction with their preferred tools (Traditional condition), then does the same for a different interaction with Enact (Enact condition). For the Traditional condition we do not impose any restriction on the tools they can use. We only let them know that they will work for a mobile platform supporting multi-touch interactions. To avoid any influence from Enact on the participants' usual workflow, they all perform the Traditional condition before Enact. Once the Traditional condition is over, we give a short presentation of Enact and let each participant individually practice for 10 minutes before performing the Enact condition.\nMethod: For both conditions we follow the same protocol (Figure 17 ). First, we show the designer an initial version of an interaction preloaded on one of the mobile devices. Only the designer has access to this device throughout the study. The designer has 10 minutes to create a design that communicates all the details he deems relevant for creating a prototype with the same behavior. Then, the designer sends the document to the developer, who has 15 minutes to create an interactive prototype based on the received design. During this time, we show a final version of the interaction to the designer. Finally, we give the pair another 15 minutes to review the initial implementation and work together to prototype the final version of the interaction.\nMethod: For each condition we use a different multi-touch continuous interaction with two versions: initial and final. This lets us study the introduction of changes in a controlled environment: the initial version features the basic interactive functionality while the final version includes changes and adds complexity to the previous design.\nMethod: We use two custom interactions already used in the participatory design workshop (Study Three, Section 6): pinch-to-create and pan-and-stamp. We simplified certain aspects of the visual design such as gradients, texts and 3-D effects to focus on interactivity. Both interactions feature at least seven edge cases. We expect these interactions to have a similar level of difficulty.\nMethod: The first interaction, pinch-to-create, is inspired by the Clear to-do list mobile app, 18 which uses a spread gesture to create a new item between two existing items (Figure 6 ). The initial version lets users simultaneously move two rectangles with a pinch gesture. The final version lets users Enact: Reducing Designer-Developer Breakdowns When Prototyping Custom Interactions 19:39 manipulate the size of a third rectangle with the spread of their fingers. The edge cases include the following: The interaction should only work when the fingers are inside the rectangles. The third rectangle is always positioned in the middle of the other two. The upper rectangle cannot move lower than its starting position.\nMethod: The second interaction, pan-and-stamp, is inspired by the Paper note-taking mobile application's 19 cut and paste feature, where one finger drags the object and a tap with another finger copies it. The initial version lets users pan a rectangle with one finger and copy the rectangle with another finger tap. The final version lets users pinch to resize the rectangle and, with a third finger, tap to create new copies of it. The edge cases include the following: New rectangles are centered at the tap position. The first rectangle needs to be resized from the center, not the top-left corner, and should also be panned with two fingers. The two interactions are balanced across pairs: P1, P2, and P4 start with pinch-to-create while P3, P5, and P6 start with pan-and-stamp.\nMethod: 10.1.4 Data Collection. We recorded audio and video, both over the shoulder and with a screen recorder. We also took notes during the tasks and the post-hoc interviews.\nResults and Discussion: We performed a thematic analysis [7] of the collected data to extract the prototyping strategies used by the participants. After looking for themes, we revisited the data to specify the classification and extract relevant quotes.\nResults and Discussion: We also measured the number of mimicking gestures in the video data, i.e., when participants acted out the interaction with their hands, either to understand it or to communicate it, both away from the device and on the device. In our initial studies we observed designers and developers using hand gestures to communicate the interaction. We were therefore interested in whether Enact encourages participants to use more mimicking gestures when collaborating.\nResults and Discussion: We focused our analysis on the side-by-side collaborative phase (not the individual phases) and on on-device mimicking gestures (Figure 18 ). We also measured the amount of edge cases found by each pair from the notes and the video data.\nTools of Choice.: Traditional condition. The six designers chose vector and raster graphics software: Sketch (5/6) and Photoshop (1/6). The six developers chose mobile web (3/6) or mobile native (3/6). The developer web tools included browsers, such as Google Chrome (2/3) and Mozilla Firefox (1/3), and code editors, such as Sublime Text (2/3) and Atom (1/3). The developer native tools were IDEs: Android Studio (2/3) and Apple Xcode (1/3). All developers used search engines to look for terms such as \"JavaScript pinch\" or \"Android button onpress,\" complemented with inline help when available, such as Android Studio and Xcode documentation. Pair P1 never used the provided mobile device and only used the on-screen emulator. Pairs P2 and P3 used the mobile device but relied almost exclusively on the on-screen debugger of the web browser, even though it does not support multi-touch input.\nTools of Choice.: Enact condition. Participants did not chose a tool on this condition. We provided a laptop with Enact pre-installed and connected to a mobile device.\nTask Completion.: Traditional condition. No pair completed the final version and only one pair (P4) finished the initial version of the provided interactions. P4 was also the only pair that started the implementation of the final version of the interaction in the Traditional condition. Two of the three pairs that started with pinch-to-create implemented its basic functionality, but none of the pairs that started with pan-and-stamp finished it.\nTask Completion.: Enact condition. All pairs provided the basic interactivity of the initial version of the interaction. Five out of six pairs provided the basic interactivity of the final version and one of them implemented all the details of the final interaction. All pairs implemented the basic and extended versions of pinch-to-create and the basic interactivity of pan-and-stamp. Two of the three pairs also implemented the extended version of pan-and-stamp. Despite our efforts to use interactions with similar complexity, we found that pan-and-stamp seemed more difficult to prototype than pinch-to-create. However, this should not pose a threat to validity because these techniques were counterbalanced across pairs. Since Enact is explicitly designed to prototype the type of interactions provided to the participants, we expected a higher completion rate with Enact than with Traditional tools. On the other hand, no participant was familiar with Enact and they only had a short training session. Even so, participants were able to complete much more with Enact than with their Traditional tools. Since we gave participants very little time and did not expect them to finish all the tasks, we concentrate the following points on the their strategies to prototype the interaction rather than on performance measures.\nTransitioning Problems.: Traditional condition. All the designers (6/6) used the same workflow to communicate the design to the developer in the Traditional condition. Designers created a storyboard document depicting the visual states of the interface with a graphical software. Then, they sent it to the developer either in the original format, as a PDF, or through a specialized tool such as InVision. Designers illustrated the visual design with different screens and explained the user inputs with circles, icons, traces, text annotations or a combination of these. None of the designers used animations nor video to communicate interactivity. P6 ds mentioned her intention of using video but she felt that it would require too much time with her Traditional tools. None of the designers updated their design specification at the beginning of the side-by-side phase to communicate the new design. Rather than using a design artifact, they described the final version of the interaction on top of the current implementation, verbally or by using mimicking gestures.\nTransitioning Problems.: In the Traditional condition, developers ran into problems when interpreting the interactivity and reproducing the visual look. Developers expected text annotations. For example, P5 dv said \"I don't understand this\" when viewing the design for the first time. When text annotations were minimal, developers expected more details. For example, P3 dv said \"I don't know if these are multiple interactions or different steps of the same interaction\" and that she \"prefer[s] comments saying \"when this happens then that happens\".\" Four developers ignored the graphic design and used either no visual elements at all (only console logs), gray buttons or wrongly colored rectangles. Two developers used external color pickers to extract the right color from the design and copy the corresponding hexadecimal string. All the developers ignored the precise size of the rectangles: for pinch-to-create the height of the rectangle in the design was not replicated and for pan-to-stamp developers generally used rectangles instead of the square shown in the design.\nTransitioning Problems.: Enact condition. Designers used the animation feature to refine the storyboard and developers used it to understand the interaction. Thus, the generated animation worked as a contact point between the two activities while working asynchronously. Only one designer asked for icons to represent user input and another expressed the need for text annotations. Enact does not currently support text annotations but developers did not mention the lack of this feature. One explanation for this could be the use of Enact's generated animation as a communication medium instead of textual descriptions. Developers also mentioned the usefulness of showing the touch input information on the device mirror while the animation was being played.\nTransitioning Problems.: In the Traditional condition, all developers used print logs to confirm the triggering of input events. Enact's live state machine diagram provided the same level of confidence to the developer, without extra effort and with more detail. In the Traditional condition, P4 dv finished the basic interactivity of pinch-to-create but had issues to preserve the right offset between the touch point and the shape during the interaction. In the Enact condition, while using the map function to implement pan-to-stamp, he emphatically expressed that \"it is really cool that I don't have to think about the sh*tty offset.\"\nTransitioning Problems.: The reduction in rework and redundancies was evident throughout the study. For example, four out of six developers did not follow correctly the user interface specification with the Traditional tools. In Enact there is no transition step, eliminating these issues altogether. With Traditional tools all the designers relied on copy-paste instead of using \"smart objects\" to create the design specification. In Enact, the storyboard propagation helps designers maintain consistency between the storyboard visual elements. Moreover, none of the designers used a design artifact to communicate the second task with their traditional tools. Instead, they relied on mimicking gestures or verbal communication. With Enact, the design artifacts are part of the prototype and were heavily used in both tasks.\nIncreasing Participation.: Both designers and developers performed significantly more interactions on the device with Enact than with Traditional tools (F = 1.4; p < .007) (Figure 19 ). This suggest that designers were much more involved during the side-by-side collaboration with Enact than with Traditional tools. With Enact, all designers interacted with the target device (M = 8 times, SD = 3.65) while only three designers did it with Traditional tools (M = 1.17 times, SD = 1.46).\nIncreasing Participation.: One reason for the increased participation is that developers were faster in creating a functional implementation with Enact, which gave designers and developers time to collaborate on the created artifact. Also, even if the implementation was not finished, with Enact, the designer Fig. 19 . Number of on-device mimicking gestures, either on the mobile device or the emulator provided by the IDE, per participant during the collaborative side-by-side phase. Designers and developers performed significantly more interactions on the device with Enact than with Traditional tools. This suggests that with Enact, designers participate more during the side-by-side phase and developers perform more contextualized actions on the real device during implementation.\nIncreasing Participation.: had \"something to play with\" while with Traditional tools they usually did not. Nevertheless, we found instances, in both conditions, where designers and developers were acting out the interaction on the device even when the implentation was not finished.\nIncreasing Participation.: Another explanation for this increased interaction with the target device could be the sense of ownership of the prototype. With Traditional tools, developers recreate the design with their own tools. It is not the designer's design that \"comes to life\" but a mere replica. With Enact, developers literally add interactivity to the artifact provided by the designer. Designers might therefore feel a stronger sense of ownership over the prototype under construction, thus increasing participation. P4 ds said that \"you have the impression to be living in the same environment, that we share the same language.\" 10.2.5 Finding More Edge Cases.\nIncreasing Participation.: Traditional condition. Only two pairs (2/6) found one and two of the seven edge casesremember that only one pair finished the basic and final version of the interaction in this condition. For example, while coding pinch-to-create in a text editor, P2 dv did not notice that the to-do list items should only move along the y-axis. He noticed this edge case only when he tried the interaction in the simulator. However, the simulator runs on the desktop browser instead of the target device, so that the mouse cursor events are treated as touches of a single finger, making it impossible to find edge cases related to the use of multiple fingers. In contrast, P4 ds found an extra edge case because he was interacting with an actual multitouch device: The Apple Xcode IDE was connected to a phone, and he noticed that the items moved beyond their initial position. However, P4 dv needed to investigate the implementation carefully in order to find which piece of code was in charge of the erroneous behavior. The lack of a clear connection between the runtime effect and the build-time code hindered P4 dv 's ability to quickly fix the issue.\nIncreasing Participation.: Enact condition. Five pairs (5/6) found three or more of the seven edge cases in the final version of the interaction. For example, while interacting with her first implementation attempt, P3 dv realized that the shapes where dragged even when her finger was outside of them. Similarly, P5 ds noticed that pinch-to-create behaved differently depending on which rectangle was touched first. He shared his finding with P5 dv , who added an if statement to determine which touch should be associated with each rectangle.\nIncreasing Participation.: Enact interactive representations helped designers and developers find more edge cases. Enact contextualizes the interaction in the final device, giving developers and designers insights that are difficult to find in their indirect Traditional tools: Emulators do not provide the same experience that the final context of use; IDEs connected with a real device impose a time-consuming buildrun-test cycle; graphic-authoring tools are focused only in the visual properties of the interaction. Enact provides a faster design-test cycle, with more chances to find edge cases on the design by actually interacting with it. In other words, it is easier to find edge cases because the whole interaction lives less on the participants' head and more on their hands.\nOpportunities for Co-creation.: Traditional condition. Designers took a more passive role and were often just observing the developer. In the two occasions where they communicated, the participation of the designer was minimal. P2 dv had an emulator on his screen and P2 ds had to stand up every time she wanted to point to the screen to make a comment. In contrast, the target device was positioned between P4 dv and P4 ds , facilitating their collaboration and participation. Nevertheless, during the side-by-side phase only the developer modified the interaction, while the designer only made suggestions.\nOpportunities for Co-creation.: Enact condition. Enact creates a kind of \"gray area\" between design and development with interesting opportunities to cross boundaries. P2 ds was not sure she should create an input example: \"am I supposed to do this?\" On the contrary, P5 ds was really interested in the programming capabilities of Enact: he started adding interactivity by himself and forgot to finish the description of the interaction on the storyboard. When the developer of the same pair (P5 dv ) received the design, he said \"What should I do now? This is already coded!\"\nOpportunities for Co-creation.: We observed that providing multiple viewpoints helped break the silos between the two communities. With Enact, most of the designers were intimidated by the code editor but not by the live state machine diagram. During the side-by-side collaboration, P5 dv built on top of P5 ds 's implementation, even extending the storyboard himself. Similarly, P6 ds added some interactivity to the pinch-to-create prototype and P6 dv directly started to fix several edge cases in the implementation, such as checking that the touches are inside the shapes and that the rectangle is constrained to a vertical movement. Finally, we observed that intermediary representations, such as state machines, align the designer-developer vocabulary for abstract concepts, such as transition events or the state of the interaction. 10.2.7 Summary. We observed that Enact reduced rework and redundancies, allowed designers and developers participate more, helped them find more edge cases and provided a friendlier environment to co-create with the other community of practice. Both designers and developers performed significantly more mimicking gestures on the device with Enact than with Traditional tools, indicating a higher degree of participation. With Enact, five out of six pairs found three or more of the seven edge cases in the final version of the interaction. With the Traditional tools, only two out of six pairs found one or two edge cases. Finally, Enact helped designers and developers to cross their boundaries, creating more opportunities to co-create the interaction. We observed that providing multiple viewpoints while maintaining a single source of truth helped break the silos that traditionally insulate the two communities of practice. Designers were able to manipulate developer-oriented artifacts, such as the state machine and the code editor, thanks to the use of design references. On the other hand, developers also manipulated the storyboard and the target device, either to modify the recorded inputs, or to reveal touch and measure information on the device mirror.\nCONCLUSION: Despite their different backgrounds and skills, designers and developers need to collaborate to create interactive systems. Our goal was to investigate how better prototyping tools can support their collaborative process.\nCONCLUSION: This work makes three main contributions:\nCONCLUSION: -At the empirical level, we report on three studies to better understand and classify current collaboration issues; -At the theoretical level, we introduce a set of principles to design better collaborative prototyping tools; and -At the technical level, we present a new tool guided by these principles to reduce collaborative breakdowns during the creation of custom touch-based interactions.\nCONCLUSION: In Study One, we showed that current workflows and tools induce unnecessary rework. We found that designers create a multitude of redundant design documents and developers must recreate them with their own tools. This process often introduces mismatches with the original design. We proposed a classification of key design breakdowns: missing information, when designers do not communicate a specific detail; edge cases, when designers do not think about a particular case; and technical constraints, when designers are not aware of developer's technical limitations. The interviews also showed that when developers are not involved in the initial design phase, implementation tends to be problematic or even impossible.\nCONCLUSION: In Study Two, we found that even if the early involvement of the developer mitigated design breakdowns, new breakdowns appeared in subsequent meetings. We also found that the inability of current design artifacts to represent interaction generates breakdowns.\nCONCLUSION: In Study Three, we observed that the limitations on the representations used to communicate interaction result in missing information and ignored edge cases. The successful communication and representation of interactions required an iterative process, from concrete examples to more general rules.\nCONCLUSION: Based on this empirical work, we introduce four principles for collaborative prototyping tools to reduce designer-developer breakdowns: Provide multiple viewpoints, to allow developers to participate early, maintain a single source of truth, to reduce reworking and redundancies, reveal the invisible, to avoid missing information, and support design by enaction, to find more edge cases. Collaborative prototyping tools based on these principles can play the role of a boundary infrastructure [6] that supports the flow of multiple interconnected objects between designers and developers.\nCONCLUSION: To demonstrate these principles in action, we created Enact: a novel interaction prototyping tool for touch-based interaction on mobile devices. Through multiple interconnected representations of the interaction under construction, Enact reduces reworking, redundancies, and design breakdowns. Storyboard propagation reduces redundancies within representations while design references reduce redundancies across representations. The interactive state machine diagram and the device mirror let designers and developers quickly explore the interaction under construction and detect edge cases. The connected target device lets designers and developers enact the interaction at each stage of the process.\nCONCLUSION: Finally, we conducted two studies of Enact to validate our design goals, gather feedback from professional designers and developers, and analyze the impact of Enact during collaborative prototyping. In the first study, participants adopted design by enaction and appreciated the pedagogy of the tool to understand the details of the interaction. Both groups highly appreciated the reduced time-to-interaction, but wanted more powerful tools to describe the relationships between user inputs and system outputs. Based on this feedback, we added two new viewpoints for finer control: an interactive state machine and a code editor with design references.\nCONCLUSION: In the second study, we assessed the new version of Enact in a collaborative setting and compared it with traditional tools. We found that participants completed more of the proposed interaction with Enact than with traditional tools. All the pairs finished the initial task while only one pair finished it with the traditional tools. Participants not only interacted more but they also found more edge cases in the design. We also found that with Enact, both designers and developers performed significantly more mimicking gestures on the device than with traditional tools, and designers were much more involved during the side-by-side collaboration. This shows that Enact encourages a more active role during design and development, compared with the passive or indirect approach encouraged by traditional tools.\nCONCLUSION: Our future work will focus on improving Enact, studying designer-developer collaborative prototyping in the wild, and going beyond touch-based interactions. Evaluating Enact in the wild will let us study the effects of collaborative prototyping on a long-term project, and hopefully will provide interesting findings about the use of interactive prototypes to collaborate with other stakeholders, such as clients, users, testers, and managers. We would also like to apply the design principles to other interaction styles, such as mid-air gestures, multimodal interaction, or mixed reality.\nCONCLUSION: We hope that these principles and Enact will inspire others to create new collaborative tools to support communication through prototyping. Initiatives such as the Hour of Code [14] try to spread programming to a broader audience. Undoubtedly, more and more designers will know how to code in the future. However, developer-based representations, such as code or visual programming, are not the only-neither the most adequate-representation for every aspect of an interaction. We need to provide integrated and multiple representations, e.g., symbolic, visual, and enactive, to create collaborative spaces in which professionals with different skills, mindsets, and values can work together. Digital tools need to provide ways of navigating these representations in a seamless way, thus reducing reworking and mismatches between design and implementation. We invite other researchers to test and extend these design principles to create new tools that better support the collaborative prototyping of interactions.",
    "relatedWork": "RELATED WORK: We grouped the related work on designer-developer collaboration in two areas. First, we review descriptive research focused on the processes and artifacts currently used during the designerdeveloper collaboration. Second, we review research on novel prototyping tools for creating interactive systems.\nStudies of Designer-Developer Practices: Researchers have studied collaboration among communities of practice [63] , such as designers and developers of interactive systems, from multiple perspectives. Some seek to understand the implications of combining software development processes, such as agile methodologies, 9 with design methodologies, such as user-centered design.\nStudies of Designer-Developer Practices: Although user-centered design methods for interactive systems emerged in the 1980s [48] , their integration with software engineering processes is challenging. By the mid-1990s, Poltrock and Grudin [53] found that large corporations' \"development practices blocked the successful application of accepted principles of interface design.\" Agile methodologies were created to address a number 9 http://www.agilemanifesto.org. of software engineering issues, with an emphasis on delivering working software as quickly as possible. However, Ferreira et al. [19] showed how this approach is often at odds with the usercentered design tradition of iterating the design from the user's perspective, before development begins. Letting developers understand what they are expected to implement, as soon as possible, remains one of the most important challenges of integrating these methodologies [54] .\nStudies of Designer-Developer Practices: In the context of agile methodologies, Brown et al. [9] analyzed two major aspects of the designer-developer collaboration process: collaboration events and artifacts. Their study of collaboration events shows that designers and developers constantly perform alignment work [10] and that the collaboration process is organized around the use of artifacts [8] . However, the transition from design to implementation is not only a translation from sketches to dynamic behaviors [18] . Most of the joint work of designers and developers is focused on managing the tensions between them. Brown et al. [10] introduce the term alignment work as the activity of aligning designers and developers in order to seek, expose, and resolve tensions. These tensions can result in breakdowns, which are considered to be a normal part of the collaborative work. However, even if breakdowns are commonly found in designer-developer work, we are interested in whether the limitations of the artifacts themselves are responsible or not for some of these breakdowns. Our goal is to identify current designer-developers breakdowns and propose actionable design principles that will help mitigate them.\nPrototyping Tools: Researchers have explored a number of novel tools for prototyping interactions, but most are targeted at a single community rather than the collaborative work of designers and developers. An exception is Apparition [35] , a tool that helps designers prototype web-based systems in real time by crowdsourcing the creation of behaviors to workers, such as the position of objects in a simple platform video game. Lee et al. [37] studied the communication problems between designers and workers when using Apparition and found that designers \"may speak in a style that makes it difficult for crowdworkers to comprehend their requests.\" Also, this system relies on the availability of crowdsourcing workers with the right expertise to fulfill the task at hand, which could be a problem when designing custom interactions and more complex prototypes. Other approaches, such as programming by demonstration [45] or the use of state machines and inference engines, have not been studied in a collaborative context.\nPrototyping Tools: Outside research, practitioners have acknowledged the gap between design artifacts and their subsequent implementation, leading to a number of commercial tools. Within the past few years, over 40 commercial prototyping tools have appeared, 10 and a 2015 survey of 4,000 designers found that 53% of them use such tools [61] . These commercial tools focus on supporting remote communication between communities, assisting the extraction of design information, and helping to prototype standard and recurrent interactions. However, they do not support co-located collaboration, ignore the back-and-forth interplay between activities, and disregard the creation of custom interactions.\nTools for Interaction Developers.: Code-oriented artifacts can be enhanced with other representations such as notations, diagrams and test-cases. For example, InterState [49] combines constraints and state machines to facilitate reuse. InterState provides a live editor where developers can edit a program and visualize the states as they interact with the interface. Proton [33] and Proton++ [32] let developers use Regular Expressions to express multi-touch interactions. Juxtapose [26] lets developers create code alternatives and modify variables at run-time to facilitate the exploration of multiple alternatives. CodePilot [62] supports novice programmers by integrating coding, testing, bug reporting, and version control management into a collaborative system.\nTools for Interaction Developers.: While these tools are heavily inspired by developer practices, we believe that they provide representations and enhancements that could also be suitable for designers. We are interested in how these mechanisms might be adapted to meet the needs of both audiences.\nTools for Interaction Designers.: Another approach focuses on augmenting traditional design artifacts. For example, SILK [34] lets designers quickly create interactions using interactive sketches and envisioned \"a future in which the user interface code will be generated by user interface designers using tools.\" More than 20 years later, however, the most common approaches still require the collaboration with developers to create user interface code.\nTools for Interaction Designers.: DEMAIS [2] provides an interactive multimedia storyboard also based on sketches. The designers' strokes and text annotations are used as an input design vocabulary to transform static sketches into working examples. Similarly, FrameWire [38] infers interaction flows from paperprototype videos to detect hot spots and generate page-based prototypes. Forsyth and Martin [20] use tagged digital storyboards to infer behavioral information, such as states and actions. These representations and mechanisms are closer to current practices of designers, and need to be present during the collaboration. However, we also want to bridge designer-friendly representations with more abstract representations, such as code, without excluding the participation of either community of practice [63] .\nTools for Interaction Designers.: While these tools support standard discrete interactions such as button clicks and menu selection, a few let designers prototype continuous, custom interactions. Monet [39] lets designers prototype continuous widgets by demonstrating interactions on top of sketches. Designers explicitly define interaction states and the system infers the correct state through multiple examples. Using inference improves informal prototyping, but these interaction descriptions are opaque and are of limited use to the developer for the final implementation.\nTools for Interaction Designers.: Other tools use intermediate representations to allow non-programmers to create interactions. EventHurdle [30] is a visual authoring tool for prototyping gestural interactions designed to facilitate designers' understanding and generate code automatically. While we want to provide similar mechanisms to facilitate the transition from design to development, we also want developers to be part of the prototyping process. Our work is closer to d.tools [25] , which brings test-driven development benefits to physical prototypes. d.tools lets designers rapidly test their design and analyze results, for example to identify the most frequently used interaction. Testing is a great intersection for the design and implementation of a prototype, and it can work as a \"hinge\" between the two activities.\nTools for Interactive Illustrators.: Recently, several authors have proposed integrating graphical and symbolic vocabularies to create dynamic graphics. Kitty [29] is a dynamic drawing tool supporting the creation of animated scenes through functional relationships between graphical entities. Kitty relies on direct manipulation of graphics but also supports indirect manipulation of input-output functions. Apparatus 11 is a graphics editor that combines direct manipulation with data-flow programming. This combination of representations, with direct and indirect manipulation, enables users to think both spatially and symbolically.\nTools for Interactive Illustrators.: While these tools provide authoring capabilities based on various representations, they focus on the creation of dynamic drawings, illustrations, and diagrams, not on prototyping interactions. For this reason, user input follows a fixed path, e.g., a constrained drag and drop, and does not take into account all the input capabilities of the target device. Victor [60] proposes a tool that 19:8 G. Leiva et al.\nTools for Interactive Illustrators.: lets artists interactively create dynamic drawings with behavior simulations. The tool provides designer-friendly direct manipulation of graphics but also relies heavily on developer-friendly concepts such as linear algebra, parameterization, and recursion. We are building on this trend of tools that bridge the worlds of visual output and dynamic input, but with a particular focus on the creation of interactions instead of dynamic graphics.\nSummary: Designer-developer collaboration has been studied from two main perspectives: processes and artifacts. With respect to processes, practitioners and researchers both apply iterative methodologies for creating interactive systems. One of the challenges of this iterative process is to manage tensions and breakdowns in the collaboration. Some researchers see these breakdowns as a normal part of the collaborative work. We argue that we need to better understand these collaborative tensions to determine which can be mitigated by new design artifacts.\nSummary: With respect to design artifacts, most of the literature describes their use in a collaborative context, but does not provide guidelines for building new collaborative tools that reduce breakdowns. Researchers proposed myriad prototyping tools, but they have been studied with a single community, either designers or developers. Involving designers and developers in the construction of interaction requires tools to support both design and development activities [13] . Such tools do not exist yet, to the best of our knowledge, especially for the collaborative creation of custom interactions.\nSummary: Our goal is to create better tools that support designer-developer collaboration during the prototyping of custom interaction. To do so, we must first better understand how designers currently represent interactive behaviors to developers, as well as which aspects of these representations hinder collaboration. 12 ",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Sweaty palms, a shaky voice, and trembling hands -most of us have experienced these sensations while giving a public speech. Fear of public speaking is prevalent in one-fourth to one-third of the general population [15, 40] and usually begins in adolescence [47] . When the symptoms of this fear persevere and become habitual, they may lead to unfortunate consequences such as social anxiety, depression, academic failure, and more limited employment opportunities [2] .\nINTRODUCTION: Virtual reality (VR) is used as a tool in therapy for exposing patients with performance-type anxiety disorder to their fear of public speaking [21, 22] . Exposure therapy scenarios in VR are ordinarily designed using computer-generated imagery (CGI), which requires careful 3D modeling and animation that can be time-consuming and costly to create. Furthermore, such designs commonly feature generic environments and scenarios that take a \"one-size-fts-all\" approach to targeting speaking anxiety.\nINTRODUCTION: The emergence of afordable and readily available 360\u00b0 stereoscopic video cameras has opened up a new design space. This technology allows realistic VR scenarios to be created based on recordings of real people [16, 24] . These 360\u00b0 cameras have recently been used in VR exposure therapy [41] , and according to Seol et al. [35] , they provide a tool with \"easier means for creating many diferent scenes/situations and customizing for the particular needs of the patient\" [35, p. 2] .\nINTRODUCTION: Given that fear of public speaking usually begins in adolescence [47] , we argue that there is a need to tailor exposure therapy designs to this age group. Since we are targeting the exercise tool at adolescents, we have used a participatory approach [14] to create designs that are based on the lived experiences of adolescents aged 15-17 years.\nINTRODUCTION: To address the need for incorporating adolescents' lived experiences in design, we devised a workshop inspired by experience prototyping techniques [9, 46] in the design of VR scenarios. Over two days, ffteen adolescents -with the help of fve facilitators -produced four scenarios that depict situations in which one might experience fear of public speaking. The participants played out their scenarios as live-action performances, which were captured by a 360\u00b0 stereoscopic video camera. After some stitching and editing, these videos became VR prototypes aimed at exposing the user to public speaking.\nINTRODUCTION: We reviewed and transcribed video recordings of the participants' work process to analyze their approach and identify analytical themes [6, 7] . In this analysis, we identifed themes relating to how the adolescents used their lived experiences to design VR scenarios, envisioned and composed virtual environments, and made sense of the subject matter. After the VR prototypes were completed, six clinical psychologists evaluated the VR scenarios by viewing them with a head-mounted display (HMD) and assessed each scenario's feasibility for use in therapeutic contexts. The evaluators emphasized the scenarios' authenticity and realism, stating that they considered the prototypes viable for use in exposure therapy.\nINTRODUCTION: Our results show that 360\u00b0 video is a viable tool for designing VR scenarios and is well-suited for constructing realistic and detailed virtual environments. The primary contribution of this paper is the composition of tools and methods we present for designing such scenarios and including participants in the design process.",
    "relatedWork": "RELATED WORK: This section introduces previous research on fear of public speaking, exposure therapy, and virtual reality.",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: In the recent big data era, scientific experiments need to handle massive amounts of heterogeneous data [43, 47] . While data-intensive experiments open up the possibilities for interesting discoveries (such as through statistical analysis and applying advanced machine learning algorithms), there are several challenges with complex, data-intensive computation and the analysis process, including dealing with failure handling, optimal task scheduling, big data visualization, distributed job execution and real time execution monitoring [43, 46] . The effective analysis and management of complex, multi-dimensional, and high volume data is challenging for an individual and often requires collaboration with multiple scientists [87, 88] . In any case, scientific research often demands collaboration among scientists from multiple domains and with diverse expertise [46, 73, 74, 87, 88] .\nINTRODUCTION: Given the collaborative nature of modern complex scientific experiments, recent research identified Computer-Supported Collaborative Work (CSCW) technologies as necessary to support scientific experiments that require collaboration among multiple researchers [13, 27, 34, 88] . Jirotka et al. determined that, while the relationship between scientific experiments (i.e., e-Science) and CSCW is relatively nascent, they together could yield significant benefits in answering complex research questions and in important knowledge discoveries [34] . With this motivation, we studied the concept of collaboration or groupware systems in the context of Scientific Workflow Management Systems (SWfMSs) for aiding with complex scientific experiments among multiple scientists via realtime collaboration. In this paper, we present several challenges and differences with collaborative SWfMSs in contrast to text or graphics editing groupware systems, and present a general framework for collaborative SWfMSs that leverages CSCW technologies to support scientific experiments.\nINTRODUCTION: A scientific workflow is a facilitation or automation of a process as a part or whole [3, 33] during which the targeted data are passed from one computational step to another for certain actions or processing as per some set of pre-defined rules or instructions [3, 33, 43] . A SWfMS automates a scientific workflow life cycle: composition, deployment, execution, and analysis [43, 47] , which is discussed in detail in Section 2. While SWfMSs are widely used in recent years for handling and managing the overall execution of complex scientific experiments [43, 46] , none of them directly support collaborative work among multiple users; hence users need to follow several time consuming manual steps for any required collaboration on a given data analysis task [73, 74, 87, 88] . For example, for a collaborative design of a scientific workflow, a user first builds a part of a workflow (e.g., a sub-workflow), exports it from the local workflow engine and shares it with a collaborator for possible updates on the sub-workflow. Around 3910 such scientific workflows have been shared among 10665 members (as last noted in August 2018) for collaboration in myExperiment [14] a shared social space for scientific artifacts. The manual collaboration process is repeated a number of times to complete building the entire workflow comprising of several sub-workflows. This manual back and forth process for collaboration is often very time consuming, does not support real-time editing, and is often impractical as the collaborating group size increases over time.\nINTRODUCTION: While the above statistics and scenario reveals the importance and necessity of collaborative SWfMSs, designing a real-time groupware system for workflow collaboration is non-trivial and differs from text or graphics editing groupware systems in a number of ways: i) Different Roles, scientific experiments often require adequate access control policies for sharing the workflow components, data products and provenance information among researchers with varying roles [5] and in the context of scientific data analysis, the varying roles might include domain user, pipeline composer, tool developer, data specialist, and so on depending on a given use-case scenario [46] (we present further details on varying roles in Section 5.3); ii) Collaborative Job Scheduling, collaborative job scheduling is required to orchestrate and efficiently schedule the independent workflow execution requests of researchers [87, 88] ; iii) Collaborative Job Management, in addition to the primary requirements of workflow job execution, monitoring, or failure handling, collaborative SWfMSs need to have a feedback system to orchestrate the overall data analysis process among the collaborators; iv) Plugin Architecture for Collaboration, for effective collaboration among research groups, a collaborative SWfMS must allow easy and real-time plugins of workflow tools; and, v) Collaborative Data Visualization, a collaborative SWfMS should facilitate collaborative data visualization to fully exploit collaborative data analysis.\nINTRODUCTION: To address these challenges and requirements, we present a framework towards an effective design of a collaborative SWfMS for scientific data analysis. Our proposed framework adopts a plugin based architecture for workflow tools. As a proof of concept of the proposed framework, we also implement a collaborative SWfMS SciWorCS. As our proposed framework is not restricted to any particular research domain, we evaluate it with use-cases from two different research areas Designing Groupware Systems to Support Complex Scientific Data Analysis 9:3",
    "relatedWork": "RELATED WORK: In this section, we first present related work on CSCW in aiding with scientific experiments (i.e., in Section 3.1), and we then discuss recent work for supporting collaborative data analysis with SWfMSs (i.e., in Section 3.2).\nCSCW to Support Scientific Experiments: Several recent researches assert the necessity of CSCW in supporting complex scientific experiments that require collaboration among multiple researchers [13, 27, 34, 88] . Jirotka et al. from their investigation studies presented that, while the relation of scientific experiments (i.e., e-Science) and CSCW are relatively nascent one, they exhibit significant potentials in answering complex research questions and in important knowledge discovery [34] . Hence, over the past few years several research studies have been conducted in understanding human behavior [21, 27] . Besides some of the study targeted on discrete aspects of collaborative data analysis systems, such as consistency management or locking schemes [34, 88] . However, to the best of our knowledge none of the previous study addressed the overall architecture comprising different primary requirement or components for a collaborative data analysis system. Hence, our study and proposed architecture is different than the existing studies in the sense that we formulate the discrete research problems towards designing an overall architecture towards collaborative data analysis.\nCSCW to Support Scientific Experiments: A number of studies have been conducted in recent years for gaining in depth understanding of Designing Groupware Systems to Support Complex Scientific Data Analysis 9:5 scientific work practices, such as how scientific experiments are conducted, how research artifacts are shared, and how scientists interact with tools and technologies [34, 48, 60, 67] . While such investigations often target different areas (e.g., the Electronic Medical Record (EMR) [32] and Breast Cancer Screening [35] ), they generally aim to provide important insights into the challenges and design implications of CSCW systems for collaborative scientific experiments [34, 48] . As we are proposing a high-level architecture for a collaborative data analysis platform, the insights from the previous studies are useful for identifying the requirements to integrate with our proposed architecture and towards designing a domain specific collaborative data analysis platform for EMR, Breast Cancer Screening, and so on.\nTowards Collaborative Data Analysis: A Scientific Workflow Management System (SWfMS) automates the process of life cycle phasescomposition, deployment, execution, and analysis of a scientific workflow [43, 47] . Large-scale scientific experiments often take advantage of SWfMSs for modeling the overall data analysis and manipulation process comprising of different computational steps for input data loading, transformation, aggregation, and so on [43] where SWfMSs provide the framework for supporting the specification, modification, execution, failure handling, and monitoring of the data-intensive tasks [43, 46] . With the increase of data complexity and volume, extensive research has been done in this domain resulting in a number of proposed SWfMSs architectures and corresponding implementations. Some of the modern popular SWfMSs are: Galaxy [25] , Taverna [62] , Kepler [47] ,\nTowards Collaborative Data Analysis: Pegasus [16] , VisTrails [9] , Triana [82] , VIEW [42] , Chiron [61] , and GridNexus [8] . However, to the best of our knowledge none of the SWfMSs supports collaboration directly and so require manual effort in contrast to our proposed architecture for supporting real-time collaboration for data analysis and scientific experimentation. Lu et al. [46] studied several motivations and opportunities for collaborative SWfMSs from the perspective of large-scale and multidisciplinary research projects. A number of methods have been proposed for consistency management of shared workflow in a collaborative environment. Zhang et al. [87] studied the concept of a turn based locking scheme in the context of collaborative SWfMSs for facilitating consistency management. In such a setup, each collaborator generally has only Read access to the shared workflow. Collaborators request and compete for the floor for carrying out any update or transaction on the workflow (e.g., Read & Write access). Fei et al. [20] and Zhang et al. [88] presented locking schemes by allowing only descendent module locks (e.g., descendent nodes of the workflow DAG [43] ).\nTowards Collaborative Data Analysis: Sipos et al. [73] used two lock modes -User and System locks. Fei et al. [20] proposed a lock compatibility matrix for a set of six pre-defined modes of locks. Besides, techniques have also been studied for extending the single-user Grid portals to a collaborative environment [74, 76] .\nTowards Collaborative Data Analysis: While the locking schemes for consistency management is one of the primary requirements of a collaborative system [79] , the requirements of collaborative SWfMSs are often more than that [46] . For example, those studies did not consider several important aspects such as collaborative job scheduling, collaborative job execution, collaborative visualization, and user interaction for problem solving. Hence, in our study we addressed those requirements as well for a successful collaborative data analysis platform, which can significantly contribute towards further improvement in the research domain.\nTowards Collaborative Data Analysis: Table 1 provides a summary of the proposed architecture in comparison to some of the existing related research. The table shows that, while existing research targeted different discrete components, our proposed architecture addresses all of the primary requirements towards the effective design of collaborative data analysis platform.",
    "rq": " rq 1: how can we design collaboration in scientific workflow management system?\n rq 2: how can we implement a real-world collaborative swfms that is functional and performant by supporting consistency management, sub-workflow execution, visualization in various formats and so on?\n rq 3: how can we utilize our collaborative platform for different scientific analysis domains?\n rq 4: how does the proposed architecture impact the network overhead?\n rq 5: how do scientists perceive real-time collaboration in data analysis?"
}
{
    "intro": "INTRODUCTION: Previsualization (previs) is a collaborative process of planning scenes and shots within the pre-production stages of filmmaking. Traditionally, this process has been performed with drawings, concept images, sketches, etc. [Janson 2019 ], and it is not until recently that previs has been performed with 3D animation tools and software. The use of computer graphics-based (CG) technologies for providing visual effects within filmmaking is a well-established practice. However, the costs of these technologies are substantial, therefore it is important to carefully plan shots that require the expertise of CG professionals [Honthaner 2017 ]. With recent advancements in Virtual Reality (VR) technologies, the film industry sees a shift where planning for different scenes and takes can be done in a more immersive and tangible way. VR enables a user to be present in virtual environments resembling specific locations, and thus may help filmmakers to work creatively in a familiar setting. Furthermore, remote work is becoming increasingly common in the film industry and VR can be the ideal solution to foster collaboration within these settings. The advantage of using online multi-user VR is that it may enable a sense of being transported to real environments where co-workers can socially immerse with their colleagues. Remote collaborative previs may thus not only be an important tool for creativity and communication, but also for saving the time, expenses and environmental costs of travelling.\nINTRODUCTION: The main research question addressed in this paper is: How useful and familiar can VR technologies be for remote collaboration in previsualization?\nINTRODUCTION: To research this topic, we describe and evaluate a tool that provides an immersive and collaborative previs environment where filmmakers can create, discuss and validate different takes, shots and entire scenes. The tool features a VR interface where the intended users do not need advanced computer literacy to understand and master the functionalities. A within-subjects experiment was conducted, where the participants (professional filmmakers, primarily from the Stockholm Academy of Dramatic Arts) performed previs sessions in the proposed collaborative setup and in a control solo setup. In this paper, we analyze the results from our experiments to assess the impact of, and provide design insights on, remote collaborative VR technology that enables previsualization of film scenes.",
    "relatedWork": "RELATED WORK: In face-to-face interaction, communication can be achieved in a delay-free and multi-functional way, as is explained in [Clark and Brennan 2004] , where Clark and Brennan claim that grounding, i.e. finding common ground, is important for how communication works effectively. Thus, the costs of constraints of mediums can affect formulation, production, and understanding. Collaboration calls for similar grounding as for general communication in order to work effectively. The importance of shared visual space, mental models, context, and speech is evident in earlier research [Carroll et al. 2006; Fussell et al. 2000; Gergle 2005; Gergle et al. 2005; Shakeri et al. 2017] , where the context and understanding of how to collaborate and what to collaborate on can be made clear. In [Fussell et al. 2000], Fussel, et al., accounts for four types of visual information that is important for grounding in collaborative contexts: participants' heads and faces, participants' bodies and actions, shared task objects, and shared work context. Similarly, having a shared visual space in collaborative work environments makes participants' more likely to let their actions speak for themselves instead of having to explain verbally, simultaneously [Clark and Brennan 2004; Gergle et al. 2005] .\nRELATED WORK: Technological solutions for remote collaboration have taken many different forms. From letters, telephone, video-conference systems to more advanced forms of technology that are now starting to show potential for creating efficient and natural forms of collaboration [Celata et al. 2018; Fussell et al. 2003; Johnson et al. 2015; Khan et al. 2016; Piumsomboon et al. 2016 ]. However, most of these authors also claim that the lack of developed technology can hinder good collaboration. Poor field-of-vision in interfaces and lack of asymmetry in controls are two examples of how the design of technical solutions could have been standing in the way of usable interfaces for interactive collaboration [Bateman et al. 2015] . This is something that VR technologies can be better suited for, as the immersive qualities of virtual headsets and its controls are extensive enough to facilitate grounding in collaborative environments. Advances in VR technologies has led to decreased prices and a more widespread usage of the technology [Feltham 2018; Ladwig and Geiger 2018; Perry 2015] , which might represent clear opportunities for remote collaboration purposes. VR can be seen as a way of achieving near perfect co-presence with other people [Perry 2015]. Co-presence is an especially important aspect for attaining grounding in virtual 3D environments [Gergle et al. 2005; Ramsbottom 2015 ]. However, there are still problems with VR that prevents the medium of fully delivering on the promise of full copresence. In [Anthes et al. 2016 ], Anthes et al., raises a number of issues on the state of VR technologies. It is clear that representation of users is still a problem as a result of lack of information on facial expressions, eye-gazing, and body movement. Nevertheless, recent advancements are attempting to tackle these problems with the use of eye-tracking inside VR headsets, gloves that can represent fingers, etc. Thus, user representation is likely to improve in the near future [Ladwig and Geiger 2018; Pfeil et al. 2018 ]. As such, the affordances of elements in a VR environment needs to be carefully decided upon, when designing for VR. In [Ellis 1995], Ellis makes clear that affordances in VR should mimic the real world as much as possible, as the affordances of real artifacts are expected to have the same affordances in the virtual environment. Furthermore, in the virtual environment, extended affordances can be developed, as the virtual environment can in its nature facilitate this [Ellis 1995] .\nRELATED WORK: In this paper, we focus on designing collaborative VR technologies that enable previsualization, a pre-production stage within filmmaking. Previsualization is a process where filmmakers create visual prototypes of scenes in a film that are otherwise difficult to visualize. Different use-cases are e.g. scenes with visual effects, expensive set constructions or digital set extensions. While hand-drawn floor plans and storyboards are still the dominating previsualization technique within creatives, high-budget productions commonly make 3D animated visualizations of the film, predominantly using 3D modelling and animation software such as Autodesk Maya 1 or Blender 2 . However, this process is not only time consuming and costly, it also delegates creative control over the previs to external animators. The possibilities to use real-time technologies such as game engines to provide more interactive and user friendly interfaces have been attempted before. The main promise of such technology is the ability for the creative team of a film-production to create the previs by themselves. In the work by [Nitsche 2009 ], a previs system was implemented using Unreal engine together with various custom built input devices for puppeteering characters and camera. The study argues that animation and camera control are the most important areas for previs. Several other studies [Ichikari et al. 2007; Mori et al. 2011; Spielmann et al. 2016; Tenmoku, R. and Ichikari, R. and Shibata, F. and Kimura, A. and Tamura 2006] have explored Mixed Reality (MR) interfaces for previs. These interfaces are however restricted to camera work (although [Spielmann et al. 2016] has basic key-framing functionality), and and thus does not exploit the possibilities to previsualize character movements in real time.\nRELATED WORK: In [Ramsbottom 2015], a VR previs system is presented where the user can create low resolution animations by adding and editing key frames on a timeline. The system used a rotation sensing headset (Oculus DK2) and a game pad for input, which limits the immersion and familiar interaction mechanisms included in later hardware. The system closest to ours is presented in [Muender 2018] . By evaluating their system on 6 expert users from film and theater (surprisingly none being a director or photographer), they found that their tool was practical for real-world applications. Like us, Muender et al., uses room scale VR with positional tracking, and a miniature approach where the users move scale models and cameras. Unlike us, they only create snapshot still-images of stiff characters, and they do not use a collaborative setting. We will contribute to the state-of-the-art in this field by performing a larger study with a greater number of film professionals, and by investigating the usefulness of distributed collaborative previs in VR.",
    "rq": "the main research question addressed in this paper is: how useful and familiar can vr technologies be for remote collaboration in previsualization?\n it can be argued that the reason for why some participants enjoyed and preferred the solo condition to a slightly higher extent could have been due to several effects: how well the participants knew each other?"
}
{
    "intro": "INTRODUCTION: Sound is an important design feature in toys. We often see toys that include background music, button triggered sounds, or are designed as instruments [16] . Yet, we rarely see toys that are specifically designed to encourage a child to actively listen to environmental sounds in different ways, outside of musical training. This paper explores children's experience of sound through the lens of active listening. We use the term active listening to mean 'listening with attention' to details within the sound, or in how sounds in the environment relate and contribute to the overall sound [15] . Children are naturally active listeners. Such ability directly aligns with their perception of the affordances of sounds as information within their world. R. Murray Schafer, while promoting the term \"acoustic ecology\", passionately discussed the deterioration of listening skills in children. We align ourselves with Schafer's belief of the importance of listening as inspiration to design sound-based toys [19] . We have designed three prototypes to observe how toys may support the child's process of exploring/learning the relationship between sounds and their personal agency, their experience of their environment, and their relationships to people around them. Our current work frames this interest by asking two questions: 1) Can we encourage young children's awareness of sounds in their environment? 2) Can we encourage the sharing of their experience with peers and teachers? We explore this project as a step towards further studies comparing toys with different approaches to sound. This paper presents our design process and a mixed-methods evaluation of 3 toy prototypes.",
    "relatedWork": "",
    "rq": " 1) can we encourage young children\\'s awareness of sounds in their environment?\n 2) can we encourage the sharing of their experience with peers and teachers?\n based on these ideas we ask the question: can we encourage young children's awareness of their sonic environment and encourage social listening by designing sound toys that highlight or isolate sounds from that environment?"
}
{
    "intro": "INTRODUCTION: Both the concept of 'having a voice' and the concept of 'being verbal' are heavily relied upon in the design process but, we question, how does one have a voice in design if one is not verbal [1, 43] ? It is estimated that around 30% of children on the autism spectrum 1 are minimally-verbal, which means they communicate with; no spoken language at all; atypical non-speech sounds only; some words or phrases in limited contexts; or echolalic language (repeating the language of others but not generating it) [60] .\nINTRODUCTION: Despite the high proportion of children on the autism spectrum who are minimally-verbal, these children are seldom catered to in existing co-design approaches. Many classic co-design techniques (e.g. Cooperative Inquiry [11, 23] ) rely heavily on activities that require verbal communication, high cognitive load (such as brainstorming and group discussion), and fine motor skills (such as detailed drawing). Successful participation also requires broad metacognitive and social skills and the ability to attend to design activities for long periods. This is often beyond the current skillset of minimally-verbal children on the autism spectrum.\nINTRODUCTION: Design work which does focus on minimally-verbal individuals (such as Augmentative and Alternative Communication) often has the focus of increasing verbal ability or monitoring progress [1, 36, 44, 64] . While this is valuable, there is little work that supports the child's existing communicative abilities [40] during co-design and/or seeks to build on their existing strengths and interests.\nINTRODUCTION: We argue that this highlights a fundamental need to rethink our approach to design with this group -moving away from an expectation that children should fit within neat, existing methods, and taking responsibility for the possibility that our existing methods may not be inclusive enough. We argue that the onus is on designers and researchers to adapt methods which can support minimally-verbal children on the spectrum to be fully included in the design process as they are [68, 69] i.e. through understanding existing abilities and building on these. We ask, how can we support and understand existing communicative competencies, as opposed to focusing on the negating of perceived limitations? Further, how do we move towards ensuring that 'having a voice' in design is not solely for the verbal?\nINTRODUCTION: We turned to the fields of Education and Speech and Language Therapy, where there are many effective existing practices which provide approaches to understanding and supporting children as they are. These include approaches like More Than Words [59] and the Floortime Approach [20] , which are widely implemented techniques for interacting with children with complex communication needs [46, 50] . They focus on following the child's lead (their existing interests and attentions) in order to better understand their actions and interactions. However, there currently exists no translation of these tried-and-tested methods from special needs practice into Human-Computer Interaction (HCI).\nINTRODUCTION: Working in an autism-specific primary school over two school terms (20 weeks) with ten children aged 5 to 8, we undertook design sessions in which prototypes were developed using Co-Design Beyond Words, an approach which draws upon techniques from Speech and Language Therapy and utilises them in design. The approach has three phases; the Foundation Phase (where we prepare for interaction), the Interaction Phase (where we interact and reflect-in-action) and the Reflection Phase (where we reflect-on-action).\nINTRODUCTION: Central to this approach are moments of interaction -micro instances of Joint Attention (JA), Turn Taking (TT) and Imitation (IM) -which result in an interaction with another person. We have observed that these moments of interaction arise when children are supported to follow their interests and utilise their strengths in a design setting. These moments can provide design insights, which, in our experience, have been inaccessible through existing co-design methods.\nINTRODUCTION: We contribute Co-Design Beyond Words, a new approach to co-design with minimally-verbal children on the spectrum which follows their interests, builds on their abilities, and supports their interaction. Through this approach, we are able to build a cumulative picture of a child's design needs and elicit new design directions.",
    "relatedWork": "RELATED WORK: Current landscape: Autism and HCI Individuals on the spectrum may typically have challenges in social communication and interaction, as well as restricted, repetitive patterns of behaviours, interests or activities [2] . We suggest that it is critical to design with children on the spectrum because, as Ringland et al. [49, p.39 ] assert, \"children with autism are doubly disempowered, both as children and as people living with disabilities\". Spiel et al. [56, 57] identify important ethical considerations pertaining to the skewed power relations [15] between researcher and child. Recent thinking on Neurodiversity positions autism not as a neurological disorder but as a natural neurological variation [30] , moving away from the idea of causation and cure, and instead towards the \"celebration of autism as an inseparable aspect of identity\" [32, p.59] . Researchers in HCI have begun to emphasise the importance of this paradigm shift in thinking [39] ; from the biomedical model of disability, which is limitation-centric, focussing on perceived deficits of individuals, to the social model of disability, whereby our societal structures (e.g. access to services) are held accountable for the way in which individuals experience disability. Ability-Based Design [69] reflects this, representing a shift from deficit-focused design to design which allows, and indeed encourages, people of all abilities to remain as they are [68] while technology is shaped by and with them.\nRELATED WORK: In design, the child's role has evolved from design informant to design partner [12] , with recent research exploring how children can initiate design processes [70] and become full design protagonists [29] and participatory evaluators [58] . While involving children with special educational needs and disabilities in design is an increasing priority in Child-Computer Interaction (CCI) e.g. [4, 5, 12, [16] [17] [18] , the progressive approaches to participation in this field are not yet fully inclusive of minimally-verbal children on the spectrum. As Read et al. [48] suggest, the core aim of Participatory Design (PD) with children is ensuring their active engagement in the design process. However, existing codesign approaches involve e.g. sustaining attention for long periods, group interaction, and processing numerous verbal cues, which present barriers to active engagement for minimally-verbal children on the spectrum. We suggest there is an opportunity to extend current co-design approaches to address this gap.\nRELATED WORK: Our work draws on the traditions of Co-Design and Participatory Design. Here, we explain our use of these terms and our choice of the term \"Co-Design Beyond Words\" to describe our approach. Participatory Design [9] has two fundamental tenets -pragmatic and moral. First, from a pragmatic perspective, people for whom the design is intended are experts in their own lives and have vital knowledge to contribute. Second, from a moral perspective, people should have a say in design that affects their lives. That is, PD concerns issues of power and working out how power in design is shared [3] . Co-Design [54] is a more recent formulation of Participatory Design [55] that focuses on aspects of collaboration of stakeholders in design, but it is not in essence different from Participatory Design, if designers ensure the fundamental tenets of Participatory Design and power sharing are upheld. We choose to use the expression Co-Design here, because, to many, the term Participatory Design suggests that participants are informants that do not have a significant input in design. This was never the intention of Participatory Design, however, we feel that the term Co-Design, emphasising collaboration, is simply more fitting. There may be debate as to whether minimally-verbal children can participate as equals and leaders in Co-Design. We argue that this can be the case, if designers are willing to offer sufficient choices and open-ended opportunities, and pay attention to children's responses. We return to this in the discussion section.",
    "rq": " we asked questions such as: how are the children transitioning spatially?\n as we work towards answering the question: how does one have a voice in design if one is not verbal?"
}
{
    "intro": "INTRODUCTION: Augmentative and Alternative Communication (AAC) can complement or replace speech using non-verbal communication systems and intervention strategies [24] . AAC systems and devices provide tools that allow users to select words, symbols, and images to communicate their thoughts, intentions, and conversations with others through digitized or synthesized voice [2] . People with speech impairments often have motor impairment associated, which makes the use of traditional input devices such as mouse, keyboard or touch difficult, since these users may not have sufficient mobility or motor control to manipulate these devices [10] . Therefore, the development of alternatives technologies is necessary to address these issues. The existence of different technologies and interaction modes opens new paradigms in Human-Computer Interaction (HCI), that can benefit people with motor and speech impairments.\nINTRODUCTION: Easy access to camera devices gave rise to a new generation of Assistive Technology (AT) resources that allows personalized and non-invasive interaction methods to be considered, including movements recognition of the hands, head, face, and other body parts to perform actions on computer systems [36] [33] . Such possibilities represent a valuable opportunity for disabled people because they promote the more inclusive and active participation of disabled people in society by influencing their affective and emotional states and strengthening their ties with the environment in which they are inserted [4] . In Brazil, different initiatives have been proposed to promote innovation in AT, whether through families [41] , companies [57] , universities [8] [7] , or federal government [22] [23] .\nINTRODUCTION: Regarding gesture recognition, however, most existing studies work with predefined gesture sets and tracking a specific body region. After leaving the laboratory conditions, in case there is a need to replace any pre-configured gesture or to include new ones, no studies were identified describing how it would be possible for the user and his caregiver to perform this type of customization. To contribute in this direction, this paper presents a Computer Vision system that allows users and caregivers to create a personalized set of interactive gestures, taking into account the abilities and limitations of the user when performing movements, and providing support to other people to recognize these movements using an AAC system based on personalized gestural interaction. The developed system is presented along with the results obtained in an experiment carried out with students with motor and speech impairment. The information obtained through interviews with teachers, caregivers and LIBRAS (Brazilian sign language) interpreters that accompanied the experiment are also presented and they have contributed to the identification of some challenges and possibilities to improve the system.",
    "relatedWork": "RELATED WORK: Possibly, the earliest example using video to recognize hand movements as an interaction mode is the study of Krueger et al. [31] . Jacob [28] was one of the first to introduce appearancebased interaction techniques into real-time applications for people with disabilities. Since then, several studies supported by resources of AT and interaction modalities based on recognition of body movements have been developed to improve the quality of life of people with physical disabilities.\nRELATED WORK: Betke et al. [9] developed the CameraMouse, a system capable of tracking the movement of various parts of the body specified by the user (such as nose, eyes, chin, foot) employing an algorithm based on spatiotemporal model matching. This study was quite representative for the AT area and inspired other studies such as [32] , [19] , [3] . Similar systems were also developed by [42] and [21] . Simple devices such as webcam were used by a large number of initiatives ([3] [34] [16] ) mainly because they represent an interesting and viable alternative for detecting and tracking of movements, and especially due to their low cost. The same advantage holds for mobile devices, that are increasingly accessible and used in AT resources [38] [39] [63] . Recent studies have investigated the use of depth data ( [11] , [25] [40] ), using devices such as Kinect, BumbleBee and Monocular infrared.\nRELATED WORK: Gaze detecting/tracking research has received increasing attention from the academic community ( [6] [12] [18] ), possibly because eye movements may be the only movements some people with severe disabilities can control voluntarily.\nRELATED WORK: In studies of [13] and [48] , the gaze is used in conjunction with movements performed by other parts of the body or other devices, generating modalities of multimodal interaction. The existing studies generally do not provide details on the difficulties encountered in creating the dataset, possibly because they use existing datasets, or created with gestures performed by people without disabilities. Tracking specific regions of the human body as a way of interaction for a specific target audience tends to generate solutions more adapted to the users' interests. However, the accessibility of these solutions may fail if they do not offer adaption mechanisms that allow themselves to be tailored for their users.\nRELATED WORK: Although not employing cameras but sensors on users (magnetic tracker and electromyography), studies from Roy et al. [46, 47] show that many non-speaking individuals are able to make gestures that are repeatable and recognizable by family observers in greater number than previously expected, and these gestures can be mapped into words or concepts. When a person has severe limitations to self-expression, the knowledge that the observers (e.g. caregivers, family members) have about an individual's ability to perform movements is fundamental to create a personalized sign language. The research we present in this paper aims to support in this direction, by allowing a personalized gesture configuration so that people can interact with a computer system.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: There is long-standing interest in the human-computer interaction community in designing shared displays to support group processes in different task settings [3, 28, 34, 36, 47] . Previous research, both on vertical wall-sized displays and tabletops, demonstrates the importance of referencing gestures over the shared displays for group communication and suggests multi-user input as a key feature for the design of shared displays [26, 41, 48] . These studies often focused on peer collaboration, where all users make similar amount of physical contributions to the tasks. Yet, we have limited understanding on gesturing over a shared display in instructional collaborative tasks, where a novice is primarily performing the task on a display, monitored and guided by an expert, such as in simulation training, remote repairing, and minimally invasive surgery [11, 22] . In these tasks, experts mainly supply expertise, or in-situ knowledge, for the novices to accomplish the task, without physically contributing to the task itself.\nINTRODUCTION: Most current gesturing tools for instructional collaborative tasks are single-user input, enabling only the experts to interact with a display [12, 21, 23, 37] . These tools have been demonstrated to improve task performance and communication efficiency [10, 11, 21] . However, much less discussion has been on the ability of novices in gesturing over a shared display. Although novices may use task-related actions to substitute their speech, these actions are mainly for acknowledging the acceptance of a statement [13] . In contrast, novices prefer to use gestures to present a piece of information and make contributions to the group decision making [1] . In addition, compared to experts, who tend to articulate the task objects, novices are more likely to describe the location of the objects when making a reference [19] .\nINTRODUCTION: When a single-user gesturing tool was provided in an instructional task for experts to use, the novices' language use was found to decrease [21] . On one hand, it indicates improved communication efficiency [5] . On the other, it may reflect the reduced participation of novices in the group process [25] . To build tools to support the instructional collaborative tasks over shared displays, we need to elucidate the process in which the current single-user gesturing tools affect the language use of the novices. Although previous research has demonstrated that novices raised fewer questions with more directive instructions [21] , we have limited information on these questions themselves. For example, do these questions contain any new information? Is there any embedded knowledge elicited by these questions? Why are these questions reduced? A more detailed understanding of the communication process allows us to comprehensively evaluate the impact of single-user gesturing tools on group communication in instructional collaborative tasks, as well as make informed design decisions on designing the interactive shared display.\nINTRODUCTION: In this study, we investigate the potential communication costs -the efforts for speakers to take over the floor by formulating and producing their communicative acts [5] -incurred by the use of a single-user gesturing tool in an instructional collaborative task. Previous studies have shown that experts and novices co-constructed the knowledge through hand gestures over the display [29, 30, 32] . In our study, we provide a gesturing tool to the experts on top of the standard interactions to identify any changes in the structure and content of the team communication. With a thorough examination in the communication process, our study reveals that the communication became less balanced with reduced active participation from the novices when using the single-user gesturing tool in the instructional collaborative task. Based on our findings, we discuss the design directions on supporting equal communicative access to the shared display.",
    "relatedWork": "RELATED WORKS: In this section, we first present the theories that lay the ground for the study. We then identify the knowledge gap by comparing current studies in shared displays for peer collaboration with collaboration between experts and novices. Finally, we present the context of our study and state our hypotheses.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Communication skills are vital in everyday life from professional to personal interactions. The ability to communicate effectively is fundamental to success at work place. Good verbal and non-verbal behavior are important for effective communication. The words that we choose and the body language, gestures, posture, prosody, facial expressions etc. play a crucial role in delivering correct information. Non-verbal information in particular is very important to judge candiate's behavior in contexts such as job interviews [1] . For example, audience might lose attention when the speaker speaks too slow. Automatic assessment of communication skill using multimodal technology has gained attention in the recent years. Automatic assessment methods help to practice and improve the communication skills of the candidate [2] . Automatic methods are not only used from the candidate's front but are also changing the way interviews are being conducted. For example, Asynchronous Video Interviews (AVI), video resumes etc. started gaining attention in the personnel selection process particularly in the preliminary stages. Many online platforms such as Talview [3] , Hirevue [4] , InterviewStream [5] record video interview of the participant and they are further used to assess, train and shortlist on candidate's behavior.\nINTRODUCTION: Until recently, the traditional interviews were all conducted faceto-face where both interviewer and interviewee are collocated. The advent of economically available sensors for example webcams, microphones and usage of internet led to conducting these interviews online. Both AVI's and video resumes enable the interviewers to view the candidate behavior and shortlist for the further recruitment process. In spite of their own advantages, they lack interaction with a social partner. Web conferencing interviews enable internetbased communication between interviewer and interviewee. They are used to conduct interviews remotely by streaming both audio and visual content of both parties simultaneously. Software platforms that allow conducting web conferencing interviews are Skype, GoToMeeting etc. But all these platforms do not enable recording and they do not provide automatic assessment.\nINTRODUCTION: In this work, we automatically assess the communication skill rating of participants giving real face-to-face interviews through an online platform. The online platform allows interviewers to interact with interviewee using a skype-like interface. This interface not only allows real time streaming of audio-visual content of both parties simultaneously but also record the interaction. This solution is advantageous as 1) it does not require collocation of both Figure 1 : Summary of our approach parties, 2) purely social as it involves interaction with a conversational partner unlike an AVI and 3) the recording feature allows further self-evaluation and training from the candidate's front. The automatic assessment framework involves extraction of non-verbal acoustic and lexical cues from the interviewee. We use previously trained Machine Learning (ML) models to predict communication skill rating of participants in real face-to-face online interviews. The ML models were trained on AVI data collected in a controlled environment. The main contributions of this paper are 1) Collecting a database of 45 real face-to-face interview videos using an online platform (the interview videos collected in a generalized environment is referred as wild dataset. This data varies w.r.t population, age, recording hardware.), 2) Using pre-trained models to predict communication skill for complementary data (controlled Vs. uncontrolled), 3) Incorporating automatic diarization and automatic speech-to-text conversion systems for extracting interviewee speech Vs. manual transcription for pre-trained model. This paper is organized as follows. Section 2 summarizes the literature in automatic assessment of communication skill and few related variables. Section 3 details about our approach for prediction framework. In Section 4, we describe non-verbal audio-visual behavioral feature extraction. Section 5 presents the experimental results along with data collection in various modes of online interviews. In Section 6, we conclude with discussion and future work.",
    "relatedWork": "RELATED WORK: Computational methods have been proposed in the past for automatic behavior assessment of candidates in social interactions. Assessment of communication skill is important at various phases of professional life and is looked previously in different contexts such as self-presentations, face-to-face interactions and group interactions. Communication skill is contextualized and is analyzed in self-presentation scenarios by [6] , [7] . Research is carried out with both data collected in lab environment and data collected in the wild. The main difference comes from the data recording hardware and background noise. Nguyen et al. analyzed the first impressions in the wild using online conversational video resumes along with other traits such as personality, communication skills, social skills using non-verbal audio and visual features [7] . Rasipuram et al. assessed the communication skill of participants in AVI's [8] along with face-to-face interviews [9] using verbal and non-verbal audiovisual cues in controlled lab settings. In face-to-face interviews, Naim et al. predicted the job interview performance along with other traits such as excited, engagement, overall performance etc. of candidates using verbal and non-verbal behavioral features [10] . Other studies that automatically predict and assess candidate's behavior in face-to-face interactions include [11] . Both these studies conduct experiments with the data collected in lab settings. Muralidhar et al. automatically predicted first impressions of hospitality students involved in job interview and reception desk scenarios using a non-verbal framework [12] . The paper also summarizes observations across genders.\nRELATED WORK: Automatic assessment of communication skill in web-based faceto-face interviews has been looked previous by Rasipuram et al. in [13] . The authors conduct online peer-to-peer discussions for automatic assessment of both candidates. The data in this kind of scenarios is subjected to challenges such as bandwidth and audiovideo streaming. Besides this, automatic speaker diarization i.e., the problem of identifying \"who spoke when\" is a challenge in face-toface or group interactions. Although many automatic diarization methods are available [14] , [15] , using them for the particular context (type and number of people involved in the conversation), noise in the audio recording hardwares etc. are challenging. Some of the studies employ manual diarization [9] , Microcone microphone array [11] that automatically segments the speaker turns using a filter-sum beamformer. Employing manual diarization is not only expensive but also a time consuming task.\nRELATED WORK: Considering human-agent interactions, Hoque et al. in [2] conducted 28 face-to-face interviews with professional career counselors. Non-verbal analysis along the dimensions of facial expressions, listening behavior, acknowledgements, duration was made to design an interactive and intelligent virtual agent intended to provide feedback to participants who seeked career counseling. Other virtual agent platforms that provide social skills training include SST (Social Skills Trainer) [16] , TARDIS [17] . In this work, we automatically predict the communication skill rating of candidate involved in three interview scenarios: 1) Asynchronous video interviews where data is collected in lab settings, 2) Asynchronous video interviews where data is collected in the wild and 3) online face-to-face interviews in the wild. Automatic predictions are made with the models trained on asynchronous video interview data collected in a controlled lab environment. This study is different from the previous work in the context of predicting communication skill rating in real interview videos with models trained on complementary data. We make non-verbal audio-visual analysis of the interview videos in all the three scenarios along with gender-based study on training data.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Current trends in computer-mediated communication (CMC) have allowed people to take advantage of a large variety of options for communicating with other individuals online, especially when co-located face-to-face communication is not possible. These communication options typically come in two forms: high-bandwidth and low-bandwidth communication. The former allows people to closely experience the intimacy and expressiveness that is found in co-located faceto-face communication, as it allows individuals to see and hear each other's gesticulations and intonations at distinct locations from each other. This is achievable through audiovisual software such as Skype or through online messaging systems such as iMessage or WeChat in the form of embedded media clips. The latter offers people the ability to communicate conveniently and discreetly, as it comes in the form of text and images that generally require less setup. Examples of this form include Facebook Messenger, which allows users to communicate in real-time primarily via text, and Snapchat, which allows users to communicate via images and short videos.\nTraditional Visual Communication Markers: Generally, the advantages offered by one type of communication are lacking in the other. The expressiveness and intimacy that people enjoy in higher-bandwidth communication usually does not manifest as strongly in lower-bandwidth communication [37] , just as the convenience and discreetness of lower-bandwidth communication typically is not found in higher-bandwidth communication. One of the most common ways that people have attempted to bring the advantages of higher-bandwidth communication to lower-bandwidth communication is through the use of visual communication markers (VCMs) such as emoticons, emojis, and stickers that are embedded in existing text messages [72] . As more people are shifting their attention to lower-bandwidth online communications, these markers provide relatively more recent forms of visual or nonverbal communication cues in digital interaction [25] for uniquely expressing emotions via graphic icons [9, 27, 33, 55, 66] .\nTraditional Visual Communication Markers: Several factors have led to the increasing popularity of VCMs in lower-bandwidth online communication. They have shown an ability to convey users' emotions and enrich their communication [78] as well as to substitute for nonverbal cues that are missing in lower-bandwidth CMCs [72] . Furthermore, there has been a growing cultural acceptance of their use in place of text (e.g., [63] ). Various studies have further supported the advantages of VCMs in lower-bandwidth online communications, specifically noting users' enjoyment and satisfaction with VCMs for reasons including users' enjoyment and satisfaction with them [27, 55] and their ability to convey emotions to users without the benefit of facial cues [78] as well as an observed positive effect on attributes such as personal interaction, perceived information richness, and perceived usefulness [27] .\nTraditional Visual Communication Markers: On the other hand, VCMs have their limitations, as they typically fail to fully capture the advantages inherent in higher-bandwidth online communications. This is largely because VCMs generally lack fully analogous mapping to the expressive gesturing commonly found in higherbandwidth online communications [18, 39] . Text-only VCMs, in particular, are limited in their ability to achieve emotional valence, especially for groups such as non-native speakers [22] . One other major limitation of VCMs-especially emojis-is that a large set of VCMs makes inputting them challenging for users, as it is difficult to quickly find and select the specific VCM desired [70] . This is a problem that researchers have made attempts at alleviating, specifically with regard to emojis. Ali et al. proposed Face2Emoji, a system that filters out any emojis that are not relevant to the user's facial expression [11] . Pohl et al. developed a system that displayed all emojis on a single screen and allows users to select their desired emoji by zooming in [46] .\nOnline Video Chat Communication: Unlike CMCs that present lower-bandwidth information, online video chat provides a richer form of long-distance communication that more closely emulates intimate face-to-face communication [43] . With improvements in Internet speeds and greater ubiquity in both mobile computing devices, online video chat has become a reliable and widely used form of communication. However, there are several factors that prevent online video chat from being more widely used. One such factor is privacy, as this is a key concern of end users [44, 59] who have expressed concerns with using online video chat in public areas [47] . Users not only emphasized the importance of privacy from others outside of the conversation but also within the conversation. For example, users who communicated with strangers online preferred to express themselves through text over video communication [60] . Even people who were familiar with the other person in an online conversation felt strongly about maintaining privacy, especially for situations not appropriate for face-to-face conversations (e.g., coming out of the shower) or for retaining their self-image (e.g., not being publicly presentable) [74] . Another factor is accessibility, as despite the major strides in Internet speeds and reliability that have taken place, there are still challenges in Internet bandwidth availability that limit the use of online video communication [29] .\nHand Gesture Communication: In addition to these efforts, conventional means of accessing VCMs continue to rely on input entry methods that are not directly or intimately tied to expressive nonverbal cues. This is important because we gesticulate when we talk, and these gestures \"share the burden of communication with that speech\" [19] . In fact, when used in conjunction with speech, gestures \"reveal meanings that speech cannot accommodate\" [18] . As such, by recognizing and using the gestures that people naturally make while expressing the emotions represented by VCMs, it becomes possible to develop a low-bandwidth CMC system that is more immersive and intuitive.\nRepresentative Scenarios: The role of hand gestures has been well studied in communication [31, 39] and some work has been done toward capturing the expressiveness of face-to-face communication through both facial (e.g., smiling [69] ) and limb (e.g., hugging [41] ) interactions. However, research into its utility in emotion-enhanced CMC remains largely lacking [71] . In this work, we elicit user-defined gestures that are highly representative of VCMs, and develop a two-stage hand gesture recognition system (trajectory-based and shape-based) to distinguish between these gestures. To provide further insight into the benefits of this work, we provide several potential representative scenarios (Figure 1 ).\nScenario 1: Distance: Learning. Jill, age 19, is one of several hundred university student attendees of a live-streaming debate via a web seminar (i.e., webinar). Throughout the video, Jill is sitting on her couch in the comfort of her dorm room while watching the webinar on her desktop computer with other virtual attendees. During the webinar, the presenter periodically prompts the virtual audience to express their approval of various topics with either a thumbs up for approval or an unamused or stop gesture for disapproval. When the session starts heading into a negative direction, Jill makes make a sob gesture toward her web camera. At the end of the webinar, the host prompts the virtual audience to vote with an emoji to give a descriptive rating of how the session went. Jill gestures a thumbs-up to her web camera to indicate her general positive experience with the completed webinar. The interface automatically locates the gesture's corresponding emoji to the interface, which is tabulated with the input emojis of the other webinar attendees and finally displayed as responses on a real-time notification sidebar in the interface.\nScenario 2: Public: Transportation. Amy, age 15, is browsing the Internet on a tablet device while riding a bus to school. One of Amy's friends sends a text message to the tablet on a mobile messaging app asking whether she is interested in going to a social event. Amy wishes to quickly respond back to the friend. However, the bus is too crowded and shaky to type text by touch or select emojis by tap, too noisy to reply with an audio message, and too impersonal to reply with a self-captured (i.e., selfie) video or photo. Amy instead taps-to-gesture in the messaging app with one hand to activate the tablet's built-in camera, then one-handed gestures to the camera-which automatically selects the corresponding emoji that best represents her intentions to her friend-as a quick response with her other hand.\nScenario 3:: Computer Gaming. Todd, 31, is playing a fast-paced online computer game on a desktop computer. He usually plays with his fellow co-workers, but is currently playing a new game with a team of anonymous strangers since his friends have not yet purchased this particular game. During his gaming session, Todd wishes to communicate messages to his current team but does not have a microphone and wishes to not communicate with the team via audio or video owing to his privacy concerns. Although the game alternatively provides text typing and keyboard macros for quick message communications, the current game session is too fast paced for text-typing, and Todd is also not yet familiar with the various keyboard macros specific to this game. Todd instead gestures to the desktop computer's camera peripheral to express quick team-based messages to the other players in the team for coordinating his team to victory.\nScenario 4:: Window Shopping. Clara, 28, is visiting a public space at a large metropolitan shopping center. While walking around the shopping center, Clara passes a large-screen display located behind a large window of one of the stores. Since the large screen is behind a window and directly inaccessible by touch to shoppers, the store set up a camera for shoppers to interact with the display using gestures with a depth-sensing camera. The display faces outwardly toward nearby shoppers such as Clara, and the display's prompted visuals and a nearby store employee's Fig. 2 . We conducted a series of three experiments. In the first experiment, we collected gestures for all 30 emojis (E 1 -E 30 ); distinct gestures are shown as G 1 , G 2 , etc. Note that G 1 for E 1 is not necessarily the same as G 1 for the other emojis. In the second experiment, participants were asked to view distinct gestures being performed for each of the 30 emojis and select which gesture most represented each emoji. In the third experiment, participants were shown all 30 selected gestures from the second experiment and asked to select which emoji they felt that gesture represented.\nScenario 4:: inviting calls beckon shoppers to interact with the display. Clara is curious and first approaches the display, discovering that the display allows her to perform a variety of gesturing interactions, such as motion-driven gestures to navigate the store's items and communicative gestures to select various corresponding VCMs for providing quick and simple feedback and opinions of those items.\nScenario 4:: 1.4.5 Scenario 5: Family Talk. Doris, 67, is an at-home retiree with a 16-year-old granddaughter. Doris's son recently bought Doris a tablet to communicate with him and his daughter on a weekly basis. However, Doris has deteriorating fine-motor skills, which prevents her from sophisticated typing and tapping on smaller on-screen touch targets and requires using her smartphone's accessibility features to be able to make use of the smartphone's common functionalities. One day, Doris is seated at her kitchen table with her tablet mounted upright. While watching a streaming video of one of her favorite television shows on the tablet, Doris receives an online message notification directly from her grandchild. Doris's grandchild had sent a cheerful message with an animated sticker, and Doris wishes to respond back with a specific animated sticker that conveys her joyous reaction. Doris switches to the online messaging app, clicks the tap-to-gesture feature on the app to activate the app's camera, and performs a two-handed gesture that corresponds to her intended emoji, which is subsequently sent to her granddaughter.\nContent Organization: The rest of the article is organized as follows. In Section 2, we address related work on emotionenhanced interaction in CMC and hand gesture recognition. Section 3 provides our approach to eliciting user-defined hand gestures from participants. This approach is summarized in Figure 2 . Section 4 describes how we collected gesture data using Kinect. In Section 5, we discuss our twostage hand gesture recognition system. Section 6 outlines our future research steps and Section 7 presents our conclusions.",
    "relatedWork": "RELATED WORK: This section discusses prior work in related areas, providing supporting insight into the motivation behind our two-stage system. The related work is divided into three sections: emotionenhanced interaction in CMC, hand shape-based gesture recognition, and trajectory-based gesture recognition.\nEmotion-Enhanced Interaction in CMC: The development of sophisticated systems that are able to process multimodal input has increased the interest in recognizing and conveying emotions in CMC. Over the past several years, researchers have developed a variety of systems designed to embed emotions in CMC in an effort to provide users with more intuitive and effective means of communication. These systems have taken a wide range of approaches, including using facial expressions, multi-touch gestures, and body posture as well as some more direct approaches.\nFacial Expressions.: Several studies have mapped a user's facial expressions to an avatar or illustration viewable by the person with whom the user is communicating [12, 74] . While these systems do successfully convey the user's emotion, they require a user to pay attention to both the avatar depicting the expressed emotion and the actual text content of the message, which also demand more mental effort of the user. Other studies have taken a different approach, choosing to simply augment the text itself. Filho et al. detected a user's facial expressions using computer vision; when the user was happy or surprised, they added a notification to the conversation thread indicating that emotion [14] . Ali et al. designed Face2Emoji, which took a similar approach but instead conveyed the user's emotion by adding the analogous emoji to the conversation [11] . Our work takes a similar approach to this last work but instead uses gestures to detect the appropriate emoji.\nMulti-touch: Gestures. There has also been a significant effort put toward studying the viability of using tactile interfaces to support emotional expression in CMC. Pirzadeh et al. identified text-based emotional cues in instant messaging and developed a set of analogous multi-touch gestures. However, these gestures were defined by the authors, who emphasized developing gestures that could be performed by users and recognized reliably by the system rather than on developing gestures that intuitively represented the emotional cue [45] . Several studies have explored the use of haptics to augment instant messaging. Rovers and van Essen explored the use of haptics to augment instant messaging, developing haptic instant messaging (HIM), an open framework designed to support the use of hapticons and haptic I/O devices [56] . Shin et al. introduced a tactile emotional interface in which users could send and receive six emotional expressions through vibration [61] . Although these studies were able to successfully augment instant messaging, vibration as input directly conveys emotions.\nBody: Posture. Tan et al. define gestures as the user's body postures that occur in ubiquitous computing environments in order to infer the user's affective state with a system called mASqUE [65] . Wang et al. use animated texts and galvanic skin response (GSR) as methods conveying emotional information in order to encourage online users to interact with each other efficiently [73] .\nOther Methods.: There have been several online messaging systems that have explored more direct means of embedding visual media in existing mainstream online messaging systems. Two such systems include mobile apps React Messenger 1 and SmileChat 2 , both of which perform similarly by allowing users to use their mobile device's video camera to record themselves and then send these recordings as looping animated images. These systems allow users to explicitly convey their emotions within low-bandwidth CMC but do so at the expense of the user's privacy, that can make such a system especially unappealing to use with strangers [59] .\nOther Methods.: In this work, we propose the use of hand gestures to provide users with some of the expressiveness and intimacy characteristic of high-bandwidth CMCs while retaining the privacy afforded by low-bandwidth CMCs. Despite how ubiquitous and expressive hand gestures are [71] , there are relatively few works that explore their utility in enhancing CMCs.\nHand Shape-Based Gesture Recognition: While there has been little work done in exploring the use of hand gestures in relation to CMCs, there has been a significant effort toward recognizing hand gestures. In this section, we cover the most common camera-based approaches to recognizing hand gestures as we take a similar approach in our work.\nRGB Video Camera-Based Approaches.: One approach to hand gesture recognition is using RGB video cameras. These algorithms rely on the color information from video camerasgenerally, low-cost cameras such as commercial webcams-for filtering the hand and fingers from complex backgrounds. For example, work from [8] focused on a survey of implementations that use machine-learning techniques-specifically, artificial neural networks, fuzzy logic, and genetic algorithms-to recognize hand shapes in gesturing. The authors discovered from the survey that, regardless of approach, the majority of researchers relied on fingertip detection in some capacity to improve their approaches to identifying the user's hand in real-time video. Such works include a system that relies on finger tracking and hand pose detection approaches for recognizing 24 different types of hand poses [32] , a system that also includes skin segmentation to supplement hand pose detection and curve fitting for hand motion detection [74] , and a system that relies on a variety of computer vision algorithms and rule-based approaches for recognizing 13 different types of hand poses [26] .\nRGB Video Camera-Based Approaches.: Other hand shape-based gesture recognition approaches using RGB video cameras have focused on the contour of the hand and its convex hull. One work by Hongyong and Youling relied on computer-vision and machine-learning techniques to segment skin color, detect the hand's edges, and perform blob tracking of the overall hand, which was then evaluated by performing hand gestures for manipulating a virtual keyboard application [24] . Another work by Rautaray and Agrawal similarly relied on computer-vision techniques for detecting the hand's contour and its convex hull for recognizing 7 different hand poses [53] .\nRGB Video Camera-Based Approaches.: Several other studies using RGB video cameras focus solely on detecting the hand shape and its different poses. One work focused on a hand shape-detection algorithm that could handle recognition quickly using multiple algorithms (e.g., template matching) and could accommodate recognition for 15 different hand poses [42] . Other works pursued the use of geometric shape descriptors in conjunction with conventional machine-learning and computer-vision techniques to try to provide more sophisticated recognition [58, 68] . Another work took into account the user's head in detection for greater context of the hand's location and was evaluated in a simple application selector program for simple hand poses [1] .\nDepth-Sense Camera-Based Approaches.: With the growing accessibility of commercial depth-sense cameras, which can also sense depth of the user's body and limbs for richer digital interactions, researchers are taking advantage of these sensors to provide more sophisticated tracking of the hands and fingers beyond the default hand location detection that conventional depth-sense cameras provide. Some approaches for detecting such hand shape gestures rely on identifying the contour of the hand and its convex hull. One work relied on hand-contour algorithms to recognize 18 different hand poses [34] ; another work used a hand convex hull algorithm to recognize a small set of gestures that was evaluated in a mobile gaming app [23] ; one other work used a hand-contour algorithm and dynamic time warping to recognize 23 different hand poses [21] .\nDepth-Sense Camera-Based Approaches.: More detailed hand shape gesture recognition approaches have also incorporated detection techniques for other parts of the body to potentially handle more sophisticated hand gesturing interactions. For example, some works have also used finger detection for gestures that have richer finger interactions [35, 48, 54, 75] , while other works have used head detection to handle hand gestures that require the location of the user's head for correct context in the domain of Japanese sign language [15] .\nTrajectory-Based Hand Gesture Recognition: Expanding even further from hand shape-based gesture recognition approaches that specialize more on recognizing static hand poses are trajectory-based hand gesture recognition approaches that explore recognizing dynamic hand gestures and hand motion trajectories. One of the early works for this research area relied on the user wearing a wired dataglove and tracked the user's static hand poses and dynamic hand gestures in a gestural command set called Charade to manipulate interactions for presentation slides [3] .\nTrajectory-Based Hand Gesture Recognition: As computer vision and machine-learning algorithms became more sophisticated for noninvasive detection of a user's hand, researchers began investigating how RGB cameras could be leveraged to recognize their dynamic hand gestures and hand motion trajectory as well. For example, Tran and Trivedi implemented their own skeletal joint tracking on the user's upper body motion; tracking their hands to recognize a set of six dynamic hand gestures [67] , Bhuyan et al. used a variety of features to recognize different hand poses that change the interaction type for when the hand is motioning in mid-air [4, 5] , Mckenna and Morrison used hidden Markov models, moment features, and template matching on users' skin color for modeling hand trajectories [38] , and Singha et al. provided a heuristic-based approach for dynamic hand gestures on 40 gesture classes solely without depth information [62] .\nTrajectory-Based Hand Gesture Recognition: With the introduction and growing ubiquity of reliable depth-sense cameras, the inclusion of depth information has enabled researchers to rely on these sensors for global skeletal joint motion trajectories, such as for the hand. By having these sensors automatically handle the calculations for retrieving these global hand motion trajectories, approaches that use such sensors have focused on more sophisticated dynamic hand gesturing with motion trajectories. For example, Geetha et al. focused on techniques for recognizing more dynamic signs in Indian sign language [16] , Zhu and Yuan focused on techniques that recognize hand gestures with richer motion trajectories for a video game [80] , Ramirez-Giraldo et al. focused on more local hand motion trajectories to supplement the depth sensor's existing automated detection for global hand motion trajectories [52] , and Taele and Hammond leveraged global hand motion trajectories for detecting shapes made on and above touchscreen surfaces [64] .",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Remote collaboration technology can enable a user at a local work place to quickly receive help from another expert user in a remote location. For example, a video call allows the remote guest user to quickly understand the situation of the local host user. In remote collaboration it is important to consider how the local host user can capture and broadcast a view of their surroundings. However, video calls have many limitations such as sharing a small field of view (FOV), limited resolution, or fixing the view of the remote guest user to that of the local host user. To overcome such limitations, it could be more efficient if the remote guest user could immerse themselves in a view of the local host user's environment. Virtual Reality (VR) technology enables this by having the user wear a Head Mounted Display (HMD) that provides an immersive viewing experience with a wider FOV compared to a standard phone or monitor.\nINTRODUCTION: Using VR technology, 360 views of the surroundings can be shared from a panorama camera. Alternatively, other systems allow sharing a 3D reconstruction of a real world scene using a depth sensor and/or photogrammetry. Both of these techniques allow sharing the local host user's environment to a remote guest user, but each has some limitations. Sharing 360 panorama views can provide a high quality view without consuming a large amount of bandwidth but it is a 2D presentation that provides limited depth perception. In contrast, sharing a 3D reconstruction supports depth perception as well as the ability to navigate through the 3D model. However, the quality of 3D reconstruction and the amount of bandwidth required to transfer it are directly proportional to each other. So a high-quality 3D reconstruction of the user's environment would require a significant amount of bandwidth and is difficult to update in real time.\nINTRODUCTION: In this paper, we present a novel Mixed Reality (MR) remote collaboration system (see Figure 1 ) that combines 360 and 3D reconstructions into one. This creates a system that aims to merge the advantages of the individual approaches while minimizing the limitations stated above.\nINTRODUCTION: Compared to prior work, this paper makes a number of novel and significant contributions:\nINTRODUCTION: (1) A novel MR Remote Collaboration technique that merges 360-views and 3D Reconstruction. (2) The first user study that compares 360 live panorama and 3D reconstruction based MR remote collaboration systems. (3) The first user study that explores the benefits and implications of combining 360-view and 3D reconstructed scene into a hybrid MR remote collaboration system.",
    "relatedWork": "RELATED WORK 360 Video Sharing Remote Collaboration: Recently, researchers started to explore how 360 panorama camera can be used instead of a standard camera for remote collaboration. This allows the local host user to capture and broadcast the 360 surrounding view to the remote guest user who could turn their head while wearing the HMD. For example, JackIn Head [16] was a remote collaboration system using 6 cameras constantly capturing videos from different angles and processing them into a 360 high-quality spherical video image to live stream to another user wearing an HMD to view. The system is constructed as a headband that allows easy wearability on the head for the local host user. Tang et al. [37] created a 360 video chat system with a similar set up on the local host user side. In their system, they used a 360 camera on a monopod fixed to a user's backpack to broadcast the 360 surroundings to the viewer watching it on a tablet device. Most recently, the Shared Sphere system [21] used a 360 panorama camera attached to a Microsoft HoloLens [23] to capture and share the user's surroundings. With this system, both local host and remote guest users could look around independently while sharing visual communication cues through MR visualisation.\nRELATED WORK 360 Video Sharing Remote Collaboration: These systems provide easy access to the 360 surroundings of the local host user by the remote guest user, who can look around independently. However, the viewing position of the remote guest user is strictly controlled by the local host user. So the remote guest user will not be able to look at a certain corner of a room or behind any occluded objects by walking closer, unless the local host user goes there.",
    "rq": "(1) how does the remote collaboration medium affect the task performance of collaborative object searching task?\n (2) how does the remote collaboration medium affect the user experience including usability, sense of being together, and motion sickness?"
}
{
    "intro": "INTRODUCTION: 360-degree video introduces many benefits for collaboration compared to regular video chat, especially for remote collaborators [23] . A remote collaborator has total autonomy over the video scene while a local collaborator is no longer responsible for performing tedious camerawork. Widely available and inexpensive cameras like RICOH's Theta make the advantages of 360-degree video face-to-face conversation sticky notes, public displays, etc. letters, email, etc.\nINTRODUCTION: Fig. 1 . The collaboration scenarios targeted by 360Anywhere lie in the \"same time, different place\" quadrant of the Time/Space Matrix, i.e., synchronous, remote collaboration.\nINTRODUCTION: accessible to a wide range of consumers and enable live streaming in mobile settings. However, existing work on potential benefits of using 360-degree video (e.g., [14, 22, 23] ) for collaboration has previously only considered regular interaction via live video, rather than placing annotations-in the form of drawings, text, or images-directly in the 360-degree video stream, which is possible with 360Anywhere. On the other hand, research on remote collaboration with annotations (e.g., [3, 5, 12] ) so far focused on plain video use cases, not taking advantage of the benefits of 360degree video. Tang et al. [23] point out that 360-degree video introduces a potentially problematic asymmetry between the local and remote collaborators and have identified three related, unresolved challenges: (C1: Gaze) because of individual perspectives, it is not clear which portion of the 360 video collaborators are seeing; (C2: Out-of-sync) unlike in 2D video, in 360-degree video collaborators do not necessarily share the same view; and (C3: Gestures) gestures performed by one collaborator may be missed by others not sharing the same view.\nINTRODUCTION: To address these issues, we present 360Anywhere (Fig. 2 ), a 360-degree video-based mobile ad-hoc collaboration framework with support for projective augmented reality (AR). Based on 360-degree input and projective AR, our framework provides a range of configurable components, such as session support, different kinds of annotations, and gaze awareness, from which the collaborators can choose, and enables distribution to a variety of output devices. For this, local collaborators, say in a meeting room, first set up a 360-degree camera connected to a computer and specify its live stream as the input for 360Anywhere in the configuration step. Remote collaborators can then access the live stream showing the meeting room simply via their mobile phones by opening a website provided by the system. With our framework, however, they are no longer restricted to just consuming a 360-degree video, but can now also provide input directly through the stream-in the form of drawings, text, and images-which is then synchronized between all collaborators. In particular, when one or more projections are available, these can be defined within the 360-degree live stream, so that remote collaborators can draw and place text and images directly in the room through projective AR. For instance, local participants in the meeting room set up a projection on a whiteboard. In the 360-degree live stream, in which the projection is now also visible, they click its four corners for calibration. Next, they place a number of designs on the whiteboard and ask remote collaborators for their preference. The remote collaborators can circle their favorites directly in the stream. These annotations appear on the whiteboard in real-time through the projection.\nINTRODUCTION: 360Anywhere: Mobile Ad-hoc Collaboration in Any Environment 9 :3 In an exploratory study structured into three design jams with interaction design Master's students (N=10), we evaluated the feasibility and effectiveness of our 360Anywhere framework for a range of mobile, ad-hoc scenarios. These scenarios are located in the \"same time, different place\" quadrant of the Time/Space Matrix (Fig. 1 ). Participants were divided into three groups and provided with different devices. All of the groups were introduced to three scenarios and asked to reason about possible configurations for 360Anywhere while generating remote collaboration applications for those scenarios. We also elicited additional scenarios in which they deemed different features of the framework particularly useful compared to existing solutions. Results indicate that the above challenges can be solved using our framework and that users are able to rapidly design and deploy 360-degree collaboration setups.\nINTRODUCTION: With 360Anywhere, this paper makes three primary contributions:\nINTRODUCTION: (1) To support synchronous, remote collaboration in any environment, we provide a framework that extends 360-degree video with a set of multi-user awareness features and annotation tools (Sec. 3.1), and support for mapping 360-degree video annotations back to projections in the real-world environment (Fig. 4 ). (2) To support a variety of collaboration scenarios and user needs (e.g., 1-to-1, 1-to-many, manyto-many), we build on a component-based architecture (Fig. 3 ) and identify and implement the minimal set of required framework components (Sec. 3.2). (3) To support a range of configurations and mobile ad-hoc collaboration scenarios, we explore the power and flexibility of our framework through a user study with 10 participants and three scenarios in a dynamic setup. Our evaluation shows that 360Anywhere: a) can mitigate existing challenges with 360-based collaboration (e.g., gaze awareness); b) is feasible for use with mobile devices in ad-hoc scenarios; and c) can be configured for various collaboration scenarios.\nINTRODUCTION: The technical contribution of our work is the 360Anywhere system. In line with Olsen [17] , we show that 360Anywhere addresses unresolved challenges, reduces solution viscosity, empowers new design participants, and leverages power in combination. While out of scope here, subsequent work may explore the impact of systems like 360Anywhere on users' collaborative behavior.",
    "relatedWork": "RELATED WORK: The research presented in this paper combines work from three so far independent research streams-projective augmented reality, 360-degree video, and remote collaboration. We present a representative sample of related research and point out the differences to our proposed solution.\nProjective Augmented Reality: Our work on 360Anywhere adds to existing research on smart rooms and multi-display environments, e.g., [2, 6, 8, 18, 20] . Most closely related is the set of research projects carried out by Wilson and Benko on projective augmented reality. With LightSpace, Wilson et al. [25] describe a room equipped with multiple depth sensors and projectors that allow physical surfaces to be turned into interactive displays. In [9] , Jones et al. introduce IllumiRoom, which extends the space around a television by augmentation in order to complement gaming and other entertainment experiences. The RoomAlive project [10] builds on a set-up with multiple projector-depth camera units to map a whole room and turn it into an immersive entertainment experience. MeetAlive [4] is a room-scale omni-directional display system involving multiple depth cameras and projectors that lets users share content from their computers at any place in the room that has been equipped. Anyone participating in the meeting can then interact with all of the shared content through a perspective-corrected mouse cursor.\nProjective Augmented Reality: While these systems are impressive in their ability to transform physical rooms into interactive multi-display environments, they all have in common that they require a good amount of calibration and instrumentation of user and/or environment, and are therefore not designed for mobile and adhoc collaborative settings. With our work on 360Anywhere, we were prepared to trade complexity for flexibility and aimed at a lower fidelity solution. We want to enable end-users with an easy and quick set-up with minimal and affordable hardware requirements, namely a 360-degree camera and a mobile projector, and in this way, rather than instrumenting the room itself with interactive capabilities, we provide a virtual smart room in the 360-degree stream.\n360-degree Video: As previously mentioned, 360-degree video is receiving increased attention in research where it has so far been studied primarily as a medium to consume immersive media content. For example, in recent work by Lin et al. [14] , two techniques are described for directing a viewer's focus to certain points of interest in a 360-degree video: autopilot and visual guidance.\n360-degree Video: On the other hand, Tang et al. [22] started to investigate how multiple users can effectively view 360-degree videos together in a guided tour scenario. Another project by Tang et al. [23] that we already mentioned in the introduction explored how pairs of users collaboratively experience environments as they are being live streamed from one participant to another via a 360-degree camera. Tang et al. identified several challenges, the most important ones of which include gaze awareness, out-of-sync views, and missed gestures.\n360-degree Video: In terms of gaze awareness, there is research that looked at possibilities to achieve bidirectional gaze awareness, e.g., based on regular webcams [13] . In ParaFrustum [21] , Sukan et al. explored visualization techniques designed to guide users to a specific set of viewing positions and orientations in virtual or augmented reality interfaces.\n360-degree Video: While these works investigated 360-degree video and, to some degree, explored the benefits and trade-offs for collaborative scenarios, there is no prior system that addresses the issues in a comprehensive manner. Motivated by this lack of support, with 360Anywhere, we wanted to explore how to best enhance 360-degree live streams for remote collaboration in augmented reality. Our framework incorporates components for remote collaboration that the above systems lack.\nRemote Collaboration: In research on remote collaboration, one of the most important requirements is support for remote annotations. A number of recent papers [3, 5, 12] investigated remote annotations in mobile scenarios, with a particular focus on scene exploration and how to enable the remote collaborator to provide stable annotations with shaky camera feeds. While these papers provide valuable input for parts of our implementation when it comes to freezing the live feed upon input, they do not consider support for 360-degree video or projective AR and the additional issues this introduces.\nRemote Collaboration: There is existing work on remote collaboration in augmented reality that is motivational to 360Anywhere. For example, Project Chalk by Vuforia [24] provides tools for remote assistance in ad-hoc scenarios based on a mobile app, so that users can assist remote collaborators with their regular, everyday smartphones.\nRemote Collaboration: Skype on HoloLens [15] takes a similar approach, where a remote collaborator with, e.g., a tablet PC can annotate the environment of the HoloLens user by drawing on walls and tables, among other things. This solution provides AR capabilities similar to 360Anywhere, but is limited to 1-on-1 calls. Also, it introduces the exact problems with standard, non-360-degree video collaboration described by [23] (i.e., remote collaborators have little autonomy and the HoloLens user is responsible for framing objects, effective overviews, steady camerawork etc.).\nRemote Collaboration: 360Anywhere: Mobile Ad-hoc Collaboration in Any Environment 9:5 To give a last example, Sketchboard.io is designed to be a collaborative sketching tool. However, compared to 360Anywhere, it does not feature video collaboration or AR capabilities for annotating an environment.\nRemote Collaboration: Most existing annotation tools have in common that they build on 2D annotations drawn on a virtual or physical screen. Sodhi et al. [19] investigated possibilities to provide 3D and spatial input for remote collaboration. However, this solution has additional hardware requirements in the form of a depth camera that we considered to be too limiting from a practical standpoint. In 360Anywhere, local collaborators can draw directly on physical whiteboards and remote collaborators can annotate a 360 live stream while the system deals with handling the live stream annotations on spherical video and perspective projection back into the physical world.\nRemote Collaboration: Other research has, e.g., looked at recreating parts of an environment from a wearable camera for a remote observer [1] and collaborative AR for mathematics and geometry education [11] . Yet, such solutions have specialized use cases and therefore limited flexibility and require sophisticated hardware set-ups. Also, they do not leverage the advantages of 360-degree video for their existing telepresence or AR solutions.\nRemote Collaboration: Informed by this existing research, we present a first framework that integrates 360-degree video with projective augmented reality. Overcoming many of the shortcomings of existing systems with remote annotations of 360 live streams and tool support to address the challenges of gaze awareness, out-of-sync views, and missed gestures, 360Anywhere enables a variety of multi-user scenarios with local and remote collaborators in an ad-hoc manner.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Whether from car accident, gunshot wound, or traumatic injury due to natural or man-made disasters, trauma often requires timely life-saving interventions; however, the expertise required to perform these actions is not ubiquitously present even within frst world medical care systems. In situations where medical expertise or facilities are not available locally, surgeons rush to perform lifesaving interventions that stabilize patient vitals before transferring the patient to a specialized surgical facility. When local surgeons lack the expertise to perform these life-saving interventions, they rely on remote guidance from expert surgeons through telementoring. Unfortunately, current telementoring systems limit how remote experts can guide and collaborate with local, inexperienced surgeons.\nINTRODUCTION: Telementoring is the use of information systems to provide realtime remote guidance to an inexperienced surgeon from an expert surgeon [4] . Today, telementoring systems typically require expert surgeons to instruct novice surgeons by annotating a live video feed of the surgical environment. Remote experts have to map actions they would normally express through gestures and actions into limited video-based interactions and verbalization. Novices operating on the patient have the extra burden to map instructions from a nearby screen to the operating feld, with increased possibilities to make mistakes in the process [8] .\nINTRODUCTION: Mixed Reality (MR) technology enables the creation of remote collaboration experiences where participants can interact with remote collaborators more naturally [6] . The space of remote collaboration in MR has seen a lot of promising works with a focus on training [11, 20, 58] and, more recently, also on telementoring projects [4, 52] .\nINTRODUCTION: In this work, we build upon existing works and take a humancentered approach to designing a surgical telementoring system. Through role-playing sessions with US Navy surgeons and iterative prototyping, we gain insight into the features of MR that are useful in surgical and collaborative applications. Based on the communication needs of expert surgeons, and on novice surgeons' constraints, we designed ARTEMIS, a collaborative mixed-reality system for immersive surgical telementoring.\nINTRODUCTION: ARTEMIS is the frst surgical telementoring system to give experts an immersive VR operating room where they can use gestures and 3D annotations on a 3D reconstruction of the patient's body to guide novice surgeons in Augmented Reality. ARTEMIS intuitive AR experience allows local surgeons to focus on the patient and the AR interfaces.\nINTRODUCTION: Overall, this paper introduces three key contributions to the human-computer interaction, surgical innovation, and user interface software and technology research space:\nINTRODUCTION: \u2022 ARTEMIS's design process, including software artifacts that facilitated our technology exploration \u2022 The development of ARTEMIS: a real-time, mixed reality collaborative system for telementoring surgical procedures \u2022 An initial qualitative evaluation of ARTEMIS through mannequin and cadaveric subjects tasks at hand, rather than having to interact with complicated",
    "relatedWork": "RELATED WORK: ARTEMIS is inspired by previous works that use Mixed Reality as a medium for remote collaboration. This section summarizes recent advances in Mixed Reality and telementoring. Throughout this paper, we use the term Mixed Reality to represent the the spectrum of technologies ranging from Augmented Reality to Virtual Reality [39] .\nCollaboration in Mixed Reality: Studies show that participants collaborating on physical tasks over video usually under perform relative to participants that are collaborating side-by-side [28] . Unlike video systems, collocated collaborators can see and understand spatial relationships between each other, the task, and the environment. They use shared visual information to communicate more efectively through a combination of verbalization, gestures, and actions [21] .\nCollaboration in Mixed Reality: The continuous development of Mixed Reality input and display technology has been addressing the limitations of video communication by introducing support to key components that make sideby-side collaboration efcient: hand gestures [2, 49] , sketches [23] , annotations [11, 20, 38] , real-time representations of a remote environment [12, 30, 46, 57] , gaze [1, 12] , shape of remote objects [30] , collaborators as avatars [50] , virtual replicas of task object [41] , haptics [13] , and more [32, 58] .\nCollaboration in Mixed Reality: These works provide a range of contributions, from enabling technology [46] to user evaluations of new interfaces [41] to new interaction modalities [50] . They approach remote collaboration from artifcial domains, environments, tasks, and people in the environment to uncover general principles, but, as pointed out by Ens et al. [16] , we need a more specifc distillation of these principles in other to apply them to a specifc domain (in this case, trauma surgery). For example, both remote laparoscopic procedures in practice [26] and studies like the ones by Fussell et al. [18] are based on a system where a mentor can annotate a live video of the mentee's task space; yet, communication in the robot-building task proposed by Fussel et al. [18] shares little resemblance to what is communicated in laparoscopic surgery [17] . Thus, while the interface in [18] leads to faster performance and better coordination between mentor and mentees, it can lead to mistakes and misunderstandings in a laparoscopic environment [53] .\nCollaboration in Mixed Reality: In our work, instead of adapting physical-collaboration systems such as Microsoft Remote Assist [38] and LOKI [58] to the surgical domain, we take a step back to better leverage the existing remote collaboration literature, and understand its limitations and unique uses in surgery. We involve surgeons in the design process and explore the use of diferent enabling technologies (including Microsoft Remote Assist) through a design exploration grounded in participatory design techniques such as role-playing [56] and body-storming [47] .\nTelementoring in Mixed Reality: The domain of telementoring has greatly benefted from advances in telecommunication technologies [40] . In a conventional telementoring scenario, a senior or \"expert\" surgeon coaches a novice surgeon through a video stream. A common drawback for this conventional telementoring approach is that it requires novice surgeons to memorize the instructions (e.g., the position length and orientation of annotations) received on the display to perform them on the patient. This continuous focus in shift from patient to nearby tablet and back causes cognitive overload and potentially surgical errors [7] .\nTelementoring in Mixed Reality: Similar to the general remote collaboration literature, the introduction of Mixed Reality in telementoring aimed at overcoming drawbacks of conventional telementoring. STAR (2016) [3, 4] uses a tablet display between the novice surgeon and the patient body and prevents unnecessary focus shift by showing instructions directly over the surgical feld. However, these tablet displays can only represent a fat image (no depth or 3D cues); thus, more recently, a new version of STAR (2018) [31, 51, 52] was introduced to provide novices with a head-mounted display so that they can have stereo view (depth cues) of annotations and instructions on the patient body. Situated instructions are known to increase procedural adherence as well as decrease execution times [25] . A key limitation of all STAR systems is that the expert surgeon sees and annotates the patient view on a fat monitor. These annotations then sit fat onto the patient as the expert surgeon is unable to give them a custom depth, required for example, to show the depth of an incision.\nTelementoring in Mixed Reality: As we detail in the next section, surgeons annotating a 3D body need 3D annotations that they can shape freely with their hands. ARTEMIS builds on previous research to combine not only a novel 3D(expert)-3D(novice) annotation interface, but also to introduces non-verbal communication channels that bring telementoring closer to situated mentoring (e.g., gestures) and even improve upon it (e.g., procedure videos).",
    "rq": " 1) how do mentors guide mentees?\n 2) how much information does the mentor need from the mentee side?\n 3) what do mentors want to show to mentees?\n this section answers the following questions: what would help you best when guiding a novice surgeon?"
}
{
    "intro": "INTRODUCTION: The development of accurate sign language models has the potential to improve access for millions of signers. Many people use sign languages worldwide, including about 70 million deaf people who use a sign language as their primary language [30] , as well as sign language students, and friends and family of deaf signers. There are many applications that accurate machine learning models could enable for sign language users, including personal digital assistants that respond to signed commands, automatic sign language transcription services, and automatic translation between a signed and spoken language.\nINTRODUCTION: However, no existing sign language recognition or translation systems are robust enough for real-world adoption. The primary obstacle is lack of sufcient real-world training data. Existing datasets primarily consist of videos recorded in-lab, with a small set of homogeneous signers. They are also small in size (e.g., state-of-the-art datasets consisting of about 100,000 signs compared to 5 million spoken or 1 billion written words [1] ). Many datasets also sufer from lacking or inaccurate labels. As current sign language recognition software cannot provide accurate labels, labeling is a high-skilled, labor-intensive task. As a result, recognition and translation models trained on these datasets do not work well in real-world settings, which requires generalization to diverse settings and diverse signers.\nINTRODUCTION: At the same time, there are few sign language-based entertainment and education resources. Existing digital resources are almost exclusively built for people who use a spoken or written language. For example, informational resources including search engines, encyclopedias, dictionaries, and written publications all almost exclusively support interacting with the material through written queries and read text. Similarly, entertainment and social resources, such as online games or forums typically involve written or spoken language. As sign languages are not spoken, and do not have a standard adopted writing system, these resources generally fail to include sign language users. In rare cases, a sign language interpreter may be hired so that these resources may be accompanied by signed interpretations. Creating more sign language resources is important both for providing access to people whose primary language is a sign language, but also for supporting people who are learning one.\nINTRODUCTION: In this work, we present ASL Sea Battle, a sign language smartphone game prototype designed to collect and label real-world sign language videos. The game serves two primary purposes: 1) creating real-world sign language corpora, while also 2) providing a fun and educational resource for sign language users. The game is based on the traditional game of battleship, where two opponents hide ships on two grids, and take turns guessing cells where ships might be hidden. Instead of referencing a cell by row and column number, each cell in ASL Sea Battle is labelled with a sign. A player attacks a cell by recording a video of him/herself executing the desired label. By recording videos on their smartphones in their daily lives, players provide real-world videos of themselves signing. Their video is sent to their opponent, who unlocks the cell by tapping it to match it with the incoming video, which seamlessly provides a label for the opponent's video. In this way, ASL Sea Battle not only generates videos of people signing, but also labels them.\nINTRODUCTION: To evaluate the feasibility of using ASL Sea Battle to collect and label real-world datasets, we conducted a user study comparing collection through the game to collection through a traditional, straightforward collection app. We analyzed the quality of the data and labels collected, and feedback from participants on their experience. Our results suggest that ASL Sea Battle can be used to sustainably collect high-quality real-world sign language videos, and provide accurate labels. It can also provide players with entertainment, education, and social connections. However, these benefts come at the cost of slower data throughput rates, as the game incurs delays between turns, for example as players strategize.",
    "relatedWork": "BACKGROUND AND RELATED WORK: In this section, we provide background on the required qualities of sign language datasets used for training machine learning models, the limitations of existing datasets, and the landscape of existing sign language resources and games with a purpose. To this space, we add the frst game designed to collect and label sign language videos, while simultaneously providing the benefts of fun and connection to the signing community.\nSign Language Datasets: In order to successfully train models for the desired application(s), sign language datasets need to have several properties. There should be many examples of each sign, and signs should be labelled in a machine-readable, consistent format. This is a challenge, as there are no widely-used conventions for notating signs. As a result, lemmatizing-determining whether two sign productions are examples of the same sign or diferent signs-is not trivial. The training data also needs to match the videos that will be supplied in the fnal application. This means having a diversity of signers: diferent people, of diferent skin tones and body types, who use diferent regional and sociolinguistic varieties of the sign language, and have diferent skill-levels in the sign language. The data would also need to be diverse with respect to the flming conditions: including a range of camera angles, lighting conditions, backgrounds. If the end application is a phone app, the dataset will need to include videos of people signing while holding a phone, which can signifcantly change how signs are produced. Signs produced in isolation are likely insufcient for training models of continuous signing and vice-versa, as sign production in context is markedly diferent from production of single signs.\nSign Language Datasets: While there are datasets of sign languages that meet some of these criteria (e.g., there are large quantities of signing videos that are unlabelled, and there are sets of labelled videos of people signing that are not diverse and/or are relatively small), there remains a dearth of large-scale video datasets that meet enough of these criteria to adequately develop real-world sign language technologies [1] . For example, state-of-the-art sign language datasets include corpora of small numbers of homogeneous sign language interpreters (e.g., [14, 15] ), other corpora with small sets of signers (i.e. < 15) (e.g., [29, 46] ), and poorly labelled videos of unverifed quality scraped from online sources (e.g., [24] ). In this work, we explore the possibility of gamifying sign language video collection and labelling, to help address the lack of appropriate real-world data.\nSign Language Resources: Currently, existing digital resources for ASL and other signed languages are extremely limited, and digital sign language games are virtually non-existent. Existing resources are comprised of a small number of dictionaries (e.g., [5, 16] ), educational materials (e.g., [10, 12, 21, 43] ), lexical databases ( [6, 20] ), and mobile vocabulary apps (e.g., [27, 28] ). However, the landscape of existing digital sign language resources pales in comparison to the rich landscape of resources for spoken/written language users, who are typically considered by default. Existing research on how well existing apps meet DHH needs also suggests large room for improvement [33] .\nSign Language Resources: Tools and resources that involve machine learning or experimental interfaces are almost always research projects with limited real-world viability. For example, attempts have been made to create browser tools that provide signed translations of written content [23] , to create signing avatars ( [13, 37, 37] ), and to more generally creation recognition and translation systems (e.g., [4, 9, 9, 31, 45] ). Research projects have also tackled educational aspects, for example proposing a learning game for DHH children and families [8] , an app for learning fngerspelling [35] , and data-collecting fashcards to help students learn vocabulary and identify sign features [3] . Our game adds a resource for both fun and education to this landscape. The sparse landscape also provides an opportunity for our game to achieve higher adoption and thereby collect a larger corpus, due to reduced competition with other apps and games.\nGames with a Purpose: Games with a purpose have been proposed in other domains, some with great success. For example, researchers have proposed games to crowdsource protein folding [11] , to label audio [26, 36] and images (including for accessibility purposes) [38, 39, 41] , and to collect common-sense knowledge [40] and user preferences [18] . Several of these games have achieved millions of users, demonstrating the viability of gamifcation as a technique for scaling human contributions. Accessibility of gaming interfaces has also been explored (e.g., [17, 44] ). Most existing games that curate data focus either on collection or labelling, and our game provides both. It is also the frst game with a purpose serving sign language users. More generally, \"organic crowdsourcing\" [25] refers to genres of data-collection methods that provide non-monetary benefts to users. In addition to gamifcation (described above), other examples include citizen science [22, 34] , where people are incentivized to contribute by a desire to help advance science, and systems that incentivize contributions by providing people with personalized insights [32] . Crowdsourcing provides the opportunity to break tasks down into small \"microtasks\", which have been shown to take longer to complete, but can result in higher quality results by enabling people to make use of smaller units of available time [7] . ASL Sea Battle is designed as an asynchronous mobile game, to enable people to take advantage of small units of available time to contribute and have fun.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Going to sleep and waking up together is an everyday yet intimate experience for partners living together. According to the 2013 international bedroom poll conducted by the National Sleep Foundation [1] , 89% of people in the United Kingdom living with someone sleep together with their significant other on most nights (82% in USA, 85% in Canada, 79% in Mexico, 87% in Germany, 63% in Japan). Sharing of this time and space among partners separated geographically -in a long-distance relationship (LDR) -is, however, limited to voice/video calls and messaging.\nINTRODUCTION: Previous work showed that people in LDRs commonly used SMS texting for short greetings such as \"Good Morning\" or \"I love you\". In [2] , 6 out of 14 participants reported that they would watch their partner fall asleep, or vice versa over Skype or Google Chat, despite having issues with location, stability and overheating of their devices, so there is clearly an opportunity to address these needs of remote couples.\nINTRODUCTION: Furthermore, as highlighted by the Covid-19 pandemic, the need to self-isolate can introduce situations whereby people who normally co-habit may find themselves needing to avoid physical contact with family and loved ones for weeks or even months. This is especially the case for healthcare workers who wish to shield their families from the risk they face, as well as those with particular health concerns. For these groups, the experience of living with loved ones is transformed to one that shares some similarities with those in LDRs.\nINTRODUCTION: We present the design, implementation and evaluation of the Bedtime Window system, connecting partners in LDR with the aim of enabling them to share bedtime, the time before going to sleep and after waking up, and therefore improving communication and intimacy in long-distance couples.\nINTRODUCTION: The main contributions of this paper are as follows:\nINTRODUCTION: 1. Exploration of bedrooms as an environment for interactive technology. We show that even systems with full-featured display and touch interfaces can become invisible and desirable in bedrooms. 2. The first system for LDR couples that allows them to share bedtime, falling asleep and waking up. We show that partners value spending this time together, and hence identify a gap in the available technology in its ability to facilitate sharing of bedtime between partners. Our system is unique in specifically targeting this scenario. 3. Evaluation of a slow photo-stream as a balance between privacy and remote presence. We show that the qualities of always-on video channels are achievable with a slow photostream. 4. We introduce a collaborative drawing experience in the home environment for everyday communication. Disappearing, shared-layer real-time inking was found to be useful, versatile, and engaging communication channel, that may have many other applications. While this paper is focused on the bedtime scenario, our findings can also have relevance for other domains that could potentially benefit from the interactivity and feeling of connectedness, but also the privacy afforded by Bedtime Window. This includes domains such as remote healthcare, social services, and remote education, subject to further research in those areas.",
    "relatedWork": "RELATED WORK 2.1 Long-Distance Relationships: Long-distance relationships are fairly common -many individuals were in a LDR at least once (75% in USA in 2017 [3] , 54% in Germany in 2014 [4] ) and the number is still increasing [5] , which is attributable to the continual advancement of travel and communication technologies [6] , as well as job market and globalization forces [5] . Despite popular beliefs, partners in LDR can be as satisfied as partners in normal relationships, if properly maintained [7] . Canary & Stafford define relational maintenance behavior as actions and activities used to sustain desired relational definitions and suggest that sharing activities play important role in maintaining relationships [8] .\nRELATED WORK 2.1 Long-Distance Relationships: Many prototypes and designs for relationship maintenance were published in the past, from remote hugging [9, 10] , touch [11, 12] or kisses [13, 14] , to sharing heart rate [15] or glasses [16, 17] , from abstract, multimodal interaction [18] to single bit of communication [19, 20] . One system that shares some aspects with our work is Pictures' Call by Pujol & Umemuro [21] , which takes up to 7 pictures at different times of the day at random intervals, and sends them to the remote system, which is continuously showing a slideshow of up to the last 20 pictures received. Users cannot see when or what pictures they are sending, but they can delete any in the past 2 hours for which the transfer is delayed, and they can also ink on received pictures and send them back.\nRELATED WORK 2.1 Long-Distance Relationships: The main difference of our system is in specifically target bedtime sharing, i.e. connecting partners when going to sleep and waking up. Our system also employs a situated interactive display with synchronous shared inking, and a transparent slow photo-stream.\nRELATED WORK 2.1 Long-Distance Relationships: Outside of academic research, several commercial mobile apps have targeted couples in LDR in the past, including Between, Couple, Twyxt, Couplete and others. These apps were primarily designed for memory curation and relationship organization, offering features like shared calendars and lists, timelines of memories, and dedicated text messaging. In contrast, our system foregrounds ephemerality and \"being together\" through a situated device.\nAlways-On Channels: Always-on or at least always-available media spaces have been studied for decades in the context of work environments and sharing workspaces [22] , summarized by Harrison et al. [23] . Pang et al. looked into creating an always-on channel between workspace and personal space [24] and substantial research also went into connecting homes and families, across generations [25] [26] [27] , living apart [28, 29] . A good summary of this work can be found in the Connecting Families book [30] .\nAlways-On Channels: Judge et al. designed a Family Window to connect part of families living separately [31] . This work was later extended to more than two connected homes through Family Portals [32] , and to multiple cameras and/or displays [33] . All these systems are summarized in [34] . The Family Window, offering video and back and forth ink messages is the closest system to ours, albeit targeting a different environment and users.\nBedrooms: HCI research in the bedroom environment is still very sparse, although it seems to be getting some traction. Wan prototyped a situated shopping experience from the bedroom [35] , and Odom looked at virtual possessions in the context of teen bedrooms in a lab study [36] . Beds themselves have been used as a medium for intimate communication in the past [37, 38] , with recent focus on pillows [39, 40] , but these channels always conveyed some secondary information about the partner, a physiological state or their activity. We are not aware of any prior work where partners in the bedroom could communicate directly to each other for prolonged periods of time.\nPrivacy: According to a survey of private moments in the home by Choe et al., the bedroom is the most frequently mentioned place where people do activities that they don't want to be recorded [41] . Hindus et al. found even a simple presence light based on remote activity to be perceived as a surveillance device that threatened home privacy despite conveying minimal information [42] . However, as Neustaedter et al. stated [43] , video use in homes cannot be stopped; we can only identify and try to resolve the related problems.\nPrivacy: Previous research into always-on video media spaces in homes found the transmission of audio to be more privacy-invasive than audioless video [44] . For video streams, various approaches have been explored to provide some level of privacy to users. This includes image processing filters such as pixelization or blur [45] and mechanical metaphors such as blinds or curtain overlays [31] . However, the usage of these was found to quickly cease with time [31] .\nInking: Much research has been devoted to ink input in the work environment, especially for document annotation and review, much of which has been summarized in a survey paper by Sutherland et al. [46] . In the home environment, inking has been used for sharing notes in households [42, 47] as well as for remote communication [32, 48] .\nInking: Research into collaborative drawing focused only on the work environment so far, with the pioneering prototypes of VideoDraw by Tang & Minneman [49, 50] and several ClearBoard iterations by Ishii et al. [51] [52] [53] using the metaphor of 'talking through and drawing on a transparent glass window'. Similar experience were supported by DigitalDesk [54] and PlayTogether [55] , where remote drawing was overlaid on paper using a projector. In all these cases, each participant had their own inking layer that no one else could interact with.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Community policing is a proactive approach that empowers residents to participate in preventing crime and maintaining order, enlisting them as partners with the police in activities such as neighborhood watch and community patrolling [7, 19, 54] . In recent years, information and communication technologies (ICT) have been increasingly adopted to foster community policing. Online communities and social media such as Twitter and Facebook, for example, have been used to share information, build relationships with the police, and take collective action [20, 22, 33] . Mobile technology has enabled novel forms of community policing with crowdsourced sensing, allowing community members to collaboratively share local safety issues such as crime and suspicious activities with mobile sensing technologies such as GPS data and cameras [5, 25, 53, 56] . In addition, connected devices-home surveillance cameras and vehicle dashcams, for example-equip people to capture street videos as digital evidence and then share for collaborative neighborhood watch [8, 45] . These novel kinds of community policing activities with mobile devices generally belong to the body of research about mobile crowdsourcing and crowdsensing in the field of HCI [47, 59, 60] .\nINTRODUCTION: Previous studies of mobile crowdsourcing and crowdsensing for community engagement have focused primarily on designing methods to effectively/accurately perform tasks online [29, 45, 59, 60] , and on encouraging user contribution and motivation (for example, situated crowdsourcing and financial incentives) [27, 44, 47, 57] . Less attention has been given to designing and assigning tasks based on participants' spatio-temporal mobility patterns and contextual factors, and encouraging community members' participation beyond the well-known incentives in mobile crowdsourcing and crowdsensing.\nINTRODUCTION: In this paper, we consider communitysourcing [21] -an alternative crowdsourcing (or crowdsensing) approach requiring specific knowledge in a community-for community patrolling. Unlike the existing prior studies on mobile crowdsourced community engagement systems [28, 29, 45, 59, 60] , communitysourced patrolling requires considerable physical efforts (i.e., walking/driving) and local knowledge/experience [44, 47] for collecting local issues with mobile devices and sharing the local issues with community members or security services.\nINTRODUCTION: Our goal is to design a communitysourced patrolling campaign that allows community members to schedule and execute their location-based tasks and that leverages social media to share local safety and public order issues. Towards this goal, we explore following three research questions:\nINTRODUCTION: (1) What are the implications in designing a communitysourced patrolling campaign? (2) How to design a patrolling task for a communitysourced patrolling campaign? (3) How do people perform patrolling tasks in a communitysourced patrolling campaign, and why?\nINTRODUCTION: To deploy communitysourced patrolling, we targeted a university campus because university students are digital natives and mature enough to participate in policing activities. A large university is also the proper community size to conduct a field trial of a community patrolling campaign. We first performed a user study to understand students' perceptions and experiences of local problems and their mobility patterns to assess possibilities for communitysourced patrolling. We then designed and built a CampusWatch campaign system that provides a suite of tools: task scheduling, task execution with mobile video capturing, and online sharing. Finally, we conducted a four-week field trial at a large university in South Korea. Our field trial aims to answer the following questions: 1) how do people set task schedules? 2) what events are captured and reported? 3) what factors affect their task selection and execution? and 4) what would affect participants' motivation and long-term engagement?\nINTRODUCTION: Our study results showed that, in communitysourced patrolling, participants have strong spatialtemporal preferences; task selection tended to be opportunistic. Task selection and execution were influenced by several factors, including uncertainty and busyness of work schedules, tiredness and fatigue, social contexts, and environmental conditions. Leveraging diverse motivations and the fostering an online community are required for proactive user engagement. Detailed guidelines and policing authority can enhance participant's engagement. We discuss how our findings can be leveraged in scheduling micro-tasks, sustaining user engagement, fostering online communities, establishing guidelines and authority, and addressing privacy concerns. \nCommunity Policing Systems: Recent advances in smartphones and the Internet of Things (for example, intelligent connected cameras) make possible novel community policing services. Smartphones are widely used, not only to share local crime-related information but also to allow people to report any safety issues in real time (see, for example, SALUS [25] , CrowdSafe [53] , S4S [56] , ComfortZones [5] ). SALUS [25] visualizes crime related information (crime maps and safety tips) and allows users to report crimes in real time. CrowdSafe [53] recommends safe paths on a map by incorporating crime data into routing algorithms. To promote campus safety, S4S [56] supports text-and voice-based incident reporting and personalized notification delivery (for example, preferred paths and places). ComfortZones [5] is a mobile service designed to mitigate fear at night in urban locations by allowing users to capture and share location-based safety or comfort attributes and provide social support and interaction.\nCommunity Policing Systems: Brush et al. [8] explored the use of connected surveillance cameras with video sharing to realize the concept of digital neighborhood watches. They also considered potential privacy concerns related to video sharing. This kind of digital neighborhood watch can be extended to vehicular scenarios if vehicles are equipped with advanced sensors, including dashboard cameras [45, 46, 55] .\nCommunity Policing Systems: Our work also focused on community policing where the public gets involved in various activities of prevention, problem-solving, and law enforcement, but differs from such prior studies in that we designed a novel form of communitysourced patrolling campaign with mobile technologies for safety and order. Communitysourced patrolling requires physical efforts such as walking/driving through the community and sharing suspicious activities with community members or security services. We then allowed community members to freely schedule their contributions based on their personal contexts or preferences to ensure effective, high spatio-temporal coverage. In addition, we enabled community members to actively collect video evidence (location tracking and video recording) for safety issues and public disorders using their smartphones, and provided opportunities while patrolling to deal with issues such as calling security staff or distributing guidance notes, not just sharing information online.",
    "relatedWork": "",
    "rq": "(1) what are the implications in designing a communitysourced patrolling campaign?\n (2) how to design a patrolling task for a communitysourced patrolling campaign?\n (3) how do people perform patrolling tasks in a communitysourced patrolling campaign, and why?\n 1) how do people set task schedules?\n 2) what events are captured and reported?\n 3) what factors affect their task selection and execution?\n and 4) what would affect participants' motivation and long-term engagement?\n(1) how did our participants set their task schedules?\n(2) what events were captured and reported?\n(3) what factors affected their task selection and execution?\n (4) what would affect participants' motivation and long-term engagement?"
}
{
    "intro": "INTRODUCTION: Consider the following scenario: \"Beth, the only woman on a team of developers, always looks for an appropriate moment to interject during a staff meeting. But whenever an opening arises, louder voices quickly seize the opportunity to fill the silence. Beth's voice is drowned away, and she is rarely, if ever, heard. Even though Beth is by nature a confident and assertive person, her group members label her as shy and unassuming. \"\nINTRODUCTION: The above scenario reveals how slight differences in preference for conversational mechanics affect which ideas are heard and how people are judged [50] . These behavioral dynamics are common, and if not addressed properly, can impede team productivity. Thus, analyzing and reflecting on these dynamics may positively impact productivity and foster positive and respectful relationships toward other group members. In this paper, we explore that possibility using a video conferencing platform. Despite the importance of understanding group Equal contribution as the first author 160:2 \u2022 S. Samrose et al. dynamics in a video-conferencing environment, it has not been studied extensively in the past. No existing videoconferencing platforms-including Skype, Google Hangouts, GoToMeeting, WebEx, ooVoo, and others-allow easy access to an API to automatically capture the audio and video feeds.\nINTRODUCTION: What if, with participants' consent, we could automatically analyze their audial and visual data and offer intelligent analytics on conversation dynamics? Imagine the questions we might answer. Did everyone participate equally, or was it a one-sided conversation? What was the overall emotional valence? Did people talk over one another? Did they share smiles? How did participants act toward one another?\nINTRODUCTION: Due to the proliferation of smart phones and tablets, video conferencing is gradually becoming a popular alternative to traditional face-to-face meetings, which require travel, space, and other advance planning. One survey reports that 76 percent of decision-makers use and recommend video conferencing without hesitation [3] . Ninety-six percent of participants surveyed agree that video conferencing increases average attention span from 23 minutes (on regular calls) to 35 minutes. It can also, however, reduce attention devoted to nonverbal cues [57] . Virtual meetings have introduced new challenges. But they have also opened up opportunities for computer-mediated feedback to address their shortcomings.\nINTRODUCTION: Another challenge is translating the raw, sensed data into insights that are helpful to participants. The effectiveness of a group discussion depends on many parameters, such as those related to group type (one-time vs. ongoing groups), those specific to individual participants (personality, background, etc.), those related to the topic(s) of discussion (open-vs. set-agenda plans), and more. Some groups may consider the outcome of the discussion the most important factor-in a business meeting, for instance-while other groups may focus on strengthening cohesion among team members. Optimal participant behavior may be difficult to establish, as it hinges on both group dynamics and the desired result. In this paper, we present a fully-automated, online collaboration platform: Collaboration Coach (CoCo). CoCo allows multiple users to video chat and receive feedback on group dynamics as well as individual communication skills. The CoCo platform includes both a video conferencing system and a virtual feedback assistant. The conferencing system allows us to capture participants' audio and video feeds. Before a video conferencing session, participants agree to enable their webcam and to have the conversation's audio and video data transmitted to our server. Using the video feed, we analyze facial features to determine the aggregate emotional valence of participants, participants' attitudes toward one another, and how often users shared a smile with other members. Using the audio feed, we analyze how much participants talk (participation), how much each participant interrupts and gets interrupted (overlap), and who spoke after whom (turn-taking). The virtual feedback assistant, a chat-based interface, relays complex performance metrics CoCo: Collaboration Coach for Understanding Team Dynamics during Video Conferencing \u2022 160:3 in an easy-to-understand format and provides feedback and conversational insights to participants in a positive and constructive way. To validate our framework, we conducted a pilot study with 39 participants. We randomly split the participants into groups of four-and one group of three-resulting in 10 groups. Each group participated in two discussion sessions with two popular team-building tasks called Lost at Sea [1] and Survival on the Moon [24] . The ordering of the tasks are counterbalanced across all groups. In Lost at Sea, participants imagine themselves escaping a sinking ship, ranking in order of most useful for survival a pre-defined set of items. (The actual best ranking of the items is annotated by the U.S. Coastguard.) The Survival on the Moon task follows a similar pattern in which participants, as crew members of a malfunctioning space ship, have to again rank the available survival items. After receiving their scenario, each participant orders their own list. Then the participants uses the CoCo system to devise a unified, final ranking of the top five items, together representing the team's decision. CoCo records audio and video of the discussion using participants' webcams and microphones. From the audial and visual data, we extracted the facial and prosodic features of the participants. These features include: participation, interruption, turn-taking, emotional valence, attitude (engagement, attention, surprise, anger), and shared smiles. After each team made its final decision, our chat-based virtual feedback assistant debriefed each member with quantitative metrics about group and individual conversational nuances. The feedback assistant does not advise participants to change their behavior-rather, it informs them of the group and individual effects of the performance features in a positive, constructive way. We opted for this design to allow humans to control their own behavior modifications instead of influencing them with predefined suggestions. After the feedback session, the teams engaged in another round of group discussion through the video conferencing system, focusing on the other task.\nINTRODUCTION: We noticed a statistically-significant change in balanced participation-that is, everyone spoke more equallyduring the second round. Participants' self-evaluations of their awareness of their own conversational skills, their teammates' conversational skills, and how often they allowed others to speak also showed statisticallysignificant improvement. We believe the proposed CoCo system improves the efficacy of video conferencing environments with the integration of analysis and feedback tools. Because our framework is publicly available at https://github.com/ROC-HCI/CollaborationCoach_PostFeedback, it could be used by other researchers to collect more virtual meeting data and expand our understanding of group dynamics.\nINTRODUCTION: In summary, this paper makes the following contributions:\nINTRODUCTION: \u2022 The development of a fully-automated video conferencing system that captures nonverbal features of both individuals and teams from participants' audio and video feeds; \u2022 The design of six crucial group interaction features (participation, speech overlap, turn-taking, emotional valence, attitude, and shared smile) and four sub-features (attention, anger, surprise, engagement, under attitude) to better describe the complexities of group discussion; \u2022 The implementation of a chat-based system that converses with participants and constructively reveals nonverbal metrics after the video conferencing session ends; \u2022 The design of a two-stage, team-building experiment with 39 participants (spread over 10 groups) to validate the developed framework and its potential effects; \u2022 The analysis of group performance and behavior metrics from both qualitative and quantitative perspectives to better understand team dynamics, in general.",
    "relatedWork": "RELATED WORK: An automated system that allows individuals to conduct a video conferencing session and receive automated feedback on team dynamics culls knowledge from affective computing, collaboration, teamwork, and rapport. In this paper, we reviewed two primary sources of information: systems that provide feedback on team dynamics and literature on teamwork and rapport. We first discuss existing systems intended to better understand team dynamics that also provide a feedback mechanism. We highlight the fundamental features that underpin effective group collaboration. We discuss the particular contributions to teamwork of features like turn-taking, overlap, participation, and sentiment.\nExisting Systems: Providing feedback on virtual group communication is a powerful way to shape team dynamics. Previous systems have made efforts to leverage the potential of group feedback on a number of group interaction features, including participation, content, and agreement. The thrust of these systems is the idea the more aware group members are of their behaviors, the more they can improve their communication abilities.\nExisting Systems: One such system is GroupMeter, a web-based chat room that lets users instant-message and process feedback visualizations during their conversation [38] . GroupMeter displays feedback on word count, engagement, and agreement between group members. This feedback is shown visually, in real time updates as group members continue to discuss. Results demonstrate that providing visual feedback increased user awareness of language use and agreement, eliciting positive changes in behavior. DiMicco et al. support this finding. In their work, they collect audio from participants in a group discussion and visualize participation rates in a bar chart on a shared screen in the discussion room [22] . Their results show that participants who spoke too often would reduce their speech time as a result of seeing the chart.\nExisting Systems: Similarly, Tausczik and Pennebaker offer real-time feedback, but unlike the GroupMeter prototype, their system focuses on giving individualized feedback [53] . Instead of showing passive, neutral visuals, Tausczik gives users suggestions, like recommending that users pay more attention or stay on topic. During a conversation, a group member may elicit a comment like \"Your group is working alright, but could be improved. Be sure to pay attention to what others are saying.\" Their user studies show that real-time feedback can be distracting, and that negative feedback can be damaging.\nExisting Systems: Calacci et al. focused on analyzing speech overlap and turn-taking when they designed their tool, Breakout. Breakout is both an interaction platform for online, face-to-face peer learning groups and a research tool to help study real-life team dynamics [13] . The platform collects audial and visual data in real time and then uses metrics like overlap, turn-taking, and response patterns to measure social relationships and engagement to display aggregated feedback. Their modular feedback analysis is based on the Meeting Mediator [33] of Kim et al., which visually maps each teammate as a node. A ball in the center of the visualization moves toward whoever dominates the conversation. However, Calacci et al. have not yet evaluated the effectiveness or influence of this feedback on group interactions. The feedback is provided through real-time visualization, which may affect the natural flow of the group discussion. Although it takes into account various communication features (e.g. turn-taking, speech overlap), it does not include features related to rapport, like smile, valence, etc.\nExisting Systems: Sellen [49] shows a difference between same-room and video-mediated conversations, especially in terms of floor control and simultaneous talk. It shows no significant effect, however, on turn-taking among speakers when the whole group can be viewed simultaneously. Cohen [16] , on the other hand, finds significantly more speaker turnover in face-to-face meetings than video meetings. Notably, in Cohen's study, the video shifts to the current speaker, foreclosing on a view of the whole group. Thus, system settings may make a difference in the behavioral patterns of a group.\nExisting Systems: Providing feedback for improved social interaction has been studied extensively in assistive technology-based research. This research reveals the complexities of social interactions and the importance of providing feedback. Research has contributed toward improving interactions in different need-based settings [8, 46, 47, 60] . In most cases, the feedback is provided in real time without extensive explanation or analysis. SayWAT [10] entails a wearable assistive technology for adults with autism that provides real-time feedback on prosody during face-to-face conversations. Another automated system, MOSOCO [54] , which was developed on a mobile phone platform, intervenes in real-life situations by providing real-time, step-by-step guidance. Likewise, another mobile-based intervention system called TalkBetter [28] provides parents with real-time meta-linguistic analysis during conversations with their children. Rhema [52] provides speakers with real-time feedback through Google Glass to improve their public speaking. The Logue [19] system explores the instant information flow through Google Glass during social interactions. It shows, notably, that real-time feedback may be distracting during real-life conversations [20, 42, 56] . Reducing the complexity of information flow and the number of interactions can reduce distraction [12, 58] . For group interactions, complexity increases with the number of members and the number of features analyzed. Thus, real-time feedback is liable to be a major distraction in more analytical systems dealing with larger groups.\nExisting Systems: All these works lay the foundation for navigating feedback design and generation for group communication. However, these works focus on only limited dimensions of team dynamics. For instance, GroupMeter and the work of Tausczik et al. are restricted by the data available through an instant messaging platform, where only text-based information can be extracted. It is subsequently not useful in analyzing team dynamics with respect to nonverbal behavior. Additionally, the mental work required to process real-time feedback often distracts participants from the discussion. The real-time feedback also does not provide participants with explanations for the suggestions. In the CoCo system, we focus on understanding team dynamics by developing a video conferencing platform to extract and display meaningful information from audial and visual data. This enables feature extraction from both verbal and nonverbal behavior, like shared smiles and turn-taking, that users may otherwise have difficulty quantifying and understanding. Our CoCo virtual feedback assistant offers post-discussion feedback, explaining each extracted feature to the participants. Importantly, CoCo makes use of a popular communication mechanism, video conferencing. If successful, the feedback strategies established through CoCo can be readily incorporated into existing video platforms without significantly changing the user experience.\nTeamwork and Rapport: Personalities and behavioral predispositions affect the ways in which people interact in group settings. Social psychology research connects various communicational skill patterns to specific personalities and behaviors. These connections help us better understand group dynamics. Dunbar et al. [23] finds that dominant people tend to be more verbally or physically active during interactions. Evaluating the behavior of less dominant people is difficult because these people demonstrate less activity [31] . Other research [21, 35] indicates that conversational features, like turn-taking, speaking time, prosody, and others, significantly affect the social interaction experience.\nTeamwork and Rapport: Analysis by Hung et al. [27] infers who dominated or led a group discussion based on the turn-taking patterns among the group members. Meeting Mediator [32] discusses the effects of continuously updating and showing turn-taking information in group discussions. McLeod et al. [40] show how feedback on speaking time or task focus can influence a group's teamwork tendencies. Feedback on these features can positively influence the group discussion, resulting in better group performance [29, 30] . The AMI meeting corpus [14] develops a meeting browsing technology with hand annotations for transcriptions and extraction of features including emotions, headnods, and more.\nTeamwork and Rapport: There is no standard behavior that defines a \"good\" team. A number of works have examined teamwork with respect to several social variables, including team size [11] , leadership [25, 26] , goal-orientation [44] , and rapport [34] . Of particular interest to us was rapport, characterized by three related components: mutual attention and involvement, mutual positivity, and coordination [55] .\nTeamwork and Rapport: Mutual attention refers to interactions in which group members feel involved and are interested in what others are saying. Mutual positivity is best described as friendliness and caring, and can be quantified through nonverbal behaviors like eye contact and smiling [41, 55] . Coordination requires behavioral cohesion, including attributes like smooth turn-taking, acknowledgments like head-nodding, and back-channeling (e.g. uttering phrases like \"uh-huh\") [41] . Drawing from these three components of rapport, we look at mutual attention through participation, mutual positivity through smiling, and coordination through overlaps and turn-taking.\nTeamwork and Rapport: In auditory conversations, participation is related to how long a speaker holds the floor. Some argue that equal participation leads to greater group interactivity and an overall better experience because all group members have a chance to share their thoughts [22, 36] . While equal participation is not ideal for all discussions, it is important in collaborative decision-making, where input from all members is needed. Inequality in participation may indicate neglect for some group members' opinions and may subsequently hinder the group's effectiveness [48] .\nTeamwork and Rapport: Smiling is conducive to establishing trust and friendliness, and is related to positivity [15] . In both one-on-one interactions and in groups, researchers have found that individual positivity is contagious and helps improve rapport [6, 7] . Barger finds that a service employee's smile can spread to their customers and positively affect the customer's mood and perception of the quality of service [6] . Similarly, in a simulated hiring committee discussion, Barsade shows that a group member's positivity influences not only others' moods but also their judgment when making decisions [7] . In his study, groups with increased positivity saw improved cooperation and a decrease in conflict.\nTeamwork and Rapport: Overlaps are typically considered disruptive or indicative of dominance. But research shows that most overlaps are supportive, rather than obstructive [43, 51] . Peers with strong rapport, for instance, will overlap one another frequently, suggesting that they are socially comfortable with each other [51] . Likewise, one peer may overlap another because they want to be involved in the conversation [43, 45] . Whether an overlap is supportive or obstructive ultimately depends on context, making it difficult to provide feedback.",
    "rq": "a: how about signal flares?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: With remote work and video-mediated communication becoming increasingly common, we are seeing new challenges and opportunities emerge in how people conduct meetings. A popular meeting form is hybrid meetings [31, 32] , which involve physically co-located and remote participants. Such meetings sufer from reduced nonverbal communication [35] due to a lack of spatial consistency between the physical spaces from where meeting participants connect [5, 29] . Unlike in face-to-face interaction, participants struggle to attract the attention of others through subtle glances and body gestures [14, 15] . This may lead to primary room dominance [18] where remote participants feel excluded from the room of co-located participants [31, 32] . To address these issues, Media Spaces (e.g., [17, 26, 29] ) and Mixed Reality approaches (e.g., [5, 9, 28] ) create a physically realistic frame of reference for a blended interpersonal space. This creates a sense of virtually \"being there\" together and enables deictic gestures with spatial referencing. However, these approaches dictate fxed spatial confgurations of users and require elaborate (and costly) physical hardware setups.\nINTRODUCTION: We follow Hollan and Stornetta's suggestion to build telecommunication tools that go \"beyond being there\" [16] with the goal of developing new principles for supporting non-verbal communication and better inclusion of remote participants in hybrid meetings. Rather than establishing a physical frame of reference, we investigate the collaborative qualities of a malleable virtual frame of reference. Our work addresses the following research questions: RQ1 How do participants leverage a malleable blended space for going beyond being there? RQ2 How can participants use their camera feeds in a virtual frame of reference for deictic gestures? RQ3 How can we better support inclusion of remote participants in hybrid meetings? To this end, we built MirrorBlender -a prototype of a malleable video-conferencing system that enables remote and co-located participants to adjust the position, scale and translucency of video feeds in a shared 2D space. We refer to the video feeds as mirrors, which can either be mirrored camera feeds of participants or screen mirroring of shared content. MirrorBlender grants spatial consistency [29] by creating a shared 2D frame of reference in a What-You-See-Is-What-I-See (WYSIWIS) style [36] so that everyone can see the same layout of mirrors. To investigate our research questions, we deployed MirrorBlender in three hybrid meeting sessions each with three co-located and two remote participants.\nINTRODUCTION: The paper thus contributes the following:\nINTRODUCTION: \u2022 A novel approach to malleability in video communication through the manipulation of position, scale, and blending of camera and screen mirrors. \u2022 Identifcation of opportunities and challenges of enabling embodiment of camera mirrors for deictic gestures. \u2022 Insights into how hybrid meetings can be made more inclusive for remote participants.",
    "relatedWork": "RELATED WORK: We build upon recent work on hybrid meetings and motivate our study by reviewing research on media spaces and relevant commercial video-conferencing systems.\nHybrid Meetings: Hybrid meetings are a popular type of video-mediated meetings in business settings which consist of a primary room with many co-located participants and a display that connects to one or more remote endpoints [31, 32] . While this type of setup allows for fexible working conditions, research has identifed several shortcomings. Hybrid meetings sufer from primary room dominance [18] , which causes co-located participants to constantly interact with each other and forget about the remote participants [31] . Remote participants struggle with limited physical and social awareness and unequal meeting experiences to those who are co-located [40, 41] .\nHybrid Meetings: Recent research shows that remote participants feel more disadvantaged and excluded in hybrid meetings than in fully virtual ones, in which every participant connects through their own device [32, 35] . Technical limitations of video-mediated communication are often blamed for the low engagement of remote participants and set the future expectations from remote participation in such meetings [24] . Unequal distribution of participants over diferent endpoints exacerbates the already existing attention problems in video conferencing and makes it difcult for remote participants to receive and pay attention throughout the meeting [23] .\nHybrid Meetings: In Ens et al.'s recent survey of the landscape of collaborative mixed reality, they highlight hybrid collaboration as one of the central themes for future research [8, pp.90] : \"[...] further research needs to explore how to support collaboration between team-members who are both co-located and remote, as the physical embodiment of collaborators afects how they can work with one another. A key challenge to address here is to aford all the benefts of collocation while similarly realizing the presence of remote collaborators in ways that all can participate efectively. \" Neumayr et al. [27] present a descriptive framework for hybrid collaboration and coupling styles in partially distributed teams with a particular focus on transitions between synchronous and asynchronous work in hybrid settings. They emphasise that future hybrid meeting systems should support territoriality 1 and ways for remote participants to raise awareness. Lastly, Saat\u00e7i et al. propose the metaphor of \"confguring the meeting\" [31] to emphasize that meeting systems should support meeting moderator(s) and participants in setting the meeting needs and goals and adjusting the meeting fow before confguring the software. Their fndings point to the need for \"making the software adaptable to meeting needs\" rather than the other way around. These insights motivate the exploration of ways to reduce primary room dominance and grant more equal control to remote and co-located participants over their hybrid meeting settings.\nMedia Spaces and Physical Frames of Reference: The challenges of undermined non-verbal communication occur not only in hybrid meetings, but more generally in meetings via media spaces: spaces of audio, video and computing technologies that connect distributed colleagues to support their work as well as more informal social interactions [6] .\nMedia Spaces and Physical Frames of Reference: When communicating face-to-face, people rely on a shared frame of reference together with deictic expressions such as \"here\" and \"there\" when using their bodies to point and gesture around objects in the world [39] . In traditional video-conferencing systems such as Zoom 2 or Skype 3 , there is no common frame of reference for pointing to objects or towards other people [29] . While techniques such as telepointers can help users point in remote collaboration [11] , our bodies are much more expressive. We express non-verbal cues with our hands in the form of diverse gestures which can be deictic (e.g., pointing at an object), iconic (e.g., forming a circle with two fngers to indicate a round object), spatial (e.g., distance), and kinetic (e.g., a bouncing motion) gestures for non-verbal communication [3, 10] . Studies have shown that support for remote gestures improves remote collaborative activities [19, 20] . The challenge of deixis has also been a central issue in early groupware work based on WYSIWIS [36] , e.g., for remote pointing [11] . Hybrid meetings complicate remote pointing as co-located participants may not have access to a pointing device but share a large display that may be out of immediate arm's reach [22] .\nMedia Spaces and Physical Frames of Reference: A common approach to supporting gestures and other non-verbal cues is to achieve spatial consistency by creating a physically realistic 3D frame of reference. For example, Morikawa and Maesako create a shared workspace by blending camera feeds from two remote participants into one shared virtual space [26] . Camray connects two wall-sized displays in diferent rooms where the remote camera feed follows the remote user's position, giving users a sense of walking along the same wall [2] . Holoportation connects two users in separate rooms by reproducing 3D representations of each other in their head-mounted displays [28] . Previous work also explored how to help meeting participants embody the system's frame of reference, such as by aligning spatial referencing with gaze direction [1, 4] , by overlapping semi-transparent video feeds [37, 38] or by \"blending\" diferent physical workspaces into Blended Interaction Spaces \"where the spatial geometries of the local space continue coherently across the distributed boundary into the remote site, providing the illusion of a single unifed space\" [29] .\nMedia Spaces and Physical Frames of Reference: Our work is mainly inspired by ClearBoard [17] and HyperMirror [26] , two of the frst-generation mixed reality systems. HyperMirror [26] merges video feeds from diferent physical spaces in a blended space, creating the illusion of a shared mirror. ClearBoard [17] enables face-to-face collaboration on the same shared digital content between two remote users by integrating a remote video feed with a shared drawing canvas, supporting the integration of interpersonal space and shared workspace for remote collaboration.\nMedia Spaces and Physical Frames of Reference: We believe that hybrid meetings can also beneft from a common frame of reference to better support gestures and other non-verbal cues. However, systems such as HyperMirror and ClearBoard rely on hardware that are costly and hard to adapt to diferent meeting settings (e.g., a colleague connecting from home or a caf\u00e9), which motivates us to explore cheaper, more fexible software alternatives.\nVideo Conferencing and Virtual Frames of Reference: Widely-adopted video communication platforms such as Skype, Zoom, BlueJeans, and Microsoft Teams ofer cheap and simple solutions that support hybrid and fully-virtual meetings. Their interaction design is relatively conservative, primarily (and most importantly) focusing on performance, stability, and ease of use. However, the opportunities for using deictic gestures in the conversation are few in these platforms. For example, in an online presentation, the \"talking heads\" are dislocated from shared screen content and there is no spatial consistency across what two remote users see in their individual views of the meeting.\nVideo Conferencing and Virtual Frames of Reference: More recent features and software applications support a sense of \"being there\" by supporting a shared 2D virtual frame of reference. Microsoft Teams announced the Together Mode 4 that masks meeting participants out of their physical backgrounds and places them in a shared virtual background, e.g., in seats in an auditorium. MakeSpace 5 proposes a more malleable alternative, where users can move around avatars and camera feeds in a WYSIWIS 2D meeting canvas. Other solutions use mixed reality for mediating communication, such as placing participant videos on AR markers [5] , or connecting through VR spaces, such as Facebook Spaces [9] , Rec Room 6 , and VRChat 7 . In VR, users are represented by avatars in a virtual world that ofers diverse embodied afordances, such as shaking hands to \"befriend\" another user, sitting in round tables to face others while talking, or blocking others with a \"talk to the hand\" gesture [25] . Since these systems embody users in the virtual frame of reference where their avatars live, users' sense of location may shift from their real bodies to their representations on the screen [34] , transforming \"here\" and \"there\" to virtual positions instead of their own surroundings. As these technologies become more widely adopted, we fnd it important to understand how the embodiment of users in virtual frames of reference supports or hinders social interaction.\nVideo Conferencing and Virtual Frames of Reference: We believe that allowing participants to \"confgure\" their hybrid meetings requires their video-conferencing systems to provide means for adaptation and appropriation that allow them to \"make it theirs\" [13] . This motivates us to better understand how a malleable video-conferencing system (i.e., highly adaptable and appropriable) invites participants to build their own frame of reference, how it shapes their social interactions, and how it may support or hinder non-verbal cues such as deictic gestures.",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Crowdfunding platforms, such as Kickstarter and Indiegogo, have gained popularity among new entrepreneurs by providing the opportunity to collect funding from millions of common people with almost no initial investment. These web platforms allow entrepreneurs to present their ideas using project descriptions, pictures, and videos. Naturally, video plays an essential role in crowdfunding campaigns, particularly for its storytelling power [9, 28] .\nINTRODUCTION: While a video is crucial in convincing people to donate for crowdfunding campaigns, creating an appealing video is a challenging task for novice entrepreneurs [20] . Making a professional video involves several elements such as storyline, script, camera movements, editing, post-production editing, and so on. Most importantly, there are many audience persuasion factors, which are critical to optimize when making a campaign video persuasive to the audience. These factors are challenging to apply in videos without professional training in advertising and marketing. Unfortunately, novice entrepreneurs -while experts on their products -often do not even know what factors make a video generally persuasive, much less, what factors would make a video most persuasive for their specific audience.\nINTRODUCTION: Past research identified many of these persuasion factors that can make campaign videos appealing to the audience [13] . Although these findings are valuable to researchers and advertising professionals, they do not help inexperienced entrepreneurs to effectively optimize their videos. The impact of these factors depends on campaign type, and often, their impacts are not intuitive. For instance, a complex representation of a fashion product is interpreted as evidence of fine craftsmanship by the general audience. Conversely, a video showing the complexity of a technology product, does not work favorably for the campaign. In this case, the audience assumes the product is difficult to use and it negatively impacts their overall impression of the campaign [13] . Without any true guidance or examples of best practices, the implication of such persuasion factors are hard for novice entrepreneurs to understand.\nINTRODUCTION: Traditionally companies consult advertising agencies for creating persuasive product videos. However, the majority of the entrepreneurs on crowdfunding platforms are beginners and cannot afford such resources due to a limited budget. As an alternative, novice entrepreneurs attempt to learn using past campaign videos as examples, seek suggestions from peers and family members [20] , and look for tips and strategies on the web [6, 37] . Many entrepreneurs depend on free counseling from film experts to compensate for their lack of experience in making videos [20] . Since Kickstarter does not allow creators to search through failed campaigns, novice entrepreneurs even go through third-party tools and blogs to compare and contrast ideas of their campaign videos. Unfortunately, this process can take three to six months of extensive work [20] and can still result in ineffective campaign videos.\nINTRODUCTION: The goal of this paper is to understand whether and how we can develop assistive tools to help novice entrepreneurs learn how to apply persuasion factors to make effective campaign videos for their target donor audience. To this end, we focused on two main hypotheses based on the literature from HCI, and cognitive and learning science. First, given that novice entrepreneurs have difficulty predicting how combinations of persuasion factors will impact the success of their campaigns, we hypothesize that a tool that assists them in exploring effects of these factors on campaign outcomes through a process of structured interactivity will help them to a) learn the impact of these factors and, b) apply them in making their own videos more persuasive. Second, given that people learn new concepts better by thinking deeply about the concept and by generating materials by themselves, we hypothesize that a tool that guides novice entrepreneurs, in a step-by-step manner, to actively think about the critical aspects of persuasive campaign videos and to generate the planning materials of their videos by themselves will result in more appealing and creative plans for their videos.\nINTRODUCTION: In this paper, we developed a functional prototype of VidLyz, an assistive web-based tool which consists of the following two modules: an interactive interface module that helps novice entrepreneurs learn the nuances of persuasion factors and enables them to make a comprehensive plan for their campaign video with the help of positive and negative example videos, interpretable explanations and measurement scales of persuasion factors, crowd-sourced feedback, and prediction models towards the success of the campaign (H1). From our second hypothesis, we designed another module, called the guided planning module, that guides novice entrepreneurs to actively think of the campaign video from the perspective of their own target audience and specific product category (H2).\nINTRODUCTION: To evaluate the effectiveness of the VidLyz tool, we conducted an in-lab user study with 45 participants and interviewed five previous campaign creators with different backgrounds and experiences. The primary objective of our evaluation was the following: 1) whether VidLyz can assist in learning of the implication of persuasion factors and 2) whether this learning can help them make better plans for their campaign videos. To this end, we created two different versions of our VidLyz tool: 1) a non-interactive version (a simple version without any interactive properties or guided active thinking) and 2) a comprehensive version (highlighting the interactive features and incorporating the guided planning module). We also recruited participants in the control group, which included no tool but a list of example campaign videos categorized based on the final outcome (success/failure) of their corresponding campaigns.\nINTRODUCTION: We randomly assigned each version of the VidLyz tool to 15 different participants and asked them to explore the tool to understand how persuasion factors can impact the effectiveness of campaign videos for prospective donors. In the end, all participants had to make their own plans for a campaign video of their pre-assigned product using a storyboard (a pre-production planning tool widely used by advertising agencies and film makers to make low-fidelity, easily customizable plans for videos).\nINTRODUCTION: Results show that the comprehensive version of the VidLyz tool helped novice users gain a deeper understanding of the relative importance of the persuasion factors. The combination of the interactive interface and the guided planning module helped participants create coherent and persuasive storyboards for their proposed campaign videos. Overall, their storyboards were suitable for their target audience, which is a key element for an effective campaign video. A follow-up user study showed that crowd workers found the storyboards of the comprehensive group to be persuasive. Finally, semi-structured interviews with participants and five prior and one future campaign owners informed us of the aspects of the VidLyz tool that can be improved in the future to better assist novice entrepreneurs in making their campaign videos persuasive.",
    "relatedWork": "",
    "rq": " rq1: can vidlyz assist novice entrepreneurs in learning about the significance of persuasion factors in making effective campaign videos from example videos, crowd-sourced feedback, and interpretable explanations?\n rq2: can this learning assist them in making a well-structured plan for their persuasive campaign videos?\n(1) what are the challenges you may face in the process of making a persuasive campaign video?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": " but a question remains: how to map from sensor data to meaningful feedback as a teacher would do?"
}
{
    "intro": "INTRODUCTION: Meetings are a primary mode of work [58] , but many employees find them frustrating and even counter-productive when good meeting practices are lacking or violated [1, 46] . The violations of general meeting norms and disrespectful behaviors have been shown to be negatively correlated with meeting effectiveness and satisfaction [45] . In 2020, the \"stay at home\" orders and travel restrictions of the COVID-19 pandemic dramatically accelerated the use of the video-conferencing meetings for work. By March 2020, daily usage of video-conferencing services such as Zoom and Microsoft Teams had increased by 300% and 775% respectively, and video-conferencing apps jumped to the top of the Apple app store. 1 Although video-conferencing has the potential to reduce the cost and effort behind organizing travel, space, and scheduling of in-person meetings 2 , the \"fractured ecologies\" [37] of videoconferencing can aggravate negative outcomes and marginalize some members of the meeting [38] . Video-conferencing has consistently presented communicative challenges [19, 52] and introduced distractions [61] which can lead to ineffective and non-inclusive meetings [23, 30] . A primary goal of remote collaboration tools should be to support the most effective meetings possible for all participants. Cutler et al. [9] conducted a large scale e-mail survey on remote meeting effectiveness (N=4,425) at a technology company (pre-COVID- 19 ) and showed that online meeting effectiveness correlates with meeting inclusiveness, participation, and comfort in contributing. They identify a large potential financial benefit to companies that can achieve these goals, and an opportunity to establish and maintain a workplace culture in which everyone feels free to contribute. There are clearly opportunities for technological solutions that assist attendees in feeling more included and improving meeting effectiveness by helping them understand their own and others' core meeting dynamics.\nINTRODUCTION: This paper reports on an exploratory study with in situ work teams to identify the challenges they face in video-conferencing based meetings, and proposes a post-meeting feedback system to address the issues. In particular, we aimed to provide insights on the following research questions: (1) What aspects of meetings do videoconferencing attendees need help with?; (2) How can we leverage AI systems to make video-conferencing meetings more inclusive and effective?; (3) How should AI-extracted meeting features (including content, behavioral measurements, and sentiment) be categorized and visualized in a feedback dashboard?; and (4) What concerns exist regarding data privacy and accuracy for such systems?\nINTRODUCTION: Our work addresses these research questions through a series of user studies and design iterations. Via an initial exploratory requirement analysis survey of 120 information workers, we identified the communicative signals (e.g., participation, facial sentiment) which are important in improving meeting effectiveness and inclusion.\nINTRODUCTION: We conducted a longitudinal user study to record in situ videoconferencing meetings from eight teams over a four-week period. We used the insights from the requirement analysis survey to create a wireframe prototype of a post-meeting dashboard and 16 participants from the user study teams evaluated the design and helped us further refine the components. Finally, we developed an AI sensing system to quantify these meeting dynamics features and created personalized, interactive, post-meeting dashboards for the participants. Through surveys (N=23) and interviews (N=9), we found that participants were able to become more aware of the group dynamics by reviewing the dashboard. Our study shed light on the privacy concerns participants had regarding such insights within the dashboard. The dashboard also helped participants identify and recollect important events of the past meetings. Our findings also showed that participants perceived that the dashboard would improve meeting effectiveness and inclusivity. In addition, participants expressed that actionable suggestions were more helpful than data visualizations alone. The main contributions of this work are as follows:\nINTRODUCTION: \u2022 We developed MeetingCoach, an AI-driven dashboard that provides both contextual and actionable insights based on meeting behaviors. \u2022 We implemented both behavioral (e.g., participation) and topical (e.g., questions) meeting dynamics features in our feedback system using state-of-the-art AI. \u2022 We identified and implemented shared and private design approaches for different feature components based on users' preferences from two design iterations and evaluations. \u2022 We demonstrated that MeetingCoach helped improve behavioral awareness and recollection of past meeting events, and bears the potential to improve perceived effectiveness and inclusivity. \u2022 We proposed design guidelines explaining the need for actionable suggestions, reminders or highlights based on timing, and multi-modal feature implementations to be adopted for future video-conferencing feedback systems.",
    "relatedWork": "RELATED WORK 2.1 Factors in Meeting Dynamics: Meeting effectiveness includes both task processing and interaction efficiency by a team [10, 14, 31, 47, 53] . Dickinson and McIntyre [10] emphasized the importance of goal specification, entailing identification and prioritization of tasks and sub-tasks, in agendas and other meeting resources. Even with clear goals, though, interaction efficiency has a clear impact on both outcomes and satisfaction. Balanced, active, and equal participation have been found to improve team performance [13, 32] . Depending on the type of meeting, equal participation may not always be applicable or feasible, but in a collaborative decision-making discussion, participation from all members ensures at least the exchange of opinions and a sense of \"being heard\", which ultimately improves team satisfaction [32, 50] . Turn-taking patterns also influence team performance and satisfaction, as some members may dominate the discussion without realizing they are doing so, reducing time for other members to voice their opinion or expertise [14] . Lawford [31] found that rapport building through verbal and non-verbal signals was an important factor in effective and inclusive discussions. To ensure coordination and rapport, affect management has been found to play an important role in a team's success [4, 8] . Barsade [4] has shown that a member's positivity can improve the mood of the whole team, making it more inclusive and improving the quality of decision-making. Cannon-Bowers et al. [8] discussed the importance of effective strategy formulation to consider alternative courses of action in case of disagreement or task failure. Non-verbal gestures, through head nodding and shaking, indicate signs of agreement or disagreement, and levels of interest, acknowledgement, or understanding [22, 34] .\nRELATED WORK 2.1 Factors in Meeting Dynamics: While the face-to-face views of video-conferencing intuitively seem to support the above, they have been found to constrain attention to the non-verbal signals and the overall progress of the meeting [19, 23] . Sellen [51] showed that having video did not improve the interruption or the turn-taking rate for video-conferencing meeting participants compared to audio-only ones. This implies that even though video is important in online meetings, it cannot fully resemble in-person meeting dynamics. Especially during long meetings, additional support for monitoring meeting progress and participation may be needed. Taking into consideration these concerns, we designed and developed an automated feedback system to summarize meeting content and attendee behaviors, with the goal toward improving meeting dynamics over time.\nFeedback Systems for Videoconference Meetings: Researchers demonstrated the impact of feedback systems on meeting dynamics and discussion outcomes for in-person, text chat, and video-conferencing meeting setups [7, 11, 24, 27, 35, 43, 48, 56] . Feedback on participation [48] , turn-taking [13] , interruption [49] , agreement [25] , and valence [15] , have effectively improved group discussion dynamics. These studies show that the timing (e.g., realtime, post-meeting) and the design (e.g., number of features, visualization strategies) of the feedback are important in effectively modulating collaboration behaviors in a group discussion.\nFeedback Systems for Videoconference Meetings: Researchers explored feedback systems with affective, behavioral and topical group discussion features. Dimicco et al. [11] presented a real-time, shared-display feedback system measuring speaking contributions from audio recordings, visualized as bar graphs during a co-located meeting. They showed that effective visualization can help improve the group discussion, even though shared visualization can also motivate behavioral changes due to social pressure [39] . Nowak et al. [44] provided feedback on vocal arousal and explored the impact on oneself and one's partners behavior during a negotiation-based task conducted over the phone. They found that real-time feedback can be difficult to process during an ongoing task and can negatively impact user performance.\nFeedback Systems for Videoconference Meetings: Therefore, even though real-time feedback has been found to be effective in modulating behaviors during a discussion, it can also be distracting and cognitively demanding [54, 56] . Samrose et al. [48] presented CoCo, an automated post-meeting feedback system providing summarized feedback on talk-time, turn-taking, speech overlap and sentiment through a chatbot for a video-conference group discussion. They showed that post-meeting feedback can effectively make successive discussion more balanced. Through a longitudinal study in a video-conference learning environment, EMODASH [15] , an interactive dashboard providing feedback on affective meeting features, improved behavioral awareness over time. Instead of showing numeric or categorical feedback on linguistic features, Tausczik and Pennebaker [56] provided real-time and individualized actionable suggestions in text chat group discussions. Their findings showed that individualized suggestions helped teams shape their behavior; however, too much information in the feedback can be cognitively taxing. Suggestion-oriented feedback has been found effective in behavior modulation [54] , and could be useful post-meeting. As we will later explain, our design incorporated an individualized and suggestion-oriented approach to a post-meeting feedback system. While identifying the feedback features for our dashboard during our requirement analysis, we prioritized features from these prior systems, such as talking time and turn-taking, but with an eye toward reducing cognitive load.\nFeedback Systems for Videoconference Meetings: Beyond the meeting context, researchers have developed a number of interfaces that allow users to view emotional or affective signals captured by self-report, diaries or sensor-based systems. These have been used in several domains, such as self-reflection [21, 36, 41] , data exploration and information retrieval [18, 59, 60] , stress management [2, 20] , and studying interpersonal dynamics [28, 29] . Data portraits can help people understand more about themselves and other people [12] . However, there is still a lot left to be understood about how to best represent complex and subjective data, such as emotions or group dynamics. Affective data is often highly dimensional, multi-modal, and continuous, all difficult when designing useful visualizations. There are also important privacy concerns raised by creating digital systems and artifacts that encode highly personal information [12] .\nFeedback Systems for Videoconference Meetings: The feedback needs of organizational teams conducting videoconferencing meetings require special attention, as these teams are relatively stable over time and the members need additional support in tracking the progress and the outcomes of their meetings [8, 26, 33] . As video-conferencing discussions can be prone to distraction and multitasking [30] , we hypothesize that a meeting feedback system that helps members reflect on meeting goals and progress could be a useful tool. Feedback on the non-verbal behaviors can also help with effective and inclusive videoconference meetings. The Matrix to a Model of Coordinated Action (MoCA) presented by Lee and Paine [33] is an elaborate framework that explains a complex collaborative environment by seven dimensions. Within the context of MoCA, the post-meeting feedback dashboard that we propose is characterized as a long-term asynchronous periodic event for small teams placed in different locations.\nFeedback Systems for Videoconference Meetings: In this study, we observe the meeting challenges and the needs of several in situ work teams, and propose and test technological solutions for them. Meetings have evolved from engaging inperson, to fully computer-mediated (all members join remotely via audio/video/chat), and hybrid (some members join remotely to group/s who are together in person), and each have their distinct character [17, 19, 58] . This study focuses on the fully-computermediated meetings of teams in which all members were remote due to COVID-19's mandatory requirement to work from home. All teams used the same video-conferencing system. We followed an iterative, human-centered design process to address our research questions. Our approach was comprised of two main phases. In the first phase, we performed a preliminary investigation via survey to understand the current challenges and needs for online meetings, and gathered design requirements for our technology probe. Informed by our findings from the preliminary study, in phase 2 we conducted two design iterations through a longitudinal study of actual remote, recurring team meetings. In the following sections, we describe the details of the requirements analysis and the longitudinal studies.",
    "rq": " (1) what aspects of meetings do videoconferencing attendees need help with?\n (2) how can we leverage ai systems to make video-conferencing meetings more inclusive and effective?\n (3) how should ai-extracted meeting features (including content, behavioral measurements, and sentiment) be categorized and visualized in a feedback dashboard?\n and (4) what concerns exist regarding data privacy and accuracy for such systems?"
}
{
    "intro": "INTRODUCTION: Giving presentations is a necessary part of many jobs such as teaching, management, and sales [3, 37, 48, 53] , and requires presenters to continuously gauge audience responses to intervene and ensure that the message is being communicated effectively. For instance, a presenter might inject humor or provide further clarifications upon observing a bored or confused audience, respectively. In contrast to in-person presentations, however, online presentations provide limited to no audience feedback, making it very difficult to establish rapport and effectively adapt the content of the presentation. This problem is partly due to the constraints of existing video-conferencing platforms, which often prioritize showing the presenter slides and/or show only a limited number of participants. Participants in a video call also turn off their cameras, which omit non-verbal cues that are useful to the presenter. In addition, remote presenters often have limited space on their computer displays, severely constraining the potential communication bandwidth. Due to the recent increase of remote work associated with the pandemic demands, this problem is more prevalent now than ever before.\nINTRODUCTION: Public speaking is often regarded as one of the most stressful daily activities [10] and is heavily influenced by audience responses to the presenter. In fact, studies that seek to reliably induce acute stress on people often involve giving a presentation in front of a neutral-looking audience (a.k.a., Trier social stress test [29] ). While research on audience responses in online settings is still nascent, there is prior work considering the impact of in-person audience responses [43] , especially in the context of alleviating public speaking anxiety. For instance, MacIntyre and Thivierge [31] showed that low perceived audience interest, responsiveness, and evaluation of the talk can contribute to public speaking anxiety. Other studies [7, 32, 45] have identified that high audience responsiveness, in terms of head nods and smiling, induced less anxiety and promoted more communication. These findings are consistent with Motley's work [38] , that proposed a continuum for presenter's orientations based on audience reactions, ranging from low to high audience responsiveness, interest, and evaluative stance towards the presenter. The limitations of current video-conferencing applications such as the lack of an intuitive reception of audience feedback, could potentially make online presentations fall at the lower end of Motley's continuum, leading to a negative presenter experience. Thus, in this work, our research goal is to address the problem of limited access to audience responsiveness during online presentations to improve presenter's awareness of the audience by spotlighting reactive audience members to the presenter as they speak.\nINTRODUCTION: The spotlight metaphor was inspired by current approaches in theater and cinematography, in which camera recorders often highlight audience responses to capture the most relevant moments [19, 54] , draw attention to elements on screen, and potentially evoke emotional responses [13] . Beyond cinematic experiences, the spotlight technique has also been used in computer applications to direct and maintain users' attention while simultaneously making surrounding context still visible [1, 28, 41, 52] . Moreover, Khan et al. [27] showed that spotlighting can also help manage users' attention between their primary activity and peripheral information, even on small monitors. Since oral presentations constitute a similar paradigm requiring focused attention across the presentation slides, speaking notes, and the audience feedback, we embraced the spotlight analogy as a design solution to provide audience feedback to presenters in real-time.\nINTRODUCTION: Over the years, researchers have developed a wide variety of presenter-support systems to facilitate the sending and reception of both explicit and implicit feedback from the audience [15, 21, 23, 51, 56] . However, most of the work has focused on in-person presentations, in which presenters are often co-located with the audience and communication bandwidth is not limited. In contrast, we propose a presenter-support system that facilitates gathering implicit feedback from an online audience by leveraging recent advancements in computer vision-based affect sensing. In particular, we propose the AffectiveSpotlight system, which analyzes the facial responses and head gestures of audience members in real-time and dynamically spotlights the most expressive members for the presenter. We purposely decided to avoid labeling the inferred responses to empower presenters to make their own personal interpretations based on the context and their experience. To the best of our knowledge, our work is the first to explore the creation of an affect-driven spotlight that facilitates audience responses to online presenters to more closely resemble in-person presentation experiences.\nINTRODUCTION: This work is organized as follows. We first describe prior work in the context of facilitating audience feedback to presenters. We then describe an exploratory survey that helped identify what types of audience responses are most informative to presenters. Next, we use our findings from the survey to help inform the design and development of our AffectiveSpotlight system. We then describe a within-subject evaluation study that compared the proposed system with two other control presenter-support systems. We review the results of the study and provide design recommendations for future affective interfaces in the context of online presentations. Finally, we discuss our findings including the limitations and potential future directions.\nFUTURE WORK: This work has also helped identify relevant opportunities for future research. The proposed system considers the measurement of head gestures and facial expressions to identify the most reactive audience members during a specific time window. In our work, we empirically set the refresh window to be 15 seconds which helped us avoid potential distractions, but could have also missed relevant responses. To address this, we envision future spotlight systems may consider a flexible window that can more quickly reflect the behaviors as they occur. In addition, some of the participants of the study expressed strong interest in controlling the specific behavior of the spotlight which could have helped address specific needs (e.g., detecting confusing points). As we consider expanding the sensing modalities (e.g., microphones, eye tracking), we believe that the quality of the information provided by the spotlight can be improved. However, it is important to be mindful that different sensing channels may be considered more invasive than others. We recognize the comfort levels expressed in sharing video feeds more in smaller meetings and thus propose several directions for future system use. For instance, the audience could be sensed locally, and their video be provided only to the presenter. In addition, the proposed system also extends to using affect sensing for sampling strategies in existing video conferencing systems, where only a subset of audience is shown to the presenter. Our interviews also show promise in combining both explicit and implicit feedback methods to more effectively address the preferences of different audience members. Finally, we believe future work will need to focus on the development of new user interfaces that promote system transparency in terms of both capturing and reflecting audience feedback which will be critical to prevent potential misuses and maximize user adoption.\nCONCLUSIONS: This work introduces AffectiveSpotlight, a real-time feedback support system for online presenters that analyzes and spotlights audience members based on their affective responses. Informed by an exploratory survey and evaluated in a controlled within-subjects study, we demonstrated some of the potential benefits of facilitating non-verbal audience feedback via the proposed system versus two other control support systems. We hope our findings and design recommendations will help enable future work exploring the possibilities of affect sensing and AI-mediated interactions in the context of online meetings. We are looking forward towards a future when similar approaches can continue to enhance presentation experiences and help close the gap between online and in-person presentations.",
    "relatedWork": "RELATED WORK: Researchers have explored a wide variety of methods to enhance the sensing and communication of audience feedback for presenters. To help map the research in this space, Hassib et al. [24] identified four important dimensions: type of audience feedback (explicit vs. implicit), audience location (collocated vs. distributed), synchronicity of the feedback (synchronous vs. asynchronous), and sender/receiver cardinality (1 to 1, N to 1, and N to N). To help better position this work in the context of prior research, we review other work considering a person presenting to a large audience (i.e., 1:N cardinality).\nRELATED WORK: Traditional methods of capturing audience feedback frequently rely on explicit modes such as use of self-reports and questionnaires, which are then aggregated and provided to the presenter in different ways. For instance, Rivera-Pelayo et al. developed the Live Interest Meter App [46] , which is a mobile and desktop application that gathers responses from the audience on demand. In particular, the presenter introduces a question for the audience, such as comprehension level of the talk or speaking volume, and the application aggregates and displays a summary graph. In a separate work, Chamillard [16] explored the use of iClicker, 1 which enabled instructors to receive student responses during lectures. In this case, the researchers identified a strong relationship between student participation and learning. In the context of confusion, Glassman et al., [21] developed Mudslide, an anchored interaction tool that allowed the audience to indicate confusing points for different parts of online lectures. The instructors found that anchored feedback was more valuable and easier to interpret than feedback provided at the end of the lecture. In a separate work, Teevan et al. [51] designed a smartphone interface that enabled audience members to indicate thumbs up/down in real-time and visualize the feedback via a shared, projected visualization. The researchers identified that the system helped audience members pay close attention to the presentation, helped them stay connected to other audience members, and facilitated retrospective review of the session. Different forms of explicit feedback have also been adopted by some social media platforms, in which audience members can broadcast different reactions in the form of flying emojis (e.g., [30] , [22] ). Despite the benefits of explicit methods to capture audience feedback, it is commonly observed that they can also increase cognitive workload and distraction for both the presenter and audience members [46, 51] . In addition, there are scenarios where the audience members who are too attentive or distracted can forget to provide feedback.\nRELATED WORK: To help address these limitations, some studies have explored the use of implicit methods to capture audience responses, such as monitoring physiological or behavioral signals. In one of the earliest studies, Picard and Scheirer created Galvactivator [42] , a hand-worn wearable that monitored the electrodermal responses of the audience members and increased the illumination of an LED when high physiological arousal was detected. The researchers observed that presenters found the information useful, especially for detecting both engaging and boring parts of the presentation. More recently, Hassib et al. developed EngageMeter [23] , a headmounted wearable that monitored electroencephalographic signals from the audience to estimate engagement and provided the feedback to the presenter in real-time. In this case, presenters found the information useful for knowing when to change the style of communication (e.g., tone of voice, injecting pauses). While physiological sensing is a promising technique, however, the cost and availability of custom sensors is a major obstacle, preventing the wide adoption of such technologies in real world scenarios. To address this problem, Sun et al. [49] developed a system that monitored facial expressions of students via webcam to estimate different cognitive states (e.g., anxiety, flow, boredom), and provided visualizations of the flow experience for the whole group to the instructor. The research identified the value of providing a real-time flow visualization, but also acknowledged that it still increased the cognitive load for the presenter.\nRELATED WORK: Our work similarly considers the use of pervasive webcams to monitor the facial expressions and head gestures of the audience, but explores providing the information in the form of original video feeds, which may be more familiar and pose less cognitive demands to the presenter as opposed to aggregated data visualizations. This visualization approach is closely related to prior work (e.g., [11, 44] ) that purposefully avoided labeling the sensed data to support flexibility in the interpretation, based on the context and personal experiences of the viewer. Our research goal is to identify relevant audience reactions that are most helpful to presenters and develop a system that spotlights audience members accordingly in the context of online presentations.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Autism Spectrum Disorder (ASD) has been the focus of growing attention in recent years due in large part to its rapidly increasing rate of diagnosis, which the Centers for Disease Control and Prevention estimates is now about 1 in 68 for children in the United States (Christensen et al. 2016) . ASD is a neurodevelopmental disorder comprised of impairments in social communication and interaction as well as restricted and repetitive behaviors (American Psychiatric Association 2013). Effective identification and treatment of ASD are often characterized as public health emergencies (Interagency Autism Coordinating Committee 2009), given the enormous costs of ASD to those with the disorder as well as to their extended support systems. Recent lifetime cost projections exceeded $1.4 million for those individuals with ASD who did not also have an intellectual disability (Buescher et al. 2014) .\nINTRODUCTION: The social interaction difficulties faced by individuals with ASD often contribute to social isolation, distress, and functional impairment. As adults, a majority of individuals with ASD experience poor outcomes with respect to employment, education, and lasting personal relationships (Howlin et al. 2013) . Social skills interventions designed for these individuals have shown some promise, especially with regards to technology-based approaches, because of the natural affinity many people with ASD have for technology (Murray and Lesser 1999) . For example, Ramdoss et al. (2012) surveyed computer-based interventions for social and emotional skills in individuals with ASD and found that social skills interventions consistently yielded positive, though sometimes small, outcome effects. Kientz et al. (2013) outlined several reasons why computer-based approaches are effective in this population. Among the many strengths of this approach are the computing environment's ability to present tasks with unambiguous goals, give simple and distilled information, dynamically adapt to the ability of the user, and collect data (Kientz et al. 2013) . Recently, researchers have advocated for the use of computer-based social skills interventions that utilize inherently collaborative tasks designed to improve social outcomes for people with ASD (Battocchi et al. 2010; Boyd et al. 2015; Escobedo et al. 2012; Stichter et al. 2014) . In this article, the use of the word \"collaboration\" most closely matches the formalization of shared cooperative activity put forth in Bratman et al. (1992) . This definition lays out the kinds of activities that may be correctly regarded as collaborative and identifies three necessary features of shared cooperative activities: mutual responsiveness, a commitment to the joint activity, and a commitment to mutual support (Bratman et al. 1992) .\nINTRODUCTION: To date, most evidence-based intervention strategies for people with ASD rely on human observers and direct observation as the primary methods for collecting reliable data on targets (Weitlauf et al. 2014 ). Although such observation, rating, and decision making are extremely powerful, the availability of trained professionals to deliver and monitor interventions to this large population of individuals is limited, and interventions (as well as treatment outcomes) are themselves subject to providers' individual expertise and perceptions-and as such it can be challenging to disseminate, export, and bring interventions to scale across low resource environments (Gress et al. 2010; Ramdoss et al. 2012; Stichter et al. 2014) . Computer-supported alternatives, on the other hand, are characterized by reliable quantification (e.g., sensor resolution and sampling frequency), scalability (e.g., dynamic application hosting in the cloud), and accessibility (e.g., support\nINTRODUCTION: Process Measures of Dyadic Collaborative Interaction for Social Skills 13:3 for a variety of device platforms). Despite the growing interest in this area of research and the potential it holds for individuals with ASD and their families, few of the existing collaborative, computer-based, social skills intervention systems harness the ability of their respective technologies to measure and utilize such data regarding collaboration between users either in isolation or in combination with expert rating and/or outcome measures. In a survey of the literature on computer-supported collaborative learning, Gress et al. (2010) found that only 11.2% of the extant research included what the authors identified as process data (i.e., quantitative measures of participant action such as timing and frequency of events). The vast majority of the examined work relied instead on survey data or data that was collected only after the collaborative tasks had been performed.\nINTRODUCTION: In response, we have developed and pilot-tested DOSE (Dyad-Operated Social Encouragement), a novel game system for collaborative skills intervention that leverages the ability of software to log time-series, speech audio, and system event information for the purposes of fine-grained analysis of dyadic interactions. Furthermore, the authors have made DOSE open-source, permitting other investigators to obtain, modify, and evaluate our novel platform. We hope that by doing this, DOSE is used to help reduce the disparity between the need for and utilization of intervention resources, which has been recognized as a problem by others in this field (Hiniker et al. 2013) . In a prior conference paper, we described details of DOSE's software architecture and implementation (Sarkar et al. 2015) . The current article significantly expands on important aspects of the system design, introduces process measures of dyadic interaction, and presents the results of a pilot study designed to assess initial feasibility of DOSE as a tool for intervention. We hypothesized that training with this system would yield improved collaborative performance in dyadic interactions, measurable via changes in the metrics computed by the system. We further hypothesized that the system would be engaging for users with ASD as well as for typically developing (TD) users, opening the door to the application of DOSE in populations beyond ASD. Our work builds upon the extant literature concerning collaborative skill development and makes the following contributions:\nINTRODUCTION: -The system records a range of quantitative event and time-series measures computed from real-time process data, functionality that is lacking from many systems (Gress et al. 2010 ); -The system was pilot tested with two non-overlapping groups of participants arranged in dyads of ASD-TD (N ASD = 6, N TD = 6) and TD-TD (N TD = 12) in order to assess the system's acceptability and initially gauge its effectiveness as a social skills intervention tool in the target population\nINTRODUCTION: This article contains the following sections: Section 2 discusses the related work on technologies applied to the problem of improving social skills for people with ASD through collaborative interaction. Section 3 gives a detailed review of the software implementation which goes far beyond our previous conference paper (Sarkar et al. 2015) . The pilot study conducted using this novel system is described in Section 4, and the results of this study are presented in Section 5. A discussion of the results and concluding remarks are given in Sections 6 and 7, respectively.",
    "relatedWork": "RELATED WORK: The research employing computer-based interventions for individuals with ASD is diverse with regards to task content, interaction methods, and measures used to track performance and outcomes. Kientz et al. (2013) provide a rigorous taxonomy of this vast literature, partitioning the methods into categories such as Virtual and Augmented Reality (VR/AR), robotics, and shared active surfaces. Methods such as these leverage the unique attributes of their respective technologies to implement interventions designed specifically to address particular deficits associated with ASD. For example, the use of virtual avatars to facilitate social communication interventions has appealed to researchers because of the controllable and non-threatening environment in which such virtual elements can be deployed (Alcorn et al. 2011) . Humanoid robots are well suited to deliver training related to skills such as joint attention and response to name (Warren et al. 2015) . Devices supporting naturalistic touch-based gestures offer an opportunity to design collaborative tasks for collocated users, such as assembling a puzzle (Battocchi et al. 2010) or building a tower from blocks (Zhang et al. 2016) . Researchers have also explored the use of mobile devices with wearable sensors to provide real-time feedback regarding appropriate proximity in interpersonal space within naturalistic settings (Boyd et al. 2017) .\nRELATED WORK: Collaboration-centric tasks in particular are a compelling approach to social skills intervention because of the wide range of skills that can be targeted, such as turn-taking, negotiation, and planning. Stichter et al. (2014) evaluated iSocial as a means of administering an evidence-based social competence intervention for higher-functioning individuals with ASD, particularly those in rural areas who may have limited access to intervention resources. The iSocial system included a variety of collaborative tasks wherein participants worked together to accomplish a goal. For instance, one task involved the design of a restaurant, including the food style and menu options; another task involved a scavenger hunt in a virtual medieval castle. Training with this system demonstrated promising results with respect to improved social communication (Stichter et al. 2014 ), but the researchers did not fully exploit the ability of the computer system to track timeand event-related performance measures during collaborative activities.\nRELATED WORK: Specialized devices have also been developed to facilitate computer-supported collaborative interaction. For instance, tabletop systems (i.e., large surface touch-sensitive devices), have been a popular approach to social skills intervention (Battocchi et al. 2010; Gal et al. 2016; Morris et al. 2006; Piper et al. 2006) . Using one such device called the DiamondTouch table, Piper et al. (2006) designed a game in which players collaboratively assembled a path for an animated frog character to hop along. Pairs of users coordinated simultaneous input gestures in order to complete the task. With the same tabletop device, Battocchi et al. (2010) implemented a collaborative puzzle game that required enforced collaboration among users in order to assemble puzzles. These enforced collaboration gestures included jointly touching, dragging, and releasing puzzle pieces on the tabletop surface. This study was one of the few that logged real-time process data such as the number of moves required to complete the game, ratio of coordinated moves to total moves, and the proportion of simultaneous activity of paired players (Battocchi et al. 2010) . Gal et al. (2016) also used a tabletop system to deliver an intervention curriculum made up of puzzle-assembly and art-creation games to children with higher-functioning ASD. The researchers reported increased positive social interactions and increased levels of collaborative play from the participants (Gal et al. 2016) .\nRELATED WORK: Porayska-Pomsta et al. ( 2012) created an intelligent, multi-modal application that provides training for the development of joint attention skills in young children with ASD. With the system, a child interacts with a virtual agent to complete joint attention tasks while the system monitors performance, touch, eye gaze information and head pose estimation-measured using a cameraand also estimates user affect through machine learning analysis (Porayska-Pomsta et al. 2012) . Escobedo et al. (2012) implemented and tested MOSOCO, an application that uses AR to extend a validated social and educational curriculum, using a mobile platform to increase accessibility. Use of the application by dyads of ASD and typicallydeveloping (TD) participants suggests that the system was well tolerated and effective in improving social communication skills in ASD and TD participants alike (Escobedo et al. 2012) . Boyd et al. (2015) analyzed dyadic interactions among children with ASD playing a collocated and collaborative iPad game. The researchers made recommendations for future design of such games for facilitating three distinct levels of social Process Measures of Dyadic Collaborative Interaction for Social Skills 13:5 interaction: membership, partnership, and friendship (Boyd et al. 2015) . Zhang et al. (2016) implemented a tablet-based Android application that implemented enforced collaboration, turn-taking, and information-sharing activities in two different types of games: puzzle-assembly and blockstacking. The game was played by dyads who were in separate rooms but were able to hear and see one another through an embedded audio/video streaming feature. The researchers found that participants demonstrated increased collaboration and improved performance, and that participants with ASD spoke fewer words than their TD counterparts (Zhang et al. 2016) . Zhao et al. (2016) developed a collaborative virtual environment (CVE) system that included three different games, each employing enforced collaboration as well as information-sharing and turn-taking. The system used Leap Motion controllers as the input method for participants in different locations, requiring sophisticated motor coordination in addition to verbal communication and cooperative planning. The researchers conducted an intervention study with dyads of ASD-TD and TD-TD structures. Results indicated increased verbal communication as measured by the change in word count, as well as a reduction in length of time to complete games that suggested increased collaborative efficiency (Zhao et al. 2016) . The growth and diversity of recent collaboration-based social skills intervention systems suggests that this approach to intervention holds promise for improving outcomes for individuals with ASD. However, few existing systems collect collaborative interaction metrics that are objective and quantitative, relying instead on subjective, qualitative, survey-centric measures, which may limit their revelatory scope (Gress et al. 2010) . We have implemented and pilot-tested DOSE, a novel collaborative computer game and data collection platform designed to promote social interaction through engaging, collaborative activities. Our ultimate aim is to provide more targeted and effective interventions for individuals with ASD, but the potential benefits of this system may be more broadly applicable to other disorders characterized by social impairment as well.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: The growing proliferation of wearable sensors that measure biosignals has begun to open new doors for leveraging the real-time measurement of activity such as heart rate, skin conductance, and brain activity, to improve our daily lives. While major inroads have been taken to investigate the implications of physiological sensors for intrapersonal outcomes, such as health management, we are only beginning to see a glimpse of the potential for the sharing of physiological data in interpersonal, social contexts. For instance, step tracking devices such as the FitBit have enabled users to engage in social fitness competitions, while heart rate-sensing smartwatches like the Apple Watch have introduced haptic heartbeat sharing. As new modes and means for sharing and understanding our data emerge, it becomes increasingly critical to understand the implications of revealing our biosignals to 77:2 \u2022 F. Liu et al.\nINTRODUCTION: others. What are the social and psychological consequences of the ability to share our physiological data? How can we inform system design and policy that account for people's preferences and help them become informed participants in the use of physiological sensing systems?\nINTRODUCTION: Users of ubiquitous technology already share sensed data such as location and activity streams with others, in order to help them communicate and connect with each other [14, 38, 46, 52, 56] . Shared biosignals similarly have the potential to support interpersonal communication. For instance, biosignals such as heart rate are known to change with our mental states, and have been used in the field of Affective Computing to predict and reduce moods such as stress and frustration in order to increase well-being [9, 23, 25, 37, 49] . Our affective states inherently provide social information about our needs, attitudes, and intentions [28, 57] ; thus, biosignal-based affective technologies could provide new ways to convey this information and support social interactions. By expressing ourselves with biosignals, we have the potential to communicate our subjective experiences, and ultimately better understand and connect with one another. However, little is known about when, to what extent, and with whom individuals are likely to want to share their biosignals.\nINTRODUCTION: The present work explores one specific implication of ubiquitous physiological sensing technologies: the opportunity they afford for sensed and shared expressive biosignals to serve as social cues in communication contexts. Given the relative novelty of expressive biosignals, and the little empirical work that has investigated their social impact, we take a broad, exploratory approach to better understand how individuals would utilize and respond to a system that allows for the real-time sharing of their physiological responses. Using a combination of Experience Sampling Methodology (ESM) and semi-structured interviews, we investigate users' sharing patterns, including the contexts most likely to trigger or inhibit sharing, motivations underlying sharing decisions, and the communicative and interpersonal consequences of those decisions. We contribute a study that reveals how people share their heart rate through their natural communication channels, finding that heart rate can be used for interpersonal expression of emotion, daily activities, and playfulness, depending on contexts and relationships between users. We present a set of design implications based on our findings that suggest new directions for the development and integration of expressive biosignal systems into social interactions.",
    "relatedWork": "RELATED WORKS 2.1 Biosignals Sharing Systems: A number of systems have been built to sense and monitor biosignals; however, most of these systems have focused on applications for individual use. For instance, popular commercial wearable heart rate monitors, such as the Fitbit or Mio watches, and several research systems have used heart rate to support fitness and physical health [20, 44, 47, 54] . Affective Computing research has expanded biosignals to social applications (in addition to health), detecting emotional and psychological states for social skills training and virtual tutors [4, 12, 13, 32, 45] . However, these applications still target individual understanding and monitoring of physiological data.\nRELATED WORKS 2.1 Biosignals Sharing Systems: Few works have investigated biosignals systems that allow for sharing in social and communicative contexts. These include systems for supporting interpersonal relationships [42, 53, 61] and collaboration [55] , increasing interactivity and encouragement in physical activities such as marathons [10, 43, 60] , and facilitating engagement in presentations and entertainment [17, 22, 50] . All of these systems focused on very specific use cases and events, and were tested over short periods. Solv\u00e1k and colleagues sought to build on these prior efforts by investigating biosignals sharing in a more natural setting over a longer period of two weeks. They deployed a technology probe (a laptop that provided visual and aural feedback of heart rate) in the homes of five couples and analyzed their reactions to the probe, finding that heart rate was used as information about emotional states and fostered connection between household members [51] . A recent study by Hassib and colleagues expanded this work by going beyond couples' homes and deploying HeartChat, a mobile heart rate chat application, in the wild with seven pairs of close friends or partners. They similarly found that heart rate sharing was able to foster connection and awareness, and that heart rate acted as both an emotional and contextual cue [21] . However, in both of these past works, participants had limited control over sharing, and described situations in which they might not be willing to share heart rate because it was \"too personal,\" awkward, or not understandable. Authors from both works suggest that sharing one's heart rate could thus potentially undermine impression management.\nRELATED WORKS 2.1 Biosignals Sharing Systems: The present work furthers this line of research by deploying a heart rate sharing system on users' mobile phones in order to understand the everyday contexts in which users are most willing or unwilling to share their heart rate. Our system prompts users to make a decision about sharing in order to give them control over when, with whom, and how to share their heart rate with others. Additionally, sharing is conducted through existing messaging applications in order to provide a natural way through which heart rate can be communicated. Our work allows us to explore heart rate sharing at a deeper level, by investigating not only the contexts in which people would be willing to share their heart, but why they might share or not share: RQ1: When and why would people be willing or unwilling to share their heart rate with others?\nSocial Meaning of Heart Rate: Past research has shown that biosignals are inherently ambiguous and open to multiple interpretations [26] , which could subsequently affect the way they are shared. For the purposes of this study, we focus on heart rate, which people tend to associate with underlying emotional and psychological states [51] , yet is known to elicit diverse perceptions of others depending on the context. For instance, Merrill and Cheshire demonstrated that an individual's elevated heart rate is typically associated with negative mood, such as being upset or anxious, which can affect how others trust one another, depending on the situation and relationship with that person [39, 40] . In addition, the heart has been associated with feelings of closeness-for instance, the sound and feeling of one's heartbeat are both considered intimate cues [27, 61] . Solv\u00e1k and colleagues suggest that heart rate sharing could even be seen as a form of emotional self-disclosure, which may only be desired between individuals who have a close relationship [51] . Altogether, these works suggest that the relationship between individuals and the context in which heart rate is accessible plays a key role in determining how heart rate is interpreted and understood by others with whom it is shared, and subsequently how it can be meaningfully expressed through sharing. Our research extends these works by investigating heart rate sharing in a breadth of contexts, giving users the opportunity to decide if and how they want to share to others, as well as how sharers and recipients alike manage or resolve the ambiguity of heart rate. We thus explore the following research questions: RQ2: How can people meaningfully express their heart rate to others?\nSharing and Ubiquitous computing: Though research around biosignals in ubiquitous computing tends to focus on individual monitoring, several works in this field have explored the sensing and sharing of other types of user data. For instance, a number of researchers have investigated preferences and practices around location sharing [38, 52, 56] , including sharing behaviors based on hypothetical ESM requests from contacts to share location [7] , and willingness to share with different types of contacts [62] . Other ubiquitous systems have been built to record and share streams of user activity. Ubiquitous healthcare, for instance, is a growing field with multiple areas of application, including activity tracking of elderly people to inform physicians and family of their daily life activities and physiological states [15, 19] . Systems that track and share personal data, particularly physical activity, through social awareness streams like Twitter are also commonly used to connect with friends and family [14, 46] . Some systems have also monitored and publicly displayed user activity levels for the support of collaborative tasks and work\nSharing and Ubiquitous computing: 77:4 \u2022 F. Liu et al.\nSharing and Ubiquitous computing: environments [31, 59] . However, many of these systems focus on individual sharing behaviors and preferences, and have generally not explored the interpersonal consequences and interactions that might result from sharing.\nSharing and Ubiquitous computing: Our work expands on past work on sharing user data ubiquitously by investigating the sharing of physiological data-specifically heart rate sharing on mobile phones in everyday contexts. Further, we consider not only users' sharing behaviors, but also how sharing affects their subsequent interactions with others with whom they share: RQ3: What are the interpersonal consequences of sharing heart rate with others?",
    "rq": " rq2: how can people meaningfully express their heart rate to others?\n rq3: what are the interpersonal consequences of sharing heart rate with others?"
}
{
    "intro": "INTRODUCTION: Social robots and physical agents capable of recognizing others and interacting with them play a substantial role for those who experience loneliness [9] . The rapid growth of one-person households around the world has accelerated the emerging importance of social robot research in this situation [5] . To our knowledge, while many research on social robot focuses on elderly households as a main subject [14, 20, 22] , the increasing societal issue of single households no longer applies to the elder group only. For example, the number of young adults between the ages of 18-34 living in one-person households stands at five million, making it the fastest-growing population of any age group in the United States [25] .\nINTRODUCTION: Since one-person households have far more difficulties obtaining social support than other households [12, 30] , they are more vulnerable to suffering from depression and social isolation [29] . In particular, loneliness among young adults is likely to lead to high risks [19, 24] . Social isolation is associated with a lack of quality and quantity of social relationships among individuals [8] and is highly correlated with suicide [6, 17] . This phenomenon suggests an increasing need for a resolution to the social isolation of young, single adults, especially within the social robot research domain.\nINTRODUCTION: However, state-of-the-art social robot research to this end has shown a couple of shortcomings. On the one hand, it mainly focuses on one-to-one interactions between the robot and human [1, 16, 27, 31, 32] . As this approach only looks at an effect of the single unitary human-robot relationship, it is rarely promoting additional real interactions with others outside the closed relationship between a single robot and a human. On the other hand, such social robots in home environments bring about privacy concerns. Most social robots use cameras to collect information about people and their environment, so there are growing concerns about privacy invasion. In particular, robotic research mediating positive social interactions (e.g., social telepresence robots) is seen as a source of serious privacy problem when used in a home environment [3, 7, 15] .\nINTRODUCTION: To overcome these, we propose a social networking robot (SN-Robot) that recognizes auditory information (e.g., living noise) and shares extracted information about activities in the living environment (Fig. 1 ). \"Living noise\" is audio-centered information about occupant's activities that excludes the user voice in the daily living environment. Usually, previous studies emphasized the negative aspect of such living noise in Human-Robot Interaction [11, 13, 26] . However, the absence of such noise may in effect cause one to feel lonely and remind them of their socially isolated state [28] . In this sense, collecting, recognizing and sharing the living noise with others can be seen as a means to improve the feeling of social connectedness in their mind. In addition, living noise is an anonymous data that contains no identity information. This may contribute to the relief of privacy concerns between the social robot users. The overall process of the study investigating the effects of sharing anonymized activity information based on living noise is as follows. First, pre-survey and experimental research were conducted to specify the concept of the robot. Second, we developed the first prototype with minimal activity sharing and deployed the prototype afterward for a four-week field study to evaluate the experience of one-person households. Third, we developed the second prototype, the SN-Robot, \"Fribo\", which can recognize specific living noise, based on the user experience results from the field study. We believe that this research contributes to the HRI research community in three ways:\nINTRODUCTION: \uf0b7 Living noise, usually seen as a negative factor that is inevitably experienced when living with others, is used as a positive factor to enhance the sense of co-residence. \uf0b7 We utilize acoustic recognition without sophisticated sensors to recognize user activities and minimize privacy concerns when collecting and sharing activities in the home environment. \uf0b7 We suggest the possibility that the social robot could perform the role of a \"trigger\" which encourages followup social interactions among people.\nINTRODUCTION: The organization of the paper is as follows. Section 2 reports the pre-studies and their results. In Section 3, the details of the field study are introduced, which consist of a description of a robot prototype, participants, procedure and findings on the study. It is followed by the proposal of Fribo, a social networking robot based on sound recognition in Section 4, inspired by a design principle elicited by implications of the field study described in Section 3. We conclude the paper with future works in Section 5 after the discussions on limitations of this study.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Recent years have seen the rapid development of virtual reality (VR) environments and related technology, which have also been utilized as tools for improving personal well-being, for example as a platform for mindfulness-based meditation practice [28, 41, 54] . A strong presence-inducing and disturbance-free virtual environment [20, 53, 59 ] seems an ideal solution for quick stress management exercises during the work day, where longer breaks and separate physical spaces would be impractical. In this article, we present an approach to extend that basic idea in two ways: First, by incorporating respiration and brain wave-based neurofeedback and visualizations to support meditation, and, second, by enabling social interaction in a multi-user virtual space where more than one user can meditate simultaneously in VR. Additionally, we present how these two advances can be tied together by measuring and visualizing dyadic physiological synchrony-i.e., the joint changes in physiological signals-between users, enabling a system responsive to the shared physiological states of the users. Augmenting a variation of traditional compassion meditation with our VR system DYNECOM, we conducted a controlled laboratory experiment to examine how the different neurofeedback types affected both the amount of physiological synchrony between the users of the shared space and their self-reported empathy and social presence.",
    "relatedWork": "",
    "rq": "this leads us to the first research question: how are empathy and social presence affected by neurofeedback training with a partner in a vr meditation environment?"
}
{
    "intro": "INTRODUCTION: Impairments in executive function skills (EF), which include domains such as working memory, flexible thinking, and inhibition of impulses, are commonly reported among individuals with Autism Spectrum Disorder (ASD) [Hill 2004; Ozonoff and Jensen 1999; Ozonoff et al. 1994] . Executive function skills are crucial for effective coordination and completion of tasks, with EF deficits associated with poor adaptive, academic, and employment outcomes [McLean et al. 2014; Biederman et al. 2006 ]. These skills are especially important to multitasking, or the ability to plan, coordinate, and complete multiple tasks within a given time period not in a sequential fashion but rather by interweaving tasks by switching back and forth between them [Law et al. 200] .\nINTRODUCTION: Because of the relevance of EF to so many academic, adaptive, and employment tasks, several recent studies have investigated EF in ASD by using multitasking paradigms [Mackinlay et al. 2006; Rajendran et al. 2011; White et al 2009] . Multitasking involves many aspects of EF that may be hard for someone with ASD, such as switching attention between components and practicing flexible thinking. Mackinlay et al. [2006] investigated multitasking in 14 children with high-functioning ASD (HF-ASD) and 16 typically developing (TD) controls. Results indicated that the ASD group generated significantly fewer strategic plans, attempted fewer tasks, and less flexibility in switching between tasks, and that they broke rules more frequently than the TD controls. Rajendran et al. [Rajendran et al. 2011 ] used a modified version of the Virtual Errands Task (VET) [McGeorge et al. 2001] to investigate EF and multitasking in 18 adolescents with HF-ASD and 18 TD controls. They found that inflexible planning, low inhibition, as well as difficulties with prospective memory (i.e., remembering to carry out intentions) may underlie multitasking difficulties in ASD. Finally, Hutchison et al. [2019] examined EF in relation to basic functional communication (FC) and more complex verbal conversation (VC) skills among 92 children with ASD and 94 TD controls. They reported that metacognition or \"thinking about thinking\" was a strong predictor of FC, while the domains of behavioral regulation and inhibition were predictive of VC skills. Therefore, they suggested that targeting EF domains specifically might improve FC and VC skills in children with ASD. Collectively, this work supports the importance of EF when understanding how to support people with ASD in learning new skills.\nINTRODUCTION: The impact of EF on multitasking and its impact on individuals with ASD becomes more salient when one considers that multitasking is a ubiquitous requirement of everyday activities, including social interactions. Each interaction draws upon not only the need to inhibit impulses, pay attention to cues, remember what has just happened, and plan for what happens next; many also involve motor skills, another common area of deficits for many people on the autism spectrum [Rao et al. 2008] . Motor skill deficits (including gross motor, i.e., postural control and limb movements; and fine motor, i.e., object control, manual dexterity and visuomotor integration) are incredibly prevalent among children with ASD, and are estimated to occur in 90% of individuals with ASD across the lifespan [Gowen and Hamilton 2013; Ming et al. 2007; Jansiewicz et al. 2006; Forti et al. 2011; MacDonald et al. 2013] . Fine motor challenges in children with ASD include trouble with grasping and reaching [David et al. 2012] , eye-hand coordination [Crippa et al. 2013] , and handwriting skills [Johnson et al. 2013 ]. These basic tasks, which many people execute almost automatically, likely require extra effort, control, and mental attention from people with ASD.\nINTRODUCTION: When thinking about designing intervention paradigms for individuals with multiple areas warranting attention, it follows that focusing on a single area of deficit (e.g., conversation) rather than incorporating multiple integrated targets (e.g., conversation as part of game-playing) may hinder the generalizability of learned skills to complex real-world activities. In particular, it would be problematic if additional real-word components created added levels of difficulty and complexity that could impact success. Existing literature suggests that teaching motor skills to children with ASD may help create a context for practicing social skills and lead to further social success [MacDonald et al. 2013] . For example, Chetcuti et al. [2019] conducted a study with 35 children with ASD and 20 TD children to examine the role of social motivation and motor execution factors in object-directed imitation difficulties in ASD. They found that difficulties in object-directed imitation in ASD might be the result of motor execution difficulties, and not reduced social motivation. Srinivasan et al. [2015] evaluated the impacts of rhythm, robotic and standard-of-care interventions on 36 children with ASD (5-12 years of age). They found that socially embedded movement-based contexts are valuable in promoting imitation/praxis, interpersonal synchrony and motor performance. Fulceri et al. [2018] applied Artificial Neural Networks (ANNs) to understand the entire spectrum of the relationship between motor skills and clinical variables. Their findings suggested that poor motor skills were a common clinical feature of preschoolers with ASD, relating both to the high level of repetitive behaviors and to the low level of expressive language. Collectively, these findings suggest that to benefit from social skills training, some individuals might also require training in other, related functional domains, like motor skills.\nINTRODUCTION: Compared to extensive research focusing on the social deficits of ASD, motor deficits of children with ASD and their impacts on social skills are relatively underexplored, especially within the context of technological intervention. To address this issue, a training system that can provide combined social skill and fine motor skill practice is needed. One computer-assisted approach to address social challenges of children with ASD is the Collaborative Virtual Environment (CVE) [Zhao et al. 2018; Zheng et al. 2017; Battocchi et al. 2009 ]. Battocchi et al. [2009] designed a tabletop collaborative puzzle game featuring enforced collaboration to facilitate cooperative behaviors in children with ASD. However, their work did not consider the importance of haptic communication or motor skills for strengthening social interaction. Simulating the sense of touch and physical contact with shared objects in CVEs could enhance feelings of social presence and task performance [Salln\u00e4s 2010; Basdogan et al. 2000 ]. Most existing work combining technology and fine motor skill intervention has focused not on social interaction, but on handwriting analysis [Rosenblum et al. 2016; Palsbo et al. 2012; Kim et al. 2013; Zhao et al. 2018; Zhao et al. 2018] . Although important to functional outcomes, handwriting does not capture the other fine motor tasks related to social interaction for children, such as holding and moving a puzzle piece, coloring a picture, or playing a video game. If children struggle with the fine motor components of these tasks, then it stands to reason that they may have less cognitive attention and energy left to focus on maintaining positive social interactions.\nINTRODUCTION: There is, to our knowledge, no existing system that provides and measures responses to opportunities to simultaneously practice social skills (e.g., communication and collaboration skills) and fine motor skills (e.g., hand movement and grip control skills), or investigates the impact of fine motor skill improvement on social skill trajectory. The primary contributions of our work are two-fold. First, we developed a Collaborative Haptic-gripper virtual reality system (C-Hg) to simultaneously practice social skills (i.e., communication and collaboration skills) and fine motor skills (i.e., hand movement and grip control skills). In the collaborative mode, a Collaborative Haptic Virtual Environment (CHVE) provides flexible haptic interactions over the Internet between remote users. Social communication and cooperation skills are required to successfully implement the carefully designed collaborative fine motor tasks. Second, we conducted a usability study to explore the impact of our system on fine motor skills and, subsequently, the impact of fine motor skill performance on the change in social skill performance across tasks.\nINTRODUCTION: The remainder of this article is organized as follows. Section 2 introduces the C-Hg system design. Section 3 presents a usability study followed by the study results. In the final section, we discussion the contributions of our work, its limitations, and important future directions.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: With the advent of the age of information and advancements in communication, there is no denying the blessings that the application of information technology has brought about. Human life has thus become much easier, and to such extent that disabilities no longer pose any menaces in conducting regular life . A statistic from the World Federation of the Deaf [26] shows that there are approximately 72 million people who are born with partial or complete hearing or speech impairments. Among them, 80% worldwide are living in developing countries. This group of people use more than 300 different sign languages to communicate. According to Dr. William C. Shiel Jr [15] , a specialist physician in speech disorder, sign language includes any language that employs signs made with the hands and other movements, including facial expressions and postures of the body, used primarily by deaf people. Although using sign language, the deaf and hearing-impaired can communicate without problems within the community, it is a serious challenge for them to try and integrate this into educational, social and work environments at large. Especially it is very perplexing to communicate with non-deaf-mute people, those who may not be well aware of the sign language.\nINTRODUCTION: To make information accessible to the deaf-mute community many organizations like World Association of Sign Language Interpreters (WASLI) and World Federation of the Deaf (WFD) are working by employing human sign language interpreters. In news broadcasting and multinational diplomatic meetings speeches are converted to a standard sign language by these professional interpreters. But in case of general mode of communication, non-deafmute people also have to rely on an interpreter. However, with the intervention of technology, there are a few 'speech to sign language' translation systems [28] that convert human speech into certain sign languages. But what can be done if a sign language speaker wants to convey his/her message without the involvement of an interpreter?\nINTRODUCTION: Over the years many innovative methods have come forward to solve the problem faced by the hearing impaired through the use of text-audio converters, sign language interpreting systems, machine translators etc. For example, O'Connor [21] used a glove to convert the gestures which had leap motion sensor, gesture sensor and other high-end technologies for interpreting American Sign Language (ASL) to text; while Islam et al. [13] developed an efficient tool comprising a special sensor-fitted glove and a mobile application for vocally impaired people of Bangladesh to learn Bengali alphabet through sign language. A similar commercial tool is available where a dedicated device is required that uses depth imaging to map the gestures [19] .\nINTRODUCTION: Some other existing applications [6, 20, 32 ] take text input and generate audio output and others simply show corresponding signs for the texts. Thus, strong efforts have been carried out to develop intelligent and natural interfaces between users and computer systems based on human gestures. However,the performance of a system is measured to a considerable extent by its usefulness and usability. Our analysis of the existing solutions showed that none of these systems provide a complete solution. These systems may qualify in terms of usefulness but most of these lack in usability which is being considered as one of the key quality attributes of any digital solution [8, 35] . There is no full integration of all the features required for a mute person to flexibly have a conversation with another person.\nINTRODUCTION: Therefore, with this motivation of incorporating all required features into one system, the objective of this research is to develop an American sign language interpretation system that focuses on the finger-spelling method for communicating with deaf-mute people in their respective environments. Like other languages, American Sign Language(ASL) combines facial expressions, hand movements and certain changes in body language to express a message [33] . However, in this system, the finger-spelling method has been emphasized due to its abundant use [23] . Finger-spelling is a part of ASL in which each letter is represented by a unique hand gesture. Any English word can be spelled through finger-spelling method. In addition to this, this system also incorporates signs of some commonly used phrases. Spoken words or proper nouns outside of these common phrases are finger-spelled to the user, and the system can also detect finger-spelled words from hand movements.\nINTRODUCTION: The rest of the paper is organized as follows: Section 2 presents the works relevant to this field, Section 3 describes the conceptual design and implemented system features. In Section 4 overall system design along with implemented system features have been described. This is followed by Section 5 where the testing and evaluation is shown. Finally Section 6 highlights the main outcome, limitations, the future work and ends up with a concluding remark.",
    "relatedWork": "RELATED WORKS: Researchers have been working with sign language interpretation for decades now. This section contains a brief overview of the related works that focus on the communication of deaf-mute people. The research can be classified into three broad categories i.e. i) Sign Language Learning Tools, ii) Sign Gesture Detection and Conversion to Text or Speech (Hardware based and Computer Vision Based Models), iii) Gesture to Speech Conversion. The proposed system aims to integrate all of these into a coherent solution.\nRELATED WORKS: There are lots of tools that aim to teach sign language to nonmute people. Sarker et al. [29] developed a translator for text to gestures that a deaf user can understand. In [7] , a communication and educational tool has been proposed that principally works as a language learning tool and also as an interpreter with acceptable accuracy and user satisfaction. A mobile application named \"Learn to sign\" [27] has features like alphabet and number recognition, video tutorials etc for the hearing impaired as well as people who want to learn basic signs and phrases.\nRELATED WORKS: Koller [16] made a quantitative analysis on researches done on sign language interpretation between 1983 and 2020 which highlights many state of the art technologies and benchmark dataset. 30 different sign languages were detected based on RGB image, depth image, color and glove data and using Mocap as inputs. Among them RGB datasets are enriched with most vocabularies.\nRELATED WORKS: Gesture detection is an active field of research which focuses on both hardware and image processing based technologies. Rebollar [11] innovated a sign language recognition apparatus and method for translating hand gestures into speech or written text. The apparatus includes sensors such as accelerometers on the fingers and on the back of the hand to detect motion and orientation of the hand and can detect static and dynamic gestures. Similarly in [12] , a microprocessor is used to process the sensor data and generate output signals by searching through a library that can then be used to produce a synthesized voice or written text. In [22] , a glove named \"SignAloud\" has sensors to record movement and gestures then transmit the info wirelessly to a central computer. The computer then looks at the data, and if it matches a gesture, then the associated word or phrase is spoken through a speaker. Lee & Lee [18] have developed a wearable system to interpret sign languages. It uses flex sensors and an inertial motion sensor to distinguish the characters in the ASL alphabet.\nRELATED WORKS: While these hardware tools can detect static and in some cases dynamic gestures as well, there remains a question -How practical are these tools in terms of real life application? An article from \"The Atlantic\" highlighted several major facts that these hand gloves miss and utterly overlook so many of the communication difficulties (facial expression, coarticulation) and frustrations that Deaf people may face [9] . Also it is quite impractical to wear a glove with a mictrocontroller and many sensors all the time. So our research focused on a software based solution. Another study [2] highlights that similar to other naturally evolved languages, linguistics is essential for sign languages too. The study also mentions the importance of 'finger spelling' method recognition, included in our proposed system. This method of signing is especially used in spelling out names of people or organizations.\nRELATED WORKS: According to Yuan et al. [34] , the field of 3D hand pose estimation has advanced rapidly, both in terms of accuracy and data set quality. Estimation of the hand pose leads to detection of sign gestures. Most successful methods treat the estimation task as a learning problem, using random forests or convolutional neural networks (CNNs). Two methods have been introduced, including the detection-based and the regression-based method; where detection based methods produce a probability density map for each joint, and regressionbased methods directly map the depth image to the joint locations or the joint angles of a hand model. Yuan et al. [34] also found that detection-based methods tend to outperform regression-based methods. . In this vein, Gay et al. [10] proposes a deep learning approach that solves alignment and recognition problem which was implemented for recognizing sign language. Machine learning approaches that can recognize sign language with a rough word error rate of 40.7% came forward. In [4] , a better algorithm was introduced with much higher accuracy. In hand tracking, current discriminative methods divide the problem into two sub-tasks: detection and pose estimation, without using the hand shape provided in the first frame. This used skin detection algorithm and classified the image according to the sign gesture performed. The model will detect the skin color of every type using YCbCr algorithm and use Bag of features(BoF) for feature extraction and Support Vector Machine (SVM) for training and evaluation. The average accuracy of this evaluation set is 86%. Jin [14] proposed a novel framework comprising established image processing techniques to recognise images of several sign language gestures. More specifically, using Canny edge detection, seeded region growing, Speeded Up Robust Features (SURF) algorithm, BoF, Support Vector Machine (SVM) is able to recognize and translate 16 different American Sign Language gestures with an overall accuracy of 97.13%. Some more works [1, 17, 24] are done in recognition of Sign Gestures using image processing techniques for different sign languages with the same principle. In [31] , a system was proposed that uses one color camera to track hands in real time and interprets ASL using HMM. The hands are tracked by their color in the first experiment via solidly coloured gloves and in the second, via their natural skin tone. In another study [25] , the SVM classifier preprocessed images using canny edge detector, sobel edge detector and gaussian filter to extract features. The accuracy for mean of features is calculated as 96.347%. The leap in the accuracy from 86% to 96% is notable here which makes SVM one of the best classifiers in detection of Sign gestures. Similarly, in [30] a preprocessing method was used to detect edge and features. This provides an optimal method for recognition of 24 English letters. In a recent study [5] , a Leap Motion controller was used to extract some basic features including the hand palm sphere radius, the hand palm position, and fingertip position.\nRELATED WORKS: Again a recent study conducted by Camgoz [3] has directed focus on how the spoken language translation of the signs arranged in order, differs in both length and ordering. It was highlighted that this is not just due to gesture recognition issues, rather due to a lack of knowledge about the underlying linguistic properties of sign language and the assumption that there is a one-to-one mapping of sign to spoken words. A sequence to sequence(seq2seq) based deep learning method was used to get a new perspective on the mapping process.\nRELATED WORKS: In summary, this literature review brings out a few important concerns. First, it can be seen that no system integrates both sign language to speech conversion and speech to sign language conversion in the same platform. Second, there is no global common ground for all the sign language interpretation. Third, the learning tools have the portability being mobile applications, but the interpretation systems lack portability and use expensive hardware. Finally, the prototypes fabricated here merely took into account the usability of deaf and mute populace which is a crucial concern when it will be put to the test. In an attempt to bring together these key points into a solution the focus of our research has been to improve on those areas to mitigate these problems to some extent and create a simple, easy-to-use device and thereby bridge the communication and social gap that exists.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Teams commonly collaborate around a shared visual document (e.g., user interface (UI) design or presentation slides). This is an essential process in both the workplace and academic settings. For productive and successful collaboration, frequent communication is necessary to develop a shared understanding between team members [25] . However, for teams of students, communicating efectively and efciently can be challenging. Students are generally novices who lack the design knowledge necessary to express themselves efectively when discussing their design. Additionally, diferences in course schedules and the lack of shared ofce space frequently restrict them to collaborate remotely and asynchronously [42] .\nINTRODUCTION: Communication-related burdens introduced by an asynchronous setting can discourage students from discussing with their teams. Most asynchronous communication channels rely on text (e.g., text messaging and Google Docs' comments [19] ), but typing involves high physical and cognitive demand [28] . Also, although efective referencing is essential in developing a shared understanding [22] , text makes referring to visual objects in the document challengingas pointing while typing is impossible. Unfortunately, students may lack the knowledge needed to overcome the restrictions of text [51] .\nINTRODUCTION: Furthermore, regular scheduling issues between students [29] may prevent them from frequently checking on their team's messages and documents. This introduces a signifcant delay in communication and team members' communicative needs may not be satisfed in a timely manner. In addition, when students do check on messages, they may have to look through a large volume of messages [56] , and references might not be adequate as the objects referenced may have changed after the messages were sent [52] .\nINTRODUCTION: As shown by our formative study, feeling uncertain about receiving a response on time, and the aforementioned burdens when producing and consuming messages may lead to communication breakdowns in student teams [34] . Although professional teams have organizational support to handle these breakdowns [25] , instructors may be ill-prepared or time-constrained to adequately provide similar support to students [41] .\nINTRODUCTION: In this paper, we propose a novel way of communication with multimodal messages of voice and clicks-referred to as linked tapes-as an alternative form of asynchronous communication. Linked tapes are multimodal messages created by simply speaking while pointing at relevant visual objects through clicks, interactions which could require less efort when compared to typing text messages. When a tape is produced, the current version of the document is stored to preserve the temporal context of the tape to enable change awareness [44] . Additionally, bidirectional links between objects and voice snippets are automatically generated by temporally mapping the modalities onto each other (Fig. 1 ). With the voice-to-object link, playing back a voice message can display relevant objects to allow the receiver to efortlessly understand the references in the message. With the object-to-voice link, selecting an object of interest can flter through numerous voice messages to retrieve only those relevant to the object and facilitate the receiver's navigation. The ease of producing linked tapes with the multimodal input and the support provided by the bidirectional links can vitalize communication in asynchronous teams and improve a shared understanding.\nINTRODUCTION: We actualized communication based on linked tapes in Winder, a plugin for the collaborative UI design tool Figma. The plugin leverages the linked tapes' bidirectional links to implement three main features: (1) highlighting on playback (Fig. 1a ), (2) inline thumbnails on transcripts (Fig. 1b ), and (3) object-based search (Fig. 1c ). In addition, Winder also aims to tackle the problem of communication delays. The plugin periodically prompts the user to produce linked tapes with the goal of preemptively obtaining information which may be needed by team members in the future. Tape-based communication could lessen the burden imposed by this approach-the efort of producing and consuming many messages. Thus, the drawbacks can be outweighed by the potential gains in shared understanding.\nINTRODUCTION: To investigate the efect of Winder on the collaboration process of asynchronous teams, we conducted a fve-day study with eight teams of three students (N=24). Teams were tasked with designing the UI of a mobile application, which helps friends decide on what and where to eat. On the frst day, they discussed ideas as teams and, on subsequent days, each team member worked on the design on diferent time slots. Our fndings showed that the participant teams produced an average of 13.13 tapes and that the average tape length was 53.27 seconds. Analysis of survey responses revealed that, when compared to text messages, participants felt less burdened producing linked tapes due to the ease of speaking and clicking, and felt more confdent that their messages would not be misunderstood. Participants also expressed that bidirectional links facilitated navigation through and within tapes, as well as their understanding of these tapes. Furthermore, the study results suggest that tapes recorded preemptively could allow for communication at hand without having team members at hand.\nINTRODUCTION: Our work contributes a novel multimodal asynchronous communication tool, Winder. Through lightweight interactions in production (i.e., click and voice) and bidirectional links for consumption, the system advances work in asynchronous communication by simultaneously decreasing burden for both senders and receiversprevious work facilitated either but not both. Furthermore, reducing at-the-moment burdens allows for an approach to tackle communication delays that would previously be overly burdensome: prompting users for preemptive recordings to satisfy future communication needs. As a secondary contribution, we present empirical fndings that demonstrate the potential of Winder to reduce bilateral communication burden and overcome the detriment of delays in student teams.\nCONCLUSION: This paper presents Winder, a novel system that supports asynchronous communication between students in UI design collaboration. Winder provides communication through linked tapesmultimodal recordings of voice and clicks that contain bidirectional links between the comments and document objects. Additionally, by prompting the user to record tapes, Winder preemptively obtains information that can substitute direct communication when satisfying team members' needs, thus reducing the impact of communication delays. A fve-day user study showed the efectiveness of linked tapes and preemptive recording in the collaborative processes of students. Finally, we discussed how Winder can be used in practice and generalized to other contexts, and the implications of linked tapes on general asynchronous communication.",
    "relatedWork": "RELATED WORK: We frst review work on incorporating multiple modalities into asynchronous communication, then on communication anchored on documents, and fnally on handling communication delays.\nMultimodal Asynchronous Communication: Through modalities that are complementary, multimodal interaction is able to be more efcient and robust (i.e., less error-prone) than unimodal alternatives [37] . Alongside the accepted value of non-verbal methods for building common ground [22] , this has inspired a rich body of work to integrate multiple modalities into asynchronous channels. To enhance expressiveness in asynchronous ideation, SketchComm [31] provides a shared canvas onto which designers can freely interweave audio, photo, video, and hand-drawn sketches to express their ideas. RichReview [54] and RichReview++ [55] record voice annotations alongside ink and pointing gestures on a tablet. The combination of these modalities allows asynchronous collaborators to discuss around a digital text document as they would with a physical document if they were co-present. Other work such as Video Threads [4] relies on video and audio as the main components of communication by allowing users to create threads of video messages. In a more social and emotional context, FamilyStories [24] incorporates voice and physical actions to kindle a feeling of togetherness between family members distributed in place and time. As seen, previous approaches to multimodal asynchronous communication rely on combinations of sketching, gesturing, or video with voice. In our work, we instead focus on clicks to support deictic referencing-an interaction proven to be essential in remote settings [32] -while being simple enough to reduce the efort of message production. Like previous work, on the other hand, we also incorporate voice or speech as it is faster than typing and rich due to expressive nuances it possesses, such as intonation and loudness [7] . These merits have led to the incorporation of \"voice comments\" in a variety of popular applications such as Microsoft Word [23] or Google Docs, through third-party plugins [46] . However, despite being faster in production, voice recordings are slow to consume and browse, which has impeded the widespread use of this modality [21] -the feature was discontinued in recent versions of Word. A common approach to tackling this challenge is to automatically generate transcripts through speech recognition. Transcribing facilitates consumption as this allows for keyword search, like in Otter.ai [36] , or creating automatic summaries [8, 16, 49] . Speech recognition, however, is limited and may be inaccurate, which has led to the development of systems like TypeTalker [2] that allow the user to correct these errors. Considering the problems with automatic transcripts and the manual efort needed to resolve them, our work takes a diferent approach: automatically linking snippets in the voice recordings to document objects. With these links, we facilitate message browsing by allowing the user to flter out messages irrelevant to an object of interest. In addition, we provide thumbnail images of the objects inline with automatic transcripts of voice recordings to allow the user to navigate to moments in a recording when potentially interesting objects were selected and discussed.\nContextual References and Anchored Communication: While referencing items or sections of a document can be as simple as pointing in a F2F setting, this task becomes challenging and complex in online situations [10] , even more so if team members are asynchronous. The user must provide detailed descriptions or rely on workarounds such as taking screenshots to adequately express the context and prevent confusion. To reduce the cost of creating contextual references, several systems have been designed to facilitate this process in diverse application scenarios. For discussions surrounding multimedia, Korero [14] supports referencing through linking to multiple portions of a video and, on the click of a button, Snapstream [53] instantaneously creates annotatable snapshots of a live stream. In a diferent domain, systems like chat.codes [35] and Callisto [52] have also been developed to support communication between programmers by enabling 'pointing' to code segments in chat interfaces.\nContextual References and Anchored Communication: Beyond incorporating the context of a document into the communication channel through references, substantial work has also explored anchoring the communication on the context itself. For example, the tools by both Churchill et al. [15] and Zyto et al. [57] allow for anchoring discussions on specifc locations in text documents. Similarly, LemonAid [12] anchors question-and-answering communication on UI components of a web application to allow the answerers to provide more contextually adequate help. This work builds on these previous approaches on referencing and anchoring. Our proposed approach of linked tapes allows for multiple references to specifc visual objects in one message-prior work supported referencing that was either singular [12] , to general visual frames [14, 53] , or to textual content [35, 52] . Additionally, anchored communication is possible as the user can select an object to retrieve recordings in which that object was clicked and record their own comments by talking while selecting the same object.\nApproaches to Handling Communication Delays: Delays in communication are an inevitable aspect of asynchronous collaboration. While working on a document, a member of a team may need to communicate with their team; however, the other members may not always be attentive to the communication channel to respond immediately. Previous work has demonstrated that, aside from hindering the overall productivity of the team, these delays can also have social ramifcations such as team members more negatively judging their fellow team members [43] and the overall task [26] .\nApproaches to Handling Communication Delays: The detrimental consequences of delays in communication has motivated multiple researchers to design interventions to mitigate these. For example, Avrahami and Hudson [3] devised a notifcation system which distinguishes messages that require the user's immediate attention, and Pielot et al. [39] identifed features that could predict a user's attentiveness to text messages, which could help manage expectations regarding response times. However, if the message receiver is certainly unavailable, these approaches will not sufciently address existing challenges. A distinct approach explored in the domain of collaborative software development is to rely on external assistance. Codeon [11] and MicroMentor [27] connect a developer with remote helpers who can provide assistance when colleagues are not available. However, this type of support incurs a fnancial cost which may not be practical for all teams. Additionally, while these external assistants can provide technical help, they will not be able to aid with needs specifc to a team's collaboration-e.g., understanding why a certain team member performed a certain change in the document.\nApproaches to Handling Communication Delays: As an alternative, we suggest preemptively obtaining team members' explanations of actions and intentions by prompting them while they are working. Our approach is inspired by think-aloud protocols used to capture participants' cognitive processes during studies on human subjects [50] . Although this type of information does not satisfy all communicative needs, a shared understanding of team members' activities and goals allows for team coordination and is thus consequential to the team's success [48] .",
    "rq": "(1) how does multimodal communication based on voice and clicks afect the burden of sending messages when compared to typing text?\n (2) how do bidirectional links between the visual objects and voice recordings support the identifcation and understanding of information?\n (3) what type of content do preemptively recorded linked tapes contain and how do these impact team collaboration?"
}
{
    "intro": "INTRODUCTION: Generally, social bonds for maintaining and building close relationships with family members, lovers, friends, etc. are weakened when living in a place away from them. This issue has led to depression due to loneliness when assigned to work alone, divorce due to loneliness of not being able to meet, and a decrease in the success rate of long-distance relationships. Recently, the self-restraint of movement by COVID-19 has exacerbated this problem.\nINTRODUCTION: To solve it, in the field of telepresence, techniques have been developed that bring conversations with remote partners closer to face-to-face conversations. The basic approach is to present the appearance and behavior of partners in a high-definition life-size video calling [1] . Currently, because 4 K and 8 K large LCD TVs are widespread, making video calls with extremely high-definition images is possible. However, there exists a limit to the approach to high-definition video because it is difficult to recognize the difference even if the definition is further improved. In addition, regardless of the image quality, it is possible that the conversation itself through the display makes the partners feel distant. The mirror neurons that are activated when a monkey sees human behavior differ depending on whether they are close to or far from a person. It is known that when a monkey sees human behavior through a transparent board, neurons are activated in the same way as when it is far from the person [2] . Even in the case of humans, if the same brain function occurs, it is possible that the perception that the display hinders the feeling of being with a conversation partner in the same space.\nINTRODUCTION: Recently, an approach has been proposed that combines telerobotics with video calls. As one of them, the effect of reproducing shaking hands with a remote partner with a robotic hand has been verified; it is known that the remote hand shaking enhances the feeling of being in the same space as the partner (space sharing) [3] . This result indicates that reproducing social touch with another person may have changed the perception of talking with a remote partner through the display. The study also shows that even a handshake reproduced by a robotic hand can provide intimacy; therefore, it is expected that reproducing social touch will enhance social bonds. According to the findings of this previous study, as shown in Figure 1 , the robotic hand should be installed on the extension of the partner's arm, so as to make the partner's life-size image and the robotic hand in the real space to be positionally consistent. This consistency is effective in enhancing the sense of space sharing. Furthermore, since the previous study used a life-size image and a full-scale robotic hand, the size consistency may also be effective. However, with the spread of smartphones, many users are now making video calls on small displays. In the reduced video call, it is difficult to provide positional and size consistencies to the combination of the partner's video and the social touch device.\nINTRODUCTION: As shown in Figure 2 , when combining a mobile phone such as a smartphone with a robotic hand-type social touch device, the following two designs can be considered: a) separated design, in which the mobile phone is operated with the right hand while the touch device is held with the left hand, and b) combined design, in which a mobile phone is attached to the device and held with the right hand. In the separated design, the touch device can be connected to the mobile phone via a general wireless communication standard such as Bluetooth, so that the function of mediating social touch can be added regardless of the shape of the mobile phone. However, although the user faces the partner's image, the touch device simulates the state of holding the hand of the partner who is positioned to the immediate left. Therefore, positional inconsistency occurred in the separated design. In contrast, the combined design reduces positional inconsistency because the touch device simulates the state holding the hand of the partner who faces the user. However, it is necessary to design a touch device to fit the shape of the mobile phone. Furthermore, size inconsistency occurs in both designs owing to the reduced video. Positional or size inconsistencies may decrease the reality of social touch with a partner. Although such inconsistencies occur, if the conditions under which the reproducing social touch enhances the sense of space sharing and social bonding are clarified, it can be expected to become the design guidelines for social touch devices that expand mobile phones. To clarify this condition, this study developed a robotic hand-type social touch device, and investigated how the positional and size inconsistencies that occur when the device is combined with a video call on a mobile phone influences the sense of space sharing and social bonding.",
    "relatedWork": "RELATED WORKS: Studies on remote touch have long history. Previous studies have proposed various devices that add the function of remote touch to existing remote communication media, such as text chats, voice and video calls, etc. A commonly used haptic stimulus in such devices is vibration [4] [5] [6] . For example, when a user squeezes a mobile device, the partner's device vibrates [4] . Other stimuli include the force of rotation with doorknob-type [7] and roller-type [8] devices, tightening with bands attached to a wrist [9] and an arm [10] , poking a cheek with a pneumatically inflated silicone balloon [11] , and joystick force feedback [12] , etc. Body touch is known to be an effective modality for emotional communication [13] . Most of these studies used haptic stimulus as a modality to convey nonverbal information because text chats and voice calls cannot transmit the users' facial expressions and emotional gestures. Therefore, these previous studies evaluated whether a remote partner could correctly decode the non-verbal information encoded as a haptic stimulus, but the reality of the reproduced touch sensation was not focused on.\nRELATED WORKS: In contrast, previous studies on remote touch in video calls have addressed the reality of reproduced touch sensations with the aim of supporting intimate interaction. First, in a previous study that reproduced a handshake [3] , the necessity of showing the user that the robotic hand is moving in synchronization with the user's hand has been verified. When showing a partner's hand on a display to demonstrate synchronization between the human and robotic hands, the hand showing on the display and the robotic hand in the real space are redundant. The recognition of the body duplication reduces reality. To enhance the sense of space sharing, it therefore is effective to create positional consistency with the robotic hand by showing the partner's image without including the part beyond their wrist in the view angle rather than visually showing the synchronization. Regarding enhancing intimacy, no differences have been reported with or without positional consistency. As a touch device that reproduces handshake, a band-type device that tightens the user's hand and presents their body temperature has also been proposed [14] . This previous study investigated the effect of remote touch on collaborative work in the media space, but the effect of enhancing the sense of space sharing was not significant. When users place their hands on the corresponding place in the shared space, this device tightens each other's hands to reproduce a handshake. In this method, there exists positional inconsistency in that the user's hand is held by the partner even though the partner's hand is placed on the desk. As a result, the reality of body contact decreased, and hence, the sense of space sharing may have decreased.\nRELATED WORKS: In addition to handshakes, devices that reproduce kisses and hugs have also been proposed. The device that mediates kissing reproduces the pressure and movement of the partner's lips with servomotors [15] . This previous study mentioned that the device creates a more intimate interaction compared to ordinary video calls. However, even with this device, the effect of enhancing the sense of space sharing has not been obtained. As separated from the display for a video call, this device suffers from positional inconsistency. The kissing device that solves the positional inconsistency by allowing a mobile phone to be attached to the device was also been proposed later [16] . However, it has not been verified whether the solution enhances the sense of space sharing. In addition, size inconsistency still occurs in this improved device.\nRELATED WORKS: Devices that reproduce hugs have been proposed in many previous studies, some of which are clothing-type [17] [18] [19] . Clothing-type devices with multiple oscillators have been proposed for use in combination with mirror-type video calls [18] . This method simulates a conversation while looking at a mirror that shows the hugging users. The combination of a clothes-type device with multiple oscillators and a video call has also been reported in previous studies. When the user touches the touch display showing the partner's image, the corresponding part of the jacket worn by the partner vibrates [20] . These studies have evaluated whether the feeling of hugging with the other party can be obtained, or whether the position touched by the other party can be correctly recognized, but they have not investigated the effects of increasing these evaluations on the sense of space sharing and social bonding.\nRELATED WORKS: In a previous study that verified the effect of reproducing a hug, a voice call made while hugging a bolster with a built-in mobile phone was proposed [21] . The study reported that this method reduces stress hormones in saliva compared to normal voice calls. A hug that cannot see the partner's face is compatible with voice calls, and so there is no need to consider the positional and size inconsistencies. However, it is not clear whether the effect of hugging a bolster is superior to that of video calling, which allows one to talk while looking at the other person. This study will clarify whether the combination of a social touch device with a video call has a positive effect on video calls regardless of positional and size inconsistencies.",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Among numerous smartphone applications available in the market, mobile chat applications are usually ranked top on the mostdownloaded and most-popular lists. Despite the fact that a mobile phone's original purpose was to make voice calls, changes in people's lifestyles and the increase in device/network capabilities have made mobile chatting one of the most (if not the most) widely used methods of communication [49] .\nINTRODUCTION: Despite their active usage, the functionality of many popular messaging applications is still limited in the sense that they are yet text-oriented. Such text-based information exchange can be effective in many cases, but when delivering specific information or for emotional empathy, text-based conversations can take long or be misleading [10, 54] .\nINTRODUCTION: The phrase \"a picture is worth a thousand words\" suggests that images can convey the meaning and essence more effectively using a visual element than a text description. Images have been shown to be more effective than text in many areas. For example, in education, using images helps students understand information better than using text alone [9, 33, 34, 46] . Similarly in advertising, using images to emphasize information or persuasively deliver a message is considered to be more effective and faster than using text alone [19, 39] . Images are also effective in showing social intimacy [40, 42, 50] . We argue that such effectiveness of using images can also be applied to mobile chat environments.\nINTRODUCTION: Despite the potential benefits of enriching mobile chat experiences with images, for many popular mobile chat apps, using images is limited to emojis, memes, and personal photos. Emojis are good to express emotions and are often used in social communication [26, 27, 30-32, 35, 41, 56, 59] . Memes, or animated GIFs, are popular for trendy and funny images [5, 22, 23] . Personal images [11, 29] are useful in sharing deep personal and social context. We argue that public images on the Internet could be useful for sharing information and emphasizing opinion on mobile chats [28] . Moreover, current mobile chat apps require users to spend tedious steps to include public images in a chat; the user typically goes through other applications to find, store, and retrieve a desired image, then uses the original mobile chat app's features to include the image in the chat room. While many mobile apps try to offer an easy-to-access shortcut to streamline this process, putting explicit manual effort to find and attach an image can distract the users On the left, MilliCat suggests images of \"opera house\". On the right, MilliCat suggests images of \"sunny side\". Note that the suggestions are triggered with concrete nouns as a search query. This approach works even with grammatically incorrect (left) or incomplete (right) sentences, both of which are common in mobile chat.\nINTRODUCTION: from their chat conversation and discourage image usage during chat.\nINTRODUCTION: We believe that having autonomous real-time image suggestions can alleviate such user inconveniences when using images within a mobile chat. Moreover, it was reported in a survey [28] that 81.57% of survey participants prefer to be autonomously suggested images from the Internet, based on the chat application input text. To validate such hypothesis, we present the design and implementation of MilliCat, a system that autonomously and selectively suggests images with respect to the user's input text in a mobile chat application. MilliCat suggests images in real-time and allows the users to select a most suitable image that matches the context presented in the input text. The core of the MilliCat design is in analyzing the chat content to suggest relevant images in real-time when the user inputs their message. MilliCat uses part-of-speech (POS) tagging and dependency parsing to extract relevant keywords (or key phrases) from an input sentence and query external image archives to enrich information delivery within chats. Figure 1 shows examples of image suggestions that can help deliver information quickly and visually. Our image suggestion could be considered as an autocomplete with images.\nINTRODUCTION: We perform two user studies with a total of 45 participants to evaluate and understand the effect of MilliCat on mobile chat behavior. First, a small-scale controlled lab study is conducted that allows in-depth analysis of how frequently and why users actively use autonomous image suggestions. Second, a longer term in-the-wild user study is run to overcome limited external validity and possible novelty effect of the in-lab user study. Here, participants used Milli-Cat's real-time image suggestion and manual image search for 8 to 10 days on their own smartphones for all chats with the designated partner(s). Results from our studies suggest that with MilliCat participants used 1.8x more images and the system reduced the image usage delay by 3x than manual search-based image sharing.",
    "relatedWork": "RELATED WORK 2.1 Diversifying Expressions on Mobile Chats: Mobile chats have diversified their communication support by integrating various forms of visual context to the user discussions. A representative example is the use of emojis in mobile chat and social networking platforms. Emojis assist in the delivery of emotional expressions, and many studies have investigated the effect of emojis in online communications [13, 14, 32, 41, 52, 58] . While new emojis are being continuously designed to represent various objects and events, they primarily focus on representing emotions. Animated GIFs are another popular way of sharing emotions [5, 23] . However, animated GIFs mostly focus on funny memes and might not be supportive enough for expressing diverse chat scenarios. Beyond emojis and animated GIFs, we believe there are a more wide variety of visual context on the Internet that the mobile chat users can utilize in their conversation to effectively deliver information. Furthermore, by contextually analyzing the chat data, we believe that automated suggestions of such diverse resources can be made autonomously without explicit user requests.\nContext Analysis for Chat Suggestions: Recognizing context in computer-mediated communication and providing users with suggested actions has been a subject of steady research. Remembrance Agent [45] keeps track of the user's behavior in e-mail conversations and helps users remember whether they have sent the e-mail or replied to a specific message. Short reply suggestions (e.g, \"Yes, it's done\", \"Sounds fun\") for receiving emails [18, 24, 57] have been shown to reduce time in replying.\nContext Analysis for Chat Suggestions: Instead of utilizing users' past behaviors for prediction, recent research has evolved so that the users' current communication content is analyzed to assist users' social activities. SearchBot [2] is an example of such system that listens to vocal conversations, extracts entities, performs a search, and proactively provides relevant information to the speakers.\nContext Analysis for Chat Suggestions: In the context of mobile chats, the work by Buschek et al. provides design implications for augmented text messaging based on user context using heterogeneous sensing modalities, which include smartphone usage patterns, heart rate measurements and the smartphone's accelerometer readings [8] .\nVisual Support for Mobile Chats: Previous work have taken an additional step from analyzing discussion context for text-based support and have integrated visual aspects to their automated suggestions. meChat [29] analyzes and classifies mobile chat conversations and in-device photos. When users intend to share photos that are relevant to the chat content, the application presents appropriate photos to the chat. Mes-sageOnTab [11] analyzes the content of mobile messaging applications and provides a shortcut interface to third-party applications that users could use (e.g., photo gallery, calendar, and contacts). While MessageOnTab is similar to MilliCat, we focus on recommending public images from the Internet while MessageOnTab suggests external applications for a possible next action. Moreover, meChat and MessageOnTab exploit personal, in-device photos that are triggered on-demand by the users and could be useful for personal and social context. On the other hand, MilliCat uses public images that are suggested autonomously based on the input text and can help improve information that may not be clear in text only and further fact sharing.\nVisual Support for Mobile Chats: While the idea and challenges of image recommendation based on the currently typed word in mobile chat has been recently reported [28] , contrary to our work, it does not suggest technical details in image search phrase extraction, word type selection, or dependency parsing that are required in realizing real-time image suggestions in mobile chat. Moreover, it lacks user studies that investigate how image recommendation affects mobile user chat behavior.\nVisual Support for Mobile Chats: A number of mobile chat application products have added features to support using images in the chat. Facebook Messenger [36] and Dango [15] , for example, allow users to send trending animated memes using an external application (e.g., GIPHY [17]) via keywordbased search. Other applications such as Google GBoard [16] and KakaoTalk [51] support web image search (within the application) using a \"trigger\" (e.g., a search key or keyword that enables image search). However, these services require an explicit user input request for image search and also outputs other non-image related contents such as restaurant information and location information, which leads to increased mobile device overhead. The goal of Milli-Cat is to eliminate such explicit user requests and autonomously suggest images in real-time with minimal system overhead.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: According to the media richness theory, the richest communication medium is a face-to-face dialogue [1] . It is superior to video conferencing due to a number of factors, including depth perception and media naturalness [15] . However, collaborative virtual reality (VR) systems have the potential to deliver an even higher eectiveness in communication, when the perceived naturalness of the medium will eventually reach that of a face-to-face conversation. In an indistinguishable VR, virtual face-to-face communication can be enriched by computer aided tools for brainstorming, design or other specic tasks [2] . One method of improving virtual face-toface communication is to use visualization cues to convey what each user in a collaborative environment sees, freeing up other media channels like verbal or gestural communication.\nINTRODUCTION: There are several implementations of visualization cues, ranging from line-of-sight indicators to full displays of a user's actual image inside the headset (video mirroring). To nd the most appropriate type of passive visualization cue for VR collaboration scenarios, we implemented three distinct universally applicable methods and compared them in a controlled study regarding task completion time, precision and error rate. The implemented visualizations are Object Highlighting, View Cones, and Video Mirroring [3, 4, 22] .\nINTRODUCTION: Each of these perspective conveying methods is a direct or indirect interaction form between user & user or user & environment. There are many possible perceivable interaction forms as Manninen [18] denes them: \"Interaction forms are perceivable actions that act as manifestations of the user-user and user-environment interaction. These forms are used to convey the actions of the user to oneself and to others. They enable awareness of actions by oering mutually perceivable visualizations and auralizations within the environment. In addition, the feeling of presence and the level of psychological immersion are increased due to the communication, co-ordination and collaboration aspects these forms bring forward. \"",
    "relatedWork": "RELATED WORK: According to M. R. Endsley, situational awareness is the ability to understand a complex situation and predict its future states in order to make decisions [6] . A high level of situational awareness supports decision making, enabling an actor to identify one or more correct Session 3: VR + Games and Learning courses of action. In collaborative environments, being aware of other users' status and activities is crucial to coordinate activity and avoid interferences [7] .\nRELATED WORK: As collaborating team members develop shared mental models [11] , their situational awareness grows [6, 7] . This in turn allows them to coordinate more eciently [5] . Stemming from this situational awareness, the team's reliance on verbal communication is gradually reduced [17, 23] , shifting the mode of operation to an implicit coordination [8, 9, 21] . This in turn lets team members utilize the verbal communication channel for exchanging additional information, or shift their attention to other cues from their working environment. While it is relatively easy to maintain social and positional awareness in co-located collaborative environments, remote collaboration proved to require a greater mental eort in regards to these types of awareness [13] . Thus, groupware research has focused on interface techniques that facilitate communication and increase group awareness and awareness cues [12] .\nRELATED WORK: In this context, Wuertz et al. [25] developed a framework to classify awareness cues in multiplayer games based on an analysis of games, game mechanics and game interfaces. They describe the information each type of cue provides, how it is communicated, and how awareness cue design choices can impact the user experience. Additionally, prior works on Workspace Awareness [12] and Gamespace Awareness [20] identied four types of information that team members use when collaborating, which also applies to virtual reality collaborative environments. The Who -Presence and identity. The What -Status, task and social structure. The Where -Location, positioning, and gaze. And nally the How -Communicating the way important events occur. By emphasizing the gaze and enriching its expressiveness through highlighting or other rendering techniques, this type of awareness cue can be used to improve the eciency of collaboration [3, 18, 22] .\nRELATED WORK: Regarding the visualization of awareness cues, Dodds and Ruddle [3] presented a suite of techniques that aim to improve the quality and quantity of communication in large-scale collaborative virtual environments (CVEs). Their social awareness increasing method, called 'group graph', which is an explicit hierarchical grouping system, visualizes who is currently talking to whom. Additionally, they addressed the diculty of users to understand what other users are currently looking at, by rendering small view ports as planes in 3D space that displayed their respective user's point of view. The suite of techniques was evaluated using an urban planning scenario, highlighting that multiple view ports increased the maximum distance of eective communication between participants.\nRELATED WORK: W\u00f6ssner et al. [24] proposed a collaborative volume rendering application, that mirrors the point of view from one CVE user to another in a master/slave conguration. They compared the tight coupling of complete master control over the point of view to a more exible approach, where the slave could change their rendered orientation, but not position. They concluded that the loose coupling approach was rated most comfortable and that user enjoyment was signicantly higher in this scenario. Sonnenwald et al. [19] found that users saw potential benets in both independent and shared perspectives, depending on the task and form of collaboration at hand. Lee et al. [16] compared three dierent view-sharing techniques for mixed reality scenarios of local users receiving support from an external expert. The real world view port of the local user is transferred either as a at 2D plane, a 360 projection on a sphere or a 3D reconstruction of the real environment. They concluded that transferring 3D reconstructions of the real world allowed the expert to give the highest degree of freedom in perspective, resulting in shorter task completion time and higher social presence.\nRELATED WORK: Trapp et al. [22] described multiple approaches to highlighting objects of interest and potential elds of application. They divided the applicability of object highlighting into the categories User Selection Preview, Visualization of Computational Results and Navigation Aid. Here, User Selection Preview refers to rendering a single selected object, which is in the user's focus, in a dierent style. Visualization of Computational Results refers to highlighting, grouping and clustering results from database queries or other computational selection processes. Lastly, Navigation Aid refers to highlighting points of interest, routes, as well as navigation way points in order to guide the user's attention. Trapp Knowledge about the user's visual attention can also be used to facilitate mediated human-to-human interaction in VR environments. Duchowski et al. [4] demonstrated this by applying the eye movements of a user onto a virtual avatar, and additionally showed advantages of a visible line of sight for the communication of points of interest. Gamer and Hecht [10] further highlighted the importance of understanding other users' view cones. They investigated the inuence of several factors such as observer distance and head orientation on user interactions and their view cones. They concluded that the processing of gaze cues plays an important role in social interactions, and mutual gaze in particular is relevant for natural as well as video-mediated communications.\nRELATED WORK: Manninen [18] compiled and described the top level interaction form categories in CVEs. These are avatar appearance, kinesics, occulesics, facial expressions, environmental details and languagebased communication. Avatar Appearance contain all forms of indirect interaction that is emanating from e.g. the clothes, hair, physique or adornments of the avatar while Kinesics describes postures, body movement, head movement or gestures. Because eye movement, eye contact and line of sight are such important aspects of interaction, they have their own category: Occulesics. Finally, the environmental Details category contains all forms of interaction with objects or the environment itself.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: In recent years, there has been increasing interest in wearable devices encompassing haptic stimulation and targeting music performers [6, 16, 23, 33] . This endeavor falls within the remits of the emerging feld of musical haptics, which relates to the application of haptics research to the musical domain [24] . Works in this feld has included the development of haptic interfaces not only for music performers (e.g., haptically-enhanced digital musical instruments [19, 34] or tactile notifcation systems [12, 15, 27] ), but also for music listeners (e.g., haptic devices aiming to enrich the musical listening experience [21, 32] ).\nINTRODUCTION: A related strand of research has prioritised accessibility aspects related to visually-impaired musicians, which represents an important aspect for social inclusion of this section of the worldwide population [3, 4] . Various devices have been developed to support ACM Reference Format: Luca Turchet, David Baker, and Tony Stockman. 2021. Musical Haptic Wearables for Synchronisation of Visually-impaired Performers: a Co-design Approach. In ACM International Conference on Interactive Media Experiences (IMX '21) , June 21-23, 2021 , Virtual Event, NY, USA. ACM, New York, NY, USA, 8 pages. https://doi.org/10.1145/3452918.3458803 visually-impaired musicians in various activities. This includes haptic tools for studio production (e.g., haptic waves, a device that allows cross-modal mapping of digital audio to the haptic domain [28] ), systems for interactions between conductors and blind choristers [11] , and haptic wearables for integrating visually-impaired musicians in orchestras of sighted musicians (e.g., [2, 18] ). A successful methodology involved in some of these studies is participatory design [22, 26, 28] , where the haptic devices are designed together with the visually-impaired musicians through an iterative process.\nINTRODUCTION: The advent of embedded and networking technologies has set the stage for the creation of new wearable devices for creative communication in musical contexts by leveraging the tactile channel. Recently, the Musical Haptic Wearables (MHWs) for performers have been proposed as instances of a wider family of Musical Things [29] within the Internet of Musical Things (IoMusT) paradigm [30] . Such a class of wearable devices targeting music performers encompasses haptic stimulation, gesture tracking, and wireless connectivity features. MHWs were conceived to enhance creative communication between performers as well as between performers and audience members by leveraging the sense of touch in both co-located and remote settings. MHWs can be utilised to support musical communication among visually impaired performers (see e.g., [1] ). Nevertheless, to date, this line of research has been scarcely addressed despite its potential to greatly beneft blind performers.\nINTRODUCTION: In this paper, we report on three workshops conducted with an ensemble of solely visually-impaired performers, following a participatory design methodology. Our work investigated the challenge of synchronisation among visually-impaired musicians, a central issue which emerged during the workshop. In contrast to other studies that investigated how to convey to visually-impaired musicians visual information related to the gestures of a sighted conductor (e.g., [2] ) our work involved only non-sighted musicians. We investigated two scenarios, choir singing and playing in ensemble, where the conductor was one of the members of the group empowered with a dedicated controller. We show the results of preliminary evaluations of MHW prototypes which highlight the potential of this technology to make collaborative music making more inclusive.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: An extensive body of research testifies to the fact that visits to cultural heritage sites are generally social in nature: people overwhelmingly come to sites in groups (e.g., as part of organized tours, school excursions, parties of family and friends), and even if alone, they still tend to be aware of other people's presence in their visiting experiences [ 21 , 60 , 82 ] . Moreover, sociality, and conversation in particular, are recognized as essential means to create engagement amongst individuals in museum environments and other cultural contexts [ 28 , 33 , 46 ] . Some visitors actually interpret the cultural visit as an \"encounter session\" wherein social interaction is their primary goal (see, for instance, the Utopian visitor type, as identified by Umiker-Sebeok [ 76 ] ).\nINTRODUCTION: Information technologies play a particularly vexed role in such social encounter scenarios. One of the most influential frameworks in research dealing with digital devices and engagement in museums is the Contextual Model of Learning, proposed by Falk and Dierking [ 20 ] . Herein the authors distinguish three different contexts of visitor interaction that should be carefully considered when designing a cultural experience-personal, physical, and socio-cultural-recognizing the interdependencies of each, but also the crucial importance of the latter (including social exchanges) to the visitor experience. What is key for our argument is that the use of mobile devices in museums often pits these contexts against one another by, for example, privileging the personalized experience (e.g., use of the visitor's own small phone or tablet; the use of individual headsets) above outward-facing engagement with others [ 84 , also see 28 , 30 ] . As we have reviewed elsewhere [ 35 , 59 ] , studies are increasingly attempting to reconcile such divides, with an explicit concern for deploying digital technologies to enhance the socio-cultural context and facilitate social interaction during cultural visits.\nINTRODUCTION: The existing scholarship hints at the positive impacts of designing sociality into digital experiences for museum visitors, but it does not yet robustly establish a relationship between visitor preferences, specific forms of social engagement, and the achievement of broader institutional and public goals. In this article, we present the results of a user study that systematically investigates the effects of group conversation and social interaction in the context of a digital cultural storytelling experience. Aiming at a detailed qualitative and quantitative assessment of the benefits and weaknesses (both for visitors themselves and for the institutions that host them) of promoting social interactions in a digital storytelling setting, we start with a story-based experience that was designed for individual use; we extend it with novel system-driven interaction prompts, and then we evaluate the two versions with 102 participants in pairs, comparing and analyzing the results over several dimensions.\nINTRODUCTION: The story described below was created as part of a tour of the archaeological site of \u00c7atalh\u00f6y\u00fck in Turkey. It was originally designed to be experienced at the archaeological site itself. In this article, the story has been re-designed as an introductory digital experience that takes place ahead of the visit to the excavation areas, either within the site's Visitor Center or even before reaching the site. The story-based experience delivers audiovisual content on personal handheld devices, similar to mobile guides. To promote social interactions between visitors, participants are directly asked to verbally or physically interact with each other, at particular points in the story. To this end, instead of inducing or stimulating conversation indirectly, as in related work like Sotto Voce [ 26 ] , or in experiments with narrative variations [ 8 ] , we adopt a more direct approach by incorporating explicit interaction prompts into the story, hence guiding visitors into purposefully framed forms of social interaction and enhancing their awareness that this is a social experience.\nContributions: In this work, we define eight research hypotheses grouped into three themes (discussed in detail in Section 3 ) relevant to the role of social interaction in general, and guided interaction in particular, on device-mediated visits to cultural heritage sites. First, building on the extant scholarship, we claim that digitally mediated social interaction can establish a more meaningful connection between visitors and the site by specifically promoting engagement, learning, reflection, and perspective-taking. Second, we present and evaluate a strategy of employing explicit and guided interaction points as a means to generate conversation and social interaction, demonstrating that the insertion of these social interaction points does not negatively affect people's sense of immersion in the story. Third, we examine the role of the visitor profile in this type of experience, focusing on how specific personality traits respond to the sociable nature of the activities.\nContributions: Through our experimental design, we confirm or partially confirm our research hypotheses, and we offer conclusions to help guide future experience design for heritage, as well as contribute to further research in the field. Our findings suggest that sociality needs to be conceptualized as an essential design pillar in the curation of cultural visits, rather than as a mere contextual factor that informs the visitor experience. Our findings also confirm that supporting social interactions between pairs of visitors by introducing concrete, digitally mediated interaction points in the story script can \"transform\" a digital story that was originally designed for single users into an impactful experience for multiple users, thereby enhancing visitor outcomes in terms of engagement and learning. This approach can significantly benefit museums and cultural institutions in general, offering them the means to re-use and enhance existing storytelling experiences for their audiences.\nContributions: The remainder of this article is structured as follows: In Section 2 , we present relevant background work on digital storytelling and social interactivity, which motivates our study, and then in Section 3 , we define our objectives and research hypotheses. In Section 4 , we describe the social interaction design that we applied to the \u00c7atalh\u00f6y\u00fck digital storytelling experience. In Section 5 , we provide an overview of the user study we performed, describing the experimental procedure and analysis methodology. Finally, we present the results of the study, followed by discussion and conclusions in Sections 6 , 7 , and 8 , respectively.",
    "relatedWork": "RELATED WORK: In this section, we briefly review key research from the domains of digital storytelling and social interaction as applied in cultural heritage. While extensive, the literature suggests gaps in the existing knowledge base around digital media for social museum experiences. These gaps provide the impetus for our current work.\nDigital Storytelling as a Tool for Museum Audience Engagement: Storytelling has long been embraced by cultural institutions as a method of communication and interpretation [ 3 ] . The widespread adoption of powerful mobile and interactive devices has also opened the way toward effective experimentation with new forms of digital storytelling in cultural heritage [ 47 ] , ranging from preorchestrated narrative-driven productions (such as [ 57 , 61 ] , amongst others) to interactive systems with virtual characters that dynamically unfold their story within the museum environment [ 47 , 67 ] . A multitude of studies advocate for the power of digitally mediated stories, and cultural institutions are actively embracing the \"digital storytelling\" paradigm, often placing it as high priority within their communication strategies and workflows (see, for example, [ 6 , 11 , 27 , 65 ] ).\nDigital Storytelling as a Tool for Museum Audience Engagement: Given that digital storytelling is a creative, multi-disciplinary task [ 6 , 64 , 68 ] , the question of how to design engaging and appealing digital stories for museum visitors is of key concern. Digital storytelling has a strong participatory potential as many recent studies show. For example, crowd-sourcing technology and digital storytelling have been combined to create participatory installations [ 70 ] . The participatory potential of digital storytelling has been explored in relation to social issues, for instance in strengthening social inclusion amongst socially excluded individuals [ 55 ] or in enabling communities to proceed with neighborhood planning [ 50 ] . In these participatory approaches, co-design emerges as a key issue, deployed in both cultural and educational settings. For example, teachers and students can co-design digital content for formal education [ 56 ] , children of different sensory abilities can co-design multisensory storytelling systems [ 13 ] , or different domain experts can co-design interactive storytelling technologies for cultural spaces [ 52 ] .\nDigital Storytelling as a Tool for Museum Audience Engagement: However, museum professionals along with experience designers arguably continue to struggle to transfer digital storytelling concepts and models from the social sciences, media communication, and the entertainment industries into forms that are actionable, specific, and sustainable by the cultural heritage sector. Wyman et al. [ 86 ] report key observations and elicit best practices from their experience at Second Story (an experience design collective) for museum clients. Yet their account makes clear that many of these best practices should precede the digital, meaning that technological considerations should come after the stories themselves. Similarly, in a piece entitled \"On Digital Content Strategy\" (strikethrough in original), Coerver focuses on the qualitative aspects of museum-based stories (regardless of platform), adopting a short list of five \"mantras\" to guide the story design process at SFMOMA [ 11 ] . The author puts emphasis on shifting from fact-based interpretations toward stories that visitors may personally relate with, evoking emotions, excitement, and surprise. Again, though, any specific affordances of the digital (if they exist) are subsumed under more generic storytelling guidelines.\nDigital Storytelling as a Tool for Museum Audience Engagement: Digital technologies and methodologies do indeed offer unique possibilities for the museological setting. While it is beyond the scope of this article to delve into the many manifestations of digital storytelling in the sector, Wong [ 85 ] offers a powerful overview and critique of the term and goes further to argue for the articulation of a \"storycraft\" to help guide practitioners in its application to museums. Herein she contends that we must invest in the storytelling capacities of museum staff, and make use of both long-standing storytelling conventions like narrative gaps and coherence, as well as more novel (digitally facilitated) \"opportunities to play with the spatial, interactive, and intermedial aspects of storytelling.\" Wong elaborates on the different requirements and capacities that are offered by \"transmedia\" and \"mobile\" storytelling, focusing on the physical and spatial relations between stories and cultural environments. She separately tackles the topic of \"interactive storytelling\" and describes four different levels of \"interactivity,\" following Ryan's onion metaphor [ 69 ] -from peripheral interaction on the interface level to fully dynamic stories where the user is a co-creator with the system, generating new stories on the fly.\nDigital Storytelling as a Tool for Museum Audience Engagement: Of special interest to us is the fact that while such research into the digital/storytelling domain is obviously profuse and rich, it is still rare to see social interaction between visitors fully considered within its remit. As will be presented in the following section, studies often explore the use of a shared interface or the synchronized delivery of an experience, but few explore social interaction at the story design level.\nSocial Interaction in the Digital Storytelling Context: Where social interaction is examined within the digital museums scholarship, it is often included as one of the many dimensions comprising the broader \"context\" of the visit (e.g., see Emmanouilidis et al.'s [ 16 ] taxonomy of mobile guides and Raptis et al.'s [ 62 ] survey of existing mobile design practices in museums). Here group behavioral patterns and visiting styles are considered at a general level [ 14 , 75 ] , from people moving together and interacting (or not interacting) to groups purposefully splitting but re-converging in order to retain spatial proximity to individuals drifting apart and following separate paths. Various researchers have thus sought to promote specific forms of social interaction between groups with such patterns in mind. For instance, in situations where visitors are not in close proximity, technology has been used to bridge physical distance, enabling remote communication, recommendations, or/and tracking the whereabouts of group members. The PIL project [ 39 , 40 ] enables group members to send predefined messages or leave \"post-its\" on exhibits that they would wish others to see, offering a series of context-aware communication services. ARCHIE [ 49 , 71 ] supports direct voice communication between the group members and also provides a framework for creating collaborative games. Game designs are often used in museum settings to promote group interactions, commonly (but not always) targeted at young people. These include \"enigma,\" treasure hunt or other exploratory/quest-style mobile experiences, aimed at explicit or implicit communication between and within groups in order to reach a shared goal (e.g., the Cicero project [ 45 ] , MuseumScrabble [ 72 ] , and \"Capture the Museum\" [ 9 ]).\nSocial Interaction in the Digital Storytelling Context: Other initiatives aim to encourage sharing of personal reflections and mobile-mediated discussions amongst a group (e.g., Find the Artwork behind the Story [ 81 ] ). The Sotto Voce project applies eavesdropping as a way to share audio information, i.e., by hearing the activities of others in close proximity [ 26 ] . Suh et al. [ 73 ] build upon the eavesdropping metaphor to implement a \"sharing mode,\" and the \"Sync\" feature of SFMOMA's mobile application allows friends and family to listen to the same stories together, providing a social listening experience on site [ 57 ] . Other approaches are based on carefully designed trajectories [ 22 ] , enabling both isolated reflection as well as shared conversation or interactions in specific locations or exploring mobile drama with coordinated narrative variations by providing different versions of the narrative to each user in order to induce conversations in small groups ( [ 8 ] ; also see Huws et al. [ 31 ] ).\nSocial Interaction in the Digital Storytelling Context: In some cases, a shared medium is employed, such as situated displays, to help visitors plan a combined path for their group visit beforehand [ 4 ] , to display shared game representations or the location of other group members during the visit [ 15 , 25 ] , or to jointly explore the \"trails\" of the group members after the visit (e.g., at the museum cafeteria) [ 40 ] . Apart from situated displays, \"fixed\" at a particular location in the museum environment, the use of shared mobile displays is also explored [ 42 , 44 ] , employing tablets and pico-projectors.\nSocial Interaction in the Digital Storytelling Context: All the aforementioned approaches (along with many others) recognize and advocate for the value of promoting social awareness and interactions during cultural visits, showcasing interesting visitor reactions or reporting positive evaluation results. However, in these studies the \"added-value\" of introducing system-driven social interactions in a group visit is roughly measured in relation to reaching particular objectives of the visit (e.g., facilitating conversation) or compared to traditional visiting scenarios (e.g., quiet or solitary experiences). It often focuses on the relation of the users to the technology, examining for example tensions between visitor control and the promotion of conversation [ 44 , 43 , 73 ] . Less weight is placed on understanding the more complex relationship between these social interactions, particular visitor needs, and broader institutional learning, value-building, and affective and transformational goals for audiences in context. For instance, Callaway et al. [ 8 ] show that the use of narrative variations leads to increased conversation between group members, compared to hearing identical narratives. But in what specific ways does the reported increase in conversation improve the visit? How exactly does it benefit visitors' experiences? Does it help the museum to reach its expected visit outcomes, which are set during the design of the experience? In their second experiment, Callaway et al. [ 8 ] compare the effectiveness of different narrative variation techniques, reporting a correlation between the amount of talking around the cultural content and user satisfaction. Their argument, however, focuses on a comparison of the different narrative variation techniques, without further reflection about what this user satisfaction means and without examination of whether the learning outcomes of the visit were actually advanced or not via the reported conversation increase.\nSocial Interaction in the Digital Storytelling Context: Moreover, in an extensive survey of the rich personalization research that has developed around cultural heritage experiences over the past 20 years, Ardissono et al. [ 1 ] point out that an individualistic approach continues to hampers the social experience of group visits. They highlight that the social context of these visits is considered in only a limited portion of the studies they reviewed. The authors suggest that extensive effort is still needed to effectively transition from \"me\" to \"us,\" and they discuss important challenges and directions toward group-level modeling and social recommendations. In our work, then, we attempt to build upon and extend the existing body of research on social interaction and its specific effects on the visitor experience through a targeted experiment examining guided social interaction points inserted into a digital storytelling experience.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Virtual and augmented reality (VR/AR) allow users to train, review, and discuss complex workflows in a comprehensive spatial environment. The natural visualization and interaction make VR and AR suitable for multi-user cooperation (e.g. [22, 57, 62, 70, 71, 73, 74, 105] ). Users perform better when communicating using full-body gestures [15] . Also, collaborative virtual environments (CVE) facilitate richer and more positive interaction and may improve team cohesion as indicated by Venkatesh and Windeler [103] . Montoya et al. [67] suggest that three-dimensional (3D) CVEs may provide more effective and efficient communication as teams can leverage affordances within the 3D environment, e.g. visual cues, environmental feedback, and sense of presence.\nINTRODUCTION: Based on the confirmed advantages of the 3D CVE and the increasing interconnection of our world, it makes sense to utilize CVEs not only locally (in the same room) but also remotely (across the globe). Remote collaboration allows teams to spontaneously discuss a topic, or experts/teachers to support workers/students with problems during task execution. Furthermore, this saves time and resources while protecting the environment and climate as people can communicate quickly and independently of their physical location. Network latencies can impact the performance of any remote collaboration system. However, Hoppe et al. [39] show that local and remote collaboration is equally suited for low-latency VR applications regarding workload, task completion time, and preference. Chen et al. [10] found that users are capable of choosing and consistently working with only one of the different representations of their partner (i.e. real person and virtual avatar). However, the ambiguity leads to perceptual conflicts and a reduction in task performance during collaboration.\nINTRODUCTION: Still, remote collaboration seems to be less affected by the physical location of users and more influenced by other factors, such as expressive communication of a user's actions and a common understanding. A natural way to convey intent is to use voice communication together with gaze, gestures, facial expressions, or full-body movements. While working in a face-to-face configuration, users can see the virtual avatar of other users and the virtual content at the same time. However, when users work with different perspectives their task performance can decrease due to left-right ambiguities [16] . In contrast, when standing side-by-side or even at the same location, users can use the shared perspective to build a common understanding. Users see objects from the same angle and can be sure that the other person sees the same. But, this might cause the avatars of the different users to overlap, which might be awkward. One way to combine the advantages of both, face-to-face cooperation and a shared perspective, is to switch between the two configurations. But, the switch takes time and users may want to be in different configurations simultaneously. Standing face-to-face and at the same location is impossible in the real world.\nINTRODUCTION: Virtual worlds, however, allow full control over the experience each user has. We use a redirected interaction technique that is applied to full-body avatars. Users can share the same perspective by standing at the same location. At the same time, the redirection allows face-to-face interaction with other avatars. For this, the virtual avatars of collaborators are shifted to see their physical appearance and movements. To compensate for the shift, the gestures and look-at/pointing locations are adjusted so that remote users still reach for and point at the correct virtual locations. The presented system provides a shifted but shared perspective (ShiSha). ShiSha aims to balance usability, task performance, feeling of teamwork, and perceived co-presence to yield a new way of easy and efficient teamwork with a high social presence. The main contributions of this work are:\nINTRODUCTION: \u2022 Concept, design, and implementation of ShiSha, a novel social redirection technique that allows users to collaborate with a shared perspective while maintaining face-to-face interaction.\nINTRODUCTION: \u2022 Findings from a user study (n=32), evaluating the impact of ShiSha on usability, task performance, and social presence and showing its advantage compared to a baseline (Vishnu [61] ). \u2022 Discussion of the implications for future social redirection design based on the results of the user study.",
    "relatedWork": "RELATED WORK: Our work combines different research topics such as a common understanding, local and remote collaboration, and redirected modifications which are discussed in the following subsections.\nProviding a Shared Perspective: During grounding [13] , users initially establish a shared understanding to make sure that \"what has been said has been understood\". Grounding is performed following the principle of the least collaborative effort and changes with the medium. For example, a user's distance or movement towards virtual objects helps to define the referential context during conversations and thereby can be used by others to disambiguate references [72] . Having a shared perspective also supports grounding and is \"useful for understanding the situation and providing effective guidance and instruction\" [69] and allows users to \"communicate with a common understanding, i.e., a sense that they are watching the same situation\" [49] . Having a shared workspace increases performance [23, 29] , especially when tasks are visually complex or difficult to describe [29] . Moreover, a shared perspective is more effective and preferred to an opposing view, which can result in left-right ambiguities [16, 99] . But, a shared view via a 2D or 360 \u2022 video can also constrain the perspective of the remote user to the recorded frame or the location of the on-site worker respectively. 3D reconstructed scenes solve this but can be challenging to dynamically update and lack in detail [77] . Also, a first-person video may induce motion sickness because of a shaking and uncontrollable moving camera [6, 69] .\nProviding a Shared Perspective: In the real world, comfortable between-user distances, i.e. personal space distances, are estimated at 45 -122 cm in a casual-personal relationship [65] . Also, people prefer to work corner-to-corner (sitting at a 90 \u2022 angle) to communicate rather than face-to-face (sitting across from each other) or side-by-side (sitting next to each other) [17, 93] . We assume that these results are applicable for CVEs, as previous work shows that real-world experiences and skills can be transferred to virtual worlds [38, 83] and that social norms of gender, interpersonal distance, and eye gaze from the physical world transfer even into non-immersive CVEs [113] . Face-to-face interaction provides powerful cues for intention by visual and acoustic information [89] . Having different perspectives enables spatial partitioning and identification/awareness of others' focus and activities [23, 99] . Following the other person's eye gaze improves performance [26] , allows to resolve ambiguity [32] , and can also increase the likelihood to speak [104] . Four of the 20 most frequent communication behaviors are listening, asking questions, discussing, and sharing information [50] and workgroups commonly switch between guided and collaborative sequences [30] .\nProviding a Shared Perspective: This shows that intentional and unintentional interpersonal communication is frequently used and having a face-to-face interaction is an important feature of a collaborative system and preferred by users. At the same time, taking the same perspective as another user facilitates a common understanding, and is more effective, but might violate personal space distances.\nMixed Reality Collaboration Systems: On-site support is more efficient than a 2D video with audio because simple video fails to create an adequate shared visual workspace and further feedback needs to be provided [23] . A virtual laser pointer, a context that is larger than the local user's current view, and stabilized annotations can help the remote expert to increase task performance [28] . Yet, a simple pointing visualization (i.e. controllable active laser) is not sufficient [25, 59] . Pointing using a virtual hand is a viable solution to spatial input [92] and visualizing the other users' field of view and their grasping behaviors can increase awareness and understanding for their actions [21] .\nMixed Reality Collaboration Systems: A live 360 \u2022 camera video gives the remote expert an independent view from the on-site worker [63, 64, 101] and can even provide the feeling of standing side-by-side [8] . The 360 \u2022 view can be reconstructed from a camera with a smaller field of view [49] and stabilized to maintain orientation [48] . Several systems [41, 43, 44, 75, 78, 100] utilize a 3D reconstruction of a scene to enable remote collaboration. Scene reconstruction [58] and adaptations [52] also allow a seamless transition between different perspectives. Voice, hand gestures, line annotations, and other features allow users to provide instruction and guidance during physical tasks [102] . Integrating the hands of the remote expert into the workspace of the worker offers a greater sense of co-presence and allows flexible gestures [45] . Vishnu [60, 61] allows two users to take the same perspective in an AR-VR supportive task by standing in the same position. Besides the user's arms, there exists one additional pair of arms that originates from the user's shoulders and represents the movement of the respective other user. Vishnu is faster compared to an annotated 2D camera stream, increases the feeling of co-presence, and eases the mapping between the guide's instructions and the worker's interactions. Overlapping avatars of two users can also be used to guide movements and posture [37] .\nMixed Reality Collaboration Systems: The related work highlights that remote collaboration can be achieved using systems that create a shared visual workspace. Yet, efficient cooperation demands further communication channels such as speech or spatial gestures.\nAvatar Adaptation: Redirected walking [80] adapts the VE imperceptibly for the user to allow seemingly unlimited walking inside a constrained physical space. This concept was quickly transferred to object manipulation. By offsetting the hand movement of the user, haptic feedback can be provided [1, 9, 31, 53, 54, 81, 86] with dynamic targets [11] and arbitrary shapes [112, 114] . The redirected touching performs equally to unmodified haptic feedback [55, 56] . Furthermore, redirection can be used to improve the sense of presence [3] , increase the user's reach [18, 79] , or to provide a more comfortable interaction [19, 66] . Moreover, redirection techniques can be applied to the whole body of the user to convey slowmotion movement effects [82] or underwater drag forces [47] without breaking presence.\nAvatar Adaptation: Besides single-user modifications, redirection can also be applied to multi-user setups. Roth et al. [85] adapt the other user's avatar to imitate one's movements. They found that the modification was mostly undetected and did not impact the perception of communication. Piumsomboon et al. [76] increase the social presence and task performance by modifying the position, scale, and body pose of a remote user's avatar to ensure that her/his gestures are always inside the local user's field of view. In later work [77] , they display a miniature avatar at the location of a 360 \u2022 camera to allow collaboration between an on-site AR and a remote VR user. Hoppe et al. [40] use user-specific transformations of virtual objects and avatars to provide a shared point of view. Their system reduces mental load and increases task performance compared to an unmodified CVE. However, the transformations only support pointing at virtual objects and are not extendible to full-body avatars. Sousa et al. [94] mirror the worker's representation and the 3D workspace to provide two users facing each other with an identical point of view. They found that especially instructors benefit from the shared perspective regarding task performance and user preference as less coordinate system conversions are needed.\nAvatar Adaptation: Users focus more on the task space and hands of remote users rather than a video showing their faces [99] . Also, workspace consistency might be more important than people's representation during remote collaboration [94] . Yet, providing a full-body avatar is important and comes close to face-to-face interaction [91, 111] . However, when modifying the other user's avatar one needs to consider that avatar realism is based on appearance and behavior [27] . A realistic appearance can lead to heightened expectations for behavioral realism [90] . Co-presence is increased when the character's behavior is realistic [115] and decreased when appearance and behavioral realism mismatch [4] . Moore et al. [68] argue that free gesticulation and publicly visible actions (e.g. menu access) are important to offer better coordination between users (see also [36] ). For example, Xia et al. [110] use ghost-like copies of avatars or objects to support multi-user movement or to resolve conflicts during simultaneous object interaction.\nAvatar Adaptation: In conclusion, collaborative VR or AR systems should aim to use realistic avatars, but also provide expressive tools to facilitate cooperation and grounding. Our research aims to combine social multi-user redirection techniques with remote assistance. The related work highlights that sharing the same perspective is useful, but it can also limit the interpersonal communication and might be inferior to face-to-face interaction. Using social redirection, i.e. redirection that is applied to increase group performance or cohesion, we can provide users with the same point of view and a face-to-face arrangement at the same time by shifting the avatar of the respective other users to the side. The redirection is applied to full-body avatars to support diverse interpersonal communication. In the next sections, we describe the design and implementation of our technique and then report on a user study evaluating the efficiency and social presence compared to a baseline.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Social interactions amongst people, especially within close relationships, are important for people's well-being and quality of life [24] . Even short interactions with strangers, for example in buses and trains, can improve people's moods [12] . With the upsurge of ubiquitous mobile and wearable devices, there has been a digital revolution in the enhancement of social interactions amongst collocated individuals. These systems typically aim to improve awareness of strangers to connect with each other and facilitate the ice breaking phase and support maintenance of conversations [27, 36, 37, 47, 51, 54, 56] .\nINTRODUCTION: As the purpose of these systems suggest, these studies thus far have been limited to facilitating interactions between strangers i.e., people that have not been in prior contact. Yet, there are many consumer applications supporting face-to-face (F2F) interactions amongst friends [14, 60, 65] . For instance, Octi [65] a social augmented reality app displays a virtual belt around people of their social media applications and photos. Despite this growth in the industry, there is very little research investigating how technologies can support social F2F interactions between closer relationships, such as friends and close friends, beyond the current stranger focused landscape.\nINTRODUCTION: To gain an understanding of how to augment F2F interactions upon closeness of relationship, we ran a study in which 20 participants first created a faceted profile; one profile aimed at a particular friend (later termed private profile) and one profile aimed at strangers (coined public profile). Participants then attended a gathering where they could view each other's profiles depending upon the relationship. Profiles were accessed through head-mounted displays as illustrated in Figure 1 . This builds upon prior studies on investigating how F2F interactions can be augmented with wearable devices amongst strangers (e.g., [36, 47] ) or friends (e.g., [43, 60] ). In these prior works the usage of augmentations in F2F interactions have been investigated either within strangers or friends, but not both at once, although it is common to have different closeness of relationships collocated in everyday life [39, 69] . In this study, we contribute by providing:\nINTRODUCTION: 1. Knowledge of how digital self-presentations differed depending on closeness of relationship, 2. An understanding of how people use and perceive the faceted digital self-presentation when the different closeness of relationships are collocated.\nINTRODUCTION: Our research is pertinent to social technology designers and researchers: for interaction designers our research gives indications on how to support faceting in digital self-presentations in F2F interactions; for researchers in social technologies we provide initial insights on how digital self-presentations are created depending on the closeness of relationships and towards multiple audiences.",
    "relatedWork": "RELATED WORK 2.1 Faceted Self-Presentations in Multiple Audience Problems: Drawing from social identity theories, the multi-faceted nature of human beings was first identified by Goffman [18] . For instance, people may maintain an occupational role in their workplace whilst playing a family role amongst their family [21] . These roles are performed to 'audiences', which vary according to many dimensions such as gender, interests, occupation, spiritual life, and community [18, 40, 59] . The separation of self-presentation towards different audiences is called faceting [18] . In this paper, we focus on faceting according to closeness of relationship.\nRELATED WORK 2.1 Faceted Self-Presentations in Multiple Audience Problems: Strangers and known people are often mixed in daily F2F interactions [42] . This mixture of closeness of social relationships induces a social situation known as multiple audience problem [16, 17] . Multiple audience problems cause tension as individuals try to perform themselves differently depending on the audience and norms of the F2F settings [18] . For example, self-presentation towards a friend is typically more modest than towards a stranger [66] , even when collocated. Although people traditionally aim to avoid multiple audience problems [39] , it is common that people need to give two different impressions to two different audiences in the same social situation [69] ; such as one towards strangers and another towards friends [39] . To manage multiple audience problems, different people use different strategies; such as whispering and gestures, and amongst stronger ties, conveying clues that are understandable only by a certain group [9, 16, 17 ]. Yet, as mobile devices become part of our daily interactions with each other, most of the research into faceting has not included the digital aspect [5, 16, 17, 69] .\nFaceted Self-Presentations in Online Social Media: In addition to F2F interactions, the importance of keeping information separate according to the audience has been found to be important in online social media networks [10, 15, 23, 29, 34, 38] .\nFaceted Self-Presentations in Online Social Media: Techniques to control digital self-presentation in social media networks include, for example, using different social media networks for different purposes [48] , using multiple social media accounts within the same application [64, 67] and using audience settings [32] . The ability of the user to manage their privacy and access control settings in social media are particularly important regarding closeness of relationship [29, 71] . In particular, closeness of relationships have been found to be a primary factor affecting media sharing decisions [29, 71] . For example, Jones and O'Neill [29] noted that users were twice as likely to share photos with users classified as strong ties (such as close friends) than with those considered weak ties (such as acquaintances).\nFaceted Self-Presentations in Online Social Media: People pursue deliberate goals and disclose different information on the perceived social media features mediating between disclosure intimacy and privacy [2, 55] . This is partly due to the ability to share information online has come with the juxtaposition of public self-disclosure no longer consisting of dyadic exchanges but instead often aimed at an unknown audience [2] . This can lead to a phenomenon called 'context-collapse', for instance in Twitter where multiple audiences are merged into one [44] . Hogan [23] argued that, in collapsed contexts, users of social media typically choose to present themselves according to the 'lowest common denominator'. This means that users avoid posting anything that would be problematic to someone in the audience, such as a 'friend' amongst all 'friends' on Facebook. Although the posted content might be relevant or interesting only to some of the friends, for the rest the posting should not be offensive.\nFaceted Self-Presentations in Online Social Media: Building on this, it is largely unknown how the techniques and strategies for managing multiple audiences in social media apply to digital self-presentation in F2F interactions. For example, whilst Tufekci [67] found that users of online social media facet their self-presentation by limiting the visibility of profiles (e.g., using nicknames) rather than adjusting the information within their profiles, this technique is less feasible in F2F situations as pseudonymity is violated.\nSupporting Faceted Digital Self-Presentations in F2F Interactions: There is little work studying how to support multiple closeness of relationships when a digital self-presentation co-exists within F2F interaction. One example of a system that has allowed faceting according to closeness of relationship is a music-sharing application for collocated strangers and friends [19] . This peer-to-peer musicsharing system enabled sharing songs to collocated users within the range of the same WIFI network. Researchers found that most of the motivations for people to share varied according to closeness of relationship [19] . For instance, when sharing with strangers, some of the users preferred automatic sharing in a non-intrusive and less social way that would not evoke F2F interactions [19] .\nSupporting Faceted Digital Self-Presentations in F2F Interactions: On the other hand, the main motivations to share with friends included sharing songs that a friend would like, 'memory aids', 'to get something to talk about', and to prank. As this study indicates, there is potential for digitally shared information to enhance F2F interactions amongst known people. However, as the study only shared music, which could be argued is non-personal information, there is yet work to be done to scaffold these findings towards richer and more personal self-presentation. Jung et al. [30] scratched the surface of this research area by looking at the use of a mobile app that allowed nearby people within Bluetooth range to access each others' user-generated profiles. The profiles consisted of an image, a short description, and a 'guestbook' that allowed profile visitors to leave messages. Here they found that users had privacy concerns about sharing information with a 'non-trusted' person. Jung et al. [30] suggested limiting the access of the guestbook feature to only close friends as a potential approach to facet the content and improve privacy. However, the option that would have enabled faceting was not implemented, and thus the experiences with this feature remain unknown. Speculative investigations have also been undertaken by interviewing individuals on what they would like to share in profiles according to social relationships in F2F [46] . Within this speculative scenario using mockups, the stronger the closeness of relationship was perceived to be, the more and deeper personal information participants were willing to share in their profiles [46] . This aligns with current social identity theory on relationship practices where a correlation exists between individualized trust and self-disclosure [70] . Similar to [46] , Kao et al. [31] conducted a hypothetical evaluation of how people wish to share faceted information in F2F interactions. Kao et al. [31] developed a prototype that allowed the interchange of private and personal information on a display attached to a coffee mug. By interviewing people Kao et al. [31] found that most of the users appreciated the ability to switch between displaying private and public information. However, the mug was not actually used in F2F interactions. Therefore, it is unknown how multiple digital facets are used in multiple audience problems and what their impact might be in F2F interactions.\nSupporting Faceted Digital Self-Presentations in F2F Interactions: To conclude, previous research indicates that augmentations are useful for facilitating collocated strangers to 'get to know each other' by providing topics for starting and continuing conversations. Additionally, the technology can have supportive roles amongst people with stronger ties, as well. Moreover, previous studies also show that people's self-presentation is faceted according to closeness of relationship in both offline F2F interactions and in online social media. However, there is no knowledge beyond hypothetical considerations [31, 46] on how the faceted digital self-presentations reflecting closeness of relationships are created, used, and perceived in F2F interactions that have multiple audiences.\nResearch Questions: Reflecting upon the above discussion, we carry out a study to investigate the how faceted digital self-presentations are created, used and perceived in F2F interactions with private profiles (created for and shown to a particular friend) and public profiles (created for and shown to strangers): RQ1: How do individuals choose to represent themselves in a faceted profile depending on closeness of relationship? RQ2: How is the faceted profile used and perceived in F2F interactions inducing multiple audience problem? Having the private profile designed for a particular friend (instead of a group of friends or friends in general) allowed us to investigate the effect of closeness of relationship on faceting as the closeness of relationships between groups of friends may vary.",
    "rq": " rq1: how do individuals choose to represent themselves in a faceted profile depending on closeness of relationship?\n rq2: how is the faceted profile used and perceived in f2f interactions inducing multiple audience problem?\n but were not limited to: what content did you add to your profiles and why?"
}
{
    "intro": "INTRODUCTION: Many parents struggle when they try to start intimate dinner table conversations with their adolescent children [6, 9] . This is partly caused by the communication being impaired by factors such as relationship changes during puberty due to hormonal changes [38] . However, when the communication between adolescents and their parents wanes, typically the time spent with their peers increases considerably [45] . In today's society, meeting and communicating with peers also happens largely online, such as in video game environments [55] . Research has uncovered a complex web of relationships between video game playing and the emotions they evoke (see, e.g., [27, 69] ). Video games can contain and communicate deeply emotional experiences for adolescent gamers [37] . Consequently, the generation gap between parents and the video game life of their adolescent children can produce friction and misunderstanding [7] . This misunderstanding can, in turn, result in a lack of parental empathy which, however, is necessary to contribute to the development of healthy relationships [64] .\nINTRODUCTION: To tackle this misapprehension, information is needed on how the emotional misunderstanding can be avoided or at least decreased by communicating emotions from adolescents to parents. Thus, to help parents understand video games related emotions and connect with their children, we designed a tangible data visualization system called Motiis. The goal of Motiis is to investigate whether representing the emotions experienced by children during gaming sessions as tangible visualizations helps to raise parental awareness of their children's emotional state and, in turn, can facilitate conversation. In that sense, Motiis is not meant to replace conversations but rather to support them. For that purpose, the six basic emotions described by Ekman et al. [17] are tracked using facial recognition software and haptically conveyed through the system. Electromagnets exert different forces -corresponding to the measured intensity of each emotion -on a movable ball that lets the user feel the 'strength' of the experienced emotions. The system also provides parents with a concise summary conveyed through light, displaying whether the overall emotional experience of the gaming session was positive or negative.\nINTRODUCTION: Physical data visualization raises different potential benefits such as fostering active perception, stimulating non-visual senses, and contributing to the accessibility of data (see Jansen et al. [32] ). However, embodied cognition theory [71] suggests that our thinking is also shaped by our body and our sensory interaction with the physical world. Through making the experienced emotions tangible we hence suspect to take advantage of the above mentioned potential benefits to foster a more conscious interaction with the data than would be possible if only representing it on a screen. Conveying emotions can be challenging and can thus benefit from mediators which facilitate parent-child communication.",
    "relatedWork": "RELATED WORK 2.1 Emotions and Gaming: With the rapid developments in gaming and technology, our understanding of its impact on humans is constantly challenged. As such, research related to emotions and gaming has investigated a wide variety of aspects. On the one hand, emotions have been recognized to be a key factor of player experience that influence the decisions made while gaming (e.g., [46] ).\nRELATED WORK 2.1 Emotions and Gaming: At the same time, research has and keeps untangling a complex and varying effect of playing video games on the emotional responses they trigger. Games have shown to be able to induce positive emotions [48] or alleviate negative ones [29] and may serve as a positive coping mechanism [69] which, however, can also become maladaptive [13] . They can be used for mood management by offering distraction and escapism as a way to reduce stress (cf. [27] ) and can positively influence creativity. Yeh [72] as well as Hutton and Sundar [30] , for example, found that arousal can be a significant factor that influences creative performance after videogame play. Video games can also have benefits in terms of emotion regulation (see [69] for a review). Gaetan et al. [22] argue that adolescents struggling with describing their emotions are likely inclined to play more video games to aid them regulating their emotions and their intensity. Regular gamers, as opposed to those playing games irregularly, were found to regulate their emotions more but were also less emotionally aroused by emotional events. However, games do also elicit negative emotions such as frustration or anxiety. However, as Granic et al. [25] elaborate, games provide a safe environment to practise modulate and control such emotions.\nRELATED WORK 2.1 Emotions and Gaming: A large body of work focused on problematic gaming and the consequences of internet gaming disorder and their relationship with emotional factors (e.g., [40, 52, 53] ). Emotions have also been studied extensively in connection with violent video games. For instance, Ivarsson et al. [31] found evidence that exposure to violent gaming is affecting different physiological, emotional, and sleeprelated processes in adolescents. Montag et al. [44] could observe an impact of excessive play of first-person shooters on the processing of emotions. Jansz [33] argue that violent video games are so appealing to adolescents in the process of forming their identity because they allow them to voluntarily select the emotional situations (including those controversial in ordinary life) they would like to confront in a safe environment. This begs the question of how this opportunity for private experimentation influences emotional development in relation to peers and parents. Gentile et al. [24] flip this relationship and investigate defining factors for development of pathological gaming, including parent-child connectedness. Their study found links between these factors, pathological gaming, and signs of influences on mental health.\nRELATED WORK 2.1 Emotions and Gaming: While broad in scope and looking at emotions from different angles all these works show that gaming has diverse implications on emotional processes.\nTangible Data Visualization: Data physicalization or tangible data visualization is not a new concept but rather has existed long before the introduction of digital technology (cf. [32] ). Jansen et al. [32] define data physicalization as a physical artifact which encodes data through its form or material properties. Physical 3D models of scientific data or processes have a long history and proved valuable for scientific discoveries [14] . The 'forest of rods' built by John Kendrew and Max Perutz is a notable and well-known example in this context as it contributed to the discovery of the structure of the myoglobin protein (cf. [14] ). However, advances in sensor technology, digital fabrication, and novel displays opened up new possibilities for physically representing data and allow to change and interact with the data more dynamically.\nTangible Data Visualization: Tangible data visualizations aim to make the user feel the data, rather than only see it. This can, for example, be achieved by creating physicalizations with interesting tactile properties or by creating dynamic devices that make use of forces. For instance, both Paneels and Roberts [51] as well as Roberts and Paneels [59] provide reviews of haptic data visualizations. Haptic interfaces can support people with impairments. For instance, Trivedi et al. [67] developed a wearable haptic device which allows people with hearing disabilities to experience music. Hull et al. [28] , on the other hand, built a tangible tabletop system which integrates physical architectural models on which data streams are displayed.\nTangible Data Visualization: More closely related to our work, Khot et al. [35] transformed physical activity into material artifacts to foster reflection. In a similar sense, we aim to increase parent's consciousness through a tangible representation of emotions. In contrast to Khot et al., we do not rely on static sculptures which are printed based on the data but rather convey the data dynamically through light and haptic feedback. In short, existing work on data physicalization has shown its benefit in multiple scenarios and for diverse user groups. While research in this space is plenty there is, however, still an interesting design space to be explored: combining more abstract concepts such as emotions with tangible data visualization.\nTangible Data Visualization: We also recognize that emotion data is often used as input for tangible user interfaces, for example, Paiva et al. [49] presented an affective tangible controller through which children can influence the emotional state of their avatar. Similarly, Bernhaupt et al. [5] utilize a player's facial expressions of emotions do control the growth of a virtual flower. However, emotion data is rarely used as an output or concretely visualized. More design-focused research can utilize design artifacts to create new insights into the relationship between emotional reactions during gaming and the use of more tangible approaches towards data visualization [5, 65] . No existing research, to our knowledge, builds upon both of these areas at the same time, combining them to create a tangible data visualization of the experienced emotions during gaming. Such a tangible emotion visualization approach might provide for a more intimate and personal feel.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Individuals have the ability to adjust their behavior to reach a goal, and this phenomenon can be found in communication technology and communication behavior as well, according to adaptive structuration [17] . However, it is still unclear if additional communication and the collaborative channel are always beneficial or not. As an example, Fussell et al. investigated the performance of asymmetrical collaborative workspaces with audio-video or audio-only communication channels [16] , and found that concurrent video communication might not be superior compared to an audio-only channel in terms of adequacy.\nINTRODUCTION: Similar questions also arise in the area of VR technology-based collaborative spaces, as VR technology evolves. This technical platform shows great potential as a next-generation communication and collaboration platform due to the high level of immersion [2, 41, 45, 52] . Individuals can have different system configurations, with differing levels and kinds of immersive sensory technologies, and thus this difference gap can be significantly larger than for more-traditional remote collaborative tools, such as a telephone or video conference system. For example, imagine that you are having a business meeting with your collaborator in a moving train. You might want to open a side window, for example, to cool down the temperature in the train or to take in some fresh air. It is a natural expectation to feel the cool wind blowing, the rumble of the train beneath your feet, and maybe even smell the lavender field the train is running past. Imagine the same train ride, but without feeling the vibration, wind, and smell at all, though other passengers still feel them. Or imagine you can sense the stimuli, but your colleague seated across from you in the train cannot feel them at all. What would this be like? How would it impact your collaborative interactions? Would this influence your emotions or decision-making abilities?\nINTRODUCTION: This scenario seems fairly unrealistic in real life. However, in virtual reality (VR), this is more than likely. In fact, unbalanced sensory information among VR users has already been depicted in popular media, such as in the seminal cyberpunk novel Neuromancer [19] or, more recently, in the book/movie Ready-player One [5] . Gartner, Inc, anticipates that VR technology will be widely used [42] , and that immersive telepresence meetings, as opposed to physical meetings, are a practical example of VR technology that guards against worldwide pandemics, such as COVID-19 in 2020 [40] . Based on these technical aspects and trends, VR can be regarded as a promising tool that can have a significant impact on society in the future, similar to the influences of television or the smartphone [8, 13] . We anticipate that perceptual information imbalances between future VR users could be a serious issue, as our society has already observed the impact of perceptually unbalanced information (e.g., the so-called digital divide, and accompanying knowledge-access gaps) previously in the case of TVs (cable/satellite vs. broadcast) [6, 37] , or smartphones (high-end vs. low-end) as well. More seriously, we expect that such different levels of social expression will occur in the shared virtual spaces for communication or collaboration scenarios since VR is an immersive platform. Using the example of a virtual battlefield, if one can smell smoke, and feel the wind and vibration, and even NPCs respond to them, when tanks are passing by, it could give deeper immersion and even better performance for completing tasks. If we compare this to someone who is not receiving this additional sensory feedback, they might come to different conclusions about the state of things, which could lead to poor decision making, miscommunication and potentially to different collaboration performance in VR.\nINTRODUCTION: To investigate this topic, we simulated the anticipated future VR scenario and conducted preliminary research. For deeper understanding regarding the impact of symmetric sensory channels between VR users, using the train scenario described above, we conducted two independent studies, one with a symmetric VR system and one with an \ud835\udc4e\ud835\udc60\ud835\udc66\ud835\udc5a\ud835\udc5a\ud835\udc52\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc50 VR system, with two levels of sensory cues, rich, multi-sensory VR (MS) and typical sensory VR (TS), in each experiment. We provided visual and audio cues in TS, while in MS we added floor vibration, wind, and smell cues. To give a realistic situational scenario we added a number of non-player characters (NPCs), and simulated two types of NPC reactions, with reaction (Yes) and without reaction (No) to the given sensory stimulus-related events in the study. In the first experiment (Symmetric VR), both participants in each pair had matched sensory cues, either MS or TS, as a between-subjects factor, and experienced both NPC reaction types (Yes/No) as a within-subjects factor during the virtual train experience. In the second experiment (Asymmetric VR), with newly recruited participants, we provided mismatched sensory cues, meaning that one had MS and the other had TS, as a between-subjects factor. Also, the participants experienced both NPC reaction conditions as a within-subjects.\nINTRODUCTION: The results show that the increased number of sensory channels and NPC reactions did not influence the user's perceptual responses (Presence, Co-presence, and Social presence) significantly under either Symmetric or Asymmetric sensory feedback, except Enjoyment in the Asymmetric case. However, we found that MS conditions in both Symmetric and Asymmetric cases showed the nine highest mean values from a total of twelve dependent variables, which indicates an advantage of MS. In addition, after accounting for individual personality traits (e.g., assertive, passive) and relationship between the pairs, we found that increasing the number of sensory channels can significantly improve subjective responses. More detail is given in Section 6.\nINTRODUCTION: The main contribution of our research is a preliminary study to gain a deeper understanding of, and preparation for, future widespread social and collaborative VR usage. Here, we mainly look in terms of the impact of balanced and imbalanced perceptual and cognitive sensory information, and consider personality traits in paired-user immersive applications. To the best of our knowledge, this is the first controlled exploration of asymmetric sensory cues for computer-supported cooperative work and social computing using VR. We hope this work will be a stepping-stone for creating more social and collaboration equity in immersive media.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Augmented Reality (AR) has been used for collaborative experiences. It allows users to interact with shared virtual content while having a view of the real world [6, 43, 58] . For these collaborative experiences to be efficient and positive, there needs to be fluid communication between the collaborators. In addition, knowing what the collaborator is doing or which objects are being looked at can improve the sense of awareness and social presence [17, 32, 33, 36] . One advantage of using AR head-mounted displays (HMDs) is their ability to capture users' head and gaze movements and use the data to provide visual augmentation cues [15, 26, 57] . Such cues can enhance situational awareness and social presence [3] and improve user performance and usability in collaborative AR [45] .\nINTRODUCTION: Previous research on AR primarily focused on remote collaboration (e.g., [24] [25] [26] 47] ). It is not clear how visual cues could be helpful in enhancing user collaboration when they are in the same physical environment. For pointing tasks specifically, many applications involve such pointing tasks during co-located collaboration (e.g., games, education, training). It is therefore important to understand the appropriate techniques for such pointing tasks with different object states and density levels. Users could be interacting with objects that are not easily touchable (e.g., moving objects) and clustered in dense regions with other objects with similar properties. Attempting to describe or pinpoint an object of interest using verbal and hand gestures may not be practical and efficient. In this research, we explore the use of visual cues to enhance the identification of target objects in collaborative AR where these objects can be static or dynamic and be in various levels of density.\nINTRODUCTION: Pointers [15, 34, 60] and annotations [27, 38] are two main visual cues that have been explored. The majority of prior research focused on hand-based techniques, e.g., to carry a pointer and to draw an annotation in a virtual environment. However, these hand-based techniques may not be efficient or ideal for AR systems [41, 42] . For one, it can pose severe occlusion because the field-of-view of AR HMDs is usually narrow. Also, AR systems, with the exception of the Magic Leap, do not come with a pointing device or controllers.\nINTRODUCTION: Therefore, in this research, we explore two hands-free, headbased techniques to simulate visual cues provided by pointers and annotations. Pointing Line (PL) is used to simulate pointers. It indicates a user's line of sight and focus of attention. Moving Track (MT) simulates the use of annotations. It records a continuous moving track to help identify a target. Based on the two techniques, we investigate the effect of visual cues on task performance, usability, and social presence when interacting with virtual objects in a co-located AR. To do this, we ran a user study with two visual cues (PL and MT) to allow paired users to share information about the object of interest and identify it. The experiment involved both static and dynamic objects in three levels of density (low, medium, and high). Figure 1 (above) shows an overview of the experiment setup.\nINTRODUCTION: In short, the paper makes the following two contributions:\nINTRODUCTION: \u2022 We report the results of a user study comparing the two types of visual cues based on hands-free, head-based techniques in co-located AR scenarios for pointing and selection tasks; \u2022 Based on the results, we propose implications on the design and use of these visual cues for co-located AR collaboration.",
    "relatedWork": "RELATED WORK 2.1 Collaborative AR Systems and Social Presence: Collaboration is the \"mutual engagement of participants in a coordinated effort to solve a problem together\" [49] . Collaborative AR systems allow users to interact with shared AR content as naturally as with physical objects, and to complete a task or achieve a common goal [5, 12, 40] . Researchers have shown that AR systems can effectively support a group of users to perform collaborative activities [7, 31, 53, 54] .\nRELATED WORK 2.1 Collaborative AR Systems and Social Presence: During collaboration, it is key to be aware of where and which object(s) the collaborator is interested in or interacting with [2, 13, 37] . Providing visual, nonverbal cues is beneficial for improving the awareness of users' actions and the sense of being together, namely social presence [10, 18] . In the early days, telepointers and cursors [19] were explored to support user awareness of others' actions on a shared workspace in traditional platforms (e.g., desktop displays). Recently, multimodal cues are used in collaborative environments, typically using auditory and visual elements. For example, virtual avatars have been explored to represent each collaborator and to provide an increased awareness of others in the shared environment [21, 30, 46] . In AR environments, researchers explored the use of virtual arrows to represent collaborators' head directions [11] and miniature virtual avatars to show collaborators' gaze directions and body gestures [46] . Although using avatars can contribute to users' perceived social presence, it adds extra visual elements to the limited display and field-of-view of current AR HMDs. Thus, this solution may not be ideal when there are multiple objects in the environment. Using pointer and annotation cues is an uncluttered alternative to support social presence and collaboration in AR. Specifically, sharing gaze pointing cues has been explored in remote collaboration with AR devices [24, 29, 39] . For example, Ishii et al. [29] and Higuchi et al. [24] reported that when users shared their workspace, gaze pointing cues provided better understanding where their partner was looking at. Lee et al. [39] and Higuchi et al. [24] found that sharing gaze pointing cues significantly improved users' awareness of each other's focus and joint attention.\nRELATED WORK 2.1 Collaborative AR Systems and Social Presence: In addition to gaze pointing cues, augmenting hand pointing cues via gestures was shown to be able to facilitate users' perception of others' actions. For example, Piumsomboon et al. [46] reported that redirected gaze and gestures in their Mini-Me system improved users' awareness of the partner in a collaborative AR interface. Yang et al. [59] stated that visual head frustum and hand gestures intuitively demonstrated the remote user's movements and target positions. Kim et al. [33] added sketch cues in addition to gestures and demonstrated an improved task efficiency with the enhanced visual cues.\nRELATED WORK 2.1 Collaborative AR Systems and Social Presence: Although previous research has proven the positive effects of visual cues on supporting social presence in collaborative activities, most of these studies focused on remote AR systems. The effect of visual cues in co-located AR collaboration is still largely underexplored. It is not clear how different visual cues may enable and enhance users' perceived social presence when they are co-located in the same physical space. Therefore, we put our focus on the study of visual cues in co-located AR environments.\nVisual Cues in Collaboration: Two noticeable visual cues were presented in previous work on collaboration: pointer and annotation [15, 27, 34, 56] . Pointer cues provide a pointing line, indicating a user's line of sight and focus of attention. Annotation cues record a moving track to identify a target, such as a track of hand or head positions. Here, we discuss these two types of visual cues in detail.\nVisual Cues in Collaboration: Pointing Line Cues. Previous work showed that pointing line cues can effectively support communication [14, 19, 20, 44, 50, 51] . For example, Gupta [20] reported that presenting users' gaze directions using a pointer significantly improved the sense of co-presence between users in remote collaboration. Piumsomboon et al. [45] visualized three types of pointing line cues: the field-of-view frustum, eye-gaze ray, and head-gaze ray. They reported that these cues significantly improved user performance, usability, and subjective preferences. They also found that head-gaze ray was significantly less confusing to use than field-of-view.\nVisual Cues in Collaboration: Most of the previous research explored the effect of pointing line cues with hand gestures or eye gaze [3, 51] . For hand gestures, if users' hands are occupied, it requires users to deliberately release the object in their hand before pointing to a target. Some research used eye gaze to provide pointing line cues [8, 52] . They stated that eye gaze can lead to better performance and better teamwork than head gaze, i.e., the direction that a user is facing towards but not necessarily looking at. However, people's eye gaze has a lot of micro movements. Having to keep their eyes stable when pointing at an object may lead to fatigue. Head-based techniques, on the other hand, could mitigate this issue. By providing a gaze ray from a user's head to a target object, this can support observers' awareness of their collaborator's attention and allow them to view the same object [1] . However, studies on head-based pointing line cues are very limited, especially in co-located AR collaboration.\nVisual Cues in Collaboration: Moving Track Cues. Moving track is one of the mostly studied visual communication cues [48, 55] . It has been found to be more effective than pointing line cues for communicating spatial information [16, 34] . While the pointing line cues present a point around the target object, moving track cues provide users with a track, namely a series of points leading to the target, and can help guide users' focus to key locations [41] . Early research has studied the use of moving track cues on a shared video view [28, 55] . Researchers have explored extensively the use of moving track cues in remote collaborative physical tasks. For example, Billinghurst et al. [4] presented a method to record stabilized moving tracks in an annotation-based AR system. Teo et al. [56] reported that local users in remote collaboration felt the task was easier and more enjoyable with moving track cues, and the cues helped them understand the attention and focus of the remote user. A recent study [33] compared four combinations of visual cues provided by hand gesture, pointer and sketch. They reported that participants completed tasks faster and felt higher level of usability if sketch (moving track) cues were provided.\nVisual Cues in Collaboration: Although there have been studies on moving track cues, such as annotations and drawings, this research mainly used hand-based techniques and focused on remote collaboration [26, 56] . Besides, previous research on moving track cues also demonstrated some limitations. Much of this research explored these cues as an annotation tool. In this case, the track traces needed to be erased after completing each step of a task [34] . Huang et al. [26] also mentioned that moving track cues that blocked users' view could have a negative impact on user experience and task performance. Based on these findings, we have made the moving track cues to gradually fade away and disappear in our study. This solution neither requires users to actively erase any virtual element, nor prevents users from viewing the workspace with additional visual elements.\nVisual Cues in Collaboration: In summary, our research is framed on previous work and compares two head-based visual techniques, pointing line cues and moving track cues, to explore their effect on pointing tasks in colocated AR collaboration.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Collaborative work allows to combine knowledge and shape collective solutions that incorporates different perspectives. This can benefit various application areas ranging from problem-solving and content layouting, to architecture and manufacturing tasks [26, 31, 54] . A promising technology to enable collaboration across distance is VR. One can imagine a workspace where a user is collaborating at a table. The table and its objects are physical, spatially registered with and visualized in the virtual scene. This provides the advantage of physicality, the intuitive familiarity of working on a table and the tactile sensation of grabbing and manipulating objects. At the same time, the benefits of virtuality can be exploited -one can perceive the remote collaborators' presence and the interaction is synchronized over the network in a unified VR experience.\nINTRODUCTION: Lots of future collaborative work can shift to VR. With technical advancements, future VR systems could integrate physical objects and provide them as haptic props that are ubiquitously applicable during collaboration. This could enable remote users to create a collective solution by using physical objects present in their location.\nINTRODUCTION: Here a fundamental challenge is interaction with remote physical objects. One can manipulate the own local object as well as the virtual representation of a remote object but not manipulate the actual remote physical object.\nINTRODUCTION: Yet, collaborators may be of distinct expertise, where an ability to manipulate remote objects directly can become helpful. Related work suggests augmenting objects with motors [12, 44] or using teleoperated robot arms for remote control [15] . Or, to indicate the manipulation virtually, which the partner can then physically recreate in their location [20, 23] . However, as these approaches either require extensive hardware augmentation or user effort, it is desirable to seek alternative ways to tackle this challenge.\nINTRODUCTION: We investigate a new approach that engages users more actively and closely preserves the notion of physical manipulation across local and remote spaces. Our idea is to use passive haptic props in each physical location for the interaction with virtual objects. These haptic props have a variable representation in the virtual environment and can be used to control the available virtual objects. We explore this in two ways of ownership of the assigned virtual objects: SingleOwnership and SharedOwnership. SingleOwnership restricts collaborators to manipulate only the virtual objects that are associated with their local haptic props, whereas SharedOwnership allows transferring virtual objects between remote locations by taking over ownership with haptic props.\nINTRODUCTION: We implemented a distributed multi-user VR system that allows remote collaborators to interact with haptic props to solve a spatial arrangement task. The system incorporates haptic props registered at two locations by optical tracking. The spatial information is shared live across the network. Collaborators experience virtual objects assigned to remotely located haptic props at the correct 3D location and orientation in the virtual room. We conducted a user study to gather insights about the performance, experience, and trade-offs of the collaboration with different ownership strategies. We implemented a puzzle task that required the collaborators to create a certain arrangement of puzzle pieces using haptic props. To fulfill the task, the collaborators had to exchange knowledge with the given ownership techniques.\nINTRODUCTION: For SingleOwnership we employed two conditions. (1) collaborators could either use haptic props to arrange their own puzzle piece and then rely on verbal communication and gestures to communicate the solution of the task to each other. (2) Collaborators could create virtual instructions that indicate the correct arrangement of puzzle pieces using blank haptic props (Instruct). Therefore, we provided an additional set of haptic props. These haptic props were 'blank' and could be assigned to a puzzle piece by the user. For SharedOwnership, we employed two transfer techniques namely copy and cut. Copy allowed collaborators to use blank haptic props to retrieve a copy of a virtual object that is assigned to a remotely located haptic prop. Cut allowed collaborators to reassign virtual objects from remote haptic props to blank local haptic props. In this case, the remote haptic prop turned blank.\nINTRODUCTION: By having the ability to transfer remote objects using local haptic props, collaborators perceived that they communicated less using speech or gestures. Our results indicate that which strategy to handle ownership works best -SingleOwnership or SharedOwnership -is depending on the underlying scenario. We found that collaborators were significantly slower when using virtual instructions compared to verbal communication or transferring ownership via copy or cut. By having the ability to transfer remote objects using local haptic props, collaborators perceived that they communicated less using voice or gestures. Overall, we found that SingleOwnership techniques are more useful if awareness between collaborator's actions is needed (e.g., novice/expert scenario), while SharedOwnership techniques provide benefits when collaborators want to use their own expertise to solve a task with fewer dependencies on each other. For example, creating a collaborative solution to which collaborator contributes with their own knowledge to shape the best result.\nINTRODUCTION: The contributions of this work are as follows:\nINTRODUCTION: (1) The design of SingleOwnership and SharedOwnership collaboration techniques. SingleOwnership techniques rely on instructions either verbally or by using haptic props. Share-dOwnership techniques allow transferring virtual objects between remote locations using haptic props. This allows the collaborators to work more independently. ( 2) An open-sourced VR system for multi-user remote collaboration with tracked physical objects at both locations. (3) A user study comparing our techniques and revealing insights on the unique trade-offs between spatial task efficiency and communication engagement of the users.",
    "relatedWork": "RELATED WORK: In the following, we review previous work on video-based remote guidance, 3D-based collaboration, as well as tangibles and haptics with a focus on their use in collaboration.\nVideo-based Remote Guidance: As one of the first efforts, Kuzuoka investigated experts collaborating remotely over a screen with a head-mounted device (HMD) user [30] . The user's HMD included a small display that showed the collaborator's finger pointer image to indicate position. The evaluation showed that gestures led to improved task performance, with fewer words needed. Other methods for providing input to the collaborator are annotations through gestural sketching [41] , visual hand embodiment, and cursor pointers [16] . The latter describes two fundamental ways to support the collaborator: pointing gestures for reference and representational gestures to convey form and nature of actions. Kirk and Fraser compare unmediated hands, hands and sketch, and digital sketch only, either presented on a monitor or projected into the workspace [28] . No difference was found for output location (monitor or projected), but hand gestures had the highest performance. Others extended screen-based input to communicate rotation and translation of objects [2] .\nVR-based Collaboration: Traditional voice or video-based remote guidance confine the collaboration, e.g. from ambiguous language [52] or confusion [24] . Hence, VR-based collaboration approaches that go beyond voice and video-based guidance are promising for co-located [40] and remote collaboration [5] . It is beneficial for the experience when collaboration happens synchronously [19] . However, collaboration can be asymmetric, meaning users can collaborate using different technologies [27, 50] . Moreover, previous work shows that 3Dbased collaboration is not limited to two users [22, 46] and allows group-to-group telepresence [4] . Yet, it requires efficient interaction concepts to enable fluid collaboration [55] .\nVR-based Collaboration: An essential part of remote 3D-based collaboration is reconstructing collaborators' bodies [25, 29] and their physical environment [17] , allowing scenarios such as Holoportation [39] . Awareness cues, such as gaze and head movement, can be added for a more realistic collaboration [42, 45, 51] . Previous work found task performance to benefit from a combination of these awareness cues [21] . Others have explored how hand gestures and sketches can be integrated in collaboration scenarios [18, 49] . Studies found that hands are very intuitive [53] and increase task performance accuracy [49] . Hence, we utilize them for our scenario, empowering collaborators to communicate via pointing and gestures.\nPhysical Object Integration: 2.3.1 Tangibles. Tangibles allow computer interfaces to be closer to the physical world by providing users with haptic feedback [20] . They can enhance task performance (e.g., allow for more precise input [12] ), achieve a higher learning gain, and perceive problemsolving as playful [47] . Tangible interfaces for collaboration were introduced in 1998 [6] . They often require active components to reflect the movement of the tangibles at other physical locations [43] . Prior work explores scenarios, such as playing air-hockey over distance [37] or transmission of shapes [32] . Nonetheless, the active components remain technically challenging, making them less generalizable (c.f., air-hockey scenario [37] ).\nHaptic Props.: Combining tangibles with VR is promising as aspects of the physical object (e.g., visual appearance) can be added in real-time, enabling a more universal usage [23] . These generic physical objects are often referred to as haptic props, designed to give users the sensation of touch (e.g., as passive [33] or active haptic props [23] ) without a strict 1:1 mapping between object and function. How to enable more expressive physical sensations in VR has been explored before. For example, people can physically move objects in the background of the virtual session [8, 10] , the prop itself can dynamically change weight [56] , and can be actuated through robots [35, 58] or quadcopters [1] . It is also possible to use dynamic repurposing of interaction elements, such as the passive haptic [3] or the user's manual input [9, 57] to be able to interact with a more diverse set of props. Complementarily, our research investigates increasing expressiveness of haptic props for remote collaboration where local and remote props are mixed.\nCollaborative Haptic Props.: For co-located collaboration, these haptic props can be shared by users [23] , whereas if remote, each collaborator needs their own set of haptic props [13] . Previous work frequently studied asymmetric collaboration that uses a combination of AR for a novice user on-site and VR for a helping remote expert [7, 14, 38] . Nevertheless, in many scenarios it makes sense to utilize symmetric collaboration between VR users (e.g., problemsolving [31] , content creation [11] , or training [22] ). Different types of active haptic props have been proposed in the literature that can reflect manipulations by remote collaborators [13, 23] . Additionally, the teleoperation of a robotic arm can allow users to manipulate remote objects [15] . However, these systems are more challenging to construct and require additional components such as motors or displays. We extend the prior work by a study of how remote and local users can interact with passive haptic props and utilize them in a synchronous collaboration task.",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": " rq1: how will the high-and low-level agent affect ns clarifications respectively?\nrq2: how will the high-and low-level agent affect nns levels of understanding?\nrq3: how will the high-and low-level agent affect nns level of participation?\n rq4: how will different types of agent affect nns motivation to request clarification?\nrq5: how will the two types of agent affect ns and nns workload?\n one ns noted: why would you ask me about a basic vocabulary word?"
}
{
    "intro": "INTRODUCTION: Virtual conferences are rapidly becoming the dominant medium for online education, remote collaboration, and casual meetings with families and friends. However, gaze awareness is not conveyed accurately in virtual conferences, so it is difcult to determine who is looking at whom from video feeds. Moreover, users often turn of their cameras in video conferences, due to low network bandwidth, shared environments, or concerns about privacy. This leads to similar challenges as for audio conferences, in which people can only see each others' still profle photos. Motivated by these limitations, we wonder: What if we could augment virtual conferences in commodity hardware by using interactive 3D photos to add proper gaze tracking? Prior art such as True-view [83] , GAZE-2 [79, 80] , and Multi-View [51, 52] has leveraged multiple cameras or eye trackers to obtain users' gaze directions and synthesize view-dependent images in video-mediated conversations. Additionally, seminal work such as Photoportals [7, 38] , MMSpace [56] [57] [58] [59] , Sirkin et al. [71] , and TeleHuman [35] has investigated a shared large display, kinetic displays, or a cylinder display to convey gaze awareness in remote group conversations.\nINTRODUCTION: Besides, commercial software like Memoji 1 and Loom.ai 2 directly map user's facial keypoints to control points of a 3D avatar for enhancing video conferencing experiences. With the recent advances in eye tracking and neural rendering, we investigate the following questions: What if we do not have access to eye trackers and special displays? How can we design a virtual conferencing system to accommodate users' concerns about privacy and limited bandwidth? Can we enhance virtual conferences with gaze-aware 3D photos for a greater level of engagement?\nINTRODUCTION: Towards these goals, we design, deploy, and evaluate GazeChat (Fig. 1 ), a virtual conferencing system, which conveys gaze awareness with augmented 3D photos. Our focus is on bringing realistic gaze awareness to virtual meetings. Diferent from previous work like Memoji and Loom.ai, we render the relative gaze rather than absolute gaze: rendering the eyes to reveal gaze awareness information rather than just replicating the actual eye positions. GazeChat consists of four components: a WebRTC 3 confguration to support videoconferencing and logging, a real-time eye-tracking module inspired by WebGazer.js [60] to recognize gaze targets, deep-learning modules to infer depth maps and synthesize novel photos by redirecting the gaze, and a rendering module implemented with the three.js 4 . In terms of bandwidth, GazeChat only adds a small overhead of gaze data and a one-time packet of 22 images to conventional audio conferences.\nINTRODUCTION: To evaluate GazeChat, we conducted four user studies with 16 remote participants (ages 21-38, 7 female and 9 male). In our analyses of video recordings, post-activity questionnaires, and post interviews, we found that GazeChat can efectively engage participants in small-group conversations by visualizing gaze awareness. gaze awareness provides more eye-contact feeling than classic video and audio conferencing, can improve the conversation experience, bring greater social presence and richness, and provide better user engagement than audio conferencing while saving bandwidth and ofering privacy protection compared to videoconferencing. Our main contributions are:\nINTRODUCTION: (1) Conception, development, and deployment of GazeChat, a virtual conference system that can convey gaze awareness in augmented 3D photos. (2) A low-cost, low-bandwidth, in-situ pipeline to turn users' profle pictures into animated, gaze-aware 3D photos on ordinary laptops without special hardware. (3) Reporting evaluation results and refections about the opportunistic use of GazeChat in virtual conferences (VC)benefts, limitations, and potential impacts to future VC systems. (4) Open-sourcing 5 . Our system is web-based and crossplatform compatible, making it easier to adopt for future research. We plan to make our software available to facilitate future development in VC systems with real-time neural rendering of nonverbal cues.",
    "relatedWork": "RELATED WORK: We review prior art on multi-user experience in distributed collaboration, gaze tracking, gaze redirection technology, and how gaze awareness is integrated into virtual conferences.\nMulti-user Collaboration in Distributed Environments: Distributed multi-user collaboration has been widely researched from the perspective of locomotion, shared proxies, and lifesize reconstruction, as well as for diferent purposes including communication [25] , presentation [27, 77] , and object manipulation [22, 43] . Your Place and Mine [72] creates experiences that allow everyone to use real walking for locomotion in collaborative VR. Three's Company [78] presents a three-way distributed collaboration system that places remote users either on the same side or around a round table. Besides, Three's Company provides non-verbal cues like body gestures through a shared tabletop interface. Remote users' arm shadows are displayed locally on a tabletop device, which is benefcial for collaborative tasks with shared objects. Tan et al. [77] focus on presentation in large-venue scenarios, creating a live video view that seamlessly combines the presenter and the presented material, capturing all graphical, verbal, and nonverbal channels of communication. The concept of Blended Interaction Spaces [54] is proposed to provide the illusion of a single unifed space by creating appropriate shared spatial geometries. TwinSpace [66] is a generic framework discussing brainstorming and presentation in cross-reality that combines interactive workspaces and collaborative virtual worlds with large wall screens and projected tabletops. SharedSphere [42] is a wearable MR remote collaboration system that enriches a live captured immersive panorama-based collaboration through MR visualization of non-verbal communication cues.\nMulti-user Collaboration in Distributed Environments: Immersive collaborative virtual environment (ICVE) and Augmented Reality (AR) are widely used to develop new forms of teleconferencing, which often leverages multiple cameras setup and 3D reconstruction algorithms. EyeCVE [73] uses mobile eyetrackers to drive the gaze of each participant's virtual avatar, thus supporting remote mutual eye-contact and awareness of others' gaze in a perceptually coherent shared virtual workspace. Jones et al. [33] design a one-to-many 3D teleconferencing system able to reproduce the efects of gaze, attention, and eye contact. A camera with projected structure-light is set up for reconstructing the remote user. Billinghurst and Kato [8] developed a system that allows virtual avatars and live video of remote collaborators to be superimposed over any real location. Remote participants were mapped to diferent fducial markers. The corresponding video images were attached to the marker surface when markers are visible. Room2Room [62] is a telepresence system that leverages projected AR to enable life-size, face-to-face, co-present interaction between two remote participants by performing 3D capture of the local user with RGBD cameras. Holoportation [55] demonstrates real-time 3D reconstructions of an entire space, including people, furniture and objects, using a set of depth cameras. Gestures are preserved via full-body reconstruction and headset removal algorithms are designed to convey eye contact.\nMulti-user Collaboration in Distributed Environments: Prior art values the importance of immersion and non-verbal cues especially for eye contact information. Various devices are included such as cameras, markers, big screens, and headsets. We design GazeChat to investigate how we can use minimal hardware to provide essential information for distributed communication.\nEye Tracking: There have been extensive research focusing on developing eyetracking devices such as the Tobii eye tracker [63] and on using webcams for eye tracking. Such methods typically involve an explicit calibration phase and are less accurate than infrared eye trackers [23] . One of the early-stage appearance-based methods employed video images for neural networks [5] . Recent work like Lu et al. introduced an adaptive linear regression model that requires sparse calibration however is sensitive to head movement [46] . Later Lu et al. overcame this by using synthetic images for head poses, however, need extensive calibration [47] . Another trend of research takes advantage of image salience to estimate gaze for calibration purposes [74] , and salience is only a rough estimate of where a user is looking. Alnajar et al. designed a webcam eye tracker that supports self-calibration, though still requires users to look at the \"ground truth\" gaze patterns [1] . Similarly, PACE [30] and TurkerGaze [84] also predicts gaze information through webcam. Diferently, GazeChat focuses on providing region-level gaze awareness instead of identifying the pixel-level gaze information. We support adapting webcam-based eye tracking technology for GazeChat use as well as eye-tracking device such as the Tobii eye tracker.\nConveying Gaze Awareness in Collaborative Tasks: Buxton started a series work researching shared space in remote collaboration since decades ago [11] , especially discussed how eye contacts behave in such kind of videoconferencing [68] . Sellen et al. [68] present Hydra, a prototype for supporting four-way videoconferencing. Three picture-in-picture devices were used to represent three remote participants. Separate devices also native support diferent view points. Hydra raises the common motivation for conventional videoconferencing and has strong impacts for following work like MMSpace [56] . Additionally, deep discussion on personal space and social space was made and diferent principles were proposed for mediaspace, meaningspace, and meetignspace [10] .\nConveying Gaze Awareness in Collaborative Tasks: To better describe gaze information in the context, we use the term \"gaze awareness\" to represent the information we want to convey during videoconferencing. Gaze awareness is related to gaze direction information. It is an ability to perceive an accurate spatial relationship between an observing person and the object, that is being observed [50] . In this work, we focus on gaze awareness related to person. Many prior work visualizes gaze awareness for various purposes. \"An eye for design\" [15] breaks down attributes of eye movements and is inspiring. Eye-write [40] and the follow-up work \"Efects of Shared Gaze\" [41] emphasize the effects of gaze awareness on a shared screen. In the meantime, \"Look together\" [89] enhances collaborative search via gaze. Gaze is a complementary modality to be included in videoconferencing and is efective for multimodal communication [9] . EyeCVE [73] uses mobile eye-trackers to drive the gaze of each participant's virtual avatar, thus supporting remote mutual eye contact and awareness of others' gaze in a perceptually coherent shared virtual workspace. LookAtChat [26] used symbols to show gaze awareness without changing original video feed. Jones et al. [33] design a one-to-many 3D teleconferencing system able to reproduce the efects of gaze, attention, and eye contact. A camera with projected structure-light is set up for reconstructing the remote user.\nConveying Gaze Awareness in Collaborative Tasks: Inspired by previous work, GazeChat integrates gaze awareness into virtual conferences as well as separating personal space and social space. We will elaborate on the design and validate the work.\nGaze Correction and Redirection: To enable gaze redirection, the traditional approaches are typically based on 3D modeling [6, 81] by ftting eye texture and shape against 3D morphable models, but they are not ideal for handling images with eyeglasses and the high variance of facial details. Some others [13, 39, 85, 90] render a scene containing the face of a subject from a given viewpoint to mimic gazing at the camera.\nGaze Correction and Redirection: Various hardware setups have been explored for gaze correction including hole in screen, long distance, and half-silver mirror. The hole in screen concept is about drilling a hole in the screen and placing a camera. Long distance uses a screen at a far distance while placing the camera as close as possible [75] . Half-silver mirror allows a user to see through a half-transparent mirror while being observed by a well-positioned camera at the same time. This idea was adapted in ClearBoard [24, 32] and Li et al.'s transparent display [44] . Despite their advantages in terms of system complexity and costs, such solutions are rarely used outside of laboratory due to the availability of hardware. In the meantime, quite a few 2D videobased (or image-based) approaches are proposed for eye contact including eye correction with a single camera [2, 3] and multiple cameras [14] while applying image-based approaches like texture remapping and image warp [20] . However, the technology was not sufciently accurate to avoid visual artifacts . 3D video-based solutions including 3D reconstruction is another trend for maintaining eye contact while the head is reconstructed. RGB camera [83] , depth camera [90] , Kinect [39] , or motion capture system [49] are used for 3D reconstruction. Eng et al. [18] propose a gaze correction solution for a 3D teleconferencing system with a single color/depth camera. A virtual view is generated in the virtual camera location with hole flling algorithms. Nourbakhsh also used one webcam to apply gaze redirection for one-to-one video conferencing [53] . Compared to single camera setup, multiple cameras are popularly used for providing gaze [4] in videoconferencing.\nGaze Correction and Redirection: The deep learning era gave rise to various learning-based methods [19, 28, 36, 37, 61, 86] . These methods mostly train neural networks that predict the fow feld for warping the eye pixels in the original image, and additional techniques such as inpainting [88] and latent space interpolation [82] have been proposed to improve improve the visual quality and redirection precision. However, these methods still fall short of reliably generating to data in the wild, with large variations in head poses, gaze angles, and lighting. A crucial reason for the lack of success for many learning-based method is the limited training data, since it is very difcult to collect a large amount of high-quality eye gaze images with correctly labeled gaze angles.\nGaze Correction and Redirection: The First Order Motion model [70] estimates unsupervised keypoints from the input images and predicted a dense motion feld to warp the source features to the target pose. Thanks to its unsupervised nature, this method benefts from a much larger training data set and produces results compelling visual quality. Although it is not specifcally designed for the task of gaze redirection, we manage to repurpose the FOM model to generate the redirected eye gazes at desired angles.\nEye Contacts and Gaze Visualization Applied in Video-mediated Conversation: True-view [83] was implemented with two cameras (one on the left and the other on the right). The synthesised virtual camera view image at the middle viewpoint is generated to provide correct views of each other and the illusion of close proximity. GAZE-2 [79, 80] utilizes an eye tracker with three cameras. The eye tracker is used for selecting a proper camera closest to where the user is looking. GAZE-2 prototypes an attentive virtual meeting room to experiment with camera selection. In each meeting room, each user's video image is automatically rotated in 3D toward the participant he is looking at. All the video images are placed horizontally so the video image turns left or right when the corresponding camera is chosen. Likewise, MultiView [51, 52 ] is a videoconferencing system that supports collaboration between remote groups of people with three cameras. Additionally, MultiView allows multiple users to be co-located in one site by generating a personal view for each user even though they look upon the same projection surface, which they achieve by using a retro-refective material. Photoportals [7, 38] groups local users and remote users together through a large display. All users are tracked and roughly reconstructed through multiple cameras and then rendered within a virtual environment. MMSpace [56] [57] [58] [59] provided realistic social telepresence in symmetric small group-to-group conversations through \"kinetic display avatars\". Kinetic display avatars can change pose and position by automatically mirroring the remote user's head motions. One camera is associated with one transparent display. Both camera and display can be turned to provide corresponding video input image and output angle. Sirkin et al. [71] developed a kinetic video conferencing proxy with a swiveling display screen to indicate which direction in which the satellite participant was looking for maintaining gaze and gestures to mediate interaction. Instead of rendering a video image on a rectangular display, a cylinder display is proposed in TeleHuman [35] with 6 Kinects and a 3D projector.\nEye Contacts and Gaze Visualization Applied in Video-mediated Conversation: GazeChat is designed to be used with a minimum requirement of a laptop/PC and a single webcam. While multi-view cameras and external hardware may yield better eye tracking and 3D rendering solutions, such systems typically require very high computational power and exclusive hardware setups. Since it is possible for users with low-cost video conferencing setup to learn to interpret gaze direction to a very high degree of accuracy [21] , we decided not to apply extensive image-based manipulation on video streams but rather to focus on the design of a widely accessible online system to empower video conferencing users with real-time visualization of eye contacts.",
    "rq": " we wonder: what if we could augment virtual conferences in commodity hardware by using interactive 3d photos to add proper gaze tracking?\n we investigate the following questions: what if we do not have access to eye trackers and special displays?"
}
{
    "intro": "INTRODUCTION: Efective communication is important in everyday social interactions. Within many organisations, large investments are made by training employees to communicate better. It is also well recognized that non-verbal communication plays a signifcant role in the competency of communication styles in a wide range of contexts [63] . Most training interventions rely on human trainers to provide feedback to learners, but this is costly, labor intensive, subjective and heavily reliant on the skill and experience of individual trainers. Research within the domain of afective computing and social signals processing have started to explore the potential of augmenting or replacing human trainers through the use of automated recognition of nonverbal signals with promising results [7, 14, 30, 31, 59] . However, most studies have focused on a narrow range of channels. They also tend to focus on evaluating the performance of one individual, rather than considering the interplay of signals between communicators.\nINTRODUCTION: Our previous work has explored which multimodal signals best predict human ratings of communication skills in the context of TV interviews and developed a usable feedback display to provide participants with information about their performance [47] . Efective media skills are important for many organisations including commercial companies, political parties and non-profts, since performance in this context can have huge implications for organizational reputation and outcomes. To our knowledge, none of the previous afective computing interventions for communication skills have focused on this specifc domain.\nINTRODUCTION: In the current paper, we report an experiment which was conducted to assess the impact of the feedback intervention we had developed on the efectiveness of training to improve media interview performance. We present a controlled between-groups pre-post experiment study where half of the trainees received standard media skills training and half received the standard training augmented with tailored feedback based on automated recognition of facial expression, vocal signals, hand movements and 'honest signals' [45] . We compared performance before and after training across both groups using subjective measures of performance and using measurements of the participants' displayed social signals. The methods used here have potential to be adapted to support real world training interventions for media skills. Longer-term, the results are relevant to the development of an automatic training feedback system to help learners self-refect upon their performance.",
    "relatedWork": "RELATED WORK: The social signal processing (SSP) domain aims to understand and interpret social interactions using nonverbal cues [65] . Signal expression depends highly on context. To recognize the signifcance of an expression researchers must note where an expression is displayed, when it is displayed and who the presenter is [65] . Later, researchers included the signifcance of why and how a cue is expressed [49] . Research in this feld has been successful in capturing postures [55] , gestures [11] , vocal behaviour [15] and inferring emotions from facial expression and eye movements [68] . The contexts which have been investigated includes job interviews [42] , healthcare [26] , public speaking [53, 54] and in the classroom [3] . Earlier research investigated social signals in isolation (i.e. facial expression only during an interaction); however, research has demonstrated that multimodal analysis is more informative of understanding naturalistic interactions. Van den Stock and colleagues (2007) investigated emotions associated with body expression and found that when investigated in isolation, the recognition of emotions is incorrectly recognized and cannot be interpreted [57] . It is noted that this is a result of visual integration of such cues which are necessary for adaptive behavior when responding to others [2] .\nRELATED WORK: Augmentation of social interactions requires the use of sensor and visual displays that provide trainees with real-time feedback on nonverbal behaviours. The purpose of this is to increase trainees awareness of their use of nonverbal signals and improve the quality of their behaviour in any given context. The behavioral feedback method has been used to provide the user with real-time feedback that is suited to the user, the context and the scenario [17] . This method of feedback provision includes observational learning, operant conditioning, social cognitive theory, perception, refection and action [18] . Several studies have investigated the efcacy of this method [6, 52, 53] . Even though these studies found promising results, researchers did not investigate whether this method was distracting. In contrast, a study found that visual displays during an interaction was not distracting [18] . However, research in cognitive functioning postulates that an increase in visual load is cognitively taxing [1] and could impact trainees overall performance.\nRELATED WORK: Studies using technology enhanced training have been successful in improving social skills or communication skills [31] . Researchers [27] developed My Automated Conversation CoacH (MACH). MACH is a social skills training platform which allows users to communicate with a virtual character. MACH captures facial expression and speech and generates information on the users use of nonverbal cues. Similarly, a study by Damian and colleagues [16] found that this technique was useful in improving job interview training in underprivileged adolescents. Another group of researchers developed a feedback system called ROC Speaking Framework [68] [70] . Researchers found that feedback of social signals during job interview training signifcantly improved in comparison to traditional methods of training. Similar results were found when attempting to improve communication skills in those with social impairments [60] , public speaking [13, 52, 53] , medical students [39] , job interviews [3, 9, 31, 32, [41] [42] [43] . However, no studies investigate whether social skills can be improved using automated feedback in the context of media skills training.",
    "rq": "1. can communication skills be improved by providing automated feedback in the context of media interview training?\n 2. are there diferential training efects for social signal feedback when tested after 6 months?"
}
{
    "intro": "INTRODUCTION: Nowadays, persons who need assistance in completing a complex task often rely on experts that are physically present. Typically, such assistance is practical if the person in need is not experienced or can only succeed with additional advice. An expert visiting the person on site can be a solution in situations, such as a car breakdown where the driver acts as the unqualified worker during an accident and a professional repair mechanic gives instructions as the expert. In a business environment, an exported machine could be maintained by a local field technician assisted by a remote certified expert. In a casual scenario, a traveler could get one-directional tourist information from friends at home. However, while highly effective to provide help colocated, it is also time-consuming to travel large distances, especially if the time to solve tasks is shorter than the time to travel. Further, traveling results in high costs for the involved parties. For instance, the US Travel Association reported that business travel in 2016 totaled in $ 307.2 billion 1 .\nINTRODUCTION: To tackle such high costs and travel times, remote assistance systems emerge to provide support over a distance. Traditional, those systems are based on voice-only or video communication, such as phone or video conferencing tools. Hence, they are limited within their scope of verbal describing or pointing at objects (e.g., \"look at this\", \"reach there\"). This can be cumbersome if the parties do not share a common ground or have language constraints [5, 23] . Moreover, voice communication can interfere with noise or overload the worker's perception requiring a high amount of conscious attention while solving the actual task. Some more advanced systems suggest video-mediated approaches, where the field worker has a separate display showing instructions of the expert [1, 7, 18, 21] . Some of them are completely stationary with a fixed setup, while others add another degree of freedom by giving the expert extra control (i.e., movable camera [10] ).\nINTRODUCTION: To overcome those limitations, the research community focused on supportive methods via 1) audio, 2) visual or 3) tactile communication channels. Each provides subtle context information and may reduce the need for verbal agreements. This context information may concern the head orientation, the hand motion or even the whole body movement (e.g., walking somewhere or turning around). Figure 1 shows an example of a car breakdown scenario and how a remote expert could assist a person with different communication approaches.\nINTRODUCTION: In this paper, we contribute a first prototype implementing three different cues: Audio, Visual, and Tactile. We evaluate these cues individually in an exploratory study to identify their benefits and limitations. Finally, we report the results based on the user feedback as lessons learned.",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Think of a work-separated family. Two partners who have been appointed to different schools, or a military officer serving at a base far from her husband and children. It is unfortunate but yet they can manage to see and talk to each other every day, thanks to cutting-edge mobile, networking, and multimedia technologies. Imagine a fictional system that superposes two distant homes and the people therein. It provides real-time self-mobile avatars directly in the physical space of both homes; e.g., the person at Home A is present in Home B through her avatar, and vice versa. The avatar could be a hologram or a humanoid robot looking, moving around, and acting exactly in the same way as she is doing at her remote home, despite all the spatial and interior differences between those homes. For example, a military officer enters her quarters after work; she sees his young son's avatar running to her at the porch, welcoming him with open arms. At the same time, the little boy sees his mother's avatar entering his porch and runs to her. They dine together; each of them is dining at one's own table with the other's avatar dining at the same table. They sit on the couch together, talk and laugh, and she sees him falling asleep next to her.\nINTRODUCTION: This fictional system highlights an avatar intelligently mimicking the remote person in real-time. The avatar does not simply copy every inch of one's activity and location as-is; it relocates and adjusts itself to make it look perfectly natural in different home environments. This system enables a distant couple to be mutually co-present in each other's living space, with all activities and movements intelligently mirrored to each other. Such co-presence brings many pseudo living-together experiences incomparable to those of today's telecommunication. They naturally perceive each other's live presence and context directly in one's living space. It could be even a peripheral perception [48] with little cognitive cost. Making an interaction is hassle-free, with no interference. Also, small interactions may naturally blend with other on-site activities such as cooking or household chores, which are often discouraged in remote conversation. In essence, they may communicate over a distance, but they commune together when being co-present.\nINTRODUCTION: Realizing such a system opens up a vast spectrum of technical and design challenges. In this paper, we build an initial prototype, HomeMeld, a zero-hassle self-mobile robotic system serving as a real-time, co-present avatar to create a persistent illusion of living together. HomeMeld is built on top of a commercial telepresence robot hardware [2] and a CNN-based computer vision technique, letting the person be device-free at all times.\nINTRODUCTION: Among many questions along the path towards this ambitious dream, HomeMeld focuses on intelligent simultaneous positioning of the avatar from a human point-of-view. Figure 1 shows two scenes of HomeMeld operating at two distant homes. The first scene shows the wife's home (Figure 1a ) and the husband's home (Figure 1b ) at the same time. The wife is sitting at her desk and talking to her husband's avatar standing against her kitchen bar and facing her. Conversely, the husband is standing against his kitchen bar and talking to his wife's avatar located at his desk and oriented towards him. The second scene shows those two homes a few moments later (Figure 1c and 1d, respectively). The wife has moved to her couch, calling her husband's avatar that has turned around towards her kitchen bar. In the husband's home, her avatar has moved autonomously to his couch, as she has, and calls him from behind. Having the other person's presence at such an equivalent location is important because it is an intuitive indicator to his/her context, for example, she may be busy with her work and he may feel like drinking something.\nINTRODUCTION: As illustrated above, HomeMeld synchronizes the avatar's presence with the remote person in terms of both space and time. We highlight that, given an arbitrary location/orientation in Home A, defining the human-perceived equivalent location/orientation in Home B is a fuzzy problem susceptible to uncertainty, subjectivity, and dependency on individual home environments.\nINTRODUCTION: We address this problem through a two-tiered approach. We begin with establishing human insights; we collect extensive, humanlabeled anecdotal point-to-point mappings between heterogeneous floor plans. This data set revealed interesting trends of varying human-perceived equivalence between in-home locations, depending on a location's proximity to household objects with inherent functions. Then we leverage those trends to derive a generalized computational model that evaluates the functionally equivalent location in a home for a given location in another home.\nINTRODUCTION: Our model of functionally equivalent location synchronizes the avatar's presence in terms of space, but not necessarily in terms of time. Our model translates a person's real-time path in Home A into a sequence of functionally equivalent locations in Home B. It does not necessarily ensure the same traveling distance, resulting in the avatar lagging behind. We devise predictive path warping to ensure the user experience of same-time co-presence.\nINTRODUCTION: To continuously detect a device-free person's in-home location/orientation, HomeMeld takes a single-camera vision approach. We instrument a room with a ceiling-mounted 180\u00b0camera in favor of obstruction-free, accurate localization. This design choice led us to train a custom CNN model localizing a person and detecting her orientation viewed from a very unusual angle, at every possible location and orientation in various homes. We devise unique strategies to collect, augment, and synthesize the data set to mitigate the data-hungry challenges of training a custom-purposed CNN model.\nINTRODUCTION: Although our initial prototype is based on a simplified robotic hardware that is unable to mimic every possible human motion, the key features of HomeMeld could be seamlessly extended towards higher fidelity mimicry as more sophisticated hardware with higher degrees-of-freedom such as a humanoid robot, as well as higherprecision indoor human activity sensing [37] become available.\nINTRODUCTION: Our contributions are threefold. First, we envision a physical co-presence service superposing two distant, heterogeneous living spaces through self-mobile avatars. Second, we build a devicefree working prototype, through which we address self-navigating telepresence robots featuring human-perceived spatio-temporal equivalence across heterogeneous homes. Third, we conduct smallscale deployments with real participants suffering from involuntary distant living and discuss their experiences with HomeMeld.\nRELATED WORK: Lack of situation awareness in remote interaction: In spite of rapid advance in communication technology, Walther et al. pointed out that the lack of nonverbal cues is the key weakness of remote interaction [61] . The lack of non-verbal cues decreases opportunities to understand a remote other's situation [23] . Brave et al. tried to overcome this issue with tangible interfaces, which create the illusion that distant users are interacting with shared physical objects [14] . Yarosh et al. designed a remote parent-child interaction by augmenting video chatting with a camera-projector system [64] .\nRELATED WORK: Natural remote spontaneous interaction: While there have been many attempts to investigate remote spontaneous interaction, most of them focus on implying simplified other-related information to the smartphone alert [57] , text message [58] , video chatting [20, 42, 52] , household object [43] , and public social agents [56] . Representing the remote other's contexts, including real-time behavior, to facilitate natural spontaneous interaction remains underexplored.\nRELATED WORK: Indoor localization: Within a wide range of indoor positioning techniques, passive wireless indoor localization has the advantage of freeing people from wearable devices. Such localization techniques have employed WiFi base stations [65] , visible light arrays [37] , and a custom FMCW antenna array [7] . They exhibit various trade-offs among localization accuracy, gesture support, and complexity of infrastructure setup. While our approach using CNN-based visual recognition of user location and orientation provides reasonable accuracy with relatively simple setup, the aforementioned techniques may complement HomeMeld in further extensions that capture user contexts at higher fidelity.\nCONCLUSION: We proposed HomeMeld, a zero-hassle self-mobile robotic system serving as a co-present avatar to create a persistent illusion of living together. We explored the practices and challenges for work-separated families to keep their bonds. We developed models and a system to ensure synchronous co-presence experiences with human-perceived spatial equivalence between heterogeneous living spaces. HomeMeld has been evaluated through both controlled experiments and deployments to real participants under workseparation. We envision further emergence of versatile, higherfidelity co-presence services, and HomeMeld would serve as an early catalyst therein.",
    "relatedWork": "RELATED WORK: Lack of situation awareness in remote interaction: In spite of rapid advance in communication technology, Walther et al. pointed out that the lack of nonverbal cues is the key weakness of remote interaction [61] . The lack of non-verbal cues decreases opportunities to understand a remote other's situation [23] . Brave et al. tried to overcome this issue with tangible interfaces, which create the illusion that distant users are interacting with shared physical objects [14] . Yarosh et al. designed a remote parent-child interaction by augmenting video chatting with a camera-projector system [64] .\nRELATED WORK: Natural remote spontaneous interaction: While there have been many attempts to investigate remote spontaneous interaction, most of them focus on implying simplified other-related information to the smartphone alert [57] , text message [58] , video chatting [20, 42, 52] , household object [43] , and public social agents [56] . Representing the remote other's contexts, including real-time behavior, to facilitate natural spontaneous interaction remains underexplored.\nRELATED WORK: Indoor localization: Within a wide range of indoor positioning techniques, passive wireless indoor localization has the advantage of freeing people from wearable devices. Such localization techniques have employed WiFi base stations [65] , visible light arrays [37] , and a custom FMCW antenna array [7] . They exhibit various trade-offs among localization accuracy, gesture support, and complexity of infrastructure setup. While our approach using CNN-based visual recognition of user location and orientation provides reasonable accuracy with relatively simple setup, the aforementioned techniques may complement HomeMeld in further extensions that capture user contexts at higher fidelity.",
    "rq": "confidence-aware speed adaptation: what if there are multiple functional objects towards alice's moving direction?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Video conference has long been a prevalent telecommunication tool. In providing moving images of remote users, video conferencing Session 5: Remote Interaction and Initiative HAI '18, December 15-18, 2018 , Southampton, United Kingdom approximates real, face-to-face communications. However, video is unable to convey the experience of co-existence between users. Remote users do not feel as if they are physically close to local users. Similarly, local users do not feel as if they are sharing space with the remote users. Telepresence is a technology that addresses these problems of existence by providing the sensation of being in a remote space. While our communication experience is limited in many aspects with video conferencing systems, telepresence systems enable us to explore a room, look around, point at objects and make gestures. One of the most important factors of these activities is gaze, which also plays a crucial role in face-to-face interaction [2, 10] , a direction of attention, and non-verbal and emotional expressions. These communication channels are used unconsciously in our daily lives and in-person conversations. Thus, a substantial amount of telepresence research has focused on gaze. They have proposed mechanical systems which simulate movements of the neck or the eyes to represent gaze direction [9] . Graphical systems with non-flat screens have also been developed to serve a similar purpose [17, 24] .\nINTRODUCTION: Especially, we focus on communication using an omnidirectional camera. An omnidirectional camera provides a panoramic and immersive view in a telepresence system [4, 6, 7] . Our research question is whether gaze information serves as a rich expression of remote user's presence in an omnidirectional telecommunication.\nINTRODUCTION: This paper introduces a telepresence system, \"OmniGaze\", that tracks remote users' gaze information and conveys the information to local users through an LED animation on a modular spherical LED display covering an omnidirectional camera in the local conference situation (Figure 1 ). We evaluated OmniGaze in terms of the accuracy of communicated gaze direction and conducted a user study to confirm the effect of gaze in conversation.",
    "relatedWork": "RELATED WORK: Our work is based on the following two factors: gaze representation and omnidirectional video communication.\nGaze Representation: The importance of gaze and eye gestures was established early in the history of telepresence [22] . Paulos et al. proved that exploring remote locations using Mechanical Gaze [20] facilitated remote users' creation of coherent image maps. Paulos et al. also asserted that the gaze of a telepresence robot is an effective tool for humanlike behavior such as focusing attention on specific local users [21] . The teleconference systems MAJIC [17] and GAZE-2 [24] have realized communication of gaze direction or eye contact by using a camera in clever ways. Sirkin et al. proposed a telepresence robot that can make nonverbal gestures with hand and head motion [23] . One limitation of conventional flat-screen systems is the Mona Lisa effect in which viewers perceive eye contact from any viewing angle of the face on when presented as a flat image. To overcome this problem, researchers have proposed systems with face-shaped screens [3, 13, 15, 18] that more accurately convey the direction of gaze. ScalableBody [14] and Transgazer [11] focused on eye contact with a mechanical system, but neither is capable of representing omnidirectional gaze. In that aspect, TEROOS [9] and SphereAvatar [19] are well-known systems that support omnidirectional gaze representation. TEROOS [9] employed a pan-tilt mechanism to realize freedom of gaze, while SphereAvatar [19] presents remote users' faces on a spherical screen. However, we do not consider these systems to be functional equivalents to the omnidirectional camera described in the introduction. During the observation study, we reference studies that investigate the role of gaze in human-avatar communication [1, 5, 16, 26] .\nOmnidirectional video communication: Before omnidirectional cameras became a common tool for recording an omnidirectional video, Ikeda et al. developed the technique of acquiring high-resolution panoramic movie with a multi-camera system [7] . Nagahara et al. suggested a telepresence system with which a head-mounted display (HMD) provides remote users with a panoramic view. Similarly, JackIn Head [8] is an immersive telepresence system that employs a light-weight head-mounted omnidirectional camera and a collaborative interface with image stabilization. By combining numerous cameras to function as a high-performance omnidirectional camera, Gaemperle et al. obtained high-resolution omnidirectional images, which they applied to a telepresence system with an HMD [4] . Although all these studies share a focus on immersiveness, their systems are single-sided and they do not support representation. Li et al. proposed an interesting omnidirectional video communication device [12] , and they also measured effects of this symmetric communication in collaborative tasks. Xu et al. [25] proposed a suggestive remote collaboration system which had an omnidirectional camera and enabled a remote user to look around the local meeting space. This system shows where the remote participants are pointing a camera by drawing arcs on a tablet display. Xu et al. found that this direction of virtual cameras, called \"simulated gaze\", offered feeling of remote user's presence. However, this simulated-gaze method had two limitations. First, this system shows only a horizontal gaze, because they adopted a flat animation and screen. Second, this system doesn't track remote user's eye, and it simulates the position of gaze from the direction of view. We considered that remote communication would improve by accurate gaze tracking because we use a variety of gaze gesture in face-to-face communication.",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Mockups are widely recognized in Human-Computer Interaction (HCI) as invaluable tools for communicating and evaluating design ideas. A mockup can help ground descriptions of UI functionality and can serve as a \"boundary object\" that allows designers to communicate with developers and other stakeholders. Mockups are useful throughout the User Interface (UI) development lifecycle, from exploration to refnement. However, most tools for mockup creation are built for the earlier (exploratory) stages of UI development.\nINTRODUCTION: There are several challenges when creating mockups as communication tools in the later stages of UI development-for example, to propose changes to a UI that already works or to describe a desired behavior in a UI that contains an error. First, most tools for creating mockups cannot import assets or behaviors from existing UIs, and it can be tedious to replicate the intricate details of a working UI in a mockup. Second, it can be difcult to communicate how an existing UI should change because it is not easy to point out the diference between the existing behavior and the mockup's behavior-particularly when the change is nuanced or dynamic [43, 48, 51] . Third, mockups that propose changes to existing behaviors often need to be mixed fdelity [40] -with highfdelity representations of existing components and low-fdelity renderings of proposed changes-but few mockup tools support this. These limitations led to our research question: How can we make communicating about changes to existing UIs easier and more efective?\nINTRODUCTION: In this paper, we introduce CoCapture, an interactive system that enables users, like UI designers, to easily create and then accurately describe dynamic UI behavior mockups. These mockups could represent changes the users want to propose or questions they want to ask about an aspect of the existing UI. With CoCapture, users frst record the existing UI behavior by demonstrating an example interaction on the existing UI (Fig. 1 , Step 1). Building on this scene, users can further create dynamic behaviors via demonstrations that manipulate DOM elements (Fig. 1, Step 2) and remix these demonstrations as a frst-class animation object (Fig. 1, Step 3) through direct manipulation and low-fdelity sketching. To help accurately specify the visual changes, users can write Natural Language (NL) descriptions in CoCapture that contain hypertext references to specifc aspects of the mockups (e.g., specifc DOM elements, new animated efects) (Fig. 1, Step 4 ).\nINTRODUCTION: We conducted two within-subjects studies to evaluate the communication efectiveness of multiple aspects of CoCapture: the efort of creating visual context and the accuracy and clarity of the description. In these studies, we asked \"requester\" participants to describe a UI behavior and \"helper\" participants to read the descriptions that requester participants generated. Our results show that compared to traditional sketching and communication tools, the requester participants using CoCapture spent less than a third of the time on text writing, and their descriptions of UI behavior were signifcantly more accurate. Additionally, the helper participants reported that the descriptions in CoCapture were more accurate, concrete, vivid, and easier to follow.\nINTRODUCTION: The key contribution of CoCapture is a novel interactive method that combines the DOM element-based recording technique with a demonstrate-remix-replay approach. This makes it easier to prototype on pre-built UIs and to describe user needs regarding dynamic UI behaviors more accurately than is possible with existing approaches. With CoCapture, users can efortlessly explore diferent possible designs, capture feeting ideas, and communicate with others about behavior ideas on existing interfaces. Specifcally, our contribution includes:\nINTRODUCTION: \u2022 A set of novel interaction designs and techniques that allow users to capture, demonstrate, remix, and then describe the Yan Chen, et al., Yan Chen, Sang Won Lee, and Steve Oney UI behaviors they want to add or inquire about on an existing UI via direct manipulation. \u2022 CoCapture, a system that integrates all these techniques to make communicating about changes to existing UIs easier and more efective. \u2022 Evidence showing that CoCapture can help designers more easily create UI behavior descriptions that are easier to understand and follow compared to those created by existing approaches.",
    "relatedWork": "RELATED WORK: As Myers et al. [43] explain, interactive behaviors defne the \"feel\" of a UI (as opposed to its \"look\"). Tools like VisBug [3] and Poirot [57] help designers quickly change the look of their UIs. We focus on communicating about the feel of the UI. This process often consists of two tasks: making visual references and referring to the visual references. In this section, we review related work and techniques in these felds.\nCreating UI Prototypes and Mockups: Systems like SILK [31] and DENIM [39] lower the overhead cost of prototyping by recognizing designers' sketches as interface elements and implementing the idea of wireframing, respectively. However, they do not support creating prototypes in later stages of UI development. More recently, tools like Rewire [56] and Poirot [57] have made prototyping new designs easier by enabling the users to directly edit elements of existing examples. However, they do not support interactive UI behavior editing, which is more difcult than designing static layouts, as the behaviors are complex to demonstrate and designers have access to limited tools [51] . Commercial tools like Figma [2] and Adobe XD [1] can ease the creation of interactive behaviors but assume their users would reconstruct existing interfaces from scratch, making it hard to scale. Other layout-capturing tools such as WebToLayers 1 and PageLayers 2 automatically convert websites to Photoshop documents. However, they only support static layouts; they do not preserve the DOM structure, element constraints, or dynamic UI behaviors. Crowdpowered systems like Apparition [32] and SketchExpress [34] allow designers or even non-experts to more rapidly create or reconstruct a prototype than they could with existing tools, but these systems also fall short of recreating particularly complex interfaces. Unlike these systems, CoCapture helps create interactive mockups in the later stages of UI development, proposing changes to a UI that already works or describing desired behaviors in a UI that contains an error. These UIs can be arbitrarily complex, requiring efort to create mockups that replicate existing functionality. Compared to layout-capturing tools, CoCapture also captures the DOM structure, allowing designers to easily add behavior mockups on existing interfaces. This gives them the ability to immediately envision the new behaviors and complete the process of creating a mockup as a reference.\nVisual References as Shared Context in Communication: Many prior studies have reported that people often include screenshots, drawings, or sketches as visual context in communication. These studies have explored questions of how designers communicate desired interactive behaviors to engineers [43, 51] , how In-foVis novices describe data visualization [20, 42] , how programming novices explain PC game behaviors to the computer [49] , and how end-user developers communicate about application extensions with other developers [17] . However, they also consistently found participants' responses to be vague, ambiguous, or imprecise, suggesting future systems should provide a tight feedback loop in which users see immediate results to refne ambiguous descriptions.\nVisual References as Shared Context in Communication: Creating visual references can help ground communication when discussing visual design and providing feedback. In a face-to-face or video conference setting, we can use pointing gestures in shared visual spaces to make references to visual information that is difcult to express with words. Much work has studied methods for referring to visual content in communication, including text annotation [44] , remote gestures [19, 27, 28] , and awareness widgets [13, 18] in diferent computer-supported cooperative work contexts such as authoring [41, 55, [60] [61] [62] and groupware [23, 24] . They have shown that referring methods can facilitate mutual understanding by reducing verbal efort and its associated complexity [22] . The ability to leverage non-verbal communication is an important factor in decreasing the efort of writing clear messages [16] .\nVisual References as Shared Context in Communication: In programming communication, systems like chat.codes [45] and Callisto [59] use deictic code pointing techniques to facilitate creating code references and connections with text descriptions that help developers discuss code. Codeon [12] is an in-IDE support environment to help requesters and helpers exchange code context easily. MarmalAid [15] allows users to start a real-time conversation on a geometric location in a 3D workspace. Building on these approaches, we aim to address the problem of referring to dynamic and interactive visual references for more efective communication.\nRecord, Replay, and Manipulate Existing Interfaces: A core technical part of CoCapture's system is the record and replay (R&R) technique, which is used to record an existing behavior once and then replay it repeatedly and automatically without user interaction. Prior work has used this technique for various purposes. Systems such as Scry [7] , Telescope [26] , Unravel [25] , Doppio [14] , FireCrystal [47] , and WebCrystal [9] use this approach to help people understand existing UI behaviors. Systems like Chronicle [21] , Timelapse [6] , and MobiPlay [53] record meta-data (e.g., operations, code editing) and allow users to easily capture the rich data of application behaviors. Our techniques enable designers to not only record arbitrary web interface behaviors, but also to easily add new designs on top.\nRecord, Replay, and Manipulate Existing Interfaces: To ease the creation of new behaviors, FrameWire [38] decreased the efort of communicating new interactive designs by automatically extracting interaction fow from paper prototype video recordings. Many other recording augmentation systems have been developed to help efectively prototype new digital content, such as CHI '21, May 8-13, 2021, Yokohama, Japan Montage [35] and Augmented Reality (AR) experiences like Proton [36] , or to provide video feedback, like VidCrit [52] . Park et al. developed a technique that enables users to edit text content in a text-based recording while preserving the recording's overall consistency [50] . CoCapture also allows its users to easily add new behaviors over recordings, but instead of simply supporting overlaid visual annotations or user comments anchored to specifc parts of the content, it allows users to edit existing elements through direct manipulation.\nRecord, Replay, and Manipulate Existing Interfaces: The feld of Programming by Demonstration (PbD) has used similar techniques to augment users' ability to perform tasks with which they often lack expertise. Rousillon [11] and Sugilite [37] allow their users to record and edit the operations via domain-specifc languages. CoCapture also supports a set of mockup creation operations that allow designers or even novice users to edit the recording and simulate the desired interactive behaviors.",
    "rq": " these limitations led to our research question: how can we make communicating about changes to existing uis easier and more efective?\n q1: can designers ask a more accurate ui behavior question using cocapture than with a text-based communication tool (e.g., email)?\n q2: can they also create the questions more quickly?\n q3: can helpers easily understand the questions in cocapture?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": " 1) how does the sharing of eye gaze or hand gestures from the remote user affect collaboration in a mr remote collaboration interface?\n 2) what are the benefits of mixing both gaze and gesture cues for mr remote collaboration compared with using each cue alone?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Collaborative virtual environments (CVEs) are emerging as a promising CSCW platform, through which distributed stakeholders can create, discuss, and review spatial content (e.g., 3D models, room layouts, animations, as in [7, 28, 71, 79] ). Such content is becoming increasingly important in creative industries such as architecture firms, video game companies, and design agencies. Simultaneously, in these domains, the modern workforce is also becoming more globalized and time-distributed, increasing the need for supporting asynchronous collaboration between teams working across different time-zones or working hours. In these settings, successful collaboration depends on support for asynchrony, which has several unique advantages over synchronous communication, such as: work parallelism, flexible time-coordination, reviewability, and reflection [40, 58] . Several contexts where asynchronous collaboration becomes crucial in the evolution of a shared artifact were discussed in-depth by Tam and Greenberg [76] .\nINTRODUCTION: With the advancement of virtual reality technology, asynchronous collaboration in immersive 3D environments has become an active research area [14, 32, 44, 50, 60, 67, 80, 85] . Multimodal recording-the capture and later playback of multiple, dynamic interaction modes, such as speech, locomotion, body language, and object manipulations-is at the foundation of asynchronous collaboration in these VR systems. In these studies, however, multimodal recording was regarded only as one of many features of a system, designed for a specific purpose such as training [85] or 3D design [80] . The state of the literature leaves room for more empirical research [84] on conceptualizing multimodal recording in VR and producing a generalizable understanding of it.\nINTRODUCTION: We aim to address the knowledge gap in conceptualizing and understanding this multimodal asynchronous VR collaboration (MAVRC) as a type of CSCW, backed by an empirical understanding of its challenges and design considerations. VR, CSCW, and HCI literature reveals three aspects of MAVRC: social behaviors in mediated-communication, awareness in cooperative work, and authoring and consuming multimedia. Using these aspects as an intellectual lens, we begin to identify and tackle these knowledge gaps by formulating the following research questions:\nINTRODUCTION: \u2022 How are social behaviors transferred from face-to-face to MAVRC? In VR, where collaborators can leverage embodied avatars with multimodal communication capacities, different aspects of social norms (e.g., proxemics, bias, anxiety, etc.) can be transferred to, or even amplified or diminished in CVEs [4, 26, 73] . However, MAVRC is fundamentally different from those of the previous studies in that asynchronous communication is one-way-the message sender cannot respond to the viewer's inquiry in real-time.\nINTRODUCTION: It is an open question to what extent, or if at all, users feel the social presence of an embodied 3D representation of their asynchronous collaborator and display social behaviors towards it (e.g., what do they feel if the recorded avatar breaches their intimate space?). \u2022 What are the challenges of maintaining workspace awareness and coordination in MAVRC? It is critical for the success of any CSCW system to offer proper support for establishing and maintaining different types of awareness about others' activities: workspace awareness is the knowledge of what is going on and what others are doing [17, 25, 36] ; asynchronous change awareness is the knowledge of how shared artefacts (e.g., 3D models, source code, documents) have evolved via the contributions of time-distributed collaborators [76] . The MAVRC setting is at the intersection of 3D and asynchrony, where the challenges of awareness may be exacerbated by the combination of spatial occlusion, limited viewpoint, and lack of real-time feedback mechanisms (e.g., dynamic activities of one's asynchronous collaborator can occur out of the user's view, and the collaborator would not know how to draw one's attention to it). \u2022 What are the challenges of navigating and creating multimodal recordings in MAVRC? Benefits and challenges of multimodal recording have been deeply studied in non-VR CSCW contexts, such as document annotation [86] , design prototyping [49] , and video production [62] . On the one hand, the rich, communicative capacities of recorded multimodal interactions help \u2026this plant should be by the window\u2026 \u2026this plant should be by the window\u2026 \u25ba REPLAYING \u2022 RECORDING Fig. 1 . Capturing and replaying speech, pointing, and head movement for a spatial collaborative task using our experimental system; (left, in the view of a recording producer) a remote collaborator makes a request about moving the position of the plant; (right, in the view of a recording consumer) an asynchronous listener replays the recording, watching the avatar of the remote collaborator as they express themselves through multimodal interactions.\nINTRODUCTION: collaborators express and understand nuanced, complex ideas effectively [13, 55, 87] . On the other hand, as Grudin pointed out, when consuming, browsing recorded speech can be tedious and slow for the consumer [34] , and, when producing, it is known that multimodal recordings are harder to revise than text [72] and can provoke self-consciousness [1, 52] . When it comes to supporting spatial tasks in immersive CVEs, however, there is a significant gap of knowledge regarding the challenges and design considerations of authoring or consuming multimodal recording in VR.\nINTRODUCTION: To answer these questions, we conducted a qualitative, exploratory need-finding study based on our experimental system, in support of multimodal recording in immersive VR. Our participants were tasked to perform asynchronously collaborative 3D design tasks by viewing and recording multimodal interactions. Key findings from our observations and interviews include: (1) participants felt the social presence of asynchronous collaborators when viewing recordings, leading to proxemic behaviors and empathy, (2) immersion in VR can cause challenges in viewing multimodal recordings, such as viewpoint disorientation or confusion from different versions of 3D scenes, (3) recording (and viewing) co-expressive speech and body language enabled participants to express (and understand) nuanced ideas effectively but highlighted needs for editing (and browsing) tools. We discuss implications of these findings for designing MAVRC systems and present a set of design recommendations. Demonstration and preliminary evaluation of four proof-of-concept interfaces provide support for the feasibility of our design implications.\nINTRODUCTION: The contributions of this paper are three-fold: (1) a conceptualization of multimodal recordingbased asynchronous collaboration in VR; (2) empirical findings on the user challenges of respecting proxemics, maintaining awareness, and consuming/producing multimodal recordings in MAVRC; and (3) design implications for multimodal recording for asynchronous collaboration: proactive proxemics management, animating changes in 3D, viewpoint display for awareness, and 3D navigational cues. We also touch on the transferability and generalizability of our findings on MAVRC to non-VR asynchronous 3D interactions (e.g., AR, mobile 3D, etc.). Through the exploration of asynchronous collaboration in VR, this work opens up new opportunities for richer forms of collaboration, lessening the challenges presented by geographical and temporal barriers.",
    "relatedWork": "RELATED WORK: Our study combines themes of asynchronous collaboration, CVEs, and multimodal annotation from HCI, CSCW, and VR literature.\nTools to support asynchronous collaboration: Asynchronous collaboration refers to cooperative scenarios where participants interact at different times. CSCW researchers established theoretical frameworks to help identify benefits and challenges of asynchronous collaboration. Olson and Olson theorized advantages of time-distributed collaboration as work parallelism, flexibility in coordination, reviewability, and reflection [58] . Tam and Greenberg suggested change awareness, the ability of the user to trace the non-real-time changes made to the shared workspace, as a critical consideration in designing asynchronous collaboration systems for shared documents and graphical artifacts [76] . One of the main foci in HCI, informed by these CSCW frameworks, is to develop asynchronous collaboration tools for a variety of applications: video meetings [77] , data visualizations [39] , web search [54] , and review & feedback [63, 64, 86] .\nTools to support asynchronous collaboration: However, since these frameworks and tools are developed for applications in a 2D environment, it is still unclear how the challenges and design considerations for asynchronous collaboration reveal themselves differently in VR. Our study extends past research by identifying issues unique to VR collaboration.\nMultimodal asynchronous collaboration in virtual reality: Virtual reality systems with digitally constructed environments and robust spatial tracking can offer compelling opportunities in aiding collaboration. In the field of collaborative virtual environments (CVEs), researchers have focused on harnessing VR to provide more effective means in supporting communication and information sharing [74] . Most research, however, focuses on applications of synchronous collaboration, such as joint-tasks [24, 65] , giving instructions via a ghost-hand metaphor [78, 85] , tele-presence [59] , or review & feedback [56] . Our study aims to discover the design considerations specific to asynchronous VR collaboration.\nMultimodal asynchronous collaboration in virtual reality: The premise of MAVRC in our research is grounded on many successful previous studies on VR systems for asynchronous collaboration. V-Mail [44] and MASSIVE-3 [32] are the most relevant to MAVRC in that they supported the capture and replay of rich, multimodal interactions. Several applications were built based on the notion of multimodal recording in different domains including architectural review [35] , creative feedback [56, 80] , training [85] , and tele-communication [14, 60, 67] . The primary foci of these studies were more on developing novel interaction techniques/concepts or designing a new technical pipeline. In contrast, we aim to provide extensive empirical evidence that help us understand the range of user needs, challenges, and design considerations for MAVRC.\nMultimodal asynchronous collaboration in virtual reality: Very recently, Lindlbauer and Wilson [50] conceptualized several types of time manipulations, including pause, loop, and replay of a captured 3D scene, as part of the taxonomy of possible interactions in mixed reality. Our study adds to this by lending a fresh task-centric view to the time manipulation that takes place in a collaboration context.\nMultimodal recording and annotation: Central to asynchronous collaboration is the exchange of messages across time. Early examples of asynchronous communication are typically textual (e.g., email or text messages). With the ubiquity of audio capture devices, use of recorded speech (e.g., voicemail) for expressive messages have been extensively explored in HCI and CSCW. With a voice recording, message producers can leverage paralingual cues (e.g., inflection, pause timing, energy) for conveying equivocal and complex ideas [13] . Speech annotation/commenting, thanks to its rich, nuanced expressivity, has been widely adopted in different applications including online discussions [52] or document review [55] . Inspired by media richness theory [18] and the conceptual framework of deictic gestures [17] , several multimodal annotation systems have tried to enrich communication and collaboration by combining multiple face-to-face inspired modalities, such as inking + speech [75, 83] , inking + gesture + speech [86] , and video + hand gesture overlay [38] .\nMultimodal recording and annotation: The benefit of expressive richness comes at a cost in browsing and editing. Grudin, in his seminal work on CSCW applications [34] , insisted that the major reason why voicemail is not popular is because its browsing and navigation features are very slow and tedious. To address this challenge, HCI researchers suggested various approaches such as using semantic navigation cues like captions [82] , ink strokes [75] , gesture trace [86] , video thumbnails [66] , viewers' collective navigational trace (e.g., skips and jumps) [47] , or directly manipulating an in-video visual entity along its motion trajectory [57] . Besides the browsing issue, in Yoon et al. 's study where a speech-based document annotation tool was deployed to classrooms, it was found that the support for efficient interfaces for editing and revising audio recording is necessary for the success of speech commenting tools [87] . For this problem, researchers devised efficient multimodal editing solutions such as using time-aligned captions as a proxy for editing audio and video [8, 68] .\nMultimodal recording and annotation: In contrast to these previous studies on 2D media, our empirical setting is in 3D where the recording of body movement and scene manipulation is a multidimensional data stream. Our study identifies users' specific needs for viewing, skimming, and revising such data.\nProxemic interactions in VR and HCI: Proxemics, originally introduced in Hall's seminal work [37] , describes the way interpersonal distance manifest social context. As modern computing systems became sensitive to the physical presence and social context of users, designing proxemic interactions attracted the attention of HCI researchers [5] . Greenberg, Marquardt, and their colleagues-the pioneers of proxemics in HCI-proposed proxemics as a new genre of HCI [31] . Due to the embodied nature of VR, many researchers studied the social implication of interpersonal distance in CVEs. The most relevant work is Bailenson et al. 's study on proxemics in immersive VR [4] . They found that human proxemic behaviors, such as moving away from an embodied agent who invades one's personal space, can be transferred or even amplified in collaborative VR environments. Similarly, Schroeder and colleagues showed that VR users exhibit such behaviors to an interactive avatar with high behavioral realism [70] . Our study adds to this literature by suggesting that proxemic behaviors can be transferred to asynchronous CVEs.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: This paper explores Mixed Presence (MP) collaboration using Mixed Reality (MR) technology, where MR is defined as any display on which the real world and virtual world can be presented together [Milgram et al. 1995] . MR technologies have been researched as a tool for enhancing communication, such as enabling a person to see live virtual video avatars of remote people in the real space [Piumsomboon et al. 2018] . The problem of collaboration in MR has been partially addressed by co-presence and telepresence technology. For example the development of remote expert systems with annotations to support communications [Gao et al. 2016; Lee et al. 2017] . In most of these earlier MR collaborative systems, the users are either all remote from one another, or in the same face-to-face environment. So there is an interesting unanswered question about how to have collaborative MR systems with both co-present and telepresent collaborators. This has spurred the concept of Mixed Presence systems [Marrinan et al. 2017; Tang et al. 2004; Tuddenham and Robinson 2007] which incorporates both face-to-face and remote users. However, research in this area has been largely focused on static tabletop or Spatial Augmented Reality (SAR) systems [Hutterer et al. 2006; Wang et al. 2014 ] and there has only been a minor focus on Head Mounted Display (HMD) based collaborative AR interactions 1,2,3 .\nINTRODUCTION: In particular, we are investigating the effect of having a coordinator who can help direct a group of users in a task and how this may affect the engagement of a remote user is in a MP collaboration. This type of situation is common in command and control environments, such as in emergency response (where a coordinator is directing teams of first responders), or collaborative medical and military situations, among others. The goal of this research is to explore collaborative MR using MP to increase the level of collaboration that users can engage in by designating the remote user as a coordinator.\nINTRODUCTION: As MR technology becomes more prevalent and accessible via Mixed Presence Groupware (MPG) such as Shipspace and Holodeck Control Center, it is important to expand current research in collaborative MR to explore the best tools and implementation of MR for collaborative tasks that require many users. The motivations for our research are to expand AR and MR collaboration into MP scenarios to support and connect multiple people while maintaining an good overall quality for those collaborations and ensuring good system usability. As a first step, we conducted a user study with a MR MPG system to understand collaboration dynamics between local and remote collaborators.\nINTRODUCTION: The main contributions of this work are 1) an initial evaluation of a MR MPG 2) identifying and bringing attention to some issues for future MR MP research 3) a discussion of the effect of having a designated coordinator in a MP task. In the remainder of the paper, we first review related work and identify the contribution our work is making. Then we outline the MPG system we are using, consisting of a desktop PC and MR HMDs, and report on a user study conducted to assess the system and investigate collaboration dynamics under two conditions of role assignment. Finally we end with a conclusion and directions for future work.",
    "relatedWork": "RELATED WORK 2.1 Co-presence: Co-presence can be described as a sense of being with others in a virtual environment or in a remote physical environment [Zhao 2003 ]. Zhao [Zhao 2003 ] breaks co-presence down into three classifications (Corporeal, Virtual and Hypervirtual) based on whether the users are physically (corporeally) present at the site of the colocation and divides systems into co-presence and telepresence based on physical proximity and electronic proximity to one another.\nRELATED WORK 2.1 Co-presence: Fussell et al. [Fussell et al. 2000 ] found that when collaborators were physically co-present they performed better than when using audio-only or video and audio communication, concluding that a shared visual space is essential for repair tasks that require instruction and coordination. Kraut et al. [Kraut et al. 2002] further addressed the need for a shared visual space in collaborative problem-solving tasks as it reduced the number of errors by collaborators in complex environments.\nRELATED WORK 2.1 Co-presence: An early example of co-present collaborative technology using AR is AR2Hockey [Ohshima et al. 1998 ]. This allowed two copresent users to play virtual air hockey using Optical See-Through Head-Mounted Displays. The players could see each other in the real world, as well as the virtual game content superimposed on the real table between them, supporting a very natural collaboration.\nTelepresence: More recent work has focused on Telepresence systems in order to allow remote users to collaborate together. For example Komiyama's [Komiyama et al. 2017 ] JackIn Space or Lee's Shared Sphere [Lee et al. 2017 ] system which aim to provide environmentally aware Telepresent systems. In these cases, live 360 video of a local person's environment is shared with a remote collaborator, enabling them to have a higher degree of Presence.\nTelepresence: Telepresence is the ability of a user to remotely collaborate on a task while having a sense of 'being there' [Minsky 1980 ]. Telepresence differs from co-presence in that co-presence captures human to human relationships whereas telepresence (and Presence in general) encompasses human-object and human-environment relationships giving the user a sense that they are 'there' in a remote environment [Zhao 2003 ]. Telepresence has found uses in a number of different applications [Rae et al. 2015] , including group-to-group teleconferencing which allows multiple users to remotely meet with one another [Beck et al. 2013] and remote expert systems which allow a remote expert to assist a local user in completing a real-world task [Gao et al. 2016] .\nTelepresence: Remote expert telepresence systems have been paired with clientside AR to allow the remote expert to add annotations and navigational drawings [Gauglitz et al. 2014; Kim et al. 2014; Reitmayr et al. 2007] , and to improve the collaboration. For example, Gauglitz et al. [Gauglitz et al. 2014] showed how a local worker with a handheld AR interface could live stream video of their workspace to a remote expert on a desktop computer. The expert could then place AR annotations in the local worker's view that appeared to be fixed to real objects, and in this way could help them perform a real-world task. Other research has combined a remote expert utilizing a Virtual Reality (VR) HMD with a local user utilizing some other technology, such as an AR HMD or a 360 camera [Kasahara et al. 2017 ]. These technologies are used to improve collaboration such as placing the remote user's virtual hands into the scene to allow gesturing [Gao et al. 2016] or allowing the users to see one another's gaze point [Lee et al. 2017 ].\nMixed Presence: Mixed Presence differs from other collaboration systems in that it falls into multiple quadrants of the Space/Time Taxonomy (see Figure 2 ) [Tang et al. 2004] . For example, MPG systems allow users to synchronously collaborate when some users are at the same location and others are in different locations. This means that MPG systems cannot fall into just one of the quadrants of the Space and Time Taxonomy and must be heterogeneous. Ens et. al. [Ens et al. 2019 ] pointed to Mixed Presence as an issue that still needs to be addressed in order to match the complexity of real-world collaborations. Furthermore, the roles of the users in groups were highlighted as another issue that is yet to be addressed in groups.\nMixed Presence: Tang et al. [Tang et al. 2004 ] developed a prototype MPG system using heterogeneous displays for co-located and remote collaborators, and evaluated the MPG to identify problem areas. In their studies, they found two problems. The first problem was caused by heterogeneous displays creating a disparity due to how users were seated in the workspace. This disparity occurred due to the differing viewpoints each display provided. For example, a tabletop display user might perceive something drawn by a user on a traditional monitor as being upside down. The second problem arose from a presence disparity due to co-located users having a better perception of one another's presence than of the remote user, causing a breakdown of communication between co-located and the remote collaborators. They attempted to mitigate these issues by creating digital arms and showing each user's virtual sitting position.\nMixed Presence: Tuddenham and Robinson presented an API based solution for a table-top MPG allowing developers to easily create complex MP tabletop applications [Tuddenham and Robinson 2007] . They argued that their system catered for MP applications by allowing collaborators to interact with and reorient items in the system which is important for mitigating disparities and increasing awareness. More recent research utilizing large heterogeneous displays found that their setup improved collaboration and coordination between users [Marrinan et al. 2017] .\nMixed Presence: There have also been some MPG systems using VR. For example, Johnson et al. [Johnson et al. 1998 ] developed an MPG system using CAVE and VR technologies for educational purposes by having local and remote children collaborating together to author virtual ecosystems. This system was shown to have a positive impact on the children's learning. However, the children felt limited by the technology and the authors noted the expensive setup cost of a CAVE system at the time. Norman et. al. [Norman et al. 2019] described and developed a simple MR MPG and received mostly positive feedback when demonstrated to a number of groups.\nMixed Presence: Overall, MPG systems are shown to be compelling collaborative tools that can improve collaboration, coordination, and learning. This research also shows that although there has been some research exploring MPG systems, there has been very little research into HMD based systems, and none using HMD based AR systems that would allow collaborations that use more of the real world without the limitations of SAR. There is also a gap in research on the roles users play in group scenarios and how this may affect collaboration in this type of groupware.\nSummary: Prior research shows that physical co-presence is very important for collaborative tasks, and when that is not possible, visual cues, such as those provided by AR, can significantly improve the remote collaboration. Mixed Presence systems can combine both face to face and remote collaborators, and the benefits of this have been shown in a number of tabletop and VR systems. Previous research has shown that AR HMDs can be used to both provide enhanced face-to-face collaboration (e.g. AR2Hockey [Ohshima et al. 1998 ]) and remote collaboration (e.g. [Tait and Billinghurst 2015] ). Despite this, there has been little research done on the use of AR HMDs in Mixed Presence and especially formal studies between two local AR HMDs users and a remote helper. Furthermore, previous works have outlined the need for research into the effect of different roles in collaborations [Ens et al. 2019 ]. While previous work has proposed roles in complex collaborations [Lages 2016 ] and looked at the effect of different technologies on leadership [Pan et al. 2018] , no formal studies have been conducted on the effect of changing roles or coordinators. Our research will help address these limitations and provide valuable insight into how to create effective Mixed Presence Mixed Reality systems.\nSummary: To investigate collaboration dynamics in a Mixed Presence Mixed Reality system we must first investigate how different role allocations affect collaboration. In this paper, we especially look into the role of a coordinator. Lages [Lages 2016 ] conceptualized a role of coordinator called a 'Director' who could divide the Degrees of Freedom (DOF) among users in collaborative interactive tasks in a VR environment. The Director would divide the DOF depending on who the Director believed had the best viewpoint to perform a particular operation, but would not be able to perform any action themselves, limiting the interactions available. We apply this concept to Mixed Presence Mixed Reality collaboration.",
    "rq": ""
}
{
    "intro": "INTRODUCTION: For a long time now, educational researchers have emphasized the importance of computational thinking in STEM education for young children [10] . Such thinking enables children to use concepts from computer science to solve problems, and understand their worlds in better ways [59] . This has lead to the development of computational toolkits aimed at developing such thinking among the children [61] . In today's digitally mediated society we are surrounded by internet-of-things (IOT) based devices that assist us in our everyday lives. Hence, there is a growing need of introducing children to the affordances of these new technologies such that they don't just use them but are fluent in developing new designs using them [51] . Addressing this requirement, we developed a new toolkit DIO, based on a distributed design form-factor, inspired from IOT technologies. DIO comprises of input, output modules that communicate wirelessly, and can be connected using an augmented reality based interface to implement a variety of interactive applications. The modules can be suspended on to bodies of multiple children as wearables or into the environment like on a wall.\nINTRODUCTION: Digital-physical construction toolkits comprise of electronic building blocks hosting a variety of sensing, actuation capabilities; that can be combined together to create a variety of constructions. Their working can be described on the basis of the theory of constructionism given by Seymour Papert, who established how children learn in designing and creating things that are meaningful to them [39] . Since then researchers have developed a variety of toolkit designs, as surveyed by Kelleher et al. [31] . While engaging with these construction toolkits, children develop an understanding of the associated concepts that the toolkit operates on (powerful ideas [40] ,) like physical stability in LEGO, looping and data structures in programming kits, etc. This toolkit-based approach towards learning has been observed to be very effective in the past [7] , and through DIO, we apply it towards learning about the internet-of-things. In the paper, we show how, creating interactive applications with DIO helps children understand concepts related to IOT technologies like input-output pairing, wireless communication, in addition to sensing, actuation functionalities of different modules.\nINTRODUCTION: Toolkits based on tangible manipulatives have been successful in teaching programming fundamentals to children [21] . Recently, toolkits based on wearable form-factor have been used to create designs that are \"potentially always available\" with the child, and hence can easily become a part of his/her everyday experiences [9, 30, 38] . In DIO, this design expands the advantages of wearability to multiple children, who can all wear the modules on their bodies. DIO modules worn by children on their bodies fall into their respective personal space, while the modules attached on to the environment fall in a common social space. This arrangement allows the children to work as design partners and engage in collaborative learning. Research has demonstrated that collaborative learning can enable higher achievement and productivity, foster supportive relationships, and social competence among children. Baranauskas et al. have showed how collaboration can further strengthen the goals of the theory of constructionism [3] . In spite of such benefits, not many construction toolkits have defined collaboration as a design goal. We show how the DIO toolkit fills in this gap by enabling shared making, learning experiences for the children.\nINTRODUCTION: To build the DIO system, we followed an iterative design process spanning across a duration of a year. We started with a workshop design session with the children in which we used low-fidelity material to check as to what they would make around the ideas of body, environment-mounted input and output modules. This was followed by iteratively building and pilot testing prototypes with our target users. Informed by these insights, we build our final system that enables children to create multi-user constructions, and to learn in the process. We report on our findings related to different research goals and reflect on the general trends of collaboration that we observed. We defined 6 research goals based on prior work in designing construction toolkits. They are as follows.\nINTRODUCTION: (R.G. 1) Usability of the toolkit: Our most basic research goal was to gauge whether the children were able to use the system. This can include being able to identify the modules, using the augmented-reality based programming application to connect the modules, and to develop general understanding of the system, including the purpose of different input, output modules. (R.G. 2) Problem-solving using DIO: Problem-solving is common in programming activities, and hence a significant characteristic of systems that enable programming as explored in previous work on construction toolkits [60] . We supported this research goal by testing how easy or difficult it is for children to implement a target application using the toolkit. (R.G. 3) What do the children make?: Through the DIO system, we introduced the children to the novel formfactor of shared input, output modules that could be worn on their bodies or could be mounted in their environment. Hence, we noted what do the participants create given these new possibilities. (R.G. 4) Development of understanding regarding the associated concepts: We hypothesized that by working with the DIO toolkit, the children would become fluent with the functionalities of the technologies around them and would understand their environments better. To check regarding this hypothesis, we presented the participants with questions to gauge if they were able to apply the learnt concepts in scenarios from their everyday lives. (R.G. 5) Subjective factors: This goal includes probing the children's views on the DIO toolkit. Whether they like it, do they find it easy to use, or their general experience of engaging in the co-making activity. (R.G. 6) Trends of collaboration: Our system is rooted in co-making and multi-user experience. Thus, we expected the children to work as partners, in collaboratively making the constructions. Moreover, we wanted to validate the usage of personal and shared spaces during construction activities. Hence, we reflect on the collaboration themes that we observed. In summary, our contributions are as follows:\nINTRODUCTION: (1) The design and implementation of the DIO toolkit including the modules, and the augmented reality based interface for programming. (2) Findings from the pilot study, evaluative user studies in context of the 6 research goals (R.G. 1-6) mentioned above.\nINTRODUCTION: (3) Finally, we reflect on our learnings from engaging children in constructionism-based making activity using our toolkit, which can be applied to other future toolkit designs.",
    "relatedWork": "RELATED WORK: Our work is related to design of construction toolkits. We borrow from perspectives from scaffolding techniques in education, and from the research on shared learning experiences.\nComputational Kits and Programming Tools: Researchers have designed and developed a variety of computational kits and toys aimed at developing computational thinking among the children [59] . By letting children to program, these kits help them understand a variety of computational concepts (sequences, looping, parallel operation,) and computational practices (being iterative, debugging, etc.) [60] . In their recent survey, Yu and Roque have provided a summary of this space [61] .\nComputational Kits and Programming Tools: The programming interface is usually based on the block as a puzzle piece paradigm, wherein the blocks representing different programming structures/ commands that can be arranged by the children to design different programming behaviour [61] . Such interface is manifested in form of a Graphical user interface (controlled through a mouse) [8, 34] or as a tangible platform [19, 25] . Tangible platforms comprising of physical programming blocks are especially preferred for younger children owing to their ease of use [22] . However, Horn et al. have suggested hybrid interfaces (comprising of tangible and GUI-based parts) as a better alternative [20] . They have argued that such interfaces let the teacher and the students switch between tangible and GUI-based programming in different situations, and hence offer a great amount of flexibility.\nComputational Kits and Programming Tools: Our toolkit comprises of physical blocks that can be attached in the environment or on the bodies of multiple children. The blocks are programmed through an Augmented Reality (AR) based interface, after which they communicate wirelessly to implement various designs. AR has recently gained popularity in supporting learningbased activities related to sentence formation [12] , astronomy [13] and environmental awareness [42] . In addition to affording the dynamism, flexibility of hybrid interfaces, AR has been observed to provide effective feedback, reduce cognitive load, and enhance the enjoyment associated with learning [45] . In case of DIO, AR serves as an effective programming method supporting the distributed design of the toolkit. This approach is partially inspired from Reality Editor, an AR-based tool to design interfaces around programmable physical objects [17] . DIO is the first effort in utilizing such programming technique in developing construction toolkits for learning. We show how this novel approach leads to enhanced learning of associated concepts, and effective making activity using the toolkit.\nConstructionist Toolkits: Construction toolkits aim to teach programming concepts, computational thinking by letting children construct things. Typically, these toolkits constitute of simple parts that the children associate together through programming and hence create a variety of constructions. Such toolkits are based on the theory of Constructionism given by Seymour Papert which stated that best learning experiences happen when the learner is actively involved in designing and creating things; things that are meaningful to them, are share-able with other people [39] . Blikstein [7] and Kelleher et al. [31] have provided comprehensive surveys of such construction toolkits.\nConstructionist Toolkits: Such toolkits allow their users to make a variety of things like virtual stories [44] , programmable robots [46] , and personalized wearable devices [9] . Comprehensive framework provided by Resnick and Silverman has been used to evaluate the expressivity of these designs [43] . Specifically wide walls and high ceiling indicate the diversity and complexity of constructions that can be designed using any toolkit. Popular among different construction toolkits are the electronics-based toolkits that enable constructions around physical computing [4, 15, 48] . Typically, these toolkits consist of input (for sensing), output (for actuation) devices that can be combined to create a variety of applications. Such toolkits enable children to bring their programs to the physical world that they live in, hence allowing them to understand how the world works. This relates to the idea of technological fluency defined by Papert which states how the children should not just use the technology around them, but should also be fluent in making things using it [40] . Our toolkit consisting of input, output modules that are attach-able into the environment or onto bodies of the users can be used to create a great variety of interactive environments. Children construct much of their knowledge through active manipulation of the environment [5] . Hence, poking and playing with the interactive constructions created using DIO modules can enable children to learn a great deal about their immediate environments that are replete with smart sensing and actuation devices. Moreover, the wireless communication based transmission of data resembles the design of the internet-of-things (IOT) systems that encumber our lives.\nConstructionist Toolkits: Papert had stressed that the engagement of children is heightened when the constructions are of personal interest (i.e. personally meaningful.) This focus has recently led research on constructionist toolkits having a wearable form-factor [30, 38] . Wearables being close to our body become part of our identity facilitating personalization and expression, and hence are meaningful to the individual. DIO expands this aspect of wearability to multiple children who can all wear their designs and can interact with one another. This along with environment based constructions allow group ownership and shared interests, hence facilitating collaboration. Moreover, unlike the existing wearable toolkits wherein the modules can be attached onto a specific body part, the DIO modules can be individually attached onto the different body locations, hence increasing the design possibilities (wider walls). Distributed design is a significant design feature of the DIO toolkit that enables these affordances.\nConstructionist Toolkits: Constructionist toolkits utilizing a distributed design have been scarce. BlockyTalky [50] is computer music toolkit comprising of distributed networks of sensors and synthesizers. Through this design, the researchers enabled multiple children to simultaneously manipulate the output composition, and make music in a collaborative manner. Their implementation however used LEGO Mindstorms sensors and Raspberry Pi network hubs that made the system bulky and predominately wired, limiting its use especially for younger children. The availability of lowcost, compact BLE hardware has motivated some new designs recently. Unruly Splats [1] and Scratch Nodes [18] are two work-in-progress projects that aim to utilize distributed designs to enable play-based learning. Essentially, the distributed form-factors allows the children to move around (while interacting with the toolkit modules,) and hence engage in physical play. While Unruly Splats uses foot-sized buttons placed in the environment (on the floor,) the modules in Scratch Nodes are hand-held and are carried around by the users. In DIO, we use a comprehensive BLE-based wireless network to support modules that can placed in the environment, can be held in hands, or can be worn as wearables by multiple children.\nLearning with Constructionist Toolkits: The age-group of children that we targeted with our toolkit is 8-12 years. This age group in Piaget' classification is defined as the concrete operational stage [41] . This is when children start to develop logical thinking, and learn to reason. This reasoning though, as Piaget notes, is limited to concrete situations, i.e. thinking about hypothetical situations can be difficult for the children of this age. Using the components of a construction toolkit to create artifacts of personal significance is an important goal associated with constructionist learning [43] , as described in the previous section. Doing this can require the children to connect the toolkits' components (and their associated functionalities) to hypothetical situation from their everyday lives, a process that involves abstract thinking.\nLearning with Constructionist Toolkits: We relied on scaffolding [36] to assist this process. Scaffolding is based on Vygotsky's theory of zone of proximal development(ZPD) [57] , which is an area of learning that occurs when a student cannot do a task on his own, but can do it when assisted by someone with a higher skill set. Scaffolding is the guidance received by the children to help them work within this ZPD. It describes how adults can model the process of a learning-based task given to children and can provide support to make them do more than what they would be able to do otherwise. This support is gradually reduced, leaving the students to master the skill on their own. Past work on construction toolkits provide descriptions of constructions that the participants are able to make using the toolkit [4, 30, 38] ; but the process to take them to that stage is usually missing. During the pilot studies for DIO, we had noticed that participants facing difficulties in applying their learning to abstract problems (as established by Piaget). Hence, we used a scaffolding-based approach to support them. Specifically, we borrowed the techniques of reduction in degrees of freedom, demonstration, and discussion, from the literature on scaffolding in education [2, 55, 56] , and applied them to making and learning with constructionist toolkits. We describe these techniques and their implications as observed during our studies.\nCollaborative Learning: Collaboration is a synchronous activity that is the result of a continued attempt to construct and maintain a shared conception of a problem [47] , through which learning occurs socially [52] . Collaborative learning takes place when a group of people perform an activity together through different forms of interaction mechanisms such as awareness and grounding [62] . Awareness is the ability to observe people's behaviour or changes to an object resulting in transfer of knowledge [14] , while grounding helps in achieving collective actions. Prior work in evaluating collaborative learning has established that children prefer to work together in small groups [26, 27] , resulting in increased engagement, enjoyment and motivation towards the activities [26, 49] . Work done by Ed Hall [16] in demarcating distance zones based on interpersonal distance in social interactions is an important phenomenon in the context of social interactions called proxemics. The four zones are described as intimate, personal, social and public. A recent survey conducted by Yu et al [61] on computational kits for young children highlight the lack of kits designed for collaborative learning.\nCollaborative Learning: Interactive collaborative systems embody typical characteristics as identified by Hornecker [23] namely: embodied constraints and access points. Embodied constraints refer to the configuration of space and objects. Work done by Eden et al [11] in evaluating their setup demonstrate that constraints such as sharing of resources and spatial orientation fostered group awareness and cooperation. Access points enables users to manipulate relevant objects and provide control to the users. Stewart et al check how children collaborate while using a conventional desktop with one mouse versus using tangibles which allow multiple users to interact at once [53] . Multiple input devices allow simultaneous action and easing active participation.\nCollaborative Learning: DIO supports collaboration by enabling multiple children to conceptualize, program and then test things out in groups. Children can use multiple modules together and share them amongst themselves, enhancing the opportunities of mutual interactions. Our user studies are designed in such a manner that the children have to divide the task at hand requiring them to coordinate and plan. Due to the tangible nature, being aware of what others are doing is easy and enhanced by the wireless nature since the modules can be placed anywhere. Owing to the distributed design of the toolkit, children can place the modules on their body and in the environment. This enables them to allow or block access to certain modules depending on their placement. A child can place a module on his body, in their personal space while another one can place it on the wall, in the social space. A similar interaction is observed in [35] , wherein children adapted different strategies to stop others from accessing objects and fight for control. Using our toolkit, children are able to experience interactions based on interpersonal distance. Closest to our work, Torino [54] is a physical programming language which supports collaborative learning between children with mixed visual abilities.",
    "rq": " 3) what do the children make?"
}
{
    "intro": "INTRODUCTION: Video-mediated communication enables people to collaborate remotely. A growing domain for video meetings is video telehealth, in which clinicians conduct visits with patients remotely [67] . When clinicians communicate with patients, they must not only exchange information but also convey a sense of sympathy, sensitivity, and attentiveness. Clinicians must often juggle the competing task demands of extracting information from patients' subjective accounts and documenting the information in medical records while conveying a sense of warmth and care to establish trust and rapport with the patient. Most affective",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "INTRODUCTION: Live video streaming platforms such as Periscope, Facebook Live & Youtube Live have gained popularity in the last few years, with particularly high numbers of new users in developing countries [11] . This popularity is enabled by decreasing costs and widespread availability of smartphones and cellular bandwidth. Content creators on live video streaming platforms have found engaged audiences for game playing, remote viewing of events (e.g., music concerts), disaster response coordination, and collective witnessing of injustice or corruption [17] . As mobile video streams, low cost 360 degree cameras, and augmented reality video streams become mainstream, we expect that the scope of use-cases for live video streaming will continue expanding.\nINTRODUCTION: We see an opportunity for mobile video streaming to enable a new class of experiences: task-oriented cooperative work and play. We envision use cases such as remote shopping [16] , where parents of young children or persons with disabilities who need to stay at home can interactively shop for items that require browsing (e.g., clothing) by interactively instructing a worker with a mobile phone to browse on their behalf; or spot checking and auditing, where donors and funders who are far away from a site (e.g., a construction area or NGO's premises) employ locals to visit and verify through a live video stream that work is completed as expected.\nINTRODUCTION: These use cases have three distinguishing traits in common:\nINTRODUCTION: (1) Participants of the stream (both creators and viewers) are collaborating in real-time towards a goal and may communicate in both directions to achieve that goal. (2) Participants on either side of the stream may not know each other.\nINTRODUCTION: (3) Depending on the nature of the interaction, there may be an asymmetric power relationship between the creator and viewer of the video stream, e.g., when the viewer of the stream is paying the creator to complete a task.\nINTRODUCTION: These unique characteristics present opportunities as well as challenges. For example, there is a need to ensure ethical behavior on the platform, and provide an environment where all participants are comfortable and engaged with the experience.\nINTRODUCTION: In this paper, we explore one example of a task-oriented experience: enriched collaborative mobile video streaming in a physicalworld 'escape-the-room' puzzle game. Escape rooms involve collaborative puzzle solving where collocated team members try to escape from a series of locked rooms within a defined time frame. We tweak this setup to include remote team members who need to coordinate with other team members who are physically present in the escape room in order to solve puzzles that have been designed to require remote collaboration. We chose this particular technology probe as an exercise in understanding the types of interactions that will take place between participants, the most appropriate UX features for smooth game-play experience, and how games such as escape-the-room can be redesigned to enable technology-mediated puzzles. Similar alternate reality games (ARGs) have been studied in previous literature to explore research questions around trust, privacy and ability to collaborate efficiently [13, 14] .\nINTRODUCTION: We conducted our technology probe with 26 study participants. We report findings from our study, including observations about appropriate communication modalities for remote collaborative game playing, as well as unexpected interactions and points of friction between participants. We also describe the iterative design of the mobile video streaming system, which we call Avatar, that we built and deployed in our study. We experiment with various UX elements as part of Avatar, such as text messaging versus voice messaging, and control modalities such as a virtual joystick. Our results leave us optimistic that Avatar and other systems like it can enable new and exciting experiences.\nINTRODUCTION: In the long run we ultimately aim to demonstrate that real-time mobile video streaming can enable new and sustainable employment opportunities, especially for people in emerging regions like India & Brazil. Several important research questions on the path towards this goal remain unanswered, but we view this study as an important first step.",
    "relatedWork": "RELATED WORK: There is extensive literature from the HCI community on remote collaboration experiences. These include one-to-one video streams for immersive telepresence [3, 6] , interactions with multiple video streams [5] , attempts to guide viewer attention, asynchronous video collaborations [1, 4, 8] and sharing media during a video call. Among these studies, some focus specifically on games, social play and distributed collaboration between people.\nRELATED WORK: At a high level, our study is unique in that we focus on collaborative, task-oriented experiences, where the participants may be anonymous and there may be asymmetric relationships between the participants. While others have studied use cases that have one of these characteristics (e.g., anonymity), the conjunction of these characteristics bring about unique opportunities and challenges.\nLive Streaming: The last few years have seen an extensive number of live streaming applications like Meerkat, Periscope, 1 Twitch 2 and Youtube Live. 3 Initial studies by Juhlin et al. showcased how streamers faced technical issues involving camera management [9] . Over the last few years, the increasing pervasiveness of smartphones with decent cameras have resulted in an explosion in video streaming. It has become common to publicly broadcast user-generated streams of political events, protests, concerts and debates.\nLive Streaming: Hunter et al. showed that customization of video environments results in more social engagement for families while interacting over live video [7] . This study was primarily focused on children and their interactions with family members. Additional work by Tang et al, shows the usage of pre-recorded video reactions as a means to enable interactive meetings with remote participants [15] .\nEscape Room Experiences: Escape room experiences involve players being locked in a room or a series of room with puzzles that need to be solved in order to escape. The first observational study on collocated play in escape rooms was performed by Pan et al. [13] to identify collaboration styles, communication methods and identified conflicts during game play among peers. This was followed by a study on distributed video based experiences for escape room performed by Shakeri et al [14] which involved two geographically distributed teams playing similarly designed escape room and tried to identify the extent of collaboration and effects on team building.\nParticipatory Live Modalities: As live video streams have evolved, a number of projects have explored interaction and communication modalities for live stream experiences. Several papers found that providing contextual information during the video stream results in better engagement and more active participation from the users who are watching the live stream [5, 12] . Yonezawa et al. designed a system that enabled remote viewers to control the light and camera angle of the broadcast which resulted in more engagement from the viewers [18] .\nParticipatory Live Modalities: Hamilton et. al performed a study with remote watchers from Amazon Mechnical Turk watching a live streamed jazz event to identify the role of multiple communication modalities like the usage of hearts, event wide text chats and push to talk audio to identify which of the features enable engagement from viewers [5] . The study pointed out that the usage of hearts in multi-stream experiences are noisy and participants preferred to use the push to talk to interact with the streamers of the video. The study asked crowd workers to watch and interact with the streamers and tried to identify how they could influence the video stream where the watchers felt like they were in control.\nParticipatory Live Modalities: Among closely related work, a study by Shakeri et al. uses video links to create an escape room setting where multiple groups are playing in different physical locations within rooms that are similar [14] . The study, which aimed at finding how closely players could collaborate, showcased that video links augment connections to help players share knowledge.\nParticipatory Live Modalities: The unique contribution of our study compared to the work done by Shakeri et al. is the creation of a similar engaging experiences with remote viewers engaging with a group of streamers who are in the escape room. We also introduce gaming specific artifacts to the remote viewers to use during the video streams as a means of interaction with the streamers.",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": " we sought to answer three main research questions: what are the important characteristics of a vibrotactile glove to facilitate a sense of touch between long distance couples?\n this raises the design question: how could devices for remote touch be made even smaller in size to support even greater mobility?"
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}
{
    "intro": "",
    "relatedWork": "",
    "rq": ""
}