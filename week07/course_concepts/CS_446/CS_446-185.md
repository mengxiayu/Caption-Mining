#CS_446
#lecture
[[hinge loss hinge loss epsilon]]
[[dynamic training]]
[[reading ian goodfellow book learning chapter]]
[[gradient zero]]
[[loss las]]
[[logarithm probability]]
[[score f ground truth]]
[[operation matrix multiplie]]
[[gradient respect]]
[[para meter]]
[[ground truth probability]]
[[tensor flow]]
[[task classification concept]]
[[score probability]]
[[layer max]]
[[kernel nonlinearity objective]]
[[vector component ann signal shortcoming]]
[[output image]]
[[car detector]]
[[output nonlinearity]]
[[mumbo jumbo]]
[[net width]]
[[paper oftentime]]
[[ton computation folk]]
[[communication gpus]]
[[machine learning]]
[[rgb image depth]]
[[professor clue]]
[[training heuristic]]
[[kernel matrix]]
[[meter filter]]
[[research direction]]
[[para meter convolution]]
[[max pooling]]
[[derivative respect]]
[[bunch symbol]]
[[layer convolution]]
[[layer mix]]
[[softmax layer]]
[[deviation layer]]
[[neural net recap guy]]
[[uniform fannin]]
[[transition period]]
[[derivative dog]]
[[machine folk]]
[[structure computation structure]]
[[structure net]]
[[bayesian optimization]]
[[technique remark]]
[[neural network]]
[[inference training]]
[[option net library]]
[[detector chair detector]]
[[weight vector]]
[[parameter output]]
[[hyperparameter search]]
[[instance convolution transform]]
[[recall convolution]]
[[dayton surprise]]
[[activation concept]]
[[acyclic graph]]
[[convolution layer]]
[[top loss bottom]]
[[top respect]]
[[chair car]]
[[convolution concept trainer]]
[[filter depth net]]
[[surprise sammy]]
[[network architecture search nis]]
[[map dimension]]
[[net structure regard]]
[[sigmoid raylo]]
[[concern gradient infinity]]
[[detection layer]]
[[matrix convolution]]
[[scaling image]]
[[net bias network]]
[[convolution raylo]]
[[computation graph]]
[[weight decay]]
[[face detector]]
[[sigh colleague]]
[[technique uhm]]
[[linearity shortcoming]]
[[weight classification]]
[[descent option]]
[[activation unit]]
[[edge structure]]
[[stride formula student]]
[[vector hyperparameter stride]]
[[hat plus exponential]]
[[activation gradient]]
[[maximum folk]]
[[cross entropy cross entropy]]
[[guy convexity]]
[[classification net]]
[[support vector machine formulation]]
[[exponential score]]
[[bias hyperparameter width height depth stride]]
[[architecture search]]
[[sigmoid activation]]
[[f r]]
[[cross entropy]]
[[task loss constraint]]
[[bengio student jorge garcia banjo]]
[[remark initialization]]
[[gradient descent]]
[[height width]]
[[node graph]]
[[matrix multiply]]
[[output formula]]
[[chain manner]]
[[acyclic graph output]]
[[task loss]]
[[ground truth]]
[[probability softmax]]
[[convex optimization]]
[[package torch]]
[[weight machine]]
[[reverse derivative respect derivative respect]]
[[optimiser optimizer weight decay regularization]]
[[support vector machine loss]]
[[machine learning expert]]
[[instance optimization]]
[[net image]]
[[assumption instance]]
[[network guy]]
[[vector machine]]
[[node graph weight]]
[[pixel boundary net dimension net track output computation convolution]]
[[support vector machine]]
[[architecture folk]]
[[batch iteration]]
[[node output]]
[[weight decay guy]]
[[filter pixel]]
[[net los]]
[[sigmoid network]]
[[bias sammy]]
[[leaky radio metric]]
[[grade infinity]]
[[filter stride]]
[[propagation aspect]]
[[node derivative respect]]
[[filter hyperparameter filter]]
[[limit exponential]]
[[height channel]]
[[edge detector]]
[[layer convolution activation]]
[[leaf node]]
[[mini batch]]
[[oftentime support vector machine]]
[[part weight decay part gradient]]
[[probability ground truth probability]]
[[cross entropy loss]]
[[derivative computation]]
[[layer matrix multiplie]]
[[slack task loss]]
[[convolution net]]
[[logarithm derivative logarithm]]
[[score inter probability softmax]]
[[three gradient respect]]
[[colonel kernels mechanism kernel mix]]
[[layer fixture]]
[[competitor net]]
[[gradient weight decay]]
[[gradient respect backpropagation]]
[[ization net]]
[[height depth]]
[[dip net alex net revolution]]
[[network planet]]
[[regression multiclass eminem learning framework optimization]]
[[shortcoming formulation]]
[[output probability]]
[[max pooling ann]]
[[width image boundary]]
[[hat ground truth]]
[[top probability]]
[[library graph]]
[[matrix multiplication net]]
[[wyatt output]]
[[layer activation]]
