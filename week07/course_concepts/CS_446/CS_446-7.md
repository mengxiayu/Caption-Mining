#CS_446
#lecture
[[discriminative model]]
[[caption text]]
[[reward sequence probabilistic action transition]]
[[variance gradient]]
[[policy pi]]
[[programming semantic war]]
[[beginning dependence]]
[[artifact action]]
[[table scope discussion]]
[[parameter model]]
[[con pro reward]]
[[distribution gradient]]
[[hat index]]
[[gradient baseline]]
[[discussion variance]]
[[quality generation metric]]
[[bias gradient variance]]
[[trajectory caption]]
[[machine learning]]
[[variance estimate policy gradient bias]]
[[sequence rollout policy bunch policy sample]]
[[caption score]]
[[paper policy]]
[[respect towel interaction policy]]
[[sample trajectory]]
[[gradient policy sequence distribution gradient policy]]
[[setting parameter]]
[[learning reinforcement learning image captioning]]
[[baseline variance reduction trick baseline reward]]
[[policy gradient]]
[[highlight intuition]]
[[instance softmax]]
[[policy action policy action]]
[[policy trajectory]]
[[action member equation reward]]
[[award policy]]
[[gradient respect doubt]]
[[reinforcement learning algorithm reinforce]]
[[variance convergence]]
[[action complexity claim complexity]]
[[model mapping]]
[[trajectory length]]
[[multiplication bias]]
[[dynamic policy]]
[[distribution rollout distribution]]
[[respect utility]]
[[variance baseline]]
[[bias gradient]]
[[network machine learning context enforcement learning]]
[[noise sequence trajectory]]
[[sequence text]]
[[policy gradient bunch trajectory]]
[[action representation comment]]
[[production downside]]
[[literature reference]]
[[iteration tide equation]]
[[sequence text epoch]]
[[completeness policy optimization]]
[[gradient metric]]
[[policy gradient bias]]
[[loss surrogate]]
[[policy reward]]
[[maximization action million]]
[[randomness environment]]
[[trajectory text reward]]
[[plan cumulative quiz]]
[[finite model]]
[[content summary]]
[[gradient trick]]
[[distribution sequence]]
[[user image]]
[[part representation]]
[[gradient respect score]]
[[towel dependence]]
[[policy direction]]
[[policy estimation]]
[[scaling reward]]
[[convergence behavior]]
[[reward image captioning]]
[[parameter policy]]
[[enforcement learning pro]]
[[thousand action]]
[[gradient reward]]
[[caption image]]
[[representation policy]]
[[action pair]]
[[variance policy gradient descent]]
[[network reinforcement learning]]
[[distribution sample]]
[[descent hammer machine learning]]
[[gradient claim]]
[[learning theory policy]]
[[behavior policy gradient]]
[[reinforcement learning reward]]
[[sequence assumption reward sequence]]
[[sequence trajectory]]
[[distribution sample quantity]]
[[policy mapping]]
[[reward action pair]]
[[length trajectory]]
[[enforcement learning]]
[[gradient descent]]
[[bias variability]]
[[translation trick]]
[[reinforcement learning]]
[[model image]]
[[estimate reward]]
[[variance reduction]]
[[policy learning]]
[[utility sequence action utility reward]]
[[rollout sample]]
[[baseline measure reward]]
[[reward trajectory]]
[[descent instance]]
[[policy optimization]]
[[reinforcement learning effort benefit]]
[[policy model]]
[[policy iteration]]
[[baseline gradient]]
[[model policy]]
[[policy caption]]
[[actor critic]]
[[gradient descent variance]]
[[setting direction]]
[[cider score]]
[[policy maximization action]]
[[gradient distribution]]
[[sequence gradient]]
[[discussion reinforcement learning policy gradient]]
[[baseline gradient std policy]]
[[quantity nominator]]
[[descent respect policy]]
[[r reward]]
[[action sequence]]
[[model parameter]]
[[policy date]]
[[rl reinforcement hammer hood]]
[[nlp measure quality text]]
[[policy setting policy]]
[[model policy gradient]]
[[discount gamma]]
[[solution loss]]
[[variety model]]
[[hood reinforcement learning]]
[[dynamic policy action]]
[[reward rollout reward]]
[[takeaway policy gradient]]
[[manipulation game]]
[[variance estimate gradient]]
[[policy action]]
[[reward award]]
[[game batch variance convergence]]
[[gradient variance band estimate bunch quantity approximation reward]]
[[discussion reinforcement learning]]
[[robot navigation]]
[[action pair reward]]
[[action mapping]]
[[cartoon model dichotomy]]
[[action maximization]]
[[variance takeaway policy gradient]]
[[knowledge distribution quantity]]
[[ad bias]]
[[policy gradient family policy optimization]]
[[policy part]]
[[bunch reinforcement learning]]
[[action parody transition]]
[[reinforcement learning agent]]
[[learning gradient]]
[[variance reduction strategy]]
[[autoencoder randomness]]
[[experience rollout]]
[[score company]]
[[sequence action]]
