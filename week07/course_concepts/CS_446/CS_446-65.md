#CS_446
#lecture
[[polynomial regularization]]
[[intuition behave]]
[[amp theorem]]
[[machine learning model analysis chat]]
[[regression score risk leo loss risk loss]]
[[regularization strategy complexity]]
[[model quadratic model ad user quadratic]]
[[bottom risk polynomial]]
[[bouncy classifier]]
[[phenomenon highway]]
[[overlap probability event]]
[[guarantee stochastic guarantee probability event generalization]]
[[interval probability]]
[[testing performance]]
[[confidence claim]]
[[generalization gap]]
[[sqrt e]]
[[machine learning]]
[[mistake classifier]]
[[interval ab]]
[[assumption notion assumption story]]
[[connection flexibility]]
[[minus sci prediction]]
[[theorem understanding]]
[[probability event]]
[[gap sampling]]
[[population prediction]]
[[sample gap sample expectation]]
[[classification model]]
[[quantity probability]]
[[training performance]]
[[hat prediction]]
[[sub baye]]
[[probability population probability]]
[[generalization comment]]
[[claim probability collection event]]
[[output noise]]
[[population hat]]
[[violation event]]
[[confidence interval]]
[[model training performance]]
[[classifier mistake]]
[[model fit]]
[[utilization model]]
[[quantity inequality]]
[[regularizer complexity]]
[[discussion generalization]]
[[prediction generalization root]]
[[training classifier]]
[[population performance sample]]
[[team train]]
[[regularization instance aloe polynomial]]
[[guarantee generalize]]
[[population expectation]]
[[training prediction]]
[[multiply expectation]]
[[consequence union]]
[[probability violation walk embassy]]
[[health inequality theorem distribution]]
[[proof holding proof]]
[[slack gap]]
[[fitting phenomenon training tester]]
[[assumption phenomenon]]
[[impair collab training]]
[[learning theory sample]]
[[book source]]
[[model prior]]
[[sampling hat training risk]]
[[inverse probability]]
[[plug probability confidence]]
[[phenomenon fitting]]
[[indicator mistake indicator]]
[[overlap event]]
[[gap train]]
[[training sample]]
[[regularization chat]]
[[fitting regression]]
[[machine learning model training]]
[[curve root]]
[[training loss]]
[[generalization machine learning model]]
[[violation path]]
[[training testing]]
[[pricing model]]
[[sample hat]]
[[model training]]
[[prediction sake simplicity]]
[[machine algorithm]]
[[distribution sample]]
[[generalization trading]]
[[spot argument]]
[[generalization classifier]]
[[solution training]]
[[machine learning theory]]
[[confidence gap]]
[[distribution comment]]
[[hat training]]
[[overfitting phenomenon training]]
[[table index]]
[[analogy offhand eye]]
[[learning theory]]
[[probability context machine learning modeling assumption]]
[[context prediction prior]]
[[noise observation]]
[[learning model trading]]
[[usage intuition proof]]
[[training gap training]]
[[generalization game balance fitting training]]
[[fitting phenomenon]]
[[comment model]]
[[probability generalization]]
[[intuition loss]]
[[diagram universe probability event]]
[[index independent]]
[[degree freedom]]
[[prediction quality chat]]
[[label passion]]
[[window thread approximation prior]]
[[model risk]]
[[zi indicator classifier label]]
[[model generalization]]
[[model correlate]]
[[razor argument]]
[[assumption simplicity assumption]]
[[connection gap]]
[[sample probability]]
[[generalizing generalization solo]]
[[generalization guarantee]]
[[classification probability indicator]]
[[probability claim]]
[[algorithm designer]]
[[overfitting theory]]
[[variance model]]
[[root curve]]
[[bout task regression apology]]
[[model selection]]
[[learning assumption]]
[[classification intuition]]
[[mistake eye]]
[[generalization claim]]
[[training risk]]
[[extension equality]]
[[training model]]
[[collection impact disease]]
[[probability confidence]]
[[chat guy]]
[[production server]]
[[theory intuition]]
[[cycle learning theory sample]]
[[sample population probability]]
[[quality regressor]]
[[classifier prediction simplicity]]
[[learning symbol]]
[[train magnitude]]
[[hat expectation]]
