#CS_446
#lecture
[[surrogate setting instance classification]]
[[projection span solution]]
[[vector row]]
[[eigenvalue invert ability ve transpose]]
[[loss science]]
[[optimize solution]]
[[descent direction slash oscillator bunch]]
[[lie curve]]
[[classifier noise]]
[[possibility distance measure]]
[[bias part]]
[[neighbor statement]]
[[lease classification]]
[[lagrange multiplier lambda]]
[[solution transpose inverse transpose]]
[[regression convex]]
[[quality constraint]]
[[ton classification]]
[[flexibility family loss]]
[[adele alice]]
[[los hinge loss]]
[[family loss]]
[[loss instance]]
[[descent algorithm]]
[[lagrange multiplier]]
[[neighbor learner]]
[[classifier setting model]]
[[inequality constraint]]
[[equality constraint]]
[[gradient minimum]]
[[classification model]]
[[quantity lambda]]
[[bee transpose]]
[[star oil solution]]
[[parameter distance measure]]
[[colonel uhm]]
[[model bernoulli style model]]
[[confidence gap loss]]
[[operation convex]]
[[news constraint]]
[[solution transpose inverse ground pool]]
[[projection vector]]
[[predictor prediction label]]
[[loss regularizer]]
[[optimal convex]]
[[loss optimize]]
[[supremum lambda]]
[[solution framework]]
[[risk minimisation]]
[[likelihood model]]
[[lambda boundary]]
[[prediction model]]
[[beta quantity decay]]
[[solution regression]]
[[noise variability model]]
[[vanilla gradient descent tickler convex]]
[[classification evaluation metric fraction]]
[[descent guest parameter]]
[[intuition beta]]
[[ascent direction anna]]
[[kernel exponentiation]]
[[classification regression]]
[[motivation los loss minimizing oil]]
[[collection convex]]
[[opposite convexity]]
[[generalization prediction]]
[[risk minimization machine learning]]
[[classification label]]
[[recipe google suggestion]]
[[solution weekend]]
[[logistic squared]]
[[ridge regression]]
[[law gloss hinge loss]]
[[convexity convergence]]
[[vector pan]]
[[solution minimizer]]
[[output prediction]]
[[constraint ws]]
[[evaluation metric model]]
[[convexity gradient]]
[[loss circuit]]
[[variety loss]]
[[lee solution regression loss]]
[[descent direction descent direction]]
[[frisbee transpose]]
[[curve convex]]
[[memory kernel matrix]]
[[algorithm foundation evaluation]]
[[intro theory]]
[[penalize penalty]]
[[model fit model]]
[[convexity claim]]
[[construction kernel]]
[[matrix vector multiplication trouble]]
[[instance convex]]
[[solution intuition]]
[[vector transpose]]
[[con learner]]
[[optimization series inequality constraint]]
[[stepsize anar dissent direction optimum]]
[[validation strategy lambda]]
[[classifier model]]
[[hinge los]]
[[equation convexity]]
[[rank matrix]]
[[loss generality]]
[[convexity strategy]]
[[pseudo inverse approximation inverse]]
[[lambda arm]]
[[decay gamma alpha beta gap iterate]]
[[lambda inequality constraint]]
[[optimization equality]]
[[regression regularizer]]
[[network activation]]
[[distance pro]]
[[los gloom loss]]
[[hinge los confidence]]
[[ridge regression regularization]]
[[strategy erm]]
[[notation linear]]
[[consequence lambda]]
[[hyperparameter selection ann]]
[[neighbor bunch classification algorithm multiclass classification learning]]
[[cross validation strategy]]
[[accuracy loss]]
[[hinge las fit classification]]
[[box convex]]
[[oil solution]]
[[aplus transpose]]
[[zero possibility]]
[[parameter probability parameter]]
[[regression beta]]
[[constraint lambda]]
[[regularization parameter]]
[[majority vote]]
[[matrix restriction]]
[[descent direction]]
[[cause optimal]]
[[lee logistic]]
[[setup li]]
[[convexity gap]]
[[behavior classification]]
[[homework exam]]
[[eigenvalue transpose]]
[[neighbor model]]
[[softmax e]]
[[prediction training pro]]
[[max instance]]
[[vector transpose vector]]
[[loss behavior]]
[[distribution caution]]
[[procedure model evaluation metric]]
[[machine learning variation]]
[[las accuracy]]
[[progress optimum]]
[[gradient hessian]]
[[addition office]]
[[hinge loss]]
[[curve intuition convexity]]
[[expert uhm]]
[[claim curve]]
