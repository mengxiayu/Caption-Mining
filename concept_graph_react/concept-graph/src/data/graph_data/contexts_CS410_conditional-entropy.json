{
  "text": "conditional entropy",
  "contexts": [
    {
      "course": "410",
      "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
      "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
      "lecture_num": 11,
      "context": "this lecture is about the syntagmatic relation discovery and conditional entropy. in this lecture, we're going to continue the discussion of word association mining an analysis. we",
      "label": "Use"
    },
    {
      "course": "410",
      "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
      "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
      "lecture_num": 11,
      "context": "ng to continue the discussion of word association mining an analysis. we're going to talk about the conditional entropy, which is useful for discovering syntagmatic relations. earlier we talked about using entropy to ca",
      "label": "Use"
    },
    {
      "course": "410",
      "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
      "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
      "lecture_num": 11,
      "context": "sence or absence of meat. so these questions can be addressed by using. another concept, called the conditional entropy. so to explain this concept, let's first look at the scenario we had before where we know nothing a",
      "label": "Intro"
    },
    {
      "course": "410",
      "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
      "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
      "lecture_num": 11,
      "context": "ilities with their corresponding conditional probabilities in the entropy function, we will get the conditional entropy. so this equation now here. would be. the conditional entropy conditioned on the presence of eats. ",
      "label": "Use"
    },
    {
      "course": "410",
      "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
      "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
      "lecture_num": 11,
      "context": "the entropy function, we will get the conditional entropy. so this equation now here. would be. the conditional entropy conditioned on the presence of eats. right? so you can see this is essentially the same entropy fun",
      "label": "Use"
    },
    {
      "course": "410",
      "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
      "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
      "lecture_num": 11,
      "context": "y of meat after we have known eats occurring in the segment. and of course, we can also define this conditional entropy for the scenario where we don't see eats. so if we know eats did not occur in the segment, then thi",
      "label": "Use"
    },
    {
      "course": "410",
      "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
      "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
      "lecture_num": 11,
      "context": "or the scenario where we don't see eats. so if we know eats did not occur in the segment, then this conditional entropy would capture the uncertainty of meat in that content in that condition. so now putting different s",
      "label": "Use"
    },
    {
      "course": "410",
      "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
      "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
      "lecture_num": 11,
      "context": " in that condition. so now putting different scenarios together, we have the complete definition of conditional entropy as follows. basically. we're going to consider both scenarios of the value of eats zero or one, and",
      "label": "Use"
    },
    {
      "course": "410",
      "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
      "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
      "lecture_num": 11,
      "context": "is equal to 0 or 1. basically, whether eats is present or absent, and this of course is the entropy conditional entropy of meat in that particular scenario. so if you expand this entropy, then you have the following equ",
      "label": "Use"
    },
    {
      "course": "410",
      "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
      "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
      "lecture_num": 11,
      "context": "those conditional probabilities. now in general, for any discrete random variables x&y we have. the conditional entropy is no larger than the entropy of the variable x, so basically this is upper bound for the condition",
      "label": "Use"
    },
    {
      "course": "410",
      "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
      "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
      "lecture_num": 11,
      "context": "l entropy is no larger than the entropy of the variable x, so basically this is upper bound for the conditional entropy. that means by knowing more information about the segment, we won't be able to increase the uncerta",
      "label": "Use"
    },
    {
      "course": "410",
      "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
      "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
      "lecture_num": 11,
      "context": " case. now what's interesting here is also to think about what's the minimum possible value of this conditional entropy. now we know that the maximum value is the entropy of x. but what about the minimum? so what do you",
      "label": "Use"
    },
    {
      "course": "410",
      "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
      "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
      "lecture_num": 11,
      "context": " be interesting to think about and in what situation will achieve this. so let's see how we can use conditional entropy to capture syntagmatic relations. now, of course this conditional entropy gives us directly one way",
      "label": "Use"
    },
    {
      "course": "410",
      "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
      "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
      "lecture_num": 11,
      "context": " let's see how we can use conditional entropy to capture syntagmatic relations. now, of course this conditional entropy gives us directly one way to measure the association of two words. because it tells us to what exte",
      "label": "Use"
    },
    {
      "course": "410",
      "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
      "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
      "lecture_num": 11,
      "context": " given that we know the presence or absence of another word. now before we look at the intuition of conditional entropy in capturing syntagmatic relations, it's useful to think of a very special case listed here, that i",
      "label": "Use"
    },
    {
      "course": "410",
      "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
      "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
      "lecture_num": 11,
      "context": "turing syntagmatic relations, it's useful to think of a very special case listed here, that is, the conditional entropy of the word given itself. so, here we listed the this conditional entropy in the middle. so it's he",
      "label": "Use"
    },
    {
      "course": "410",
      "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
      "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
      "lecture_num": 11,
      "context": "listed here, that is, the conditional entropy of the word given itself. so, here we listed the this conditional entropy in the middle. so it's here. so what is the value of this? now. this means we know whether meat occ",
      "label": "Use"
    },
    {
      "course": "410",
      "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
      "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
      "lecture_num": 11,
      "context": "segment we will already know the answer for the prediction. so this is 0. and that's also when this conditional entropy reaches the minimum. so now let's look at some other cases. so this is a case of. knowing the and t",
      "label": "Use"
    },
    {
      "course": "410",
      "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
      "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
      "lecture_num": 11,
      "context": "on between meat and eats. so we now also know when this w is the same as this meat then the entropy conditional entropy would reach its minimum which is 0? and for what kind of words would it reach its maximum? well, th",
      "label": "Use"
    },
    {
      "course": "410",
      "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
      "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
      "lecture_num": 11,
      "context": "be very close to the maximum, which is the entropy of meat itself. so this suggests that we can use conditional entropy for mining syntagmatic relations. the algorithm would look as follows. for each word w1, we're goin",
      "label": "Use"
    },
    {
      "course": "410",
      "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
      "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
      "lecture_num": 11,
      "context": " for each word w1, we're going to enumerate the overall other words w2, and then we can compute the conditional entropy of w1 given w2. and we thought all the candidate words in ascending order of the conditional entrop",
      "label": "Use"
    },
    {
      "course": "410",
      "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
      "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
      "lecture_num": 11,
      "context": "onditional entropy of w1 given w2. and we thought all the candidate words in ascending order of the conditional entropy, because we want to favor a word that has a small entropy, meaning that it helps us predict the tar",
      "label": "Use"
    },
    {
      "course": "410",
      "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
      "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
      "lecture_num": 11,
      "context": "nd these words. the threshold can be the number of top candidates to take or absolute value for the conditional entropy. now this would allow us to mine the most strongly correlated words with a particular word w1 here.",
      "label": "Use"
    },
    {
      "course": "410",
      "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
      "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
      "lecture_num": 11,
      "context": "ditional entropies for w1 given different words. and in this case they all comparable right? so the conditional entropy of w1 given w2 and conditional entropy of w1 given\u00a0 w3 are comparable. they all measure how hard it",
      "label": "Use"
    },
    {
      "course": "410",
      "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
      "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
      "lecture_num": 11,
      "context": "nt words. and in this case they all comparable right? so the conditional entropy of w1 given w2 and conditional entropy of w1 given\u00a0 w3 are comparable. they all measure how hard it is to predict w1. but if we think abou",
      "label": "Use"
    },
    {
      "course": "410",
      "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
      "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
      "lecture_num": 12,
      "context": "tion. and how it can be used to discover syntagmatic relations? before we talked about a problem of conditional entropy, and that is the conditional entropy computed on different pairs of words is not really comparable,",
      "label": "Use"
    },
    {
      "course": "410",
      "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
      "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
      "lecture_num": 12,
      "context": "ver syntagmatic relations? before we talked about a problem of conditional entropy, and that is the conditional entropy computed on different pairs of words is not really comparable, so that makes it hard to discover st",
      "label": "Use"
    },
    {
      "course": "410",
      "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
      "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
      "lecture_num": 12,
      "context": "ion, which is another concept in information theory that allows us to, in some sense, normalize the conditional entropy to make. a more comparable across different pairs. in particular, mutual information, denoted by i(",
      "label": "Use"
    },
    {
      "course": "410",
      "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
      "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
      "lecture_num": 12,
      "context": "y. so mathematically, it can be defined as the difference between the original entropy of x and the conditional entropy of x given y. and you might see here you can see here. it can also be defined as a reduction of ent",
      "label": "Use"
    },
    {
      "course": "410",
      "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
      "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
      "lecture_num": 12,
      "context": "o understand becausw the original entropy is always not going to be lower than the possibly reduced conditional entropy. in other words, the conditional entropy would never exceed the original entropy. knowing some info",
      "label": "Use"
    },
    {
      "course": "410",
      "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
      "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
      "lecture_num": 12,
      "context": " is always not going to be lower than the possibly reduced conditional entropy. in other words, the conditional entropy would never exceed the original entropy. knowing some information can always help us potentially, b",
      "label": "Use"
    },
    {
      "course": "410",
      "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
      "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
      "lecture_num": 12,
      "context": "us potentially, but won't hurt us in predicting x. the second property is that it's symmetric while conditional entropy is not symmetrical. mutual information is. the third property is that it reaches its minimum zero i",
      "label": "Use"
    },
    {
      "course": "410",
      "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
      "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
      "lecture_num": 12,
      "context": "operty can be verified by simply looking at the equation above. and it reaches 0 if and only if the conditional entropy of x given y is exactly the same as original entropy of x. so that means knowing why did not help a",
      "label": "Use"
    },
    {
      "course": "410",
      "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
      "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
      "lecture_num": 12,
      "context": "t all, and that's when x&y are completely independent. now when we fix x to rank different ys using conditional entropy would give the same order as ranking based on mutual information, because in the function here h of",
      "label": "Use"
    },
    {
      "course": "410",
      "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
      "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
      "lecture_num": 12,
      "context": "ause x is fixed. so ranking based on mutual information is exactly the same as ranking based on the conditional entropy of x given y. but the mutual information allows us to compare different pairs of x&y, so that's why",
      "label": "Use"
    },
    {
      "course": "410",
      "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
      "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
      "lecture_num": 12,
      "context": "een eats and other words. and if we do that, and it's basically a based on the same intuition as in conditional entropy, we will see that words that are strongly associated with each will tend to have high mutual inform",
      "label": "Use"
    },
    {
      "course": "410",
      "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
      "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
      "lecture_num": 12,
      "context": " the reduction is maximum because knowing one would allow us to predict the other completely so the conditional entropy is zero. therefore the mutual information reaches its maximum. it's going to be larger than or equa",
      "label": "Use"
    },
    {
      "course": "410",
      "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
      "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
      "lecture_num": 13,
      "context": "he three concepts from information theory, entropy, which meshes uncertainly over random variable x conditional entropy, which measures the entropy of x. given we know why. and mutual information of x&y which matches th",
      "label": "Use"
    }
  ]
}