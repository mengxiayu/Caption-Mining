{
  "0d005f7e-0c3f-465b-b7fe-08f45355e3de": [
    "this lecture be about the word association mining and analysis . in this lecture we be go to talk about how to mine association of word from text . this be an example of knowledge about natural language that we can mine from text datum . here be the outline . we be gooe to first talk about what be word association and then . explain why discover such relation be useful and find a going to talk about some general idea about how to mine word association . in general there be two word relation , and these be quite basic . one be call a paradigmatic relation , the other be syntagmatic relation . a&b have paradigmatic relation if they can be substitute for each other .",
    "and these two word have paradigmatic relation because they be in the same class .",
    "and in general , if it replace cat with dog in a sentence , the sentence would still be a valid sentence that you can make sense of . similarly , monday and tuesday have paradigmatic relation .",
    "in this case , the two word that have this relation can be combine with each other . so a&b have syntagmatic relation if they can be combine with each other in a sentence . that mean these two word be semantically relate . so for example , cat and sit be relate because a cat can sit somewhere .",
    "so this be different from paradigmatic relation and these two relation be in fact so fundamental , that they can be generalize to capture basic relation between unit in arbitrary sequence . and definitely they can be generalize to describe relation of any item in the language .",
    "and they can even be more complex phrase than just a noun phrase . if you think about the general problem of the sequence mining , then we can think about the unit in the sequence datum , and then we think of paradigmatic relation as relation that be apply to unit that tend to occur in similar location in a sentence or in a sequence of their element in general .",
    "so these two be complementary and basically relation of word , and we be interested in discover they automatically from text datum . discover such world relation have many application .",
    "if we can learn paradigmatic relation , then we form class of word . syntactic class for example . and if we learn syntagmatic relation , then we would be able to know the rule for put together a large expression base on component expression .",
    "word relation can be also very useful for many application in text retrieval and mining . for example , in search in text retrieval we can use word association to modify a query . and this can be use to introduce additional related word to a query to make the query more effective .",
    "or you can use related word to suggest related query to the user to explore the information space .",
    "finally , such word association can also be use to compare and summarize opinion .",
    "in order to do that , we can look at what word be most strongly associate with a feature word like the battery in positive versus negative review .",
    "so how can we discover such association automatically ? now here be some intuition about how to do that . let 's first look at the paradigmatic relation .",
    "so here you see some simple sentence about cat and dog .",
    "and that , after all , be the definition of paradigmatic relation . so on the right side you can see i extract explicitly the context of cat and dog from this small sample of text datum . so i have take away cat and dog from the correspond sentence so that you can see just the context .",
    "for example , we can look at the what word occur in the left part of this context . so we can call this left context . what word occur before we see cat , cat or dog .",
    "you generally say his cat or my cat , and you say also my dog and his dog . so that make they similar in the left context . similarly , if you look at the word that occur after cat and dog , which we can call right context and they also very similar in this case , of course it be extreme case where you only see eat and in general you will see many other word . of course that can follow cat and dog .",
    "and even in the general context you also see some similarity between the two word .",
    "in contrast , how similar be context of cat and context of computer ?",
    "between the context of cat and dog , whereas in the second the similarity between context of cat and computer would be low because they be not have paradigmatic relationship .",
    "so this be the basic idea of discover paradigmatic relation . what about the syntagmatic relation ? here we we be go to explore the correlated occurrence again base on the definition of syntagmatic relation .",
    "but here we be interested in know what other word be correlate with the verb eat . and what word can go with eat ? and if you look at the right side of the slide and you will see i 've take away the two word around eat . i 've take away the word to its left and also the world to its right , in each sentence .",
    "and what word tend to occur to the right of eat ? now think about this question would help we discover syntagmatic relation . because syntagmatic relation essentially capture such correlation .",
    "so the question here have to do with whether there be some other word that tend to co - occur together with eat , mean that whenever you see eat , you tend to see the other word . and if you do n't see , it be probably you do n't see other word often either .",
    "now again , consider example . how helpful be the occurrence of eat for predict occurrence of meat ? know whether eat occur in a sentence would generally help we predict the whether meat also occur indeed as if we will see eat occur in a sentence , and that should increase the chance that meat will also occur .",
    "because eat and text be not really relate , so know whether eat occur in a sentence do n't really help we predict whether text also occur in the sentence . so this be in contrast to the question about eat and meat .",
    "so to summarize , the general idea for discover word association or the following . for paradigmatically relation we represent each word by its context , and then compute the context similarity . we can gon na assume the word that have high context similarity to have paradigmatic relation",
    "and we be go to compare their co occurrence with their individual occurrence ."
  ],
  "1cc2d7fa-3d11-49fa-b979-ef5e9442466f": [
    "this lecture be about the probabilistic latent semantic analysis or p lsa . in this lecture we be go to introduce probabilistic latent semantic analysis , often call the plsa . say this be the most basic topic model . also , one of the most useful topic model . now , this kind of model can in general be use to mine multiple topic from text document , and plsa be one of the most basic topic model for do this , so let 's first examine this problem in a little more detail . here i show a sample article which be a blog article about hurricane katrina . an i show some sample topic , for example government response , flooding of the city in new orlean 's donation and the background . you can see in the article we use word from all these distribution . so we first for example . see there be a criticism of government response , and this be follow by the discussion of flooding of the city and donation , etc . we also see background word or mix with they , so the goal of topic analysis here be try to decode these topic behind the text . so segment of the topic to figure out which word be from which distribution and to figure out the first one of these topic . so how do we know there be a topic about government response ? there be a public about the flooding of the city . so these be the task of topical model . if we can discover these topic can color this word as you see here to separate the different topic , then you can do a lot of thing such as summarization or segmentation of the topic , clustering of sentence , etc . so the formal definition of the problem of mining multiple topic from text be show here , and this be actually a slide that you have see in the early lecture , so the input be the collection , the number of topic and vocabulary set .",
    "so the idea of plsa be actually very similar to the two component mixture model that we have already introduce . the only difference be that we be go to have more than two topic . otherwise it be essentially the same .",
    "and naturally , in all case of probabilistic modeling , would want to figure out the likelihood function . so we will also ask the question what be the probability of observe a world w from such a mixture model ?",
    "so before we have just one topic besides the background topical , but now we have more topic . specifically we have k topic . now all these be topic that we assume that exist in the text datum , so the consequence that our switch for choose a topic now be multiway switch before it be just a two way switch . go to think of as flip a coin . but now we have multiple be . first we can flip a coin to decide whether we will talk about the background . so it be the background . lambda sub b versus non background . so this one minus lambda b give we the probability of actually choose a topic .",
    "after we have make this decision , we have to make another decision to choose one of these k distribution .",
    "so this be just the different design of switch , a little bit more complicated , but once we decide which distribution to use , the rest be the same . we be go to generate the world by use one of these distribution , assume here .",
    "now we 've see this problem many time now , and if you recall , it be generally a sum over all the different possibility of generate the world . so let 's first look at the how the world can be generate from the background model . the probability that the world be generate from the background model be lambda multiply by the probability of the world from the background model , right ? two thing must happen . first , we have to have choose the background model . and that be probability of lambda sub b and then the second we must have actually obtain the world w from the background , and that be probability of w given sit out submit .",
    "and i have to stress again , this be a very important formula to know because . this be really key to know to for understand all the topic model and indeed a lot of mixture model , so make sure that you really understand the probability .",
    "so next , once we have the likelihood function , we would be interested in know the parameter right ? so to estimate the parameter . but first let 's put all these together to have the complete likelihood function for plsa . now the first line show the probability of a word as illustrate on the previous slide and this be an important formula as i say . and so let 's take a close look at this . after that contain all the important parameter . so first we see lambda sub b here . this represent the percentage of background word .",
    "second we see the background language model and typically we also assume this be know . we can use a large collection of text or use all the test that we have available to estimate the water distribution .",
    "excuse i , you see two interesting kind of parameter . those be the most important parameter that we be ask , so one be pie and these be the coverage of topic in the document .",
    "so the next line then be simply to plug this in to calculate the probability of document . this be again of the familiar form where you have some and you have account of world in the document and then log of a probability . now it be a little bit more complicated than the two component because now we have more component . so the sum involve more term and then this line be just the like holder for the whole collection and it be very similar .",
    "so what be the unknown primer ? i already say there be two kind on his coverage . one be award distribution .",
    "how many unknown parameter be there ? now try to figure out that question would help you understand the model in more detail , and it would also allow you to understand what would be the output that we generate when we use plsa to analyze text datum , and these be precisely the unknown parameter .",
    "and we can do the usual thing . maximum likelihood estimator . so again , it be a constrained optimization problem like what we have see before , only that we have a collection of text and we have more parameter to estimate and we still have two constraint , different constraint , two kind of constraint . one be award distribution . all the word must have probability that sum to 141 distribution ."
  ],
  "20703c3c-ced6-4410-ace1-139baa46505c": [
    "so i just show you that empirically the likelihood will converge , but theoretically it can also be prove that em algorithm with converge to a local maximum . so here be just the illustration of what happen an a detailed explanation this . require more . knowledge about some of the inequality that we have n't really cover yet . so here what you see be on the x dimension . we have set up value . this be the parameter that we leave on the y axis . we see the likelihood function . so this curve be reach or like roller function , right ? so this one . and this be the one that we hope to maximize an we hope to find a set of value at this point to maximize this .",
    "an once we fit the lower bind we can then maximise the lower bind and of course the reason why this work be because the lower bind be much easy to optimize so we know our current gas be here an by maximize the lower bind will move this point to the top two here .",
    "and that we can then map to the original like role function . we find this point .",
    "right , because we improve our lower bind and then the original light holder curve which be above this lower bind will definitely be improve as well . i so we already know it be improve the lower bind , so we definitely improve this original like record function which be above this lower bind .",
    "so the e step be basically to compute this lower bind .",
    "and that be why eml be go . the little converge to a local maximum .",
    "in the em algorithm , we generally would have to start from different point or have some other way to determine a good initial starting point .",
    "the general idea be that we will have two step to improve the estimate of parameter in the e step . we roughly all augment our datum by predict value of useful hidden variable that we would use to simplify the estimation . in our case , this be the distribution that have be use to generate the world ."
  ],
  "2736e0b3-cd3e-4760-b07e-e9aadcc588e2": [
    "this lecture be about the syntagmatic relation discovery . an entropy . in this lecture , we be go to continue talk about word association mning . in particular , we can talk about how to discover syntagmatic relation . and we be go to start with the introduction of entropy , which be the basis for design some measure for discover such relation . by definition , syntagmatic relation hold between word that have correlate co occurrence . that mean when we see one word occur in the context , we tend to see the occurrence of the other word . so take a more specific example , here we can ask the question whenever eat occur , but other word also tend to occur . now look at the sentence be on the left . we see some word that might occur together with eat like a cat , dog or fish be right . but if i take they out and if you look at the right side where we only show eat and some other word .",
    "right , so this would force we to think about what other word be associate with eat . if they be associate with eat , they tend to occur in the context of eat .",
    "right here we can ask the question about the world w be present or absent in this segment .",
    "if you take a look at the three word show here , meet , the and unicorn . which one do you think it be easy to predict ? now , if you think about it for a moment , you might conclude that . the be easy to predict because it tend to occur everywhere , so i can just say with the in the semtence .",
    "but meat be somewhere in between in term of frequency , and it make it hard to predict because it be possible that it occur in the sentence or the segment more accurately . but it may also not occur in the segment . so now let 's start this problem more formally .",
    "when the value of the variable be 1 , it mean this word be present . when it be zero , it mean the word be absent , and naturally the probability for one and zero should sum to 1 . because a word be either present or absent in the segment . there be no other choice .",
    "now the question be , how do one quantitatively measure the randomness of a random variable like x sub w , how in general , can we quantify the randomness of a variable ?",
    "there be also some connection with the information here , but that be beyond the scope of this course . so for our purpose we just treat the entropy function as a function define on a random variable . in this case it be a binary random variable , although the definition can be easily generalize for a random variable with multiple value . now the function form look like this . there be a sum over all the possible value for this random variable inside the sum , for each value we have a product of the probability that the random variable equal this value and log of this probability .",
    "now , entropy in general be not negative and that can be mathematically prove .",
    "and sometimes when we have 0 log of 0 , we would generally find that as zero because log of 0 be undefined . so this be the entropy function and this function will give a different value for different distribution of this random variable .",
    "at the two end ,",
    "now , if we plot the function against the probability that the x be take a value of 0 and it the function would show exactly the same curve here .",
    "that be because the two probability be symmetric . and completely symmetric . so an interesting question . you could think about in general here be for what kind of x ? do the entropy reach maximum or minimum and we can in particular think about some special case . for example , in one case we might have a random variable that always take the value of what the the probability be . one .",
    "be equally likely take a value of 1 or 0 . in this case , the probability that x = 1 be .5 . now , which one have a high entropy ?",
    "use coin tossing , so when we think about the random experiment like a toss a coin it give we a random variable that . can represent the result . it can be head or tail , so we can define a random variable x sub coin so that it be one when the coin show up as head , it be zero when the coin show up as tail .",
    "so we can think about the two case . one be a fair coin , it be completely fair . the coin show up as head hotel equally likely , so the two probability would be ,",
    "another extreme case be completely biased coin , where the coin always show up as head , so it be a completely biased coin . now let 's think about the entropy in the two case , and if you plug in these value you can see the entropy , would be as follow for a fair coin we see the entropy reach its maximum , that be one .",
    "the completely biased coin correspond to the end point . we have a probability of 1.0 and the entropy be 0 .",
    "now we can assume high entropy word be hard to predict ."
  ],
  "27d06808-2624-4922-a079-04dccb301dde": [
    "this lecture be about mixture model estimation . in this lecture , we be go to continue discuss probabilistic topic model . in particular , we be go to talk about how to estimate the parameter of a mixture model . so let 's first look at our motivation for use a mixture model and we hope to factor out the background word from the topic word distribution . so the idea be to assume that the text datum actually contain two kind of word . one kind be from the background here . so the be away etc and the other kind be from our topic word distribution that we be interested in . so in order to solve this problem of factor out background word , we can set up our mixture model as follow . we be go to assume that we already know the parameter of all the value for all the parameter in the mixture model except for the word distribution of theta sub d , which be our target . so this be the case of customize a probalistic model so that we embed the unknown variable that we be interested in , but we be go to simplify other thing .",
    "now , although we design the model heuristically to try to factor out this background word .",
    "in this case , it turn out that the answer be yes , and when we set up the problem be volume . this way when we use maximum likelihood estimator we will end up have a word distribution that where the common word will be factor out via the use of the background distribution .",
    "let 's assume that the probability of choose each of the two model be exactly the same , so we be go to flip fair coin to decide which model to use .",
    "so we far assume that the background model give probability of point nine to the word the and text .1 .",
    "so now let 's write down the likelihood function in such a case . first , what be the probability of text and what be the probability of the ?",
    "so the probability of text be basically the sum over 2 case , where each case correspond to each of the word distribution .",
    "an inside each case we have the probability of choose the model which be .5 multiply by the probability of observe text from that model .",
    "so naturally our likelihood function be just the product of the two , so it be very easy to see that .",
    "now the interesting question now be , how can we then optimize this likelihood ? well , you will notice that there be only two variable . they be precisely the two probability of the two word text and the give by theta sub d. and this be because we have assume all the other parameter be know .",
    "and the exercise that we have see some simple algebra problem and note that the two probability must sum to one . so there be some constraint .",
    "so now the question be how should we allocate the probability mass between the two word ? what do you think ? now it would be useful to look at this formula for moment and to see what intuitively what we do in order to set these probability to maximize the value of this function .",
    "and they will tend to bet high probability on different word to avoid this competition in some sense . or to gain advantage in this competition . so again , look at this objective function and we have a constraint . on the two probability .",
    "if you look at the formula intuitively , you might feel that you want to set the probability of text to be somewhat large than the .",
    "and when we make they equal an , if we consider the constraint that we can easy to solve this problem and the solution be the probability of text would be point nine and probability of the be point one . and as you can see , indeed the probability of text be now much large than probability of the . this be not the case when we have just one distribution and this be clearly because of the use of the background model which assign a very high probability to the and low probability to text . and if you look at the equation , you will see obviously some interaction of the two distribution here .",
    "and , this be obvious from examine this equation because the background part be weak for text it be small . so in order to compensate for that we must make the probability of text give by theta sub d somewhat large so that the two side can be balance . so this be in fact a very general behavior of this mixture model , and that be if one distribution assign a high probability to one word than another , then the other distribution . would tend to do the opposite . basically it would discourage other distribution to do the same , and this be to balance they out so that we can account for all kind of word ."
  ],
  "2997c717-2552-411d-9dc4-7e648e16bbf0": [
    "so , as we explain different textual representation tend to enable different analysis . in particular , we can gradually add more and more deep analysis result to represent text datum , and that would open up more interesting representation opportunity and also analysis capacity . so this table summarize what we have just see . so the first column show the text recognition , the second visualize the generality of such representation , mean whether we can do this kind of representation accurate before all the text datum , or only some of they , and third column show the enable analysis technique . and the final column show some example of application that can be achieve through this level of representation . so let 's take a look at they . so as a string text can only be process by use stream processing algorithm , but it be very robust , it be general . and there be still some interesting application that can be do at this level . for example , compress of text do n't necessarily need to know the word boundary . although know word boundary might actually also help .",
    "it can enable a lot of analysis technique such as word relation analysis , topic analysis and sentiment analysis , and there be many application that can be enable by this kind of analysis . for example , thesaurus discovery have to do with discover related word and topic and opinion relate application be abundant , and there be for example , and people might be interested in know the major topic cover in the collection of text .",
    "in research literature , a scientist want to know what be the most important research topic today or customer service people might want to know know what be the major complaint from their customer about by mine their email message .",
    "and in general there be many application that can be enable by the representation at this level .",
    "syntactic graph .",
    "\" syntactical structure we can also generate the structure base feature feature and those be feature that might help we classify text object into different category .",
    "k author have actually write this article . then you generally need to look at the syntactic structure .",
    "you can also use this level representation to integrate everything about entity from scatter source .",
    "for example , we can also add ontology on top of the extract information from text to make inference . a good example of application in this enable by this level of representation be a intelligent knowledge assistant for biologist .",
    "so in order to support this level of application , we need to go as far as logical representation . now this course be cover technique mainly base on word base representation .",
    "in fact , in virtually all the text mining application you need this level of representation and the technique that support analysis of texte this level .",
    "and there be multiple way to represent text - string , word , syntactic structure and the relation graph , logical predicate , etc .",
    "for example , if even if we can not do accurately , this application of syntactic structure we can stick at partial structure extract and if we can recognize some entity and that would be great . so in general we want to do as much as we can .",
    "this course , however , focus on word base representation . such technique have also several advantage . first , they be general and robust , so they be applicable to any natural language . that be a big advantage over other approach that rely on more fragile natural language processing technique .",
    "so that be again important benefit , because that mean you can apply directly to any application .",
    "although not all , of course , as i just explain ."
  ],
  "2d0e46c7-df4e-48b3-9550-dac3fec3062d": [
    "this lecture be about the ordinal logistic regression for sentiment analysis . so this be our problem set up for a typical sentiment classification problem , or more specifically , rating prediction . we have an opinionate text document d as input an we want to generate as output already in the range of one through k , so it be discrete rating and thus this be a categorization problem . we have k category here . now we can use a regular text for categorization technique to solve this problem , but such a solution would not consider the order and dependency of the category . intuitively , the feature that can distinguish category 2 from 1 or rather rate 2 from 1 may be similar to those that can distinguish k from k - 1 . for example , positive word generally suggest a high rating .",
    "so what be the solution ? in general , we can add order to classify and there be many different approach , and here we be go to talk about one of they be call the ordinal logistic regression . now let 's first think about how we use logistic regression for binary set categorization problem . so suppose we just want to distinguish it positive from negative and then it be just a two category categorization problem . so the predictor be represent as x and these be the feature and there be m feature altogether , which feature value be a real number , and this can be representation of a text document .",
    "so this would allow we to also write the probability of y = 1 give x in this equation that you be see on the bottom , and so that be logistical function and you can see it relate this probability to probaility that y = 1 to the feature value .",
    "so this be just a direct application of logistical regression for binary categorization .",
    "so basically , if we want to predict rating in the range of one through k , we first have one classifier to distinguish k versus other . and that be our classifier one , and then we be go to have another classifier to distinguish k - 1 from the rest . that be classifier two , and in the end we need a classifier to distinguish two and one",
    "now if we do that of course , then we can also solve this problem , and the logistical regression program would be also very straightforward as you have just see on the previous slide . only that here we have more parameter because for each classify we need a different set of parameter .",
    "and i have also use offer subject to replace beta 0 . and this be to make the notation more consistent with what we can show in the ordinal logistic regression .",
    "and if its probability accord to this logistical regression classifier be large than .5 , we be go to say yes , the rating be k.",
    "it be at least k - 1 and if the probability be large than .5 then will say well , then it be k -- 1 .",
    "when we need to decide whether it be two or one , so this will help we solve the problem , right ? so we can have a classifier that would actually give we a prediction of rating in the range of one through k , unfortunately , such a strategy be not the optimal way of solve this problem , and specifically there be two problem with this approach .",
    "now the first problem be that there be just too many parameter . there be many parameter . now can you count how many parameter we have exactly here ? now this may be interesting exercise . to do so you might want to just pause the video and try to figure out the solution how many premise we have for each classifier ?",
    "you can see the answer be that for each classifier we have n + 1 parameter . and we have k - 1 classifier altogether , so the total number of premise ( k - 1 ) * ( m + 1 ) . that be alot alot of parameter . so when the classifier have a lot of parameter would in general need a lot of datum to actually help we training datum to help we decide the optimal parameter of the this such a complex model ?",
    "the second problem be that these problem these k - 1 classifier be not really independent . these problem be actually dependent .",
    "beta parameter these be the parameter that indicate the influence of those weight .",
    "so this be intuitively appeal assumption . it be reasonable for our problem set up when we have this order in these category .",
    "and the other be to allow we to share the training datum , because all these parameter be assume to be equal . so these training datum for different classifier .",
    "so we have more datum to help we choose a good beta value .",
    "so that mean we tie they together and there be only one set of beta value for all the classifier .",
    "so that be just the basically that be basically the main idea of ordinal logistic regression .",
    "idea of tie all the parameter , the beta value . we also end up have a simple way to make decision , and more specifically now the criterion whether the predict probability above be or at least .5 above and now be equivalent to whether the score of the object that be .",
    "so this mean now we can simply make a desicion of rating by look at the value of this scoring function and see which bracket it fall into .",
    "so in sum , in this approach we be go to score the object ."
  ],
  "3103be2f-681e-41cf-b0f7-21cf6ba56616": [
    "[ intro ] this vector be about a specific technique for contextual text mining call contextual probabilistic latent semantic analysis . in this lecture , we be go to continue discuss contextual text mining . and we be go to introduce contextual probabilistic latent semantic analysis as an extension of plsa for do contextual text mining . recall that in contextual text mining we hope to analyze topic in text . in consideration of context so that we can associate the topic with appropriate context that we be interested in . so in this approach contextual probabilistic latent semantic analysis or cplsa the main idea be to explicitly add interesting context variable into a generate model . recall that before when we generate the text , we generally assume we will start with some topic and then sample word from some topic . but here we be go to add context variable so that the coverage of topic and also the content of topic will be tight little context .",
    "the consequence that this would enable we to discover contextualized topic make the topic more interesting , more meaningful , because we can then have topic that can be interpret as specific to a particular context that we be interested in . for example , a particular time period .",
    "that clearly suggest that the generation of text would then depend on context , and that allow we to bring context into the generative model .",
    "and that mean depend on the time or location , we might cover topic differently . and then again this dependency would then allow we to capture the association of topic with specific context .",
    "and in this case , the estimate premise would naturally contain context variable , and in particular a lot of conditional probability of topic give certain context .",
    "so this be the basic idea .",
    "so as you see here , we can assume there be still multiple topic . for example , some topic might represent the theme like a government response donation or the city of new orleans . now this example be in the context of hurricane katrina and that hit new orleans .",
    "and these be show as view one , view tow and view three view three each view be a different version of word distribution .",
    "now on the right side you see we assume the document have contact information , so the time be know to be july 2005 , location be texes , etc . now such context information be what we hope to model as well . so we be not go to just model the text .",
    "now on the bottom you will see the theme coverage or topic coverage might also vary accord to these context .",
    "in the case of location like texas , people might want to cover the red topic more at the new audience , as visualize here . but in a certain time period , maybe particular topic like donation will be cover more so this variation be also consider in cplsa .",
    "and this view of course now could be from any of these context . let 's say we have take this view . that depend on the time in the middle . so now we have a specific version of word distribution . now you can see some probability of word for each topic . now , once we have choose a view , now the situation will be very similar to what happen in standard plsa . we assume we have get a word distribution associate with each topic , right ?",
    "now here , because we consider context so the distribution of topic or the coverage of topic can vary depend on the context that have influence the coverage . so , for example , we might pick a particular coverage , let 's say in this case .",
    "so we might get the word like government . and then next time we might choose a different topic , an will get donate , etc right until we generate all the word and this be basically the same process as in plsa .",
    "so here be some sample result from use such a model . not necessary exactly the same model , but similar model . so on this slide you see some sample result of compare news article about iraq war and afghanistan war .",
    "on iraq war and 26 article on afghanistan war . now , in this case , the goal be to .",
    "so in this case , the context that be explicitly specify by the topical collection .",
    "there be a common theme that be correspond to cluster around here in this column .",
    "now if you the background , of course this be not surprising and this be . this topic be indeed very relevant , to both war . if you look at the column far and what be interesting be that the next two cell of word distribution actually tell us collection specific variation of the topic of united nations . so it indicate that in iraq war , united nations be more involved in weapon inspection , whereas in afghanistan war it be more involved in maybe aid to northern alliance as a different variation of the topic of united nations .",
    "similarly , if you look at the second cluster .",
    "but imagine if you be not familiar with the text collection or have a lot of text article and such a technique can review the common topic cover in both set of article . it can be use to review common topic in multiple set of article as well . if you look down , of course in that column of cluster 2 you see variation of killing of people and that correspond to in different context .",
    "obtain the front block article about the hurricane katrina .",
    "trend of topic overtime . and the top one show just the temporal chain of two topic . one be oil price and one be . about the flooding of the city . new orleans . this these topic be obtain from block article about the hurricane katrina .",
    "and in addition to some other topic . but the visualization show that with this technique that we can have conditional distribution of time give a topic . so this allow we to plot this conditional probability . general curve like what you be see here . we see that initially the two curve track each other very well . but later anf",
    "the bottom curve show the coverage of this topic about the flooding of the city by block article in different location and also show some shift of coverage .",
    "so in this case we can see the time can be use as context to reveal trend of topic . this be some additional result on special pattern and this . in this case it be about the topic of government response .",
    "location , but in week four , which be show on the bottom on the left , we see a pattern that be very similar to the very first week on the top left , and that be why again the hurricane without hit the region .",
    "be yet another application of this kind of model where we look at the use of the model for event impact analysis .",
    "and the topic we focus on be about the retrieval model and you can see the top word top word with high probability be about this model on the left . and then we hope to examine the impact of two event . one be the start of trec for text retrieval conference . this be a major evaluation effort sponsor by us government and be launch in 1992 or around that time and that be know to have make an impact on the topic of research information retrieval .",
    "on the bottom we see the variation that be correlate with the publication of the language model paper . before we have those classical probabilistic model logic model , boolean model etc . but after 1998 that we see clear dominance of language model as probabilistic model and we see word like a language model , estimation of parameter etc . so this technique here can use event as context . to understand the impact of event again , the technique be general so you can use this to analyze the impact of any event . here be some suggest reading ."
  ],
  "380a7417-6702-4df8-9818-5aceba7cde2b": [
    "this lecture be \u00a0  about topic mining and analysis . \" we \" in this v \" about the language namely discovery of word association such as paradigmatic relation relation and syntagmatic relation . now , start from this lecture , we be go to talk about mine another kind of knowledge , which be content mining and try to discover knowledge about . the main topic . in the text . and we call that topic mining and analysis . in this lecture we be go to talk about its motivation and the task definition . so first , let 's look at the concept of topic . so topic be something that we all understand , i think , but it be actually not that easy to formally define it . roughly speak , topic be the main idea discuss in text datum , and you can think of this as a theme or subject of discussion or conversation . it can also have different granularity . for example , we can talk about the topic of a sentence .",
    "so different granularity of topic obviously have different application .",
    "or perhaps we be interested in know what be the major topic debate in 2012 presidential election ?",
    "in general , we can view topic as some knowledge about the world . so from text that we expect to discover a number of topic and then this topic generally provide the description about the world and it tell we something about the world , about the product , about the person , etc .",
    "similar look at the topic in different location , we might know some insight about people ' opinion in different location .",
    "and then we also would like to know which topic be cover in which document , to what extent . so for example in .",
    "and other topic perhaps be not cover .",
    "etc . , right ? so now you can see there be generally two different task or subtask . the first be to discover k topic from a collection of text datum . what be these k topic ? ok , major topic in the text datum . the second task be to figure out which document cover which topic to what extent . so more formally , we can define the problem as follow . first , we have as input a collection of n text document . here we can denote that text connection . as c. and denote a text article as d i an we generally also need to have an input the number of topic k. but there may be technique that can automatically suggest a number of topic , but in the technique that we will discuss which be also the most useful technique , we often need to specify a number of topic .",
    "to what extent do the document cover each topic ."
  ],
  "3956403f-f159-448a-9514-5dc69f314c5a": [
    "this lecture be about the evaluation of taxable categorization . so we 've talk about many different method for taxi categorisation , but how do you which method work well ? and for a particular application , how do you this be the good way of solve your problem . to understand these will have to . how to we have to know how to evaluate categorisation result ? so first some general thought about the evaluation in general for evaluation of this kind of empirical task such as categorisation , we use methodology that be develop in 1960s by information retrieval researcher call cranfield evaluation methodology . the basic idea be to help human to create test collection . where we already every document be tag with the desire category , or in the case of search for which query , which document should have be retrieve and this be call ground truth .",
    "a way to do control experiment to compare different method .",
    "so in our case , then we be go to compare our system categorization result with the categorisation ground truth create by human . and we be go to compare our system decision on which document should get which category with what .",
    "or equivalently , to measure the difference between the system output and desire ideal output generate by the human ? so obviously the high similarity be , the well the result be . the similarity can be measure in different way .",
    "in general , different categorization mistake , however , have different cost for a specific application , so some error might be more serious than other . so ideally we would like to model such difference . but if you read many paper in texture catalyzation , you will see that they do n't generally do that , and instead they will use a simplified measure .",
    "and then we should still expect the more effective method to perform well than a less effective one , even though the measure be not perfect .",
    "and see if the system have say yes to despair . basically have assign this category to this document or no , so this be denote by y or n. that be the system to decision . and similarly we can look at the human decision . also , if the human have assign a category to the document , there will be a plus sign here . that be just that just mean a human would think this assignment be correct an if the incorrect , and then there be a minus . so we will see . all combination of these end yes and nos with minus and plus . so there be four combination in total and two of they be correct and when we have y plus or minus and then there be also two kind of error . so the measure of classification accuracy be similar to count how many of these decision be correct and normalize that by the total number of decision we have make . so we know that the total number of decision be .",
    "and the number of character decision obviously be basically of two kind . one be why plus and the other be n minus be and we just put together the account . now this be a very convenient measure that will give we a one number to characterize performance of method and the high the well of course .",
    "the strength and weakness of different method .",
    "in apa category or per document basis ?",
    "and classification error classification accuracy do not address this issue .",
    "that would give we 98 % accuracy . in this case , it be go to be appear to be very effective , but in reality this be obviously not a good result .",
    "and we wonder about equal number of instance . for example , in each class the minority category or class tend to be overlook in the evaluation of classification accuracy .",
    "now , as in the general case of all decision , we can think about four combination of possibility .",
    "when the system say yes , but human say no , that be incorrect . that be a false positive fp . and when the system say no , but the human say yes , then it be a false negative . we miss one assignment .",
    "alright , so then we can have some mesh to just well characterize the performance by use these phone number and so 2 popular measure of precision and recall . and these be also propose by information retrieval researcher in 19 , six day for evaluate search result . but now they have become a standard measure use everywhere . so when the system say yes , we can ask the question how many be correct ? what be the percentage of correct decision when the system say yes ? that be call precision .",
    "the other recall the other mesh call recall an this measure .",
    "this give we a detailed view of the decision on each document . then we can aggregate they later . and if you be interested in some document and this would tell we how well we do that those document a subset of they might be more interesting than other . for example , and this allow we to analyze error in more detail as well . we can separate the document of certain characteristic from other and then look at the error . you might see a pattern here for this kind of document along document it do n't do as well as . for short document .",
    "similarly , we can look at the popular category valuation . this . in this case we be go to look at the how good be the decision on a particular category . and as in the previous case , we can define precision and recall and it will just basically answer the question from a different perspective .",
    "be sometimes also useful to combine precision and recall as one measure , and this be often do by use if mesh . and this be just the harmonic mean of precision and recall define on this slide .",
    "it be also control by a parameter beta two to indicate the weather precision be more important , or recall be more important when beta be set to one , we have a measure call f1 , and in this case we just take a equal weight on both precision and recall . if one be very often use as a measure for categorisation . now , as in all case when we combine result , you always should think about the good way of combine they . so in this case i do n't know if you have think about it and we could have combine they just with the arithmetic mean , right ? so that would still give it the same range of value .",
    "undesirable property of this arithmetic mean ."
  ],
  "39d13817-de51-4195-a33a-985b0b54e64d": [
    "this lecture be about the text categorization . in this lecture we be go to talk about the text categorization . this be a very important technique for a text , datum mining and analytic . it be relevant to discovery of various different kind of knowledge as show here . first be relate to topic mining analysis . and that be because it have to do with analyze text datum base on some predefined topic . secondly , it be also related to opinion mining and sentiment analysis , which have to do with discover knowledge about the observer that the human sensor . because we can categorize the author , for example , base on the content of the article that they have write .",
    "because we can often use text categorization technique to predict some variable in the real world that be only remotely related to text datum . and so this be a very important technique for text datum mining .",
    "possibly form a hierarchy so .",
    "which mean that text object have already be label with know category , and then the task be to classify any tax object into one or more of these predefined category . so the picture on the slide show what happen . when we do text categorization , we have a lot of text object to be process by a categorisation system .",
    "the category of new tax object that it have not see .",
    "in fact , there be many example . here be just a few . so first text object can vary , so we can categorize a document . or a passage or sentence or collection of text , as in the case of cluster the unit to be analyze can vary a lot , so this create a lot of possibility . secondly , category can also vary , and we can generally distinguish two kind of category . one be internal category . these be category that characterize content of text object . for example , topic category .",
    "the other kind be external category that can characterize the entity associate with the text object . for example , author or entity associate with the content that they produce . and so we can use their content , determine which author have write which part , for example , and that be call author attribution .",
    "there be a.",
    "or a lot of review about the product . and then these text datum can help we infer property of product or a restaurant .",
    "here be some specific example of application . news categorization be very common , have be stuide . a lot . news agency would like to assign predefined category to categorize news generate every day .",
    "another example of application spam , email detection or filtering right ? so we often have a spam filter to help we distinguish spam from legitimate email , and this be clearly a binary classification problem .",
    "so you can have the same sentiment category assign . to text content .",
    "there be also another important kind of application of route email to the right person to handle . so in helpdesk email message generally route to a particular person to handle different people attempt to handle different kind of request and in many case a person will manually assign the message to the right people . but you can imagine you can build automatic text categorization system to help route \u00a0  a request . and this be to classify the incoming request in to one of the category where each category actually correspond to a person to handle the request .",
    "distinguish it relevant document from non relevant document for a particular query . spam filter be interesting . distinguish spam from non spam . so also two category . sometimes classification of opinion can be in two category together with positive and negative .",
    "and another variation to have hierarchical categorization , where category form hierarchy , again , topical hierarchy be very common . yet another categorization be joint categorization . that be when you have multiple categorization task that be relate . and then you hope to kind of join the categorization",
    "and basically we can look at each category separately and then the binary categorization problem be whether object be in this category or not . mean in other category .",
    "semantic category assign can also be directly or indirectly useful for application . so for example , sentiment category could be already very useful , or author attribution might be directly useful .",
    "another example be when semantic category can facilitate aggregation of tax content , and this be another case of .",
    "for example , we if we want to know the overall opinion about the product , we could first categorize the opinion in each individual review as positive or negative , and then that would allow we to easily aggregate all the sentiment and it will tell we about 70 % of the view positive and 30 % be negative , etc . so without do categorization it will be much hard to aggregate such opinion .",
    "and sometimes you miss see some application , text or categorization be call a text code encoding with some controller vocabulary .",
    "and text categorisation allow we to infer the property of such entity that be associate with text datum . so this mean we can use text categorization to discover knowledge about the world in general , as long as we can associate the entity with text datum , we can always use the text datum to help categorize the correspond entity .",
    "speaker"
  ],
  "4453a049-7597-4df4-9b9b-67c2d124a116": [
    "we can compute this maximum regular estimate by use the em algorithm . so in the e - step , we now have to introduce more hidden variable because we have more topic . so our hidden variable z now , which be a topic indicator , can take more than two value . specifically , will take a k plus one value with b denote the background and one through k to denote all the k topic . so now the e step as you can recall be augment datum and by predict the value of the hidden variable . so we be go to predict for word whether the word have come from one of these k+1 distribution . this equation allow we to predict the probability that the word w in document \" d be generate from topic theta sub j",
    "note that we use document d here to index the word . why ? because whether a word be from a particular topic , actually depend on the document . can you see why ? well , it be through the pi . the pis be tie to each document . each document can have a potentially different pis , right ? the pis will then affect our prediction , so the pis be here , and this depend on the document .",
    "in both case we be use the baye rule as i explain , basically assess the likelihood of generate word in from each distribution and be normalize . what about the m - step ? well , we may recall the m step be to take advantage of the infer z value to split the count and then collect the right count to re estimate parameter . so in this case we can re estimate our coverage probability and this be re estimate base on collect all the word in the document .",
    "and then we be go to look at the to what extent this word belong to the topic theta sub - j , and this part be our guess from e - step .",
    "and similarly , the bottom one be to re - estimate the probability of word for topic . in this case we be use exactly the same count . you can see this be the same discount count , it tell we to what extent we should allocate this word to , topic theta sub - j. but the normalization be different because in this case we be interested in the word distribution . so we simply normalize this over all the word .",
    "in contrast , here we normalize among all the topic .",
    "this give we different distribution and these tell we how to improve the parameter ? and as i just explain in both e step formula , we have a maximum likelihood estimator base on the allocate word \" count to \" topic theta sub - j. now this phenomena be actually general phenomenon in all the em algorithm in the m step , you generate expect count of event base on the e step result and then you just collect the relevant count for a particular parameter . and re - estimate with normalize .",
    "so in term of computation of the em algorithm , we can . actually , just keep count various event and then normalize they . and when we think in this way , we also have a more concise way of present the em algorithm . it actually help we well understand the formula . so i be go to go over this in some detail . so as the algorithm , we first initialize all the unknown parameter randomly .",
    "and we just randomly normalize they . this be the initialization step , and then we will repeat until likelihood converge . now how do we know whether likelihood converge we be go to compute likelihood at each step and compare the current likelihood with the previous likelihood if it do n't change much and we be go to say stop right ?",
    "so if you look at the e step formula essentially we be actually normalize these count . at all , sorry , these be probability of observe the word from each distribution , so you can see basically the prediction of word from topical thetasubject be base on the probability of select that theta sub - j as a word distribution to begin to generate the world multiply by the probability of observe the word from that distribution . and i say it be proportional to this because in complete the implementation of em algorithm you can just keep count counter for this quantity and then in the end you just normalize it . so the normalization here be over all the topic and then you will get a probability .",
    "allocate count for each topic .",
    "and then we be go to normalize they in different way to obtain the re - estimate . so , for example , we can normalize among all the topic to get re estimate of pi the coverage . or we can renormalize base on the . for all the word and that would give we a word distribution . so it be useful to think of the algorithm in this way , because when you implement , you can just use . variable to keep track of these quantity in each case .",
    "now i do not put the constraint for this one and i intentionally leave this as exercise for you and you can see what be the normalizer for this one . it be of a slightly different form , but it be essentially the same as the one that you have see here . namely this one .",
    "so to summarize , we introduce the plsa model , which be a mixture model with k unigram language model represent k topic .",
    "and , we show that with maximum likelihood estimator we can discover topical knowledge from text datum . in this case plsa allow we to discover two thing . one be k - word distribution , each represent a topic and the other be the proportion of each topic in each document . and such detailed characterization of coverage of topic in document can enable a lot of further analysis . for example , we can aggregate the document in the particular time period to assess the coverage of a particular topic in a time period that would allow we to generate the temporal chain of topic .",
    "and in addition to this , we can also cluster term and cast document . in fact , each topic can be regard as a cluster , so we already have term cluster . and the high probability word can be regard as in belong to one cluster ."
  ],
  "44df41bc-04d3-41ca-ac51-dbd22dc98305": [
    "in general , we can use the empirical count of event in the observed datum to estimate probability . and a commonly use technique be call a maximum likelihood estimate , where we simply normalize the observe account . so if we do that , we can see we can compute these probability as follow for estimate the probability that we see a word occur in segment , we simply normalize the count of segment that contain this word . so let 's first take a look at the datum here . on the right side you see i list some hypothesize that datum these be segment . and in some segment you see both word occur . their indicator as once for both column .",
    "and for estimate these probability , we simply need to collect the three count . so the three count of 1st , the count of w. 1 and that be the total number of segment that contain world w one . it be just the one in the column of w one we can just count how many one we have see there . the second counter be for word 2 and we just count the one in the second column .",
    "the third account be when both word occur , so this be time we be go to count the segment where both column have one . and then so this would give we the total number of segment where we have see both w and w2 . once we have these count , we can just normalize . these count by n , which be the total number of segment and this will give we the probability that we need to compute mutual information .",
    "now , the good way to understand the smoothing be imagine that we actually . observe more datum than we actually have . we will pretend we observe some pseudo segment that be illustrate on the top on the right side of the slide and these pseudo segment would contribute additional count of these word so that no event will have zero probability probability .",
    "and these represent the four different combination of occurrence of these word . so now each event , each combination will have at least one count or at least non zero counter . from these pseudo segment . so in the actual segment that we observe , it be ok if we have n't observe all the combination .",
    "and similarly this .05 come from one single pseudo segment that indicate the two word occur together .",
    "so this basically conclude the discussion of how to compute the mutual information , how to use this for syntagmatic relation discovery .",
    "due to know why or entropy reduction of why do too know egg ?",
    "so this would give we another way to represent the context .",
    "so to summarize , this whole part about word association mining , we introduce the two basic association , call paradigmatic and syntagmatic relation . these be fairly general . they can be apply to any item in any language , so the unit do n't have to be bad than they can be phrase or entity . be we introduce multiple statistical approach for discover they ? then it show that pure statistical approach be visible ?",
    "and mostly becausw . they be base on counting of word .",
    "we can also use different way to define context and segment and this would lead to some interesting variation of application . for example , the context can be very narrow , like a few word around a word or sentence or maybe paragraph and use different context , which allow you to discover different flavor of paradigmatic relation .",
    "these discovery association can support they . any other application in both information retrieval and text datum mining .",
    "if you want to know more about the topic , the 1st be a book with a chapter on location which be quite relevant to the topic of these lecture ."
  ],
  "48b37a2f-5ca3-4b7b-9bfc-d841da37c566": [
    "so now let 's talk about the problem a little bit more and specifically , let 's talk about the two different way of estimate parameter . one be call maximum likelihood estimate that i already just mention . the other be bayesian estimation . so in maximum likelihood estimation , we define well as mean the datum likelihood have reach the maximum , so formally it be give by this expression here . where we define the estimate as arg arg max . of the probability of x give theater . andso arg max here just mean it be actually a function that would return . the argument that give the function maximum value as the value , so the value of arg max be not value of this function , but rather the argument that have make the function reach maximum . so in this case the value of argmax be theta . it be the theater that make the probability of x give theta reach maximum , so estimate . also make sense , and it be often very useful , and it seek the parameter that well explain the datum .",
    "not go to look at just the datum , but also look at the prior so the prior here be define by p of theta .",
    "and by use bayes rule that i have show here ,",
    "to give we this .",
    "now a full explanation of bayes , and some of these thing relate to bayesian reasoning would be outside the scope of this course , but i just give a brief introduction because this be a general knowledge that might be useful for you , so the bayes rule be basically define here .",
    "that mean before we observe any other datum , that be our belief about x , what we believe some x value have high probability than other .",
    "now , the two probability of related through this can be regard as the probability of the observe evidence why . here give a particular act .",
    "and we have some prior belief about which hypothesis to choose and after we have observe y , we will update our belief and this update formula be base on the combination of our prior here and the likelihood of observe this y if x be indeed true . so much for a detour about bayes rule .",
    "that include our prior knowledge about the parameter .",
    "so it represent a compromise of the two preference .",
    "this posterior probability . and this be estimator be call a maximum a posteriori or map estimate .",
    "the same as here .",
    "there be no free lunch , and if you want to solve the problem with more knowledge , we have to have that knowledge and that knowledge ideally should be reliable . otherwise your estimate may not necessarily be more accurate than maximum likelihood estimate .",
    "so now first we have the prior . the prior tell we some theta value be more likely than other . we believe , for example , these value be more likely than the value like here or here or other place .",
    "and then we have our data likelihood .",
    "and then when we combine the two , we get the posterior distribution and that be just a compromise of the two . it would say that it be somewhere in between , so we can now look at some interesting point estimate of theta . now this point represent the mode of prior . that mean the most likely parameter value accord to our prior before we observe any datum .",
    "now this point be interesting . it be the posterior mode , it be the .",
    "in general , in bayesian inference we be interested in the distribution of all these parameter value . as you see , here be there be a distribution over theta value that you can see here p of theta give x.",
    "so this be a general illustration of bayesian estimation and bayesian inference .",
    "so to summarize , we introduce the language model which be basically probability distribution over text . it be also call a generative model for text datum . the simple language model be unigram language model . it be basically a word distribution .",
    "and this function be very important ."
  ],
  "4a54f790-991c-44bb-ab62-713cbef84ad1": [
    "this lecture be about the sentiment classification . if we assume that most of the element in the opinion representation be already know , then our only task maybe just the sentiment classification as show in this case . so suppose we know who be the opinion holder and what be the opinion target and also know the content and context of the opinion . then we mainly need to decide the opinion sentiment of the review . so this be a case of just use sentiment classification for understand opinion . sentiment classification can be define more specifically as follow : the input be opinionate text object . the output be typically , a sentiment label or sentiment tag , and that can be design in two way . one be polarity analysis where we have category such as positive , negative or neutral .",
    "that can go beyond polarity to characterize the feeling of the opinion holder . in the case of polarity analysis , we sometimes also have numerical rating , as you often see in some review on the web . five might denote the most positive and one maybe at most negative , for example . in general you have just discrete category to characterize the sentiment .",
    "so as you can see , the task be essentially a classification task or categorisation task . as we 've see before , so it be a special case of text categorization . this also mean any text categorization method can be use to do sentiment classification .",
    "the other be to consider the order of these category . an especially polarity analysis , very clear that order here and so these category be not all that independent . there be order among they , and so it be useful to consider the order . for example , we could use ordinal regression to do , and that be something that will talk more about later . so now let 's talk about some feature that often very useful for text categorization and text mining in general , but some of they be especially also need for sentiment analysis . so let 's start from the simple one , which be character n - gram . you can just have a sequence of character as a unit , and they can be mix with different n(s ) and different length .",
    "and this be also robust to spell error or recognition error , right ? so if you misspell the word by 1 character and this representation actually would allow you to match this word when it occur in the text correctly . so misspelled word and the correct form can be match because they contain some common n - gram of character .",
    "so next we have word n - gram , a sequence of word and again we can mix they with different length . uni grams be actually often very effective for a lot of text processing task and that be mostly because word be well design feature by human for communication , and so they often good enough for many task , but it be not good or not sufficient for sentiment analysis clearly . for example , we might see a sentence like it be not good or it be not as good as something else . so in such a case , if you just take a good and that would suggest positive , it be not good , so it be not accurate , but if you take the bigram , not good together , and then it be more accurate , so long n - gram be generally more discriminative and they be more specific . if you match it and it say a lot and it be accurate . it be unlikely , very ambiguous .",
    "that could be useful for sentiment analysis .",
    "or could be semantic and they might represent concept in the \u00a0 thesaurus or ontology like word net .",
    "and such pattern provide a more discriminative feature than word , obviously , and they may also generalize well than just the regular n - gram because they be frequent , so you can expect they to occur also in test datum so they have a lot of advantage , but they might still face the problem of overfitting as the feature become more complex . this be the problem in general , and the same be true for parse tree base feature where you can use a parse tree to derive feature such as frequent subtree or path , and those be even more discriminating , but they also be more likely to cause overfitting .",
    "so from these word we can only derive simple world n - gram representation or character n - gram . but with nlp we can enrich the representation with a lot of other information such as part of speech tag , parse tree or entity , or even speech act . now with such enriched information , of course , then we can generate a lot of other feature , more complex feature , like a mixed gram of word and part of speech tag . or even a part of parse tree .",
    "so first you want to use domain knowledge and your understanding of the problem to design seed feature .",
    "and these feature can then be far analyze by human through error analysis .",
    "so a main challenge in designing feature a common challenge be to optimize the tradeoff between exhaustive activity and specificity ."
  ],
  "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5": [
    "so this be indeed a general idea of the expectation maximization , or em algorithm . so in all the em algorithm , we introduce a hidden variable to help we solve the problem more easily . in our case , the hidden variable be a binary variable for each occurrence of word . and this binary variable would indicate whether the world have be generate from theta on sunday or theater super b. and here we show some possible value of these variable . for example for the it be from background , z value be 1 and text on the other hand be from the topic . then it be 0 for z etc . now , of course we do n't observe those z value . we just imagine there be such a social value of z attach to all the word .",
    "the value of this hidden variable . and so . the algorithm , the em algorithm then would work as follow . first will initialize all the parameter with random value . in our case the parameter be mainly the probability of a word give by status update . so this be the initialization stage . it be initialize value would allow we to use bayes rule to take a guess of these z value . so will guess these value we can say for sure whether taxis from background or not , but we can have our guess . this be give by this formula . it be call e - step .",
    "in this step we simply take advantage of the infer value and then just group word that be in the same distribution like this from background , include this as well .",
    "so let i also illustrate we can group the word that be believe to have come from cedar sub d and as text mining algorithm for example and clustering .",
    "to help we re estimate the parameter . that be interested in so these will help we re estimate these parameter . but note that before we just set these parameter value randomly , but with this guess we will have a somewhat improve estimate of this .",
    "by the probability that we believe this would have be generate by use the theta sub d.",
    "and the m step be to take advantage of the additional information to separate the datum to split the datum account and then collect the right data count .",
    "and then once we have a new generation of parameter , we be go to repeat this . we be go to use the e - step again to improve our estimate of the hidden variable , and then that would lead to another generation of re estimate the parameter .",
    "ok , so as i say , the bridge between the two be really variable z hide variable , which indicate how likely this world be from the topic word distribution theta sub d. so this slide have a lot of content and you may need to pause the video to digest it , but this basically capture the essence of em algorithm . start with initial value that be often randomly set .",
    "so let 's take a look at the computation for specific case . so these formula be the em formula that you see before , and you can also see there be superscript here n to indicate the generation of parameter .",
    "so in this setting we have assume that the two model have equal probability and the background model be know . so what be the relevant statistic ? well , these be the word count .",
    "an in the first iteration you can picture what would happen . well , we first we initialize all the value . so here this probability that we be interested in be normalize into an uniform distribution over all the word .",
    "of the distribution that have be use to generate each word , we can see we have different probability for different word . why that be be cause these word have different probability in the background . so even though the two distribution be equally likely , and then our initialization say uniform distribution because of the difference in the background world distribution , we have different guest probability . so these word be believe to be more likely from the topic .",
    "so once we have the z value , we know in the e step these probability would be use to adjust the count . so 4 must be multiply by this point three three in order to get the allocate count toward the topic .",
    "note that if our guess say this be 100 % . if this be 1.0 then we just get the full council of this word for this topic . but in general , as i say , it be not go to be 1.0 , so we be go to just get some percentage of the count toward this topic , and then we simply normalize these count .",
    "so compare this with this one and will see at the probability be different . not only that , we also see some word that be believe to have come from the topic . we have high probability like this one text .",
    "and this these new in further value of these will give we then another generation of the estimate of probability of the word ."
  ],
  "4da6283d-6903-4be9-8bfc-ad5d330343c6": [
    "this lecture be about the discriminative classifier for text categorization . in this lecture , we be go to continue talk about how to do text categorization and cover discriminative approach . this be a slide that you have see from the discussion of naive bayes classifier , where we have show that although naive bayes classifier try to model the generation of text datum from each category , we can actually use baye rule and to eventually rewrite the scoring function as you see on this slide and this scoring function be basically a weight combination of a lot of word feature where the feature value be word count and the feature weight be the log of probability ratio of the word give by two distribution here . now this kind of scoring function can be actually a general scoring function where we can in general represent text datum as a feature vector . of course the feature do n't have to be all the word and their feature can be other signal that we want to use .",
    "on some predictor .",
    "for feature value you may recall in the previous slide we have use fi to represent the feature value .",
    "and with m feature .",
    "now the goal here be to model the conditional probability of y give x directly as oppose to model the generation of x&y as in the case of naive bayes .",
    "so most specifically , in logistic regression the assume functional form of y depend on x be the following , and this be very closed , closely related to the log or log odd that i introduce in the naive baye or log of probability ratio of the two category that you have see on the previous slide .",
    "but here we actually would assume explicitly that we would model our",
    "as directly as a function of these feature .",
    "and so it be a function of x , and it be a linear combination of these feature value , control by beta value .",
    "so this be a log odd ratio . here . and so in logistic regression , we basically assume that the probability of y = 1 \u00a0  give x be dependent on this linear combination of all these feature .",
    "so if we rewrite this question to actually express the probability of y give x in term of x by take by getting rid of the logarithm and we get this functional form and this be call a logistical function , it be a transformation of x into y.",
    "on the right side here . so that the xs will be map into a range of value from zero to 1.0 . you can see , and that be precisely what we want . since we have a probability here . and the function form look like this . so this be the basic idea of logistic regression , and it be a very useful classifier that can be use to do a lot of classification task , include text categorization .",
    "to model the classifier , then the next step be to compute the parameter value . in general , we be go to adjust these parameter value , optimize the performance of classifier on the training datum . so in our case , let 's assume we have training datum . the training datum here , x i and y i and each pair be basically feature vector of x and a known label for that x y , either one or zero .",
    "the condition likelihood here be basically to model y give the observe x.",
    "it be not like a modeling x , but rather we be go to model this . note that this be a conditional probability of y give x. and this be also precisely what we want for classification . now , so the likelihood function would be just a product over all the training case .",
    "and that be basically the logistical regression function . but what about this if it be 0 ? well , if it be zero then we have to use a different form and that be this one .",
    "and you can easily see this now . the key point here be that the function form here depend on the observed . y i if it be one , it have a different form than when it be 0 .",
    "but if the document be not we be go to maximize this value , and what be go to happen be actually to make this value as small as possible . because they sum to one . when i maximize this one .",
    "so you can see basically the if we maximize the conditional likelihood we be go to basically try to make the prediction on the training datum as accurate as possible . so , as in other case , when compute the maximum likelihood estimator basically lets go find a beta value , a set of beta value that will maximize this conditional likelihood .",
    "if we have all the betavalue already know , all we need be to compute the xi 's for that document .",
    "ok , so much for logistical regression . let 's also introduce another discriminative classifier call k near neighbor . now in general , i should say there be many such approach .",
    "the object we be interested in classify and say we have find the k near neighbor . that be why this method be call k near neighbor . and then we be go to assign the category that be most common in these neighbor .",
    "now that mean if most of they have a particular category , lets say category 1 then we be go to say this current object will have category one .",
    "but the general idea be to look at the neighborhood and then try to assess the category base on the category of the neighbor .",
    "now i be go to explain this intuition in the moment , but before we proceed , let i emphasize that we do need a similarity function here in order for this work .",
    "an in logistical regression , we do not talk about the similarity function either . but here we explicitly require a similarity function .",
    "and i 've color they differently and to show just different category . now suppose we have a new object in the center that we want to classify .",
    "now in this case , if the , let 's assume the close neighbor be the box fill with diamond and so then we be go to say . well , since this be in , this object be in category of diamond . let 's say then we be go to say , well , we be go to assign the same category to our text object .",
    "so in this case now we \u00a0  be go to notice that among the four neighbor there be actually three neighbor in a different category . so if we take a vote , then we 'll conclude the object be actually of a different category . so this both illustrate how k near neighbor work and also illustrate some potential problem of this classifier . basically the result might depend on the k and indeed k be an important parameter to optimize .",
    "the parameter k here or some other parameter in other classifier , and then you be go to assume this number that work well on your training set would be actually the good for your future datum .",
    "so the key assumption that we make in this approach be that the distribution of the label give the document or probability of a category give document . for example , probability of theta i give document d be locally smoothed and that just mean we be go to assume that this probability be the same for all the document in this region . r \u00a0  here .",
    "that be actually important assumption that would allow we to do a lot of machine learning , but in reality , whether this be true of course would depend on how we define similarity , because the neighborhood be largely determine by our similarity function . if our similarity function capture object that do follow similar distribution , then this assumption be ok . but if our similarity function could not capture that . obviously the assumption would be a problem , and then the classifier would not be accurate ."
  ],
  "5190e288-54f7-4021-9083-8e8ceac11345": [
    "this lecture be about the latent dirichlet allocation or lda . in this lecture , we be go to continue talk about topic model . in particular , we be go to talk about some extension of plsa , and one of they be lda or latent dirichlet allocation . so the plan for this lecture be to cover two thing . one be to extend the plsa with prior knowledge that would allow we to have in some sense a user control plsa , so it do n't blindly just listen to datum but also would listen to our need . the second be to extend the plsa as a generative model fully generate model . this have lead to the development of latent dirichlet allocation or lda .",
    "the standard plsa be go to blindly listen to the datum by use maximum likelihood estimator . we be go to just fit datum as much as we can and get some insight about datum . this be also very useful , but sometimes a user might have some expectation about which topic to analyze . for example , we might expect to see retrieval model as a topic in information retrieval . we also may be interested in certain aspect such as battery and memory when look at the opinion about the laptop , because the user be particularly interested in these aspect .",
    "and we may know which topic be definitely not cover in which document or be cover in the document . for example , we might have see those tag topic tag assign to document . and those tag could be treat as topic if we do that , then a document that can only be generate use topic correspond to the tag already assign to the document . if the document be not assign to a tag , we be go to say there be no way for use that topic to generate the document . the document must be generate by use the topic correspond to the assign tag . so the question be , how can we incorporate such knowledge into plsa ? it turn out that there be a. a very elegant way of do that , and that be all incorporate such knowledge as prior on the model .",
    "so in this case we can use maximum a posteriori estimate , also call map estimate , and the formula be give here . basically be to maximize the posterior distribution probability and this be a combination of the likelihood of datum and the prior . so what would happen be that we be go to have an estimate that listen to the datum and also listen to our prior preference . we can use this prior , which be denote as p of lambda to encode . all kind of preference and constraint . so for example , we can use this to encode the need of have precisely 1 background the topic .",
    "so now we can also , for example use the prior to force particular choice of topic to have a probability of a certain number . for example , we can force the document d to choose topic one with probability of 1/2 .",
    "we can also use the prior to favor set of parameter with topic that assign high probability to some particular word . in this case , we be not go to say it be impossible , but we be go to just strongly favor certain kind of distribution .",
    "and in such a estimate , if we use a special form of the prior call conjugate prior , then the functional form of the prior will be similar to the datum . as a result , we can combine the two and the consequence that you can basically convert the influence of the prior into the influence of have additional pseudo datum because the two functional form be the same and they can be combine . so the effect be as if we have more datum .",
    "so now let 's look at the specific example . suppose the user be particularly interested in battery life of a laptop , and we be analyze review . so the prior say that the distribution should contain one distribution that would assign high probability to battery , and life .",
    "now , this be a intuitively very reasonable way of modify the em algorithm and theoretically speak , this deal work , and it compute the map estimator .",
    "now what would happen if we set mu to positive infinity ? well , that be to say this price be so strong that we be not go to listen to the datum at all . so in the end you can see in this case we can do make one distribution fix to the prior . you see why ? when mu be infinity , we basically let this one dominate . in fact , we be go to set this one . to precise this distribution , so in this case it be this distribution , and that be why we say the background language model be in fact a way to enforce a prior , because with false one distribution to be exactly the same as what we give , that be the background distribution . so in this case we can even force the distribution to entirely focus on battery life . but of course this wo n't work well 'cause it can not attract other word , it would affect the accuracy of counting . topic about the battery life so in practice mu be set somewhere in between , of course ."
  ],
  "51be74e8-eb10-47c2-a768-b688605de1e0": [
    "this lecture be about the syntagmatic relation discovery and conditional entropy . in this lecture , we be go to continue the discussion of word association mine an analysis . we be go to talk about the conditional entropy , which be useful for discover syntagmatic relation . early we talk about use entropy to capture how easy it be to predict the presence or absence of a word . now we address the different scenario where we assume that we know something about the text segment . so now the question be , suppose we know eat occur in the segment , how would that help we predict the presence or absence of a word ? like meat , and in particular we want to know whether the presence of eat have help we predict the presence of meat . and if we frame this use entropy , that would mean we be interested in know whether know the presence of eat could reduce uncertainty about the meat or reduce the entropy of the random variable correspond to the presence or absence of meat .",
    "so these question can be address by use .",
    "i suppose we know eat be present , so now know the value of another random variable that denote eat .",
    "so as a result , if we replace these probability with their correspond conditional probability in the entropy function , we will get the conditional entropy .",
    "would be . the conditional entropy condition on the presence of eat . right . so you can see this be essentially the same entropy function as you have see before , except that we all the probability now have a condition .",
    "and of course , we can also define this conditional entropy for the scenario where we do n't see eat . so if we know eat do not occur in the segment , then this conditional entropy would capture the uncertainty of meat in that content in that condition .",
    "basically . we be go to consider both scenario of the value of eat zero or one , and this give we the probability that eat be equal to 0 or 1 . basically , whether eat be present or absent , and this of course be the entropy conditional entropy of meat in that particular scenario .",
    "where you see the involvement of those conditional probability . now in general , for any discrete random variable x&y we have . the conditional entropy be no large than the entropy of the variable x , so basically this be upper bind for the conditional entropy . that mean by know more information about the segment , we wo n't be able to increase the uncertainty . we can only reduce uncertainty , and that intuitively make sense because as we know more information , it should always help we .",
    "now what be interesting here be also to think about what be the minimum possible value of this conditional entropy . now we know that the maximum value be the entropy of x.",
    "i hope you can reach the conclusion that the minimum possible value would be 0 and it will be interesting to think about and in what situation will achieve this . so let 's see how we can use conditional entropy to capture syntagmatic relation .",
    "because it tell we to what extent we can predict the one word give that we know the presence or absence of another word . now before we look at the intuition of conditional entropy in capture syntagmatic relation , it be useful to think of a very special case list here , that is , the conditional entropy of the word give itself .",
    "conditional entropy in the middle . so it be here .",
    "now . this mean we know whether meat occur in the sentence and we hope to predict whether the meat occur in the sentence . now of course this be zero 'cause there be no uncertain there anymore once we know whether the word occur in the segment will already know the answer for the prediction . so this be 0 . and that be also when this conditional entropy reach the minimum .",
    "know the and try to predict the meat and this be the case of know eat and try to predict the meat . which one do you think be small ? note that a small entropy mean easy for prediction . which one do you think be high ? which one be small ?",
    "whereas in the case of eat , eat be relate to meet , so know presence of eat or absence of eat what help we predict weather meet occur so it can help we reduce entropy of meat , so we should expect the second term , namely , this one to have a small entropy .",
    "so we now also know when this be the same as this meat then the entropy conditional entropy would reach its minimum which be 0 ?",
    "like the , for example , it would be very close to the maximum , which be the entropy of meat itself . so this suggest that we can use conditional entropy for mine syntagmatic relation . the algorithm would look as follow .",
    "of w1 give w2 . and we think all the candidate word in ascend order of the conditional entropy , because we want to favor a word that have a small entropy , mean that it help we predict the target word w1 , and then we can take the top rank the candidate word as word that have potential syntagmatic relation with w1 .",
    "now this would allow we to mine the most strongly correlate word with a particular word w1 here .",
    "in this case of discover syntagmatic relation for a target word like w1 , we only need to compare the conditional entropy"
  ],
  "5350ccd0-beab-48fc-8484-d8e6a38c4cbf": [
    "now let 's look at the another behavior of mixture model and in this case let 's look at their response to the data frequency . ok , so what you be see now be basically the likelihood function for the two word document , and we know in this case the solution be to give text a probability of 0.9 and the probability of 0.1 . now it be interesting to think about a scenario where we start add more word to the document . so what would happen if we add many the ' to the document ? now this will change the game , right ? so how ? well , picture what would the likelihood function look like now ? it start with the likelihood function for the two word . as we add more word , we know that , we have to just multiply the likelihood function by additional term to account for the additional occurrence of the . since in this case all the additional term be the , we be go to just multiply by this term for the probability of the .",
    "now this obviously change the likelihood function , so what be interesting be now to think about how would that change our solution . so what be the optimal solution now ?",
    "but the question be how should we change it ? well in general there be something one . so in order to change it , we must take away some probability mess from one word . an add the probability mass to the other word . the question be which word to have a reduce the probability and which word to have a large probability ? and in particular , let 's think about the probability of the . should it be increase to be more than 0.1 or should we decrease it to less than \u00a0  0.1 ? what do you think ?",
    "now if you look at the formula for a moment then you will see . it seem that now the objective function be more influence by the than text before each contribute one turn .",
    "right , so this mean there be another behavior that we observe here that be high frequency word generally how high probability from all the distribution . and this be no surprise at all , because after all we be maximize the likelihood of the datum . so all the more word occur , then it be it make more sense to give such a word a high probability because the impact would be more on the likelihood function . this be in fact a very general phenomenon of all the maximum likelihood estimator , but in this case we can see as we see more occurrence of term . it also encourage the unknown distribution theta sub d to assign somewhat high probability to this word .",
    "now , we 've be so far , assume that each model be equally likely and that give we 0.5 , but you can again look at this like your function and try to picture what would happen if we increase the probability of choose a background model . now you will see these term for the will have a different form where the probability of ' the ' would be even large because the background that have a high probability for the word \u00a0  and the coefficient in front of point nine which be now 0.5 would be even large . when this be large the overall result would be large and that also make they less important for thetasub d to increase the probability for the because it be already very large so the impact here of increase the probability of the be somewhat regulate by this coefficient 0.5 . if it be a large on the background then it become less important to increase the value so . so . this mean the behavior here , which be high frequency word tend to get a high probability be affect or regularise somewhat by the probability of choose each component . the more likely a component that be be choose , it be more important than to have high value for these frequent word . if you have a very small probability of be choose , than the incentive be less .",
    "3rd , the probability of choose each component regulate the collaboration and competition between the component model . it would allow some component model to respond more to the change , for example of frequency of datum point in the datum ."
  ],
  "54ab232c-85cb-4829-abd0-6cbaed5f3fc8": [
    "this lecture be about an overview of statistical language model which cover probabilistic topic model as special case . in this lecture we be go to give an overview of statistical language model . these model be general model that cover probabilistic topic model as special case . so first , what be the statistical language model ? a statistical language model be basically the probability distribution over word sequence . so , for example , we might have a distribution that give today be wednesday a probability of 0.001 it might give \" \" \" today wednesday be \" \" which be a non \" grammatical sentence very , very small probability as show here . \" and similarly another sentence , \" \" the \" \" eigenvalue be positive \" \" , might get a \" probability of 0.00001",
    "it depend on the context of discussion . some word sequence might have high probability than other . but the same sequence of word might have a different probability in a different context . and so this suggest that such a distribution can actually characterize topic . such a model can also be regard as a probabilistic mechanism for generate text .",
    "so now give a model , we can then sample sequence of word . so for example , base on the distribution that i have show here on this slide , we might , let 's say , sample a sequence like today be wednesday because it have a relatively high probability , we might often get such a sequence .",
    "very , very occasionally , might get today \" wednesday be \" \" because the probability be \" so small . so in general , in order to characterize such a distribution , we must specify probability value for all these different sequence of word . obviously it be impossible to specify that , because it be impossible to enumerate all the possible sequence of word .",
    "so the simple language model be call a unigram language model . in such a case , we simply assume that text be generate by generate each word independently .",
    "basically now the probability of a sequence of word w_1 through w_n would be just a product of each . the probability of each word . so for such a model we have as many parameter as the number of word in our vocabulary . so here we assume we have n word , so we have n probability , one for each word , and they sum to one .",
    "that just mean we be gon na draw a ward each time and then eventually we 'll get a text . so for example now again . we can try to sample word accord to a distribution . we might get wednesday often or today often and some other word like eigenvalue might have a small probability , etc . now , with this we actually can also compute the probability of every sequence , even though our model only specify the probability of word . this be because of the independence assumption . so specifically we can compute the \" probability of \" \" today be wednesday \" \" . \" because it be just a product of the probability of today , probability of be and probably wednesday . for example , i show some fake number here and we might then multiply these number together to get the probability \" of \" \" today be wednesday \" \" . \"",
    "so when we have a model , we generally have two problem that we can think about . one be give a model . how likely we 'll observe certain kind of data point .",
    "so here i show 2 two example of word distribution or unigram language model . the first one have high probability for word , \u00a0  text , mining , association , etc .",
    "so in this case , if we ask the question about what be the probability of generate a particular document , then we likely will see text that look like a text mining paper of course .",
    "the text that we generate by draw word from this distribution be unlikely coherent , although the probability of generate a text mining paper publishing in the top conference be non zero . assume that no word have a zero probability in the distribution and that just mean we can essentially generate all kind of text document , include very meaningful text document .",
    "so if we sample word from such distribution , then the probability of observe a text mining paper would be very very small .",
    "so that just mean give a particular distribution , different text will have different probability .",
    "if we ask the question , what be the most likely language model that have be use to generate this text datum , assume that the text be observe from some language model , what be our good guess of this language model ?",
    "so what do you think ? what would be your guess ?",
    "what about the query ?",
    "and if you think about it for a moment , and if you like many other , you would have guess that text have a probability of 10 out of 100 . because i 've observe text 10 time in the text that have a total of 100 word .",
    "right , so that , intuitively , be a reasonable guess , but the question be be this our good guess or good estimate of the parameter ?"
  ],
  "5bb813bd-6b7d-4f77-8156-21995f5944ad": [
    "this lecture be about the similarity base approach to text for clustering . in this lecture , we be go to continue the discussion of how to do a text clustering . in particular , we be go to cover a different kind of approach than generative model . and that be similarity base approach . so the general idea of similarity base clustering be to explicitly specify a similarity function to measure the similarity between 2:00 text object . now this be in contrast with a generative model where we implicitly define the cluster bias . by use a particular objective function like a likelihood function . the whole process be drive by optimize the likeable , but here we explicitly provide a review of what we think be similar , and this be often very useful because then it allow we to inject any particular view of similarity into the clustering program . so once we have a similarity function , we can then aim at optimally partition to partition the datum into cluster or into different group . anne , try to maximize the intragroup similarity and minimize the intergroup similarity . that is , to ensure the object that be put in the same group to be similar , but the object that be put into different group to be not similar , and these be the general goal of clustering . and there be often a tradeoff between achieve both goal . now , there be many different method for do similarity base clustering . in general , i think we can distinguish two strategy at high level . one be to progressively construct the hierarchy of cluster .",
    "so as i just say , there be many different clustering method available and .",
    "but here we can talk about the two representative method and .",
    "in this case , we be give a similarity function call to measure similarity between two object and then we can gradually group similar object together in a bottom up profession to form large and large group , and they also form a hierarchy and then we can stop when some stop criterion that .",
    "there be different variation here and there mainly differ in the way to computer group similarity base on the individual object similarity . so let 's illustrate how can induce a structure base on just similarity . so start with all the text object and we can then measure the similarity between they . of course base on the provider similarity function and then we can see which pair have the high similarity and then just group they together .",
    "the next one to group .",
    "and then we can gradually group they together in every time we be go to pick the high similarity similarity pair to group . this will give we a binary tree eventually to group everything together .",
    "or we can use the threshold to cut or we can cut at this high level to get just the two cluster . so this be a general idea .",
    "and there be also different way to do that , and there be the three popular method be single link complete link an average link ?",
    "average link define the similarity as average of similarity of all the pair of the two group .",
    "and then we can in general basis on the similarity of the object in the two group .",
    "as long as they be very close order , say the two group be very .",
    "the complete link , on the other hand , will be in some sense pessimistic and by take the similarity of the two farthest appear as the similarity for the two group .",
    "every link be in between , so it take average of all these pair .",
    "now , so it be useful to take a look at their difference and to make comparison . our first single link . can be expect to generate the loose cluster . the reason be becausw . as long as two object be very similar in the two group , it would bring the two group together .",
    "the complete linker be in the opposite situation where we can expect the cluster to be tight .",
    "even the people that be unlikely to talk to each other would be comfortable with talk to each other , so ensure the whole class to be coherent . the average link , of course be in between and group decision , so it be go to be insensitive to outlier . in practice , which one be the good ? well , this will depend on the application and sometimes you need a loose class and to aggressively on cluster object together . then maybe simple english good . but other time you might need a tight cluster , then completely completely well , but in general you have to empirically evaluate these method for your application to know which one be well .",
    "which be call k mean clustering will represent each text object as a term vector and then assume similarity function define onto object .",
    "so this be . this give we the initial tentative classroom .",
    "and then we be go to recovery , compute the centroid base on the allocate object in each cluster .",
    "to adjust the centroid and then we have repeat this process until the similarity base on objective function . in this case it be within cluster sum of square converge an theoretically we can show that this process actually be go to minimize the within cluster sum of square where define objective function . give k cluster . so it can be also show this process will converge to a local minimum . i think about this process for a moment . it might remind you the em algorithm for mixture model .",
    "predator in the em algorithm , so the random inner inner initialization be similar .",
    "and so we be not go to have a probability , but we be go to just put one object into precisely one cluster .",
    "and we be not go to say exactly which distribution have be use to generate the data point . now the next we be go to adjust the centroid , and this be very similar to m step where we re estimate the parameter . that be when we 'll have a well estimate of the parameter . so here we have a well clustering result by adjust the centroid . and note that the central be adjust base on the average of the vector in the . a cluster , so this be also similar to the m step , where we do count pull together counter and normalize they , or the difference of course be also because of the difference in the instep , and we be not go to consider probability when we count the point in this case , for k mean we be go to only count the object allocate to this cluster , and this be only a subset of datum point . but in the em algorithm , we in principle consider all the datum point .",
    "but in nature they be very similar and that be why it be also maximize where define objective function and it be guarantee to convert convert local minimum . so to summarize our discussion of clustering method , we first discuss the model base approach , mainly the mixture model . and here we use be implicitly similarity function . to define the cluster bias , there be no explicit definer similarity function . the model define cluster bias .",
    "complex generative model can be use to discover complex clustering structure . we do not talk about it , but we can easily design generate model to generate a hierarchical cluster .",
    "sometimes we want to do that , but it be very hard to inject the such a explicit definition of similarity into such a model .",
    "but one potential disadvantage be that their object function be not always very clear . the k mean algorithm have a clearly define the objective function , but it be also very similar to a model base approach . the hierarchical clustering algorithm , on the other hand , be ."
  ],
  "6382e23f-d54e-4ece-a231-8df819983fb5": [
    "this lecture be continue discussion of evaluation of textual categorisation . early we have introduce measure that can be use to compute the precision and recall for each category qnd each document . now in this lecture we far \u00a0  examine how to combine the performance on these different category or different document . how do we aggregate they ? how do we take average ? you see on the title here , i indicate it be call a macro average and this be in contrast to micro average that will talk more about that later . so . again , for each category , we can compute the precision recall and f1 so for example , for category c one . we have precision , p1 recall r1 and f value f1 and similarly we can do that for category 2 and all the other category . once we compute that , then we can aggregate they . so for example , we can aggregate all the precision value for all the category to compute the overall precision and this be often very useful . to summarize what we have see in the whole datum set and the aggregation can be do in many different way . again , as i say , in case one unit to aggregate different value . it be always good to think about what be the good way of do the aggregation . for example , you can consider arithmetic mean , which be very commonly use . or you can use geometric mean which would have different behavior depend on the way you aggregate . you might have get different conclusion .",
    "and similar we can do that for recall and f score , so that be how we can then generate the overall precision , recall and f score .",
    "in general , it be beneficial to look at the result from all these perspective , and especially if you compare different method in different dimension . it might reveal which method be well , in which measure or in what situation , and this provide insight for understand the strength of a method or weakness , and this provide further insight for improve they .",
    "so we can compute the overall precision and recall by just count how many case be in true positive , how many case in false positive , etc . basically compute the value to fill in this contingency table and then we can compute precision recall just once . now , in contrast , in macro average we be go to do that for each category 1st and then aggregate over these category . or we do that for each document and then aggregate over all the document . but here we pull they together . now this will be very similar to the classification accuracy that we introduce early , and one problem here of course , be to treat all the instance , all the decision equally .",
    "but it may be appropriate for some application , especially if we associate , for example , the cost for each combination .",
    "so there could be variation of these method that would be more useful , but in general macro average tend to be more informative than micro average just because it might reflect the need for understand performance on each category or performance on each document which be need in many application .",
    "now this be because . categorisation result be sometimes or often indeed pass to human for various purpose . for example , it might be pass to human for far editing . for example , news article can be tentatively categorize by use the system and then human editor would then correct they . and all the email message might be route to the right person for handle in the help desk , and in such a case the categorization do help prioritize the task for a particular customer service person . so in this case , the result have to be prioritize .",
    "so , to summarize , categorization evaluation , first evaluation be always very important for all these task , so get it right . if you do n't get it right , you might get misleading result an you might be mislead to believe one method be well than the other , which be in fact not true . so it be very important to get it right .",
    "so then we would need to consider the difference and design measure appropriately .",
    "sometimes there be tradeoff between multiple aspect , like precision and recall , and then , so we need to know for this application be high recall more important or high precision be more important .",
    "some commonly use measure for relative comparison of different method or the follow classification accuracy be very commonly use for especially balanced tester set .",
    "finally , sometimes ranking may be more appropriate , so be careful . sometimes categorisation , task and maybe well frame as a ranking task and there be machine learning method for optimize rank measure as well ."
  ],
  "6962b043-7dd8-4050-bad0-bbdb13e2c302": [
    "this lecture be about how to use generative probabilistic model for text categorization . there be in general be two kind of approach to text categorization by use machine learning . one be generative problem risk model , the other be discriminative approach . in this lecture , we be go to talk about the generative model . in the next lecture , we be go to talk about discriminative approach . so the problem of text categorization be actually very similar to document clustering in that we assume that each document belong to one category or one cluster . main difference be that in clustering we do n't really know what be the predefined category or what be the cluster . in fact , that be the goal of text clustering . we want to find such cluster in the datum . but in the case of categorization , we be give the category . so we kind of have predefine category and .",
    "but because of the similarity of the two problem , we can actually adapt document clustering model for text categorization . or we can understand how we can use generative model to do text categorization from the perspective of clustering . and so this be a slide that we 've talk about before about text clustering , where we assume there be multiple topic represent by word distribution . each topic be 1 cluster . so once we estimate such model , we face the problem of decide which cluster document d should belong to and this question boil down to decide which thtea i have be use to generate d.",
    "now , how can you compute the probability that particular topic word distribution theta i have be use to generate this document ? in general , we use baye rule to make this inference .",
    "prior information here . that we need to consider if a topic or cluster have a high prior then it be more likely that the document have be from this cluster , so we should favor such a cluster .",
    "and we want to pick a topic that be high by both value . so more specifically , we just multiply they together and then choose which topic have the high product . so more rigorously , this be what we would be do , so we be go to choose the topic that with the maximize this posterior probability of the topic give the document .",
    "but this conditional probability here",
    "and bayes rule allow we to update this probability base on the prior and i show the detail . below here you can see how the prior here be relate to the posterior on the left hand side . and this be relate to how well this word distribution explain the document here , and the two be relate in this way . so to find the topic that have the high posterior probability here , it be equivalent to maximize this product as we have see also multiple time in this course .",
    "an we now can see clearly how we can assign a documentary to a category base on the information about word distribution for these category and the prior on these category . so this idea can be directly adapt to do categorization and this be precisely what naive bayes classifier be do , so here it be mostly the same information , except that we be look at the categorization problem now , so we assume that if",
    "and this be relate to the prior and the likelihood an as you have see on the previous slide .",
    "as you see here now here i change the notation so that we will write down the product as product over all the word in the vocabulary and even if even though the document do n't contain all the word and the product be there accurately represent the product of all the word in the document . because of this count here .",
    "now you may notice that here it involve the product of a lot of small probability and this can cause underflow problem . so one way to solve the problem be to take logarithm of this function , which do n't change the order of these category , but would help we preserve precision and so this be often the . this be often the function that we actually use to score each category , and then we be go to choose the category that have the high score by this function . so this be call a naiyes bayes classifier . now the keyword bayes be understandable because we be apply a bayes rule here . when we go from the posterior probability of the topic to a product of the likelihood and the prior .",
    "but this assumption allow we to simplify the problem , and it be actually quite effective for many text categorization task .",
    "so now the question be , how can we make sure each theta i actually represent category i accurate ?",
    "and here you see that t1 represent the set of document that be know to have be generate from category one , and t2 represent the document that be know to have be generate from category two , etc .",
    "so the estimation problem of course would be simplify , but in general you can imagine what we want to do be to estimate these probability that i mark here and what be the probability that we have to estimate in order to do categorization where there be two kind .",
    "the other kind be word distribution and we want to know what word have high probability for each category .",
    "and in general we can do this separately for different category . that be just because these document be know to be generate from a specific category , so once we know that it be in some sense irrelevant what other category we be also deal with .",
    "and this be the problem that you have see . also several time in this course .",
    "right , so think for a moment that how do you use all these training datum , include all these document that be know to be in these k category .",
    "now , if you have think about it and then you will realize the follow intuition . first , what be the basis for estimate the prior or the probability of each category ? well , this have to do with whether you have observe a lot of document from that category . intuitively , if you have see a lot of document in sport and very few in medical science , then your guess be that the probability of sport category be large or your prior on the category would be large .",
    "and we simply just normalize this count to make this a probability . in other word , we make this probability proportional to the size of training dataset in each category .",
    "now , what about the word distribution ? well , we do the same again . this time we can do this for each category . so let 's say we be consider category i or theta i i. so which word have high probability ? well , we simply count the word occurrence in the document that be know to be generate from theta i.",
    "and then we just normalize these count to make this distribution of all the word make all the probability of all these word sum to one .",
    "now you may notice that we often write down a probability estimate in the form of be proportional to certain number , and this be often sufficient . becausw we have some constraint on these distribution and so the normalizer be dictate by the constraint .",
    "so once you figure out the answer to this question and you will know how to normalize , this count and so this be a good exercise to work on it if it be not obvious to you . there be another issue in naive bayes which be a smoothing . in fact the smoothing be a general problem in all the estimate of language model and this have to do with what would happen if you have observe a small amount of datum . so smoothing be the important technique to address datum sparseness . in our case the training datum set can be small and one datum set be small . when we use maximum likelihood estimator we often face the problem of zero probability . that mean if the event be not observe . then the estimate probability would be 0 in this case if we have not see a word in the training document for , let 's say , category i , then our estimate would be 0 for the probability of this word in this category . and this be generally not accurate . so we have to do smoothing to make sure it be not zero probability .",
    "there be also a third reason , which be sometimes not very obvious , but we 'll explain that in a moment and that be to help achieve discriminative wait of term . and this be also call idf weighting inverse document frequency weighting that you have see in mining word relation .",
    "so one possible way of smooth the probability of category be to simply add small nonnegative constant delta to the count . we pretend that every category have actually some extra number of document represent by delta . and in the denominator we also add k multiply by delta because we want the probability to sum to one . so in total we 've add delta k times because we have k category . therefore in the sum we have to also add k multiply by delta as a total pseudo count that we add to the estimate .",
    "what if delta be zero ? well we just go back to the original estimate base on the observed training datum to estimate the probability of each category . now we can do the same for the word distribution , but in this case we sometimes we find it useful to use a non - uniform pseudo count for the word . so here you see we 'll add pseudocount to each word and that be mu multiply by the probability of the world give by a background language model .",
    "now that background model in general can be estimate by use a large collection of text , or in this case we can use the whole set of all the training datum to estimate this background language model . but if we do n't have to use this one , we can use large text datum that be available from somewhere else .",
    "now , this addition of background model would cause nonuniform smoothing of this word distribution be go to bring the probability of those common word , or to a high level because of the background model . now this help make the difference of the probability of such word small across category .",
    "you also see another smoothing parameter mu here , which control the amount of smoothing , just like delta do for the other probability .",
    "so mu be also non - negative constant and it be empirically set to control smoothing . there be some interesting special case to think about as well . first , let 's think about when mu approach infinity . what would happen ? or in this case , the estimate will approach to the background language model will tend to the background language model , so we would bring every word distribution to the same background language model .",
    "the other special case we think about the background model an suppose we actually set the two uniform distribution and let 's say one over the size of the vocabulary . so each word have the same probability .",
    "so in general , in naiye baye categorization we have to do such smoothing and once we have these probability , then we can compute the score for each category for a document and then choose the category with the high score as we discuss early .",
    "and also to understand why add a background language model will actually achieve the effect of idea of idf weighting and to penalize common word .",
    "ratio of probability , so this be",
    "so this be a score of a document for these two category .",
    "\u00a0 then it mean it be more likely to be in category one , so the large the score be , the more likely the document be in category one .",
    "now , we generally take logarithm of this ratio and to avoid small probability , and this would then give we this formula in the second line .",
    "and if you look at this function , we 'll see it have several part .",
    "so it do n't really depend on the document , it just say which category be more likely and then would . we would then favor this category slightly .",
    "right , so these be the word that be observe in the document , but in general we can consider all the word in the vocabulary . so here we be go to collect evidence about which category be more likely .",
    "and this count of the word serve as a feature and to represent the document .",
    "the second part be the weight of this feature . here it be the weight on each word and this weight .",
    "to what extent observe this word help contribute to our decision to put this document in category one . i remember the high the scoring function be more likely it be in category one . now if you look at this ratio basically or sorry this weight it be basically base on the ratio of the probability of the word from of the two distribution . essentially we be compare the probability of the word from the two distribution and if it be high accord to theta one , then accord to theta 2 then this weight would be positive and therefore it mean when we observe such a word . we 'll say that it be more likely to be from category one , and the more we observe such a word , the more likely the document will be classify as theta one .",
    "so this formula now make a lot of sense , so we be go to aggregate all the evidence from the document . we take a sum over all the word we can call this the feature .",
    "and then finally we have this constant of bias here , so that formula actually be a formula that can be generalize to accommodate more feature . and that be why i 've introduce some other symbol here . so introduce the beta zero to denote the bias and fi to denote each feature , and then beta sub i , denote . the weight on which feature .",
    "and then ouurscore function can be define as a sum of constant beta zero and sum of the feature weight over all the feature ."
  ],
  "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb": [
    "this lecture be about the probabilistic topic model for topic mining and analysis . in this lecture we be go to continue talk about the top mining and analysis . we be go to introduce probabilistic topic model . so this be a slide that you have see early where we discuss the problem with use a term as a topic . so to solve these problem intuitively we need to use more word to describe the topic and this would address the problem of lack of expressive power . when we have more word that we can use to describe the topic , we can describe complicated topic , to address the second problem , we need to introduce weight of word . this would allow you to distinguish subtle difference in topic and to introduce semantically relate word in the fuzzy manner . finally , to solve the problem of word ambiguity , we need to split an ambiguous word so that we can disambiguate its topic .",
    "but now we be go to use a word distribution to describe the topic . so here you see that for sport , we be go to use a word distribution over theoretical speak all the word in our vocabulary .",
    "in general , we can imagine a non zero probability for all the word and some word that be not relevant would have very very small probability and these probability will sum to one . so that it form a distribution of all the word .",
    "you can also see it as a very special case if the probability mass be concentrated entire of just one word . let 's sport , and this basically degenerate to the simple representation of topic with just one word .",
    "\" whereas in science , we see \" \" scientist , \" \" spaceship , telescope or genomic \" \" and new \" science - relate term , now , that do n't mean sport - relate term \u00a0 necessary have zero probability for science in general , we can imagine all these word . we have non zero probability , it be just that for a particular topic of some word we have very very small probability .",
    "it have the high probability for the travel topic 0.05 . but with much small probability for sport and science , which make sense .",
    "and for each word distribution , we know that all the probability should sum to one over all the word in the vocabulary . so you see a constraint here and we still have another constraint on the topic coverage , namely pis . so all the pis of igs must sum to one for the same document .",
    "for analysis , and that mean each word be a unit . now the output would consist of as first a set of topic represent by theta i be each theta_i be a word distribution .",
    "we also want to know the coverage of topic in each document so that that be the same pi_ij 's that we have see before . so give a set of text datum , we would like to compute all these distribution and all these coverage , as you have see on this slide .",
    "so the idea of this approach be actually to 1st design a model for our datum . so we design a probabilistic model to model how the datum be generate . of course this be base on our assumption . the actual datum be n't necessary generate this way , so that would give we a probability distribution of the datum that you be see on this slide give a particular model and parameter that be denote by lambda . so this capital lambda actually consist of all the parameter that we be interested in . and these parameter in general , we control the behavior of the probabilistic model , mean that if you set these parameter for different value , it will give some datum point high probability than other . now in this case , of course , for our tax mining problem , or more precisely topic mining problem , we have the follow parameter . first , we have theta_i 's each be a word distribution and then we have a set of pi 's for each document . and since we have n document so we have n set of pis .",
    "so i leave this as exercise for you to figure out exactly how many parameter there be here .",
    "i just say that depend on the parameter value , some data point will have high probability than other . what we be interested in here be what parameter value will give our datum set the high probability . so i also illustrate the problem with the picture that you see here . on the x axis , i just illustrate the lambda , the parameter as one dimensional variable . it be oversimplification obviously , but it suffice be to show the idea and the y axis show the probability of the datum observe this probability obviously depend on the setting of lambda , so that be why it vary as you change the value of lambda . what we be interested in here be to find the lambda star that would maximize the probability of the observed datum .",
    "it also allow we to assign weight on word so we can model subtle variation of semantic . we talk about the task of topic mining and analysis when we define a topic as a distribution , so the input be a collection of text article . the number of topic and vocabulary set and the output be a set of topic . each be word distribution ."
  ],
  "6a9b1334-5f53-407b-8864-2cff7edbc603": [
    "this lecture be the first one about the text clustering . this be very important that technique for do topic mining an analysis . in particular , in this lecture organ to start with some basic question about the clustering : what be text clustering and why we be interested in text clustering ? in the follow lecture , we be go to talk about how to do text clustering and how to evaluate the clustering result . \" so what be text clustering actually be a very general technique for datum mining . as you might have learn in some other course . the idea be to discover natural structure in the datum .",
    "and then our goal be to group similar texture object together . so let 's see a example here . you do n't really see text object , but i just use some shape to denote object that can be group together .",
    "so we get the three cluster in this case . and then may not be so much disagreement about these three cluster , but it really depend on the perspective to look at the object .",
    "and the problem lie in how to define similarity . what do you mean by similar object ?",
    "have to be clearly define in order to have well define clustering problem . and the problem be in general that any two object can be similar that depend on how you look at they . so for example . let 's look at the two word like car and horse . so be the two word similar",
    "physical property of car and horse . they be very different . but if you look at the they functionally , a car in the horse can both be transportation tool , so in that sense they may be similar . so as you can see , it really depend on our perspective to look at the object and so in order to make the clustering problem well define , a user must define the perspective . for assess similarity .",
    "and when you define a clustering problem , it be important to specify your perspective for similarity or for define the similarity that would be use to group similar object 'cause otherwise .",
    "so let 's look at a concrete example . here you be see some object or some shape that be very similar to what you have see on the 1st slide . but if i ask you to group these object again , you might .",
    "you might think , well , we can still group by shape , so that would give we cluster that look like this . however , you might also feel that . maybe the object can be group base on the size , so that would give we a different way to cluster the datum . if we look at the size and look at the similarity in size . so as you can see clearly here , depend on the perspective will get different clustering result , so that also clearly tell we that in order to evaluate the clustering result we must use perspective . without perspective , it be very hard to define what be the good clustering result .",
    "set up .",
    "we may be able to cluster term in this case . term be object . and cluster of term can be use to define the concept or theme or topic . in fact , the topic model that you have see some previous lecture . can give you cluster of term in some sense . if you take the term with high probability from world distribution .",
    "for example , we might extract all the text segment about the topic , let 's say by use a topic model . now , once we 've get those text object , then we can cluster . the segment that we 've get to discover interesting cluster that might also represent the subtopic .",
    "similarly , we can also cluster article write by the same author , for example .",
    "furthermore , text cluster can also be far cluster . regenerate the hierarchy that that be 'cause we can in general , cluster any text object at different level .",
    "and so a typical scenario be that you be get a lot of text datum . let 's say all the email message from customer in some time period , or all the literature , article , etc . and then you hope to get the sense about what be the overall content of the collection . so , for example , you might be interested in get . a sense about the major topic or what be some typical or representative document in the collection ?",
    "these object might be duplicate content for example , and in that case such a technique can help we remove redundancy , remove duplicate document . sometimes they be about the same topic and by link they together we can have more complete coverage of the topic .",
    "we may also use text clustering to induce additional feature to represent text datum when we cluster document together , we can treat each cluster as a feature and then we can say when a document be in this cluster and then the feature value would be one and if a document be not in this cluster , then the future value be zero and this help provide additional discrimination that might be use for texture classification as we will discuss later ."
  ],
  "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719": [
    "this lecture be a continue discussion of . discriminative classifier for text categorization . so in this lecture will introduce yet another discriminative classifier call a support vector machine or vm , which be a very popular classification method , and there have be also show to be effective for text categorization . so to introduce this classifier , let 's also think about the simple case of two category and we have two public category , season one and season 2 here . an we want to classify document into these two category and we be go to represent again a document by a feature vector x here . now the idea of this classifier be do design . also a linear separator . here that you see and it be very similar to what you have see or just for logistic regression . and we be go to also say that if the sign of this function value be positive , then we be go to say the object be in category 1 . otherwise , we be go to say it be in category 2 , so that make 0 value . the decision boundary between two category .",
    "i show you a simple case of two dimensional space with just x1 and x2 . in this case this correspond to a line that you can see here . so this be .",
    "now this line .",
    "i so let 's just assume that beta one be negative and beta two be positive . now it be interesting to examine then the datum instance on the two side of this line , so here that there be incidence be visualize as circle for one class and diamond for the other class .",
    "so what do you think ? basically work to evaluate its value by use this function .",
    "so what do you think be the sign of this expression ? to examine the sign , we can simply look at this expression . here we can compare this with , let 's say , value on the line . let 's say compare this with this point .",
    "now let 's look at the sign of the coefficient for x2 , where we know this be a positive .",
    "so we know in general for all the point on this side , the . function about it would be positive . and you can also verify all the point on this side would be negative , and so this be how this kind of linear classifier or linear separator can then separate the point in the two category .",
    "and of course there be also line that wo n't separate they , and those be bad line . but the question be when we have multiple line that can separate the both clause , which line be the good ? in fact , you can imagine there be many different way of choose the line .",
    "so the basic idea be to choose the separator .",
    "so what be the margin ? well , i choose . so i 've show some daughter line here to indicate the boundary of those datum point in .",
    "so you can see the margin of this side be as i 've show here . and you can also define the margin on the other side . and in order for the separate to maximize the margin , it have to be kind of in the middle of the two boundary , and you do n't want this separator to be very close to one side .",
    "so this be the basic idea of ecfmg . we be go to choose a linear separator to maximize the margin . now on this slide i 've also change the notation so that i be not go to use beta . do n't know the parameter and , but instead i be go to use w , although w be use to denote the word before . so do n't be confuse here . w here be actually wait set of weight .",
    "so i be also use locate be to denote beta zero , the bias constant . and there be instance do represent as x. and i also use the vector form of multiplication here . so we see transpose of w vector multiply by the feature vector .",
    "an similarly the data instance . here the text object be represent by also a feature vector of the same number of element . xi be future value . for example word count .",
    "well connect the slide with some other reading you might do . ok .",
    "when we maximize the margin of separate , it just mean with the boundary of . the separate be only determine by a few data point , and these be the data point that we call support vector . so here be illustrate to support vector for one class and two for the other class . at this , porter define the margin basically . and you can imagine once we know which be support vector , then this center separate line will be determine by they so .",
    "and you can see if they you change other datum point , it wo n't really affect the margin , so the separate with the stay the same mainly affect by the support vector machine . sorry it be mainly affect by the support vector and that be why it be call a support vector machine .",
    "the next question be of course , how can we set it up to optimize the line ? how can we actually find the line ? or the separator . now this be equivalent to find value for w&b because they would determine where exactly the separator be . so in the simple case , the linear osfm be just a simple optimization problem . so again we let 's recall that our classifier be such a linear separator where we have weight for all the feature and the main goal be to learn these weight w&b. and the classifier will say x be in category one if it be positive . otherwise it be go to say it be in the other category . so this be our assumption or setup . so in the linear be uvm , we be go to then seek these parameter value to optimize the margin and then the training error .",
    "an here we define why i as two value , but these two value be not 01 as you have see before , but rather negative one and positive one and their correspond to these two category as i 've show here . now you might wonder why we do n't define they as zero and one , but instead of have negative 11 and this be purely for mathematical convenience , as you will see in a moment .",
    "now , if , on the other hand , why i be negative one that mean it be in a different class then we want this classifier to give we a very small value . in fact a negative value . and we want this value to be less than or equal to negative one .",
    "why i multiply by the classifier value must be large than or equal to 1 ?",
    "but when yi be negative one you also see a new . this be equivalent to the other inequality , so this one actually capture both constraint in a unified way , and that be a convenient way of capture these constraint . what be our second goal ? that be true . maximize margin , right ? so we want to ensure the separate can do well on the training datum , but then , among all the case where we can separate the datum , we also would like to choose the separate that have the large margin . now the margin can be show to be relate to the magnitude of the weight .",
    "so we 've just assume that we have a constraint for the get the datum on the training set to be classify correctly . now we also have the objective that be tide to maximization of margin and this be simply to maximize sorry to minimize w transpose multiply by w and we often denote this by file w.",
    "so what you see here be very similar to what you have see before , but we have introduce the extra variable . cassie i an we in fact will have one for each data instance and this be go to model the error that will allow for each instance . but the optimization problem will be very similar .",
    "some error to the constraint so that now we allow .",
    "zero , then we allow some error here . in fact , the one ci be very large . the error can be very , very large , so naturally we do n't want this to happen . so we want to then also minimize this ci . so cassie , i need to be minimize in order to control the error . and so as a result in the objective function we also add more to the original 1 , which be only an by basically ensure that we be go to not only minimize the weight , but also minimize the error as you see here , we simply take a sum over all the instance . each one have a ci to model the error allow for that instance an when we combine they together , we basically want to minimize the error on . all of they .",
    "and we do n't really optimize the training error and then see i can be set to a very large value to make the constraint easy to satisfy . that be not very good of course , so see should be set to a non 0 value and a positive value . but when she be settle very , very large value would see the objective function will be dominate mostly by the training error and so the optimization of margin will then play a secondary role . so if that happen , what would happen ? what would happen be then we will try to do our good to minimize the training error . but then we be not go to take care of the margin and that affect the generalization capacity of the classifier for future datum . so it be also not good . so apparently this parameter c have to be actually set .",
    "now with this modification in the problem , be there a quadratic program with linear constraint , so the optimization algorithm can be actually apply to solve this different version of the program ?",
    "so to summarize , the text categorisation method we have introduce many method and some be generative model , some more discriminative method , and these tend to perform similarly when optimize , so there be still no clear winner , although each one have its pro and con , and the performance might also very different datum set for different problem .",
    "be perform similarly on the datum set but with different mistake and so their performance might be similar , but then the mistake that make might be different , so that mean it be useful to compare different method for particular problem and then maybe combine multiple method 'cause this can improve the robustness and they want to make the same mistake so .",
    "most technique that we introduce the use supervise machine learning and which be a very general method . so that mean these method can be actually apply to any text categorization problem as long as we have human to help annotate some training datum set and design feature , then supervise machine learn an all these classifier can be easily apply to those .",
    "the computer of course here be try to optimize the combination of the feature provide by human an . as i say that there be many different way of combine they and they also optimize different object and function .",
    "so as a general rule , and if you can improve the feature representation an and then provide more training datum , then you can generate do well . so performance be often much more affect by the effectiveness of feature and then by the choice of specific classifier . so feature design tend to be more important than the choice of specific classifier .",
    "but",
    "and do some analysis of the categorization problem and try to understand the what kind of feature might help we distinguish category , and in general we can use a lot of domain knowledge to help we design feature .",
    "insight for design new feature .",
    "and there be also other way to ensure the sparsity of the model . mean to recognize the weight . so for example , the svm actually try to minimize the weight on feature , but you can far for some feature to falsely use only a small number of feature .",
    "lda can actually help we reduce the dimension of feature . imagine the word be original feature representation , but the representation can be map to the topic space representation . let 's say we have k topic , so a document can not be represent as a vector of justice k value correspond to the topic . so we can let each topic define one dimension . so we have k dimensional space instead of the original high dimensional space correspond to word . and this be . often another way to learn factor feature , especially , we could also use the category to supervise learning of such low dimensional structure .",
    "deep learning be a new technique that have be develop in machine learning . it be particularly useful for learn representation , so different learning refer to deep neural network . it be another kind of classifier where you can have intermediate feature embed in the model so that it be highly non linear classifier . an some reason advance have allow we to train such a complex network effectively .",
    "it have show some promise and one important advantage of this approach in relationship with the feature design be that they can learn intermediate representation or compound feature automatically , and this be very valuable for learn effective representation for text localization . although in texas domain cause word be excellent representation of text content because these be . human invention for communication and they be generous sufficient for represent content for many task . if there be a need for some new representation , people would have invent a new word and new world . so because of this reason , the value of deep learning for text processing tend to be low than for computer vision and speech recognition , where there be n't correspond wedding design . the word . as feature .",
    "regard the training example , it be generally hard to get a lot of training example because it involve human labor .",
    "we could assume five star review be all positive training example . onstar negative but of course sometimes in five star review . we also mention negative opinion so that rain example be not all of that high quality , but they can still be useful ."
  ],
  "8717e27a-33fb-4d06-ae68-2e0d915b1568": [
    "this lecture be about the generative probabilistic model for text clustering . in this lecture we can do continue discuss text clustering , and we be go to introduce generative probabilistic model as a way to do text clustering so this be the overall plan for cover text clustering in the previous lecture we have talk about what be text clustering and why text clustering be interesting . in this lecture we be go to talk about how to do text clustering , in general , as you see on this slide , there be two kind of approach . one be generate probabilistic model , which be the topic of this lecture , and later will also discuss similarity base approach . so to talk about generative model for text clustering , it would be useful to revisit the topic mining problem use topic model .",
    "each be a word distribution and the other be a pi ij 's and these be the probability that each document cover each topic . so this be a topic coverage and it be also visualize here on this slide you can see that this be what we can get by use a topic model .",
    "in text clustering , however , we only allow a document to cover one topic . if we assume one topic be a cluster .",
    "that mean if we change the topic definition just slightly by assume that each document can only be generate by use precisely one topic .",
    "as show here . so here the output be change so that we no long have the detailed coverage distribution pi ij 's , but instead will have cluster assignment decision . an ci and ci be decision for the document i. and c sub i be go to take a value from one through k to indicate one of the k cluster .",
    "as illustrate here , we no long have multiple topic cover in each document be precisely one topic , although which topic be still uncertain . there be also a connection with the .",
    "and here we hope to estimate a topic model or word distribution base on precisely one document , and that be when we assume that this document cover precisely one topic . but we can also consider some variation of the problem . for example , we can consider there be n document , each cover different topic . so that be n document and topic . of course , in this case these document be independent and these topic also independent . but we can far allow these document share topic and then . we can also assume that we be go to assume there be few topic . the number of document . so this document must share some topic .",
    "so because of these connection , naturally we can think about how to use a probabilistic generating model to solve the problem of text clustering .",
    "as in all case of design a generative model , we hope the generative model would adopt the output that we hope to generate , or the structure that we hope to model . so in this case it be a clustering structure . the topic and each document that cover one topic , and we hope to embed such such preference in a generative model .",
    "as in the topic model .",
    "and we generate each word first make a choice between these distribution with decide to use one of they with probability .",
    "now we first make this decision regard which distribution should be use to generate the world , and then we be go to use this distribution to sample word . now . notice that in such a generative model .",
    "that mean the word in the document could have be generate in general from multiple distribution . now this be not what we want to see for text clustering . for document clustering where we hope this document will be generate from precisely one topic .",
    "if we really have one topic to correspond to one cluster of document , then we would have a document to be generate from precisely one topic . that mean all the word in the document must have be generate from precisely one distribution , and this be not true for such a topic model that we be see here , and that be why this can not be use for clustering because it do not ensure that only one distribution have be use to generate . all the word in one document .",
    "but this time , once we have make the decision to choose one of the topic , we be go to stay with this distribution to generate the all the word in the document .",
    "so in other word , we only make the choice once . for all .",
    "similarly , if i have choose the second distribution , theta sub two here , you can see we will stay with this one and then generate the entire document d.",
    "of use a particular distribution be make of just once for this document . in the case of document clustering . but in the case of topic model we have to make as many decision as the number of word in the document because for each word we can make a potential different decision and that be the key difference between the two model .",
    "model that will give we a probability of a document .",
    "and of course , the main problem in document clustering be to infer . which distribution have be use to generator a document and that would allow we to recover the cluster identity over document",
    "there be many . two difference . one be the choice of . use a particular distribution be make just once for document clustering model , whereas in the topic model it be make multiple time .",
    "but in the case of topic modeling , one distribution do n't have to generate with all the word in a document . multiple distribution could have be use to generate the word in the document .",
    "now that just mean we have no uncertainty . we just stick with one particular distribution . now in that case , clearly we will see this be no long mixture model 'cause there be no certainty here and we be go to just use precise one of the distribution for generate a document , and we be go back to the case of estimate one word distribution base on one document .",
    "but now you can see more clearly . so as more case of use a generative model to solve a problem , we first look at theta and then think about how to design the model . but once we design model , the next step be to write down the likelihood function .",
    "so in this case what be the likelihood function or it be go to be very similar to what we have see before in topic model , but it will be also different . if you still recall what the likelihood function look like in plsa , then you realize that in general the probability of observe a theta point from mixture model be go to be a sum over all the possibility of generate the datum . i in this case , so it be go to be some over these k topic because everyone can be use to generate the document and then inside the sum you can still recall what the formula look like an it be go to be .",
    "so if you be map , this formula be kind of formula to our problem . here you will see the probability of observe a document d be basically a sum , in this case over two different distribution . because we have a very simplified situation of just two cluster .",
    "choose the world distribution . be theta one or theta two right ? and then it be this probability be multiply by the probability of observe this document from this particular distribution .",
    "so this form should be very similar to the topic model , but it be also useful to think about the difference and for that purpose i be also copy the probability of .",
    "but there be also some difference .",
    "and that be correspond to our assumption of 1st make a choice of choose one distribution and then stay with this distribution with all the word . and that be why we have the product inside the sum . the sum correspond to the choice ."
  ],
  "8eaf2971-31ff-40b7-9fc6-b91c7637f916": [
    "this lecture be about a mixture of unigram language model . in this lecture we will continue discuss probabilistic topic model . in particular , we be go to introduce a mixture of unigram language model . this be a slide that you have see early where we talk about how to get rid of the background word that we have on top of estimate language model for one document . so if you want to solve the problem . it will be useful to think about why we end up have this problem . well , this be obviously because these word be very frequent in our datum and we be use a maximum likelihood estimate and then the estimator obviously would have to assign high probability for these word in order to maximize the likelihood . so in order to get rid of they , that would mean we have to do something different here . in particular , we have to say that this distribution do n't have to explain all the word in the text datum , or we be go to say these common word should not be explain by this distribution .",
    "this way the two distribution can be mix together to generate the text datum and will let the other model which we call background topic model to generate the common word . this way our target be the topic theta here would be only generate the content word that characterize the content of the document .",
    "so this be the probability of select the topic word distribution . this be the probability of select the background word distribution denote by theta sub b.",
    "and we be go to use the background word distribution to generate the word .",
    "so now let 's see . in this case , what be the probability of observe the word w ? now here i show some word like the \" and \" \" text \" \" , so as in all case , once we \" set up the model , we be interested in compute the likelihood function . the basic question be , so what be the probability of observe a specific word here ? now we know that the word can be observe from each of the two distribution , so we have to consider 2 case . therefore it be a sum over these two case .",
    "now obviously the probability of text the same be all similar , right ? so we also consider two way of generate text , and each case be a product of the probability of choose a particular word distribution multiply by the probability of observe the word from that distribution .",
    "and in each case it be a product of the probability of select that component model . multiply by the probability of actually observe the data point from that component model , and this be something quite general and you will see this occurring often later .",
    "but the way that determine this probability be quite different from when we have just one distribution .",
    "so as i just say , we can treat this as just a generative model and it be often useful to think of just the likelihood function . the illustration that you have see before , which be dimmer now be just the illustration of this generation model . so mathematically , this model . this be nothing but to just define the follow generative model where the probability of word be assume to be a sum over 2 case of generate the word . the form you be see now be more general form than . what you have see in the calculation early . i just use a simple w to denote any word , but you can still see . this be basically the first sum . like",
    "and the two term be , first , the probability of select a component like theta sub d of the second , the probability of actually observe the word from this component model . and so this be a very general description of , in fact , all the mixture model .",
    "so now once we set up the model and we can write down the likelihood function as we see here , the next question be how can we estimate the parameter or what to do with the parameter give the datum ? well , in general we can use some observe text datum to estimate the model parameter and this mission would allow we to discover the interesting knowledge about the text , so in this case , what do we discover ? well , these be represent by our parameter , and we have two kind of parameter . one be the two word distribution . those be two topic and the other be the coverage of each topic in each .",
    "it will then degenerate to the special case of just one distribution right so you can easily verify that by assume one of these two be 1.0 and the other be 0 .",
    "so to summarize , and we talk about the mixture of two unigram language model .",
    "and the model be a mixture model with two component : two unigram language model . specifically , theta sub d which be intend to denote the topic of document d and theta sub b which be represent a background topic that we can set to attract the common word .",
    "so the parameter can be collectively call a lambda , which i show here again , and you can again think about the question about how many parameter be we talk about exactly . this be usually good exercise to do because it allow you to see the model index and to have a complete understanding of what be go on in this model and we have mix weight of course also .",
    "but now we have this sum because of the mixture model and because of the mixed model we also have to introduce the probability of choose that particular component distribution ."
  ],
  "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee": [
    "this lecture be about the expectation maximization algorithm , also call the em algorithm . in this lecture , we be go to continue the discussion of probabilistic topic model . in particular , we be go to introduce the em algorithm , which be a family of useful algorithm for compute the maximum likelihood estimate of mixture model . so this be now familiar scenario of use a two component mixture model to try to factor out the background word from one topic word distribution here . so we be interested in compute this estimate . and we be go to try to adjust these probability value to maximize the probability of the observed document , and know that we assume that all the other parameter be know . so the only thing unknown be this word probability give by theta sub d update . and in this lecture be go to look into how to compute this maximum likelihood estimator .",
    "but suppose we actually know which word be from which distribution , so that would mean , for example these word : the be and we be know to be from this background word distribution .",
    "if we already know how to separate these word , then the problem of estimate the world distribution would be extremely simple , right ? if you think about this for a moment , you realize that well we can simply take all these word that be know to be from this word distribution theta sub d and normalize they . so indeed this problem will be very easy to solve . if we have know which word be from which distribution precisely .",
    "make this model no long mixture model because we can already observe which distribution have be use to generate which part of the datum , so we actually go back to the single word distribution problem , and in this case let 's call these word that be know to be from theta d pseudo document d prime and then all we need to do be just normalize these word count for each word w sub i.",
    "this idea , however , do n't work , because we in practice do n't really know which word be from which distribution . but this give we the idea of perhaps we can guess which word be from which . distribution .",
    "so let 's assume that we actually know tentative probability for these word in theta sub d.",
    "and now let 's consider word like a text . so the question be , do you think text be more likely have be having be generate from theta sub d or from theta sub b ?",
    "now , this inference process be a typical bayesian inference situation where we have some prior about these two distribution so can you see what be our prior here ? well the prior here be the probability of each distribution , right ? so the prior be give by these two probability . in this case the prior be",
    "so this be call prior because this be our guess of which distribution have be use to generate the world before we even observe the word . so that be why we call it prior .",
    "they be equally likely . alright , so it be just a flip a coin . now in bayesian inference , we typically then would update our belief after we have observe evidence . so what be evidence here ? while the evidence here be the word text . now that we be interested in the word text , so text can be regard as evidence .",
    "so intuitively , what would be your guess ? so in this case",
    "text have a much high probability here . \" by the background model , which have a very small probability .",
    "we be go do tend to guess the distribution that give the word high probability and this be likely to maximize the likelihood right so .",
    "of the word give by each distribution . but our guess must also be affect by the prior , so we also need to compare these two prior why ? because imagine if we adjust these probability , we be go to say the probability of choose a background model be almost 100 % .",
    "the prior be very high . so in the end we have to combine the two and the bayse formula provide provide we a solid and principle way of make these kind of guess to quantify that .",
    "\" the select , so we have the selection probability here , and secondly , we also have to actually have observe text from the distribution . so when we multiply the two together , we get the probability that text have in \" fact be generate from similarly , for the background model an . the probability of generate text be another product of similar form .",
    "\" theta sub d \" it be from the background so now we have the probability that text be generate from each . then we simply we can simply normalize they to have estimate of the probability that the word text be from \" theta sub d",
    "so this be application of bayes rule . but this step be very crucial for understand the em algorithm ."
  ],
  "95f92696-1963-4307-83c6-a8370ff03b30": [
    "this lecture be about the contextual text mining . contextual text mining be relate to multiple kind of knowledge that we mine from text datum . as i be show here , be relate to topic mining because can make topic associate with context , like a time or location , and similarly it can make opinion mining more contextualize , make opinion connect to context . it be relate to text base prediction because it allow we to combine non text datum with text datum to derive sophisticated predictor for the prediction problem . so more specifically , why be we interested in contextual text mining ? well that be , first , because text often have rich context information and this can include direct context such as meta datum . and also interact context , so the direct context can include the matter such as time , location , author , and source of the text datum .",
    "an indirect text context refer to additional datum relate to the meta datum . so , for example , from author , we can far obtain additional context , such as social network of the author or the author 's age and such information be not , in general , directly relate to the text yet through the author we can connect they . there could be also other text datum from the same source as this one , so the other context datum can be connect with this text , as well . so in general , any related datum can be regard as context , so there could be remotely relate to context . context .",
    "it also in general provide meaning to the discovery topic if we gon na associate the text with context .",
    "can be regard as interesting way of partitioning of text datum . so here i just show some research paper publish in different year .",
    "now , such text datum can be partition , many interesting way because we have context . so the context here just include time and the conference venue .",
    "but let 's see how we can partition datum in interesting way . first , we can treat each paper as a separate unit . so in this case a paper id and each paper have its own context , it be independent . and . but we can also treat all the paper write in 1998 as one group , and this be only possible because of the availability of time and we can partition datum in this way . this would allow we to compare topic , for example in different year .",
    "we can also partition the datum to obtain the paper write by author in the us , and that of course use additional context . of the author and this would allow we to then compare such a subset with another set of paper write by author in other country .",
    "and in particular , we can compare different context , and this often give we a lot of useful knowledge . for example , compare topic overtime , we can see trend of topic and compare topic in different context can also reveal difference about the two context . so there be many interesting question that require contextual text mining here , i list some very specific one . for example , what topic have be gain increase attention recently in data mining research ? now to answer this question , obviously we need to analyze text in the context of time . so time be a context in this case .",
    "what be the common research interest of two researcher ? in this case , author can be the context . be there any difference in the research topic publish by author in the usa and those outside ? now , in this case , the context would include the author and their affiliation and location .",
    "be there any difference in the opinion about the topic express on one social network and another ? in this case , the social network of author and the topic can be the context .",
    "what issue matter in the 2012 presidential campaign or presidential election ?"
  ],
  "9a443634-7f2e-4d3a-9ccd-0f1b6604c939": [
    "in this lecture , we continue discuss paradigmatic relation discovery . early , we introduce a method call expect overlap of word in context . in this method , we represent each context by a word vector that represent the probability of word in the context and we measure the similarity by use the dot product . which can be interpret as the probability that to randomly pick the word from the two context be identical , we also discuss the two problem of this method . the first be that it favor match one frequent term very well over match more distinct term . it put too much emphasis on match one term very well . the second be that it treat every word equally . even a common word like ' the ' would contribute equally as content word like ' eat ' .",
    "so to address the first problem , we can use a sub linear transformation of term frequency . that is , we do n't have to use the raw frequency count of term to represent the context . we can transform it into some form that would n't emphasize so much on the raw frequency . to address the second problem , we can put more weight on rare term . that is , we can reward match a rare word and this heuristic be call idf term weight in text retrieval . idf stand for inverse document frequency .",
    "and so that will be denote by tf of w&d as show in the y axis . now in general there be many way to map that , and let 's first look at the simple way of mapping . in this case , we be go to say , any non zero count will be map to one .",
    "and that create the problem that we just talk about . namely it answer too much on just match one frequent term . match one frequent term can contribute a lot .",
    "and they generally form a sub linear transformation . so for example , one possibility be to take logarithm of the raw count , and this will give we curve that look like this , right ? that you be see here .",
    "and this be what we want , because it prevent that kind of term from dominate the scoring function .",
    "look like this . i see it be ( k + 1 ) * x / ( x + k ) where k be a parameter . x be the count , the raw count of word .",
    "and it also be interesting that it have upper bind",
    "so this put a very strict constraint on high frequency term , because their weight would never exceed k+1 . as we vary k , if we can simulate the two extreme . so one case be set to zero . we roughly have the 01 vector .",
    "so this transformation function be by far the most effective transformation function for text retrieval , and it also make sense for our problem set up . so we just talk about how to solve the problem of over emphasize a frequently frequent term . now let 's look at the second problem , and that be how we can penalize popular term . match ' the ' be not surprising because ' the ' occur everywhere , but match ' eat ' with account alot . so how can we address that problem . in this case we can use the idf weighting ... that be commonly use in retrieval . \u00a0  idhfor stand for inverse document frequency . document frequency mean the count of the total number of document that contain a particular word .",
    "or document frequency .",
    "the idf function be give a high value for a low k , mean that it reward a rare term .",
    "so that be a very rare term .",
    "the low value you can see here be when k reach its maximum , which would be m.",
    "close to 0 in fact . right so this . this of course measure be use in search where we naturally have a collection .",
    "so how can we add these heuristic to improve our .....",
    "so here we define in this case we define the document vector .",
    "so in this normalization function we see we take sum over some of all the word and then we normalize the weight of each word by the sum of the weight of all the word .",
    "now the weight of bm25 for each word be define here .",
    "on this one , right ? so we only have this one and the document length or the total count of word in that context document .",
    "first , of course , this extra occurrence of this count be just to achieve the sub linear normalization . but we also see we introduce the parameter k here . and this parameter be generally non negetive number , although zero be also possible . this control the upper bind and the kind control can choose to what extent be simulate the linear transformation .",
    "and , and in this case the normalizing formula have average document length here .",
    "so this average document will be a constant for any give collection , so it actually be only affect the effect of the parameter b here . because this be a constant .",
    "affect the lenngth formalization . together with parameter b. now with this definition , then we have a new way to define our document vector and we can compute the vector d2 in the same way . the difference be that the high frequency term will now have a somewhat low weight and this would help control the influence of these high frequency term .",
    "and the xi and yi probability of",
    "so let 's take a look at the term vector in more detail here . and we have each xi , define as a normalize weight of bm 25 .",
    "but we ca n't just say any frequent term in the context that would be correlate with the candidate word .",
    "but if we apply idf weighting as you see here , we can then",
    "so for this reason , the highly weight term in this idf weight vector .",
    "but it clearly show the relation between discover the two relation . and indeed they can be discuss , discover in a joint manner by leverage such association .",
    "the main idea for discover paradigmatic relation be to collect the context of a candidate word to form a pseudo document , and this be typically represent as a bag of word . and then compute the similarity of the corresponding context document of two candidate word .",
    "these be the word that share similar context .",
    "and more specifically , we talk about use text retrieval model to help we design effective similarity function to compute the paradigmatic relation ."
  ],
  "9e5a0c5f-ff4a-42a6-b37b-f43628632860": [
    "this lecture be about the syntagmatic relation discovery and mutual information . in this lecture , we be go to continue discuss syntagmatic relation discovery . in particular , we be go to talk about another concept , the information theory , call mutual information . and how it can be use to discover syntagmatic relation ? before we talk about a problem of conditional entropy , and that be the conditional entropy compute on different pair of word be not really comparable , so that make it hard to discover strong syntagmatic relation globally from corpus . so now we be go to introduce mutual information , which be another concept in information theory that allow we to , in some sense , normalize the conditional entropy to make . a more comparable across different pair .",
    "so mathematically , it can be define as the difference between the original entropy of x and the conditional entropy of x give y.",
    "normally the two conditional entropy h(x|y ) and h(y|x ) be not equal , but interestingly \u00a0 but interestingly , the reduction of entropy . by know one of they be actually equal , so this quantity be call mutual information denote by i here and this function have some interesting property . first , it be also non negative . this be easy to understand becausw the original entropy be always not go to be low than the possibly reduce the conditional entropy . in other word , the conditional entropy would never exceed the original entropy . know some information can always help we potentially , but wo n't hurt we in predict x.",
    "the third property be that ?",
    "and this last property can be verify by simply look at the equation above .",
    "now when we fix x to rank different ys use conditional entropy would give the same order as rank base on mutual information , because in the function here h of x be fix because x be fix . so rank base on mutual information be exactly the same as rank base on the conditional entropy of x give y.",
    "so let 's examine they intuition of use mutual information for syntagmatic relation mining .",
    "so this question can be frame as a mutual information question , that is , which be have high mutual information with eat . so we be go to compute the mutual information between eat and other word .",
    "the mutual information between east and the . because know the do n't really help we predict eat . similarly know eat do n't help we predict the as well .",
    "so because in this case the reduction be maximum because know one would allow the predict the other completely so the conditional entropy be zero . therefore the mutual information reach its maximum . it be go to be large than or equal to the machine between it be an another word .",
    "so now let 's think about how to compute the mutual information . now , in order to do that , we often .",
    "now if you look at the formula , it be also sum over many combination of different value of the two random variable , but inside the sum mainly we be do a comparison between 2 joint distribution . the numerator have the joint actual observe . join the distribution of the two random variable . the bottom part of the denominator can be interpret as the expect joint distribution of the two random variable . if there be independent .",
    "so this comparison would tell we whether the two variable be indeed independent if there indeed independent , then we would expect that the two be the same .",
    "the sum be simply to take into consideration of all the combination of the value of these two random variable . in our case , each random variable can choose one of the two value 0 or 1 , so we have four combination here .",
    "so now let 's far look at the what be exactly the probability involve in this formula of mutual information .",
    "so for w1 , we have two probability show here .",
    "and similarly for the second word , we also have two probability represent presence or absence of this word , and there be something one as well . and then finally we have a lot of joint probability that represent the scenario of co - occurrence of the two word .",
    "right , so this sum to 1 cause . the two word can only have these four possible scenario . either they both occur .",
    "in these two case , one of the random variable will be equal to 1 and the other would be 0 . and finally we have the scenario when none of they occur . so this be when the two variable take a value of 0 .",
    "here . once we know how to calculate these probability , we can easily calculate the mutual information . it be also interesting to note that there be after some relation or constraint among these probability , and we already see two of they , so the in the previous slide that you have see that the marginal probability of these word sum to one , and we also have see this constraint that say the two word can only have these four different scenario of co occurrence , but we also have some additional constraint list in the bottom .",
    "and this capture the second scenario . when the seond word be not observe , so we only see the first word .",
    "now these equation allow we to compute some probability base on other probability . and this can simplify the computation .",
    "an so we this will take care of the computation of these probability of presence or absence of each word . now let 's look at their joint distribution , right ? let 's assume that we also have available probability that they occur together .",
    "specifically , for example , use this equation , we can compute the probability that the first word occur and the second word do not , because we know these probability in the box .",
    "and then finally we . this probability can be calculate by use this equation , because now this be know and this be also know and this be already know right ?"
  ],
  "a97a9d5e-48b7-4f4e-9754-4c5b30a31424": [
    "this lecture be about text base \" in this lecture we be go to start talk about mine a different kind of knowledge as you , you can see here on this slide . namely , we be go to use text datum to infer value of some other variable in the real world . that may not be directly relate to the text , or only remotely relate to text so this be very different from content analysis or topic mining where we directly characterize the content of text . \" it be also different from opinion mining or sentiment analysis , which still have to do with characterize mostly the content only that we focus more on the subjective which reflect what we know about the opinion holder .",
    "it would be useful to first take a look at the big picture of prediction in data mining in general and i call this \"",
    "for example , someone 's health condition or the weather , or etc . so these variable would be important because we might want to act on that . we might want to make decision base on that .",
    "because we in general should treat all the datum that we collect . in such a prediction problem set up , we we be very much interested in joint mining of non text and text datum . we should mine all the datum together .",
    "so this then allow we to change the world and so this basically be the general process for make a prediction base on datum include text datum .",
    "especially because of the involvement of text datum . and so human first would be involve in the mining of the datum . it will control the generation of these feature . and also help we understand the text datum because text datum be create to be consume by human . human be the good in consume or interpret text datum .",
    "sometimes machine can see pattern in a lot of datum that human may not see , but in general human would play an important role in analyze text datum in all application . next human also must be involve in predictive model building and adjust or testing . so in particular we will have a lot of domain knowledge about the problem of prediction that we can build into this predictive model , and then next , of course , when we have predict value for the variable , then human would be involve in take action to change the world or make decision base on these predictive value .",
    "and , this be so that we can adjust the sensor to collect the most useful datum for prediction .",
    "so it be useful to keep that in mind while we be look at some text mining technique . \"",
    "... this question have be address to some extent in some previous lecture where we talk about what kind of feature we can design for text datum . it have also be address to some extent by talk about the other knowledge that we can mine from text . so for example , topic mining can be very useful to generate the pattern or topic base indicator or predictor that can be far feed into a predictive model . so topic can be intermediate representation of text . that would allow we to design high level feature or predictor that be useful for prediction of some other variable . it maybe , although it be generate from original text datum , it provide a much well representation of the problem and it serve as more effective predictor .",
    "the other question be how can we join mine text and non text datum together ? now this be a question that we have not address yet . so in this lecture and the follow lecture be go to address this problem because this be where we can generate the much more enriched feature for prediction and allow we to review a lot of interesting knowledge about the world . these pattern that be generate from text and non text datum themselves can sometimes already be useful for prediction , but when they be put together with many other predictor they can really help improve the accuracy of prediction .",
    "the goal here be mainly to infer value of real world variable .",
    "and the other could be to mine knowledge about the observer so sentiment analysis or opinion analysis and both can help provide predictor for the prediction problem .",
    "in one perspective , we can see non text datum can help text mining .",
    "now the other perspective be text datum can help , but not text their mining as well .",
    "and this technique be call pattern annotation . and , you can see this reference list here for more detail ."
  ],
  "aac4e33c-97bb-46c8-a108-3e3e3322a85c": [
    "this lecture be about the method for text categorization . so in this lecture be go to discuss how to do text categorization . 1st . there be many method for text categorization in such a method , the idea be to determine the category base on some rule that we design carefully to reflect the domain knowledge about the categorization problem . so , for example , if you want to do topical categorisation for news article , you can say if the news article mention word like game and sport three time that we be go to say it be about sport . thing like that and this would allow we to deterministically decide which category a document should be put into . now such a strategy would work well if the follow condition hold . first , the category must be very well define , and this allow the person to clearly decide the category base on some clear rule .",
    "for example , if there be some special vocabulary that be know to only occur in a particular category , and that would be most effective because we can easily use such a vocabulary or pattern of such a vocabulary to recognize this category .",
    "for design these rule and so if that be the case , then such a method can be effective , and so it do have a provision in some domain and sometimes . however in general there be several problem with this approach . first , of course it be labor intensive . it require a lot of manual work . obviously we ca n't do this for all kind of categorization problem . we have to do it",
    "\u00a0 secondly , it can not handle uncertainty in rule . often the rule be n't 100 % reliable take for example , and look at the occurrence of word in text and try to decide the topic .",
    "but that may not be exactly about the sport , or only marginally touch sport . the main topic could be another topic , different topic then sport .",
    "problem with this approach , and it turn out that the both problem can be solve or alleviate by use machine learning .",
    "and this be call a training datum . and then secondly the human expert also need to provide a set of feature to represent each text object that can potentially provide a clue about the category . so we need to provide some basic feature for the computer to look into . and in the case of text , natural choice would be the word . so use each word as a feature be a very common choice to start with . but of course there be other sophisticated feature like a phrase or even policy feature tag or even syntactic structure . so once human expert can provide this , then we can use machine learn to learn soft rule for categorization from the training datum . so soft rule just mean we be go to still decide which category should be assign to the document . but it be not go to be use use a rule that be deterministic , so we might use something similar to say that if it match game sport many time , it be likely to be a sport . but we be not go to say exactly for sure , but instead we be go to use probability or weight so that we can combine multiple evidence , so the learning process basically be go to figure out which feature be most useful for separate different category .",
    "to learn a classifier to map a value of x into a map of y. so here x be all the text object . and y be all the category a set of category , so the classifier would take any value in x as input and we generate the value in y as output , and we hope the output y would be the right category for x , and here correct of course be judge base on the training datum , so that be the general goal , like in all the machine learn problem or supervise learning problem where you be give some example of input and output for function and then the computer be go to figure out how the function behave like base on these example and then try to be able to compute the value for future access that we have not see .",
    "and they will also combine multiple feature in a weighted matter with weight to be optimize to minimize the error on the training datum . so ultimately , the learning process optimization problem and the objective function be often tide to the error on the training datum .",
    "they also tend to vary in their way of combine the feature , so linear combination for example be simple be often use . but they be not as powerful as non linear combination , but nonlinear model might be more complex for training . so there be tradeoff as well , but that would lead to different variation of .",
    "so in general , we can distinguish the two kind of classifier at a high level one be go to generative classifier . the other be call discriminative classifier . the generative classifier try to learn what the datum look like in each category .",
    "and , this can then be factor out to a product of y. the distribution of label and join the probability of sorry the conditional probability of x give y so it be y. so we first model distribution of label and then we model how the datum be generate give a particular label here .",
    "and the label distribution here by use the base rule .",
    "so in such approach , the objective function be actually likelihood , so we model how the datum be generate , so only thus it only indirectly capture the training error .",
    "the other kind of approach be call discriminative classifier . these classifier try to learn what feature separate category , so they directly tackle the problem of categorisation or separation of class .",
    "so these discriminative classifier attempt to model the .",
    "probability of the label give the data point directly ."
  ],
  "aca8d826-412b-4134-8d2c-87537fdc4a76": [
    "so look at the text mining problem more closely , we see that the problem be similar to general datum mining , except that we 'll be focus more on text datum . and we be go to have text mining algorithm to help we to turn text datum into actionable knowledge that we can use in ( the ) real world . especially for decision making or for complete whatever task that require text datum to support , now because in general in many real world problem of datum mining , we also tend to have other kind of datum that be non textual . so a more general picture would be to include non text datum as well . and for this reason , we might be concern with joint mining of text and non text datum and so in this course we be go to focus more on text mining . but we can also touch how to join the analysis of both text datum and non - text datum . with this problem definition we can now look at the landscape of the topic in text mining analytic .",
    "most specifically , human sensor or human observer would look at the world from some perspective .",
    "the same person at a different time might also pay attention to different aspect of the observed world . and so the human sensor would perceive the world from some perspective . and that human ... the sensor would then form a view of the world and that can be call the observed world . of course this would be different from the real world because of the perspective that the person have take . this can often be bias also .",
    "of course , the person could have use a different language to express what he or she have observe . in that case , we might have text datum of mixed language for different language .",
    "and we hope to be able to uncover some aspect in this process .",
    "and that mean by look at text datum in english , we may be able to discover something about english ... some usage of english ...",
    "so this be 1 type of mining problem where the result be some knowledge about language which may be useful in various way . if you look at the picture , we can also \" then mine knowledge about the \" \" observe \" \" world \" \" . \"",
    "we be go to look at the what the text datum be about and then try to get the essence of it . or extract high quality information about a particular aspect of the world that we be interested in . for example , everything that have be say about a particular person or particular entity , and this can be regard as mining content to describe the observed world in the user 's mind in the person 's mind .",
    "and these property could include the mood of the person or sentiment of the person .",
    "so this picture basically cover multiple type of knowledge that we can mine from text in general .",
    "and that be why here we show that the result of some other mining task , include mine the content of text datum and mining knowledge above the observer can all be very helpful for prediction .",
    "and of course , it depend on the problem .",
    "stock price or change of stock price base on discussion in the news article or in social medium , then this be an example of use text datum to predict some other real world variable . now in this case , obviously the historical stock price datum would be very important for this prediction , and so that be example of non - text datum that would be very useful for the prediction and we can combine both kind of datum to make the prediction .",
    "when we look at the text datum alone will be mostly look at the content and opinion express in text .",
    "for example , the time , the location , of that associate with the text datum and these be useful context information .",
    "now we can analyze text datum in each time period and then make a comparison .",
    "in this course we be go to selectively cover some of those topic .",
    "first , we be go to cover natural language processing very briefly because this have to do with understand text datum , and this determine how we can represent text for text mining . second , we be go to talk about how to mine word association from text datum and word association be a form of useful lexical knowledge about a language . third , we be go to talk about the topic mining and analysis , and this be only one way to analyze content of text , but it be a very useful way of analyze content . it be also one of the most useful technique in text mining ."
  ],
  "b1854d1c-3199-4c42-ab7d-f219f70259a3": [
    "this lecture be about the latent aspect rating analysis or opinion mining and sentiment analysis . in this lecture , we be go to continue discuss opinion mining and sentiment analysis . in particular , we be go to introduce . late in the aspect of rating analysis , which allow we to perform detailed analysis of review with overall rating . first , motivation . here be two review that you often see on the internet about the hotel and you see some overall rating . in this case , both reviewer have give five star . and of course there be also review that be in text . now , if you just look at these review , it be not very clear whether a hotel be good for its location or for its service , and it be also unclear why be . if you be like this hotel .",
    "into rating on different aspect such as value , room , location and service . so if we can decompose overrate two rating on these different aspect . then we can obtain more detailed understanding of the reviewer opinion about the hotel . and this would also allow we to rank hotel along different dimension , such as valuable room , but in general such detailed understanding would reveal more information about the user , preference , review , preference and also we can understand well how reviewer view this hotel from different perspective . now , not only do we want to .",
    "but other might care more about service and therefore they might place more weight on service then value .",
    "and this be a problem call latent aspect rating analysis .",
    "an we hope to generate the three thing . one be the major aspect comment on in the review .",
    "and 3rd be the relative weight place on different aspect by the reviewer , and this task have a lot of application . if we can do this and we would enable a lot of application , i just list some here and later . i will show you some result . and for example , we can do opinion base and the ranking . we can generate a aspect level opinion summary . we can also analyze reviewer preference , compare they or compare their preference on different hotel .",
    "so of course the question be how can we solve this problem ?",
    "so with this we would be able to obtain aspect segment . in particular , we be go to obtain the count of all the word in each segment , and this be denote by c supply of wnd . this can be do by use seed word like location and room .",
    "in the segmentation stage , which be call latent rating regression , we be go to use these word and their frequency in different aspect to predict the overall rating , and this prediction happen in two stage . in the first stage , we be go to use the sentiment weight of these word in each aspect to predict the aspect rating . so , for example , if in the discussion of location use a word like amazing mention many time and it have a high weight .",
    "so we have for each aspect a set of sentiment weight .",
    "in the second stage , or in a second step , we be go to assume that the overall rating be simply weight combination of these aspect rating . so we be go to assume we have aspect weight in order by of r sub of d. and this would be use to take a weighted average of the aspect rating , which be denote by our supply of the .",
    "so this setup allow we to predict the overall rating base on the observe word frequency . so on the left side you will see all these observed information , the art , the and the count . but on the right side you see all the information that we be interested in be actually latent .",
    "now this be a typical case of generate model where we would embed the interesting variable in the generating model . and then we be go to set up a generation probability for the overall rating give the observe word .",
    "and so we have see such case before in , for example , plsa , where we predict the text datum . but here we predict the rating and the parameter of course be also very different . but if you can see if we can uncover these parameter , that would be nice because also r of d be precisely the aspect rating that we want to get , and these be decomposer rating on different aspect of our sub id be precisely the aspect weight that we hope to get .",
    "they think we be model . here be a set of review document with overall rating . and each review document denote by at and overall rating be denote by r sub d and these pre segment into k as their segment and we be go to use c sub i of r and to denote the count of world w in aspect segment i. of course it be zero if the world do n't occur in the segment .",
    "now of course , this be just what our assumption in the actual reading be not necessary generate this way . but as always when we make this assumption , we have a formal way to model the problem , and that allow we to compute interesting quantity . in this case , the aspect rating and aspect of weight .",
    "so .",
    "now this alpha",
    "a set of other value from this multivariate gaussian prior distribution and once we get these alpha value be go to use , then the weighted average of aspect rating as the mean here to use the normal distribution .",
    "now the aspect rating as i just say be the sum of the sentiment weight of word in their spectrum . note that here the sentiment weight be specifically to aspect , so beta be index by i. and as for aspect .",
    "this be neither because of the same word might have positive sentiment for once back , but negative sentiment for another aspect .",
    "what premise we have here , but i just say that the beta sub i w give we a aspect specific sentiment of w. so obviously that be one of the important parameter , but in general we can see we have these parameter . the beta value that delta and then the mu and sigma .",
    "now we can , as usual , use the maximum likelihood be make and this will give we the setting of this premise that with the maximizer observe .",
    "and of course , this would then give we all the useful variable that will interest in computing .",
    "now , what about the aspect weight ? alpha sub i of d ? it be not part of our parameter , right ? so we have to use bayesian inference to compute it ."
  ],
  "b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7": [
    "this lecture be about the topic mining and analysis . we be go to talk about use a term as topic . this be a slide that you have see in the early lecture where we define the task of top mining and analysis . we also raise the question how do we exactly define the topic theta ? so in this lecture we be go to offer one way to define it , and that be our initial idea . our idea here be to define a topic simply as a term . a term can be a word or a phrase . and in general , we can use these term to describe topic , so our first thought be just to define a topic as one term . for example , we might have term like sport , travel or science as you see here . now if we define a topic in this way , we can analyze the coverage of such topic in each document . here , for example , we might want to discover to what extent document 1 cover sport and we find that 30 % of the content of document 1 be about sport .",
    "so now of course , as we discuss .",
    "now there be of course many different way of do that and .",
    "so how can we design such a function ? well , there be many thing that we can consider .",
    "intuitively , we would like to favor representative term , mean term that can represent a lot of content in the collection . so that would mean we want to favor a frequent term . however , if we simply use the frequency to design the scoring function , then the high score term would be \" general term or functional term , like \" \" the \" \" , \" \" a \" \" \" etc . those term be very frequent in english .",
    "and tf stand for term frequency idea idf stand for inverse document frequency and we talk about some of these idea in the lecture about the discovery of word association . so these be statistical method , mean that the function be define mostly base on statistic . so the scoring function would be very general . it can be apply to any language and any text .",
    "basically , the idea be to go down the list base on our scoring function an gradually take term to collect the k topical term . the first term of course will be pick when we pick the next term . we be go to look at the what term have already be pick and try to avoid pick a term that be too similar . similar .",
    "so look at this picture , we have sport , travel and science and these topic and now suppose you be give a document how should we figure out the coverage of each topic in the document ?",
    "this form a distribution over the topic for the document to characterize coverage of different topic in the document .",
    "so now let 's examine this approach . in general , we have to do some empirical evaluation by use actual dataset and to see how well it work .",
    "so in term of the content , it be about the sport .",
    "so the count of sport be zero . that mean the coverage of sport will be \" estimate",
    "the term science also do not occur in the document , and it be estimate also zero . that be ok , but sport certainly be not ok . 'cause we know the content be about sport . so this estimate have problem .",
    "so this simple example illustrate some problem of this approach . first , when we count what word belong to the topic , we also need to consider related word . we ca n't simply just count the topic . word sport .",
    "finally , the main restriction of this approach be that we only have one term to describe this topic . so it can not really describe complicated topic . for example a very specialized topic would be hard to describe by use just a word or one phrase , we need to use more word , so this example illustrate some general problem with this approach of treat a term as topic . first , it lack expressive power , mean that it can only represent the symbol general topic ."
  ],
  "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf": [
    "this lecture be about the paradigmatic relation discovery . in this lecture we be go to talk about how to discover a particular kind of word association call paradigmatic relation . by definition , 2 word be paradigmatically relate if they share similar context . namely , they occur in similar position in text . so naturally , our idea for discover such relation be to look at the context of each word and then try to compute the similarity of those context . so here be an example of context of word cat . here i have take the word cat out of the context .",
    "now we can do the same thing for another word like a dog . so in general we would like to capture such a context and then try to assess the similarity of the context of cat and the context of a word like dog . so now the question be , how can we formally represent the context and then define the similarity function ?",
    "so they can be regard as a pseudo document . an imaginary document . but there be also different way of look at the context . for example , we can look at the word that occur before the word cat . we can call . we can call this context left1 context . so in this case you will see word like my , his or big , a , the , etc . these be the word that can occur to the left of the world cat . so we say my cat , his cat big cat . a cat etc .",
    "and here we see word eat , eat , be , have , etc .",
    "now of course , you can see all the word from left or from right , and so we have a bag of word in general to represent the context . now , such a word base representation would actually give we interesting way to define the perspective of measure the similarity . \" similarity of left1 , then we 'll see word that share just the word in the left context and we kind of ignore the other word that be also in the general context . so that give we one perspective to measure the similarity .",
    "use both left1 and right1 , ofcourse would allow we to capture the similarity with even more strict criterion .",
    "and this flexibility also allow we to measure the similarity similarity in some other different way . sometimes this be useful as we might want to capture similarity base on general content that would give we loosely relate paradigmatic relation , whereas if you use only the word immediately to the left and to the right of the world , then you likely will capture word that be very much relate by their syntactical category , or an semantic .",
    "so here for example , we can measure the similarity of cat and dog base on the similarity of their context . in general , we can combine all kind of view of the context and so the similarity function be in general combination of similarity on different context . and of course we can also assign weight to these different similarity to allow we to focus more on particular kind of context , and this would be naturally application specific , but again here that main idea for discover paradigmatically relate word be to compute the similarity of their context . so next , let 's see how we exactly compute these similarity function . now to answer this question it be useful to think of bag of word representation as vector in the vector space model .",
    "and on the bottom you can see frequency vector represent a context .",
    "so in general , we can represent a pseudo document or context of cat as one vector .",
    "an another word dog might give we a different context , so d2 .",
    "so the two question that we have to address be first how to compute each vector , that is , how to compute the xi or yi .",
    "now in general there be many approach that can be use to solve the problem , and most of they be develop for information retrieval .",
    "so the idea here be represent a context by award vector where each word have a weight that be equal to the probability that a randomly pick word from this document vector be this word . so in other word . xi be define as the normalize count of word wi in the context .",
    "now of course these xi 's will sum to 1 because they be normalize frequency .",
    "so , the vector d2 can be also compute in the same way .",
    "so that address the problem how to compute the vector ? next , let 's see how we can define similarity in this approach . well , here we simply define the similarity as a dot product of two vector and this be define as the sum of the product of all the corresponding element of the two vector .",
    "and there be this dot product infact give we the probability that to randomly pick word from the two context be identical that mean if we try to pick a word from one context and try to pick another word from another context , we can then ask the question , be they identical ? if the two context be very similar , then we should expect that we frequently will see the two word pick from the two contact be identical . if they be very different then the chance of see identical word be pick from the two context would be small . so this intuitively make sense for measure similarity of context .",
    "so if you just stay at the formula .",
    "alright , so that be one possible approach . eowc expect overlap of word in context . now , as always , we would like to assess whether this approach it would work well . now , of course , ultimately we have to test the approach it with real datum and see if it give we really semantically relate word really give we a paradigmatic relation . but analytically , we can also analyze this formula little bit . so first , as i say , it do make sense right ? because this formula will give a high score if there be more overlap between the two context . so that be exactly what we want . but if you analyze the formula more carefully , then you also see there might be some potential problem . and specifically there be two potential problem . first might favor match one frequently term very well over match more distinct term , and that be because in the dot product , if one element have a high value and this element be share by both context and it contribute a lot to the overall sum .",
    "so this may not be desirable . of course , this might be desirable in some other case , but in our case we should intuitively prefer a case where we match more different term in the context so that we have more confidence in say that the two word indeed occur in similar context .",
    "it may not be robust ."
  ],
  "d857a66b-1018-4ffb-821a-9d8acc6f5012": [
    "so now let 's talk about the extension of plsa to derive lda and to motivate that we need to talk about some deficiency of plsa . first , it be not really generate model because we ca n't compute the probability of a new document . you can see why , and that be because the pie be need to generate the document , but the pis be tie to the document that we have in the training datum . so we can not compute the pis for future document . and there be some heuristic . a work around though . and secondly , it have many parameter and i 've ask you to compute how many parameter exactly there be in plsa and \" you will see there be many that mean the model be very complex and that also mean there be many \" local overfitting and that mean it be very hard to also find a good local maximum .",
    "so lda be propose to improve that and it basically to make plsa a generative model by impose a dirichlet prior on the model parameter . dirichlet be just a special distribution that we can use to specify prior . so in this sense , lda be just a bayesian version of plsa and the parameter be now much more regularized . you will see there be many few parameter .",
    "so this be a picture of lda . now i remove the background model just for simplicity .",
    "so more specifically they will be draw from 2 dirichlet distribution respectively . \" the vector , so it give we a probability for a particular choice of a vector . take for example pis , right ? so this dirichlet distribution tell we which vector of pis be more likely , and this distribution itself be control by another vector of parameter of alpha 's .",
    "and similar here . the topic word distribution be draw from another dirichlet distribution with beta parameter and note that here alpha have k parameter correspond to our inference on the k value of pis for a document , whereas here beta have n value correspond to control the n word in our vocabulary .",
    "and then we be go to use the pi to far choose which topic to use , and this be of course very similar to the plsa model .",
    "so in the lda formula you see very similar thing . first you see the first equation be essentially the same and this be the probability of generate a word from multiple word distribution .",
    "so this be a very important formula as i have stress but multiple time and this be actually the core assumption in all the topic model and you might see other topic model that be extension of lda or plsa and they all rely on this . so it be very important to understand this . and this give we the probability of get a word from a mixture model . now next in the probability of a document we see there be a plsa component in the lda formula . but the lda formula would add some integral here , and that be to explain to account for the fact that the pis be not fix , so they be draw from dirichlet distribution .",
    "that be why we have to take the integral to consider all the possible pi 's that we could possibly draw from this \" dirichlet",
    "right , so basically in the lda we just add these integral to account for the uncertainty and we add of \" course the govern the choice of these parameter , pi 's and theta 's .",
    "influence .",
    "so you will see they when you use different toolkit for lda , or you read the paper about that these different extension of lda .",
    "by use the parameter of alpha and beta .",
    "so to summarize , our discussion of probabilistic topic model and these model provide a general principal way of mining and analyze topic in text with many application .",
    "and plsa be the basic topic model , and in fact the most basic topic model . and this be also often adequate for most application . that be why we spend a lot of time to explain plsa in detail .",
    "however , in practice , lda and plsa intend to give similar performance , so in practice , plsa , an lda , would work equally well for most task ."
  ],
  "da74c929-efc1-4b65-9635-684c7ebcab3f": [
    "this lecture be about evaluation of text cluster . so far we have talk about multiple way of do text clustering but how do we know which method work the good ? so this have to do with evaluation . now to talk about evaluation , one must go to go back to the classroom bias that we introduce at the beginning . because two object can be similar depend on how you look at they , we must clearly specify the perspective of similarity . without that , the problem of clustering be not well define . so this perspective be also very important for evaluation . if you look at this slide and you can see we have two different way to cluster these shape . an if you ask a question , which one be the good or which one be well you actually see there be no way to answer this question without know whether we 'd like to cluster base on shape or cluster base on size . and that be precisely why the perspective or clustering bias be crucial for evaluation .",
    "so the closeness here can be assess assess from multiple perspective and that would help we characterize the quality of clustering result in multiple angle . and this be sometimes desirable .",
    "and finally , you can see in this case we essentially inject the cluster bias by use human . basically , human would bring the need or desire clustering bias . now how do we do that exactly ? the general procedure would look like this .",
    "and they will use their judgment base on the need of a particular application to generate what they think be the good clustering result . and this would be then use to compare with the system generate cluster from the same test set .",
    "mutual information capture the correlation between these cluster label and normalize mutual information be often use for quantify the similarity for this evaluation purpose . f measure be another possible measure .",
    "i 've suggest some reading in the end that you can take a look to know more about that .",
    "in this case , the cluster bias impose by the intend application as well . so what count as the good clustering result would be dependent on the application .",
    "in this case what we care about be the contribution of clustering to some application . so we often have a baseline system to compare with .",
    "and then we be go to compare the performance of your clustering system and the baseline system in term of the performance measure for that particular application . so in this case we call it indirect evaluation of cluster because there be no explicit assessment of the quality of cluster , but rather its to assess the contribution of cluster to a particular application .",
    "the second application or second kind of application be to discover interesting clustering structure in text datum , and these structure can be very meaningful .",
    "model base approach and similarity base approach .",
    "now sometimes you may see some method that can automatically determine the number of cluster .",
    "so this be important to keep in mind . and i should also say sometimes we can use application to determine the number of cluster . for example , if you be cluster search result , then obviously you do n't want to generate 100 cluster , right ? so the number can be dictate by the interface design . in other situation , we might be able to use the fitness of datum to assess whether we 've get a good number of cluster to explain our datum well and to do that , you can vary the number of cluster and watch how well you can fit the datum . if it be in general , when you add more component to mixture model , you should fit the datum well , because you can always set the probability of use the new component at 0 , so you ca n't in general fit the datum bad than before , but as the question be , as you add more component would you be able to significantly improve the fitness of the datum and that can be use to determine the right number of cluster . and finally , evaluation of clustering result and can be do both directly and indirectly ."
  ],
  "db1d54dd-bb05-46c0-995b-5f7d5243e3c4": [
    "this lecture be about the text representation . in this lecture we be go to discuss text representation . and discuss how natural language processing can allow we to represent text in many different way . let 's take a look at this example sentence again . we can represent this sentence in many different way . 1st . we can always represent such a sentence as a string of character . this be true for all the language when we store they in the computer . when we store a natural language sentence as a string of character , we have perhaps the most general way of represent text , since we can always use this approach to represent any text datum .",
    "the reason be because we be not even recognize word . so as a string we be go to keep all the space and these ascii symbol . we can perhaps count how ... what be the most frequent character in english text , or the correlation between those character , but we ca n't really analyze semantic .",
    "if we try to do a little bit more natural language processing by do word segmentation .",
    "so here we see that we can identify word like : a dog be chase etc . now with this level of representation , we certainly can do a lot of thing , and this be mainly because word be the basic unit of human communication in natural language , so they be very powerful . by identify word we can , for example , easily count what be the most frequent word in this document or in the whole collection , etc .",
    "when we combine related word together \" and some word be positive , some word negative , so we can also do sentiment analysis . so represent text datum as a sequence of word open up a lot of interesting analysis possibility .",
    "so you have to rely on some special technique to identify word .",
    "but in english it be very easy to obtain this level of representation , so we can do that all the time . now if we go far to do natural language processing , we can add a part of speech tag .",
    "if we go far then we 'll be parse the sentence to obtain a syntactic structure .",
    "if we could go far for semantic analysis , then we might be able to recognize dog as animal and we also can recognize boy as a person and playground as a location .",
    "at this level , then we can do even more interesting thing . for example , now we can count easily the most frequent person that be mention in this whole collection of news article , or whenever you mention this person , you also tend to see mention of another person , etc . so this be very useful representation an it be also related to the knowledge graph that some of you may have hear of . that google be do as a more semantic way of represent text datum .",
    "now if we move far to logical representation then we can have predicate and even inference rule . and with inference rule we can infer interesting , derive fact from the text . so that be very useful , but unfortunately at this level of representation it be even less robust and we can make mistake , and we ca n't do that all the time for all kind of sentence .",
    "author of this sentence , what be the intention of say that ? what scenario , what kind of action will be make ? so this be ...",
    "and unfortunately , such technique would require more human effort .",
    "that mean there be mistake . so if we analyze text datum at the level that be represent , deep analysis of language , then we have to tolerate the error .",
    "now , this be desirable because as we can represent text at the level of knowledge , we can easily extract the knowledge . that be the purpose of text mining . so there be a trade off here between do deep analysis that might have error , but would give we direct knowledge that can be extract from text and do shallow analysis , which be more robust . but would n't they actually give we the necessary deep representation of knowledge . i should also say that text datum be generate by human an be mean to be consume by human so as a result in a text datum analysis text mining , human play a very important role . they be always in the loop ."
  ],
  "dccc8a84-66da-47ce-ab88-28e8acf192b9": [
    "this lecture be a continue discussion of generative probabilistic model for text clustering . in this lecture we be go to finish the discussion of generative probabilistic model for text clustering . so this be a slide that you have see before and here we show how we define the mixture model for text cluster an what the likelihood function look like and we can also compute the maximum liklihood estimate to estimate the parameter . in this lecture , we be go to talk more about how exactly we be go to compute the maximum likelihood estimator . now , as in most case , the em algorithm can be use to solve this problem for mixture model . so here be the detail of this em algorithm for document clustering . now , if you have understand how eml work for topic model , psa and i think here it will be very similar and you just need to adapt a little bit to \u00a0 this new mixture model . so as you may recall , em algorithm start with initialization of all the parameter . so this be the same as what happen before for topic model .",
    "and more specifically , basically we be go to apply bayes rule to infer , or which distribution be more likely to have generate this document or compute the posterior probability of the distribution .",
    "an we know it be proportional to the probability of select this distribution p of theta i and the probability of generate this whole document from that distribution , which be a product of all the probability of word for this document , as you see here . now , as in all case , it be useful to kind of remember the normalizer or the or the constraint on this probability . so in this case we know the constraint on this probability in the e step be that all the probability of z equal i must sum to one 'cause the document must have be generate from precise or one of these k topic . so the probability of the generator from each of they should sum to one .",
    "what it be proportional to , right ? so once you compute this product that you see here , then you simply normalize this these probability to make they some to 1 what over all the topic . so that be e step after e step we would know which distribution be more likely to have generate this document d , which be unlikely .",
    "before we observe anything , we do n't have any knowledge about which cluster be more likely , but after we have observe these document , then we can collect the evidence .",
    "and so this give we all the evidence about use topic . i say i to generate a document and we put they together and again we normalize they into probability .",
    "now the other kind of parameter be the probability of word in each distribution , each cluster , and this be very similar to the case of plsa .",
    "and then we normalize again . these count into probability so that the probability on all the word some to one . note that it be very important to understand these constraint as they be precisely the normalizer in all these formula , and it be also important to know that distribution be over what ?",
    "so now let 's take a look like this . take a look at the simple example of two cluster . i have two cluster . i 've show some initializer value for the two distribution .",
    "and then let 's consider one document that you have see here . there be two word , sorry , two occurrence of text and two occurrence of mining . so there be four word together .",
    "now for each document we must use a hidden variable and before in plsa we use 1 hidden variable for each word .",
    "so now how do we infer which distribution have be use to generate the d ? it be to use bayes rule so it look like this in order for the first topic be setup , want to generate the document . two thing must happen . first theater subway must have be select , so it be give by p of 01 second . it must have also be generate the four word in the document , namely two occurrence of text and two occurrence of mining . that be why you see the numerator have the product of the probability of select theta one and the probability of generate the document from theta 1 .",
    "so now it be important to note that in such a computation there be a potential problem of underflow , and that be because if you look at the numerator , the original numerator and denominator it involve the computation of a product of many small probability . imagine if a document have many word and it be go to be a very small value here , as it can cause the problem of underflow .",
    "we can use a normalize .",
    "and this do the average distribution will be comperable to each of these distribution .",
    "so we can then divide the numerator and the denominator both by this normalizer . so basically this normalize the probability of generate this document by use this average word distribution .",
    "and since we have use exact the same normalizer for the numerator and denominator , the whole value of this expression be not change .",
    "in some other time we sometimes also use logarithm of the product to convert this into a sum of log of probability . this can help preserve precision as well , but in this case we can not use logarithm to solve the problem because there be sum in the denominator , but this kind of normalize can be effective for solve this problem , so it be a technique that be sometimes useful in other situation as well .",
    "how do we estimate that ? intuitively , you can just pull together the z probability z probability from e steps , right ? so if all these document say they be more likely from silouan , then we intuitively would give a high probability to see that one right ? so in this case , so we can just take the average of these probability that you see here , and we obtain the .6 for theta 1 so theta 1 be more likely theta 2 .",
    "what about these world probability ? what we do the same ? and intuition be the same , so we be go to see in order to estimate the probability of word in theta one , we be go to look at which document have be generate from scylla and we be go to pull together the word in those document and normalize they .",
    "use all the count of text in these document to estimate the probability of tax give still awhile , but we be not to use their raw count or total account . instead , we can do that . discount they by the probability that each document be likely be generate from theta 1 . so this give we some fractional count , and then these council would be then normalize in order to get the probability . now how do we normalize they ? these probability of these word must sum to one . so to summarize , our discussion of generating model for clustering . we show that a slight variation of top model can be use for cluster document and this also show the power of generate model in general by change the generation assumption and change the model slightly we can achieve different goal and we can capture different pattern in text datum . so in this case , each class be represent by unigram language model or word distribution , and that be similar to topic model . so here you can see the word distribution actually generate a term cluster as a byproduct .",
    "and then the estimate model pamateter will give both a topic capitalization of each cluster and the probabilistic assignment of each document into a cluster ."
  ],
  "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a": [
    "this lecture be a continue discussion of probabilistic topic model . in this lecture , we be go to continue discuss probabilistic model , we be go to talk about a very simple case where we be interested in just mine one topic from one document . so in this simple setup we be interested in analyze one document and try to discover just one topic . so this be the simple case of topic modeling . the input now no long have k , which be the number of topic because we know there be only one topic . and the collection have only one document also . in the output we also no long have coverage because we assume that the document cover this topic 100 % . so the main goal be just to discover the word probability for this single topic , as show here .",
    "where our perspective just mean we want to take a particular angle of look at the datum so that the model would have the right parameter for discover the knowledge that we want , and then we 'll be think about the likelihood function or write down the library function to capture more formally how likely a data point will be obtain from this model . and the likelihood function will have some parameter in the function and then we be usually interested in estimate those parameter , for example by maximize the likelihood which would lead to maximum likelihood estimator and these estimate parameter would then become the output of the mining algorithm . which mean we 'll take the estimate parameter as a knowledge that we discover from the text . so let 's look at these step for this very simple case . \" later , we 'll look at this procedure for some more complicated case . so our datum in this case be just the document which be a sequence of word . each word here be denote by x sub i.",
    "so we will have as many parameter as many word in our vocabulary , in this case m. and for convenience we be go to use theta sub i to denote the probability of word w sub i.",
    "now , what do the likelihood function look like ? this be just the probability of generate this whole document give such a model . because we assume the independence in generate each word , so the probability of the word the document would be just a product of the probability of each word .",
    "so in this line we have rewrite the formula into a product over all the unique word in the vocabulary ,",
    "now this be different from the previous line where the product be over different position of word in the document . now when we do this transformation , we then would need to introduce account function here .",
    "and similarly , this be the count of word of m in the document . becausw . these word might have repeat occurrence . you can also see if a word do not occur in the document , it would have a zero count and therefore that corresponding term will disappear . so this be a very useful form of write down the likelihood function that we will often use later . so i want you to pay attention to this . just get familiar with this notation . it be just to change the product over all the different word in the vocabulary . so in the end , of course we 'll use theta sub i to express this likelihood function and it would look like this .",
    "this line be copy from the previous slide . it be just our likelihood function .",
    "and we also have constraint over these probability .",
    "so please take a look at this sum again here and this be a form of \" function that you often see later also in more general topic model .",
    "and this be multiply by the logarithm of the probability . so let 's see how we can solve this problem . now at this point the problem be purely a mathematical problem , because we be go to just to find the optimal solution of a constrained maximization problem . the objective function be the likelihood function , and the constraint be that all these probability must sum to one .",
    "now this content be beyond the scope of this course . but since lagrange multiplier be very useful approach , i also would like to just give a brief introduction to this for those of you who be interested .",
    "and this function would combine our objective function with another term that encode our constraint .",
    "so it be additional parameter .",
    "as you may recall from calculus , an optimal point would be achieve when the derivative be set to 0 . this be a necessary condition . it be not sufficient though , so .",
    "and this part come from the derivative of the logarithm function .",
    "and when we set it to zero , we can easily see theta sub i be related to lambda in this way .",
    "and this be just negative sum of all the count and this far allow we to then solve optimization problem . eventually to find the optimal setting for theta sub i.",
    "so after all this math , after all , we have just obtain something that be \" very intuitive , and this will be just our intuition where we want to maximize the theta by assign as much probability mass as possible to all the observed word here .",
    "so if you imagine what would we get if we use such a maximum likelihood estimator to estimate one topic for a single document d here , let 's imagine this document be a text mining paper . now what you might see be something that look like this . on the top you will see the high probability word tend to be those very common word , often functional word in english , and this will be follow by some content word that really \" characterize the topic well like text , mining etc and then in the end you also see various more probability of word that be not really related to the topic , but they might be externally mention in the document ."
  ],
  "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402": [
    "this lecture be a continue discussion of latent aspect rating analysis . early we talk about how to solve the problem of lara in two stage when we first do segmentation of different aspect and then we use a little regression model to learn the aspect rating and let the weight . now , it be also possible to develop a unified generative model for solve this problem , and that be we not only model , we not only model the generation of overrate base on text , we also model the generation of text and so a natural solution would be to use topic model . so give an entity , we can assume there be aspect that be describe by word distribution . topic and then we can use a topic model to model the generation of the review text .",
    "and then we can then plug in the latent regression model to use the text to far predict the overall rating and that mean we first predict the aspect rating and then combine they with aspect weight to predict the overall rating . so this would give we a unified generative model where we model both the generation of text and the overall rating condition on text .",
    "so now i be go to show you some simple result that you can get by use this kind of generative model . first it be about rating decomposition . so here what you see be the decomposed rating for three hotel that have the same overall rating . so if you just look at the overall rating you do n't . you ca n't really tell much difference between these hotel , but by decompose these rating into aspect rating we can see some hotel have high rating for some . dimension like value , but other might score well in other dimension like location and so this can reveal detailed opinion at the aspect level .",
    "the 2nd result be to compare different reviewer on the same hotel so the table show the decompose rating for two reviewer about same hotel again their high level overall rating be the same . so if you just look at the overall rating , you do n't really get that much information about the difference between the two review . but after you decompose the rating you can see clearly they have high score on different dimension . so this show that the model can reveal difference in . opinion of different reviewer and such a detailed understanding can help we understand well about review and also well about their feedback on the hotel . this be something very interesting because this be in some sense some byproduct in our problem formulation . we do not really have to do this , but the design of the generative model have this component and these be sentiment wait for word in different aspect . and you can see the highly weight word versus the negatively low weight word here for each of the four dimension . value , room , location and cleanliness . i add the top word , clear it , make sense , and the bottom word also make sense .",
    "and here be some result to validate the preference weight . remember , the model can infer whether a reviewer care more about service or the price . now , how do we know whether the infer weight be correct and this pose a very difficult challenge for evaluation .",
    "so for example , value versus location value versus room etc . but the top ten be the reviewer that have the high ratio by this measure . and that mean these reviewer tend to put a lot of weight on value as compare with other dimension . that mean they really emphasize on value .",
    "now these ratio be computer base on the infer weight from the model .",
    "and this provide some",
    "here be also mother result about the aspect discover from review with low rating . these be mp3 three review an these result show that the model can discover some interesting aspect comment on low overall rating versus those high overall rating , and they care more about the different aspect .",
    "so that can help we discover , for example , consumer train in appreciate different feature of product . for example , one might have discover the trend that people tend to like large screen of cell phone or lightweight of laptop etc . and such knowledge can be useful for manufacturer to design their next generation of product .",
    "and on the left side you see the weight of review like the expensive hotel they give . the whole expensive hotel five star and you can see their average weight tend to be more focused on service and that suggest that people might be expensive hotel because of good service . and that be not surprising as also another way to validate the infer weight .",
    "but if you look at the when they do n't like expensive hotel or cheap hotel and you seal it tend to have more weight on the condition of the room cleanliness . so this show that by use this model we can infer some information that be very hard to obtain , even if you read all the review . even if you read all the review , it be very hard to infer such preference or such emphasis . so this be a case where text mining algorithm can go beyond what human can do to review interesting pattern in the datum , and this of course can be very useful . you can compare different hotel , compare the opinion from different consumer group in different location , and of course the model be general . it can be apply to any review with overall rating , so this be very useful technique that can support a lot of text mining application .",
    "so because we can infer the reviewer weight on different dimension , we can allow a user to actually say what do you care about . so , for example , if a query here that show 90 % of the way it should be on value and 10 % on other . so that just mean i do n't care about other aspect , i just care about get a cheap hotel . my emphasis be on the value dimension .",
    "the non personalized recommendation result be show on the top . an you can see the top result generally have much high price than the low group , and that be because when reviewer care more about the value as dictate by this query and they tend to really have favor low price hotel . so this be yet another application of this technique .",
    "and as a task sentiment analysis can be usually do by use just text categorization , but standard technique tend not to be enough and so we need to have enrich feature representation . and we also need to consider the order of those category and we talk about the ordinal regression . for solve this problem .",
    "most approach have be propose and evaluate for product review , and that be the cause in such a context of the opinion holder an opinion target or clear and they be easy to analyze and there of course also have a lot of practical application , but opinion mining from news and social medium be also important , but that be more difficult than analyze review datum , mainly because the opinion holder and opinion target be all .",
    "so here be some suggest reading , the first 2 ."
  ],
  "f1951cf2-4293-450b-8578-4d74c72f9862": [
    "this lecture be about opinion mining and sentiment analysis cover its motivation . in this lecture we be go to start talk about mine a different kind of knowledge , namely knowledge about the observer or human that have generate text datum . in particular , we be go to talk about the opinion mining and sentiment analysis . as we discuss early , text datum can be regard as the datum generate from human as subjective sensor . in contrast , we have other device such as video recorder that can report what be happen in the real world objectively to generate the video datum , for example . now the main difference between text datum and other datum like video datum be that it have rich and rich opinion and the content tend to be subjective because it be generate from human .",
    "we can mine the text datum to understand the opinion understand the people 's preference , how people think about something .",
    "so let 's start with the concept of opinion that it be not that easy to formally define opinion , but mostly we would define opinion as a subjective statement describe what a person believe or think about something .",
    "those statement can be prove right or wrong .",
    "so in contrast , objective statement can usually be prove wrong or correct .",
    "now that be something you can check . it be either have a battery or not . but in contrast , if you think about the sentence such as this laptop have the good battery . or this laptop have a nice screen , now these statement be more subjective and it be very hard to prove whether it be wrong or correct .",
    "and next , let 's look at the keyword person here and that indicate this opinion holder 'cause when we talk about opinion , it be about the opinion hold by someone and then we notice that there be something here . so that be the target of the opinion . the opinion be express on this something .",
    "so what be the basic opinion representation like , well , \u00a0  it should include at least three measurement , right ? first it have to specify what be the opinion holder . so whose opinion be this , second must also specify the target . what be this opinion about ?",
    "and that mean we also want to understand , for example , the context of the opinion and what situation be opinion express . for example , in what time be it express ? we also would like to deeply understand opinion sentiment and this be to understand , what the opinion tell we about the opinion holder 's feeling , for example , be this opinion positive or negative ?",
    "and so such understanding obviously go beyond just extract the opinion content and need some analysis .",
    "when the review be post , usually you can extract such information easily .",
    "so you can see product review be fairly easy to analyze in term of obtain a basic opinion representation .",
    "or we want to know that the sentiment of this review be positive , and so this additional understanding of course add value to mine the opinion .",
    "now let 's take a look at the sentence in the news . in this case , we have implicit holder and implicit target . and the task be in general hard so we can identify opinion holder here and that be governor of connecticut .",
    "so what be the opinion ? well , this negative sentiment here that be indicate by word like a bad and bad . and we can also then identify the context . new england in this case . now unlike in the product review , all these element must be extract by use natural language processing technique .",
    "and these example also , suggest that a lot of work can be easily do for product review , and that be indeed what have happen . analyze sentiment in news be still quite difficult . it be more difficult than the analysis of opinion in product review .",
    "opinion target council vary a lot . it can be about 1 entity , a particular person , a particular product , that particular policy , etc . but it could be about a group of product . could be about the product from a company in general .",
    "it could be about someone else opinion and one person might comment on another person opinion etc . so you can see there be a lot of variation here that will cause the problem to vary a lot . now opinion content , of course , can also vary a lot on the surface . you can identify one sentence opinion or one phrase opinion , but you can also have long text to express the opinion like a whole article .",
    "finally , the opinion context can also vary . we can have simple context , like different time or different location , but there could be also complex text such as some background topic be discuss . so when opinion express in the particular discourse context , it have to be interpret in different way than when it be express in another context , so the context can be very rich to improve the entire discourse context of opinion . from computational perspective , we be most interested in what opinion can be extract from text datum , so it turn out that we can also differentiate distinguish different kind of opinion in text datum from computation perspective . first , the observer might make a comment about the opinion target in the observed world . so in this case we have the author 's opinion . for example , i do n't like this phone at all , and that be opinion of this author .",
    "here , so it do n't mean this author love that painting .",
    "and another complication be that there may be indirect opinion or infered opinion that can be obtain by make inference on what be express in the text that might not necessarily look like opinion . for example , one statement might be this phone run out of battery in just one hour .",
    "but from this statement one can also infer some negative opinion about the quality of the battery of this phone or the feeling of the opinion holder about the battery .",
    "so these be interesting variation that we need to pay attention to when we extract opinion . also , for this reason about the indirect opinion .",
    "so the task of opinion mining can be define as take text datum as input to generate a set of opinion representation . in each representation we should identify opinion holder , target content and context . ideally we can also infer opinion sentiment from the content and context to well understand the opinion .",
    "so now that we have talk about what be opinion mining and we have define the task , let 's also just talk a little bit about the why opinion mining be very important and why it be very useful . so here i identify three major reason , 3 broad reason . the first be it can help decision support .",
    "we also",
    "and policymaker may also want to know people opinion when design a new policy . so that be one general kind of application . and it be very broad , of course .",
    "it can also help she with advertising , of course , and we can have target advertising if we know what kind of people tend to know to like what kind of product .",
    "people need to fill in form to answer some question .",
    "now this be would be very useful for business intelligence , where product manufacturer want to know , where their product have advantage over other . what be the win feature of their product or win feature of competitive product ?"
  ],
  "f64adab4-578a-4868-8b2c-03fdd4ddf55d": [
    "this lecture be a continue discussion of generative probabilistic model for text clustering . in this lecture , we be go to continue talk about the tax capture text clustering , particularly \" generative so this be a slide that you have see early where we have write down the likelihood function for a document . with two distribution in two component mixture model for document clustering . now in this lecture , we be go to generalize this to include the k cluster . now if you look at the formula and think about the question how to generalize it , you will realize that all we need be to add more term like what you have see here . so you can just add more theta and the probability of theta and the probability of generate d from those theta . so this be precisely what we be go to use . this be general presentation of the mixture model for document clustering .",
    "and then we talk about the model . think about the model . in this case , we design a mixture of k unigram language model . it be a little bit different from the topic model .",
    "now note that , although our goal be to find the cluster and we actually have use a more general notion of a probability of each cluster . and this , as you see later , would allow we to assign a document to the .",
    "property .",
    "so the model basically would make the following assumption about the generation of the document . we first choose a theta i accord to probability of theta i and then generate all the word in the document use this distribution . note that it be important that we use this distribute generator . all the word in the document . this be very different from topic model , so the likelihood function would be like what you be see here .",
    "you can take a look at the formula here . we have use the different . notation here in the second line of this . of this equation . but you can see now the .",
    "so from x sub j to w be a change of notation , and this change allow we to show the estimation formula more easily and you have see this change also in the topic model presentation , but it be basically still just a product of the probability of all the word .",
    "note that unlike in plsa and this probability of theta i be not dependent on d. now . you may recall that the topic choice in each document actually depend on d. that mean each document can have a potentially different choice of topic , but here we have a generic choice probability for all the document . but of course , give a particular document that we still have to infer which topic be more likely .",
    "so lets look at a key problem let to compute the c sub d here and this will take one of the value in the range of one to k to indicate which cluster should be assign to d.",
    "so that mean we be go to choose one of those distribution that give d high probability .",
    "intuitively , that make sense .",
    "and if we choose theta base on this posterior probability and we would have the follow formula that you see here . on the bottom of this slide , and in this case , we be go to choose the theta that have a large p of theta i. that mean a large cluster and also a high probability of generate d. so we be go to favor a cluster that be large and also consistent with the document ."
  ]
}