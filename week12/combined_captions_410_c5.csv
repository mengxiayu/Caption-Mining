0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-04 14:31:40.390557	2	00:00:00.29	00:00:04.52	This lecture is about the Word Association mining and analysis.	this lecture be about the Word Association mining and analysis .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283111	5	00:00:14.97	00:00:21.53	In this lecture we're going to talk about how to mine associations of words from text.	in this lecture we be go to talk about how to mine association of word from text .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283112	8	00:00:22.88	00:00:30	This is an example of knowledge about natural language that we can mine from text data.	this be an example of knowledge about natural language that we can mine from text datum .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-12-14 08:55:39.870965	11	00:00:33.21	00:00:39.08	Here's the outline. We are gooing to first talk about what is word Association and then.	here be the outline . we be gooe to first talk about what be word Association and then .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283115	18	00:00:40.34	00:00:55.44	explain why discovering such relations is useful and finding a going to talk about some general ideas about how to mine word associations. In general there are two word relations, and these are quite basic.	explain why discover such relation be useful and find a going to talk about some general idea about how to mine word association . in general there be two word relation , and these be quite basic .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283116	23	00:00:56.47	00:01:10.69	One is called a paradigmatic relation, the other is syntagmatic relations. A&B have paradigmatic relation if they can be substituted for each other.	one be call a paradigmatic relation , the other be syntagmatic relation . A&B have paradigmatic relation if they can be substitute for each other .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-12-14 08:58:27.092471	32	00:01:11.37	00:01:35.75	That means the two words that have paradigmatic relation would be in the same semantic class or syntactic class, and we can in general replace one by the other without affecting the understanding of the sentence. That means we would still have a valid sentence. For example, cat and dog.	that mean the two word that have paradigmatic relation would be in the same semantic class or syntactic class , and we can in general replace one by the other without affect the understanding of the sentence . that mean we would still have a valid sentence . for example , cat and dog .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-12-14 08:58:54.175007	35	00:01:37.06	00:01:44.98	And these two words have paradigmatic relation because they are in the same class.	and these two word have paradigmatic relation because they be in the same class .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-12-14 08:59:07.72443	36	00:01:45.61	00:01:46.49	of animal.	of animal .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283122	40	00:01:47.57	00:01:57	And in general, if it replace cat with dog in a sentence, the sentence would still be a valid sentence that you can make sense of.	and in general , if it replace cat with dog in a sentence , the sentence would still be a valid sentence that you can make sense of .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-12-14 08:59:49.47817	42	00:01:58.2	00:02:02.13	Similarly, Monday and Tuesday have paradigmatic relation.	similarly , Monday and Tuesday have paradigmatic relation .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283124	44	00:02:04.83	00:02:09.68	The second kind of relation is called syntagmatic relation.	the second kind of relation be call syntagmatic relation .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283126	50	00:02:10.48	00:02:22.82	In this case, the two words that have this relation can be combined with each other. So A&B have syntagmatic relation If they can be combined with each other in a sentence.	in this case , the two word that have this relation can be combine with each other . so A&B have syntagmatic relation if they can be combine with each other in a sentence .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283127	52	00:02:25.34	00:02:29.71	That means these two words are semantically related.	that mean these two word be semantically relate .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-12-14 09:00:41.026557	54	00:02:30.63	00:02:36.96	So for example, Cat and sit are related because a cat can sit somewhere.	so for example , Cat and sit be relate because a cat can sit somewhere .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283131	64	00:02:37.97	00:03:04.24	Similarly, car and drive are related semantically, and they can be combined with each other to convey meaning. However, in general we cannot replace cat with sit in a sentence or car with drive in a sentence to still get a valid sentence. Meaning that if we do that, the sentence will become somewhat meaningless.	similarly , car and drive be relate semantically , and they can be combine with each other to convey meaning . however , in general we can not replace cat with sit in a sentence or car with drive in a sentence to still get a valid sentence . mean that if we do that , the sentence will become somewhat meaningless .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283133	73	00:03:05.9	00:03:30.93	So this is different from paradigmatic relation and these two relations are in fact so fundamental, that they can be generalized to capture basic relations between units in arbitrary sequences. And definitely they can be generalized to describe relations of any items in the language.	so this be different from paradigmatic relation and these two relation be in fact so fundamental , that they can be generalize to capture basic relation between unit in arbitrary sequence . and definitely they can be generalize to describe relation of any item in the language .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283134	75	00:03:31.5	00:03:36.71	So A&B don't have to be words and they can be phrases example.	so A&B do n't have to be word and they can be phrase example .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283137	87	00:03:37.84	00:04:10.81	And they can even be more complex phrases than just a noun phrase. If you think about the general problem of the sequence mining, then we can think about the units in the sequence data, and then we think of paradigmatic relation as relations that are applied to units that tend to occur in similar locations in a sentence or in a sequence of their elements in general.	and they can even be more complex phrase than just a noun phrase . if you think about the general problem of the sequence mining , then we can think about the unit in the sequence datum , and then we think of paradigmatic relation as relation that be apply to unit that tend to occur in similar location in a sentence or in a sequence of their element in general .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283139	94	00:04:11.57	00:04:30.29	So they occur in similar locations relative to the neighbors in the sequence. Syntagmatic relation, on the other hand, is related to co-occurring elements that tend to show up in the same sequence.	so they occur in similar location relative to the neighbor in the sequence . syntagmatic relation , on the other hand , be relate to co - occurring element that tend to show up in the same sequence .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.28314	100	00:04:33.05	00:04:46.41	So these two are complementary and basically relations of words, and we're interested in discovering them automatically from text data. Discovering such world relations has many applications.	so these two be complementary and basically relation of word , and we be interested in discover they automatically from text datum . discover such world relation have many application .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-12-14 09:10:01.883332	110	00:04:47.13	00:05:11.37	First, such relations can be directly useful for improving accuracy of many NLP tasks, and this is because this is part of our knowledge about the language. So if you know these two words or synonyms, for example, and then you can help a lot of tasks. And grammer learning can be also done by using such techniques because	first , such relation can be directly useful for improve accuracy of many nlp task , and this be because this be part of our knowledge about the language . so if you know these two word or synonym , for example , and then you can help a lot of task . and grammer learning can be also do by use such technique because
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283145	118	00:05:12.14	00:05:31.64	If we can learn paradigmatic relations, then we form classes of words. Syntactic classes for example. And if we learn syntagmatic relations, then we would be able to know the rules for putting together a larger expression based on component expressions.	if we can learn paradigmatic relation , then we form class of word . syntactic class for example . and if we learn syntagmatic relation , then we would be able to know the rule for put together a large expression base on component expression .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283146	120	00:05:32.28	00:05:37.5	So we'll learn the structure and what can go with what else.	so we 'll learn the structure and what can go with what else .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283149	129	00:05:39.73	00:06:00.62	Word relations can be also very useful for many applications in text retrieval and mining. For example, in search in text retrieval we can use word associations to modify a query. And this can be used to introduce additional related words to a query to make the query more effective.	word relation can be also very useful for many application in text retrieval and mining . for example , in search in text retrieval we can use word association to modify a query . and this can be use to introduce additional related word to a query to make the query more effective .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283149	130	00:06:01.44	00:06:03.48	It's often called query expansion.	it be often call query expansion .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.28315	133	00:06:04.16	00:06:10.76	Or you can use related words to suggest related queries to the user to explore the information space.	or you can use related word to suggest related query to the user to explore the information space .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283152	141	00:06:12.63	00:06:31.2	Another application is to use word associations to automatically construct the topic map for browsing where we can have words as nodes and associations as edge. is a user could navigate from one word to another to find information in the information space.	another application be to use word association to automatically construct the topic map for browse where we can have word as node and association as edge . be a user could navigate from one word to another to find information in the information space .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283153	144	00:06:33.36	00:06:39.74	Finally, such word associations can also be used to compare and summarize opinions.	finally , such word association can also be use to compare and summarize opinion .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283154	147	00:06:40.51	00:06:47.99	For example, we might be interested in understanding positive and negative opinions about iPhone 6.	for example , we might be interested in understand positive and negative opinion about iPhone 6 .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283155	152	00:06:49.13	00:07:00.57	In order to do that, we can look at what words are most strongly associated with a feature word like the battery in positive versus negative reviews.	in order to do that , we can look at what word be most strongly associate with a feature word like the battery in positive versus negative review .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283155	155	00:07:01.52	00:07:08.57	Such a syntagmatic relations would help us show the detailed opinions about the product.	such a syntagmatic relation would help we show the detailed opinion about the product .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283157	162	00:07:10.48	00:07:21.58	So how can we discover such associations automatically? Now here are some intuitions about how to do that. Let's first look at the paradigmatic relation.	so how can we discover such association automatically ? now here be some intuition about how to do that . let 's first look at the paradigmatic relation .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-12-15 01:22:40.526681	164	00:07:28.9	00:07:33.31	Here we essentially can take advantage of similar context.	here we essentially can take advantage of similar context .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283158	166	00:07:34.01	00:07:38.64	So here you see some simple sentences about cat and dog.	so here you see some simple sentence about cat and dog .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283159	168	00:07:39.85	00:07:43.49	You can see they generally occur in similar context.	you can see they generally occur in similar context .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-12-14 09:15:28.987645	170	00:07:44.23	00:07:48.47	And that, after all, is the definition of paradigmatic relation.	and that , after all , be the definition of paradigmatic relation .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.28316	174	00:07:49.37	00:07:59.25	So on the right side you can see I extracted explicitly the context of cat and dog from this small sample of text data.	so on the right side you can see I extract explicitly the context of cat and dog from this small sample of text datum .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283161	177	00:08:00.39	00:08:07.49	So I have taken away cat and dog from the corresponding sentences so that you can see just the context.	so I have take away cat and dog from the correspond sentence so that you can see just the context .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283162	179	00:08:08.69	00:08:12.99	Now of course we can have different perspectives to look at the context.	now of course we can have different perspective to look at the context .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283162	182	00:08:13.7	00:08:22.52	For example, we can look at the what words occur in the left part of this context.	for example , we can look at the what word occur in the left part of this context .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283163	185	00:08:23.4	00:08:32.67	So we can call this left context. What words occur before we see cat, cat or dog.	so we can call this left context . what word occur before we see cat , cat or dog .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283164	187	00:08:34.25	00:08:39.54	So you can see in this case clearly dog and cat have similar left context.	so you can see in this case clearly dog and cat have similar left context .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-12-14 09:17:36.426955	191	00:08:40.6	00:08:51.39	You generally say his cat or my cat, and you say also my dog and his dog. So that makes them similar in the left context.	you generally say his cat or my cat , and you say also my dog and his dog . so that make they similar in the left context .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-12-14 09:18:33.865259	200	00:08:53.56	00:09:15.36	Similarly, if you look at the words that occur after cat and dog, which we can call right context and they also very similar in this case, of course it's extreme case where you only see eats and In general you will see many other words. Of course that can follow cat and dog.	similarly , if you look at the word that occur after cat and dog , which we can call right context and they also very similar in this case , of course it be extreme case where you only see eat and in general you will see many other word . of course that can follow cat and dog .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-12-14 09:19:22.823323	205	00:09:17.71	00:09:26.91	You can also even look at the general context. And that might improve the all words in the sentence or in sentences around this word.	you can also even look at the general context . and that might improve the all word in the sentence or in sentence around this word .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283168	208	00:09:27.52	00:09:34.32	And even in the general context you also see some similarity between the two words.	and even in the general context you also see some similarity between the two word .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283172	215	00:09:35.32	00:09:55.04	So this is just suggesting that we can discover paradigmatic relation by looking at the similarity of context of words. So for example, if we think about the following questions, how similar are context of cat and context of dog?	so this be just suggest that we can discover paradigmatic relation by look at the similarity of context of word . so for example , if we think about the follow question , how similar be context of cat and context of dog ?
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283172	217	00:09:56.11	00:10:01.67	In contrast, how similar are context of cat and context of computer?	in contrast , how similar be context of cat and context of computer ?
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283174	224	00:10:02.42	00:10:20.82	Now intuitively, with imagine in the context of Cat, context of dog would be more similar than the context of cat and context of computer, that means the first in the first case, the similarity value would be high.	now intuitively , with imagine in the context of Cat , context of dog would be more similar than the context of cat and context of computer , that mean the first in the first case , the similarity value would be high .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283175	230	00:10:21.45	00:10:36.45	Between the context of cat and dog, whereas in the second the similarity between contexts of cat and computer would be low because they are not having paradigmatic relationship.	between the context of cat and dog , whereas in the second the similarity between context of cat and computer would be low because they be not have paradigmatic relationship .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283176	234	00:10:37.37	00:10:45.08	And then imagine what words occur after computer. In general they will be very different from what words occur after cat.	and then imagine what word occur after computer . in general they will be very different from what word occur after cat .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-12-14 09:22:41.851702	236	00:10:46.42	00:10:50.87	So this is the basic idea of discovering paradigmatic relation.	so this be the basic idea of discover paradigmatic relation .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-12-14 09:23:11.685827	240	00:10:51.93	00:11:02.61	What about the syntagmatic relation? Here we we are going to explore the correlated occurrences again based on the definition of syntagmatic relation.	what about the syntagmatic relation ? here we we be go to explore the correlated occurrence again base on the definition of syntagmatic relation .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283178	241	00:11:03.83	00:11:05.96	Here you see the same sample of text.	here you see the same sample of text .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283178	244	00:11:06.54	00:11:11.99	But here we are interested in knowing what other words are correlated with the verb eats.	but here we be interested in know what other word be correlate with the verb eat .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-12-14 09:23:29.555984	245	00:11:12.79	00:11:14.79	And what words can go with eat?	and what word can go with eat ?
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283179	248	00:11:16.24	00:11:25.83	And if you look at the right side of the slide and you will see I've taken away the two words around eats.	and if you look at the right side of the slide and you will see I 've take away the two word around eat .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.28318	251	00:11:26.97	00:11:33.97	I've taken away the word to its left and also the world to its right, In each sentence.	I 've take away the word to its left and also the world to its right , in each sentence .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-12-14 09:24:23.215319	253	00:11:34.56	00:11:41.19	And then we can ask the question what words tend to occur to the left of it.	and then we can ask the question what word tend to occur to the left of it .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-12-14 09:24:59.607508	255	00:11:43.52	00:11:47.95	and what word tend to occur to the right of eat?	and what word tend to occur to the right of eat ?
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283181	257	00:11:49.43	00:11:55.54	Now thinking about this question would help us discover Syntagmatic relations.	now think about this question would help we discover syntagmatic relation .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283182	259	00:11:56.12	00:12:01.18	Because syntagmatic relation essentially captures such correlations.	because syntagmatic relation essentially capture such correlation .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-12-14 09:26:06.045325	263	00:12:02.97	00:12:14.66	So the important question to ask for syntagmatic relation is whenever eats occurs, what other words also tend to occur?	so the important question to ask for syntagmatic relation be whenever eat occur , what other word also tend to occur ?
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283184	268	00:12:16.04	00:12:28.209999	So the question here has to do with whether there are some other words that tend to co-occur together with eats, meaning that whenever you see eat, you tend to see the other words.	so the question here have to do with whether there be some other word that tend to co - occur together with eat , mean that whenever you see eat , you tend to see the other word .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283184	270	00:12:29.48	00:12:34.76	And if you don't see, it's probably you don't see other words often either.	and if you do n't see , it be probably you do n't see other word often either .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-12-14 09:27:03.81443	272	00:12:36.45	00:12:40.24	So this intuition can help us discover syntagmatic relations.	so this intuition can help we discover syntagmatic relation .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-12-14 09:27:14.794889	273	00:12:41.38	00:12:43.29	Now again, consider example.	now again , consider example .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283185	275	00:12:44.03	00:12:48.5	How helpful is the occurrence of eats for predicting occurrence of meat?	how helpful be the occurrence of eat for predict occurrence of meat ?
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-12-14 09:28:56.118195	282	00:12:49.8	00:13:05.879999	knowing whether eat occurs in a sentence would generally help us predict the Whether meat also occurs indeed as if we will see eats occur in a sentence, and that should increase the chance that meat will also occur.	know whether eat occur in a sentence would generally help we predict the whether meat also occur indeed as if we will see eat occur in a sentence , and that should increase the chance that meat will also occur .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283188	286	00:13:08.35	00:13:15.74	In contrast, if you look at the question in the bottom, how helpful is occurrence of eats for predicting the occurrence of text?	in contrast , if you look at the question in the bottom , how helpful be occurrence of eat for predict the occurrence of text ?
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283189	292	00:13:17.22	00:13:29.41	Because eats and text are not really related, so knowing whether eats occurred in a sentence doesn't really help us predict whether text also occurs in the sentence.	because eat and text be not really relate , so know whether eat occur in a sentence do n't really help we predict whether text also occur in the sentence .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-12-14 09:30:07.136746	294	00:13:30.01	00:13:34.15	So this is in contrast to the question about eats and meat.	so this be in contrast to the question about eat and meat .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-12-14 09:30:38.856118	300	00:13:35.46	00:13:49.1	This also helps explain the intuition behind the methods for discovering syntagmatic relation. Mainly we need to capture the correlation between the occurrences of two words.	this also help explain the intuition behind the method for discover syntagmatic relation . mainly we need to capture the correlation between the occurrence of two word .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283192	303	00:13:50.31	00:13:55.94	So to summarize, the general ideas for discovering word associations or the following.	so to summarize , the general idea for discover word association or the following .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-12-14 09:31:58.541576	310	00:13:56.71	00:14:12.64	For paradigmatically relation we represent each word by its context, and then compute the context similarity. We can gonna assume the words that have high context similarity to have paradigmatic relation	for paradigmatically relation we represent each word by its context , and then compute the context similarity . we can gon na assume the word that have high context similarity to have paradigmatic relation
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283194	315	00:14:14.51	00:14:25.33	For syntagmatic relation, we will count how many times two words occur together in a context which can be a sentence, paragraph or a document even.	for syntagmatic relation , we will count how many time two word occur together in a context which can be a sentence , paragraph or a document even .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283195	318	00:14:26.76	00:14:31.69	And we're going to compare their Co occurrences with their individual occurrences.	and we be go to compare their co occurrence with their individual occurrence .
0d005f7e-0c3f-465b-b7fe-08f45355e3de	2020-11-02 23:02:18.283197	327	00:14:32.5	00:14:53.23	We're going to assume words with high co-occurrences, but relatively low individual occurrences to have syntagmatic relations because they tend to occur together, and they don't usually occur alone. Note that the paradigmatic relation and syntagmatic relation, are actually closely related.	we be go to assume word with high co - occurrence , but relatively low individual occurrence to have syntagmatic relation because they tend to occur together , and they do n't usually occur alone . note that the paradigmatic relation and syntagmatic relation , be actually closely related .
1cc2d7fa-3d11-49fa-b979-ef5e9442466f	2020-11-02 22:56:18.772803	2	00:00:00.3	00:00:05.02	This lecture is about the probabilistic latent semantic analysis or P LSA.	this lecture be about the probabilistic latent semantic analysis or P LSA .
1cc2d7fa-3d11-49fa-b979-ef5e9442466f	2020-11-02 22:56:18.772808	16	00:00:12.47	00:00:47.92	In this lecture we're going to introduce probabilistic latent semantic analysis, often called the PLSA. Say this is the most basic topic model. Also, one of the most useful topic models. Now, this kind of models can in general be used to mine multiple topics from text documents, and PLSA is one of the most basic topic models for doing this, so let's first examine this problem in a little more detail. Here I show a sample article which is a blog article about Hurricane Katrina.	in this lecture we be go to introduce probabilistic latent semantic analysis , often call the PLSA . say this be the most basic topic model . also , one of the most useful topic model . now , this kind of model can in general be use to mine multiple topic from text document , and PLSA be one of the most basic topic model for do this , so let 's first examine this problem in a little more detail . here I show a sample article which be a blog article about Hurricane Katrina .
1cc2d7fa-3d11-49fa-b979-ef5e9442466f	2020-11-02 22:56:18.77281	20	00:00:48.7	00:00:57.62	An I showed some sample topics, for example government response, flooding of the city in new orlean's donation and the background.	an I show some sample topic , for example government response , flooding of the city in new orlean 's donation and the background .
1cc2d7fa-3d11-49fa-b979-ef5e9442466f	2020-11-02 22:56:18.772825	54	00:00:59.13	00:02:12.9	You can see in the article we use words from all these distributions. So we first for example. See there's a criticism of government response, and this is followed by the discussion of flooding of the city and donation, etc. We also see background words or mixed with them, so the goal of topic analysis here is try to decode these topics behind the text. So segment of the topics to figure out which words are from which distribution and to figure out the first one of these topics. So how do we know there's a topic about government response? There is a public about the flooding of the city. So these are the tasks of topical model. If we can discover these topics can color this words as you see here to separate the different topics, then you can do a lot of things such as summarization or segmentation of the topics, clustering of sentences, etc. So the formal definition of the problem of mining multiple topics from text is shown here, and this is actually a slide that you have seen in the earlier lecture, so the input is the collection, the number of topics and vocabulary set.	you can see in the article we use word from all these distribution . so we first for example . see there be a criticism of government response , and this be follow by the discussion of flooding of the city and donation , etc . we also see background word or mix with they , so the goal of topic analysis here be try to decode these topic behind the text . so segment of the topic to figure out which word be from which distribution and to figure out the first one of these topic . so how do we know there be a topic about government response ? there be a public about the flooding of the city . so these be the task of topical model . if we can discover these topic can color this word as you see here to separate the different topic , then you can do a lot of thing such as summarization or segmentation of the topic , clustering of sentence , etc . so the formal definition of the problem of mining multiple topic from text be show here , and this be actually a slide that you have see in the early lecture , so the input be the collection , the number of topic and vocabulary set .
1cc2d7fa-3d11-49fa-b979-ef5e9442466f	2020-11-02 22:56:18.772829	66	00:02:13.46	00:02:41.47	And of course, the text data right? And then the output is of two kinds. One is the topic category characterization Seedies HCI is a water distribution and 2nd it's the topic coverage for each document. These are pie some ideas and they tell us which document covers which topic to what extent. So we hope to generate these as output because there are many useful applications if we can do that.	and of course , the text datum right ? and then the output be of two kind . one be the topic category characterization seedie HCI be a water distribution and 2nd it be the topic coverage for each document . these be pie some idea and they tell we which document cover which topic to what extent . so we hope to generate these as output because there be many useful application if we can do that .
1cc2d7fa-3d11-49fa-b979-ef5e9442466f	2020-11-02 22:56:18.772832	72	00:02:42.79	00:02:56.79	So the idea of PLSA is actually very similar to the two component mixture model that we have already introduced. The only difference is that we're going to have more than two topics. Otherwise it's essentially the same.	so the idea of PLSA be actually very similar to the two component mixture model that we have already introduce . the only difference be that we be go to have more than two topic . otherwise it be essentially the same .
1cc2d7fa-3d11-49fa-b979-ef5e9442466f	2020-11-02 22:56:18.772834	75	00:02:57.59	00:03:01.54	So here I illustrate how we can generate the text that I was multiple topics.	so here I illustrate how we can generate the text that I be multiple topic .
1cc2d7fa-3d11-49fa-b979-ef5e9442466f	2020-11-02 22:56:18.772836	81	00:03:02.6	00:03:17.38	And naturally, in all cases of probabilistic modeling, would want to figure out the likelihood function. So we will also ask the question what's the probability of observing a world W from such a mixture model?	and naturally , in all case of probabilistic modeling , would want to figure out the likelihood function . so we will also ask the question what be the probability of observe a world w from such a mixture model ?
1cc2d7fa-3d11-49fa-b979-ef5e9442466f	2020-11-02 22:56:18.772839	86	00:03:18.08	00:03:25.74	Now if you look at this picture and compare this with the picture that you have seen earlier, you will see the only difference is that we have added more topics here.	now if you look at this picture and compare this with the picture that you have see early , you will see the only difference be that we have add more topic here .
1cc2d7fa-3d11-49fa-b979-ef5e9442466f	2020-11-02 22:56:18.772847	105	00:03:26.76	00:04:12.05	So before we have just one topic besides the background topical, but now we have more topics. Specifically we have K topics. Now all these are topics that we assume that exist in the text data, so the consequences that our switch for choosing a topic now is multiway switch before it's just a two way switch. Going to think of as flipping a coin. But now we have multiple is. First we can flip a coin to decide whether we will talk about the background. So it's the background. Lambda sub B versus non background. So this one minus Lambda B gives us the probability of actually choosing a topic.	so before we have just one topic besides the background topical , but now we have more topic . specifically we have k topic . now all these be topic that we assume that exist in the text datum , so the consequence that our switch for choose a topic now be multiway switch before it be just a two way switch . go to think of as flip a coin . but now we have multiple be . first we can flip a coin to decide whether we will talk about the background . so it be the background . Lambda sub B versus non background . so this one minus Lambda B give we the probability of actually choose a topic .
1cc2d7fa-3d11-49fa-b979-ef5e9442466f	2020-11-02 22:56:18.772847	106	00:04:13.94	00:04:14.95	And background help.	and background help .
1cc2d7fa-3d11-49fa-b979-ef5e9442466f	2020-11-02 22:56:18.772849	109	00:04:15.68	00:04:23.92	After we have made this decision, we have to make another decision to choose one of these K distributions.	after we have make this decision , we have to make another decision to choose one of these K distribution .
1cc2d7fa-3d11-49fa-b979-ef5e9442466f	2020-11-02 22:56:18.77285	112	00:04:24.65	00:04:30.32	So there's a key way. Switch here, and this is characterized by the pies, and there's someone.	so there be a key way . switch here , and this be characterize by the pie , and there be someone .
1cc2d7fa-3d11-49fa-b979-ef5e9442466f	2020-11-02 22:56:18.772853	120	00:04:31.33	00:04:45.28	So this is just the different design of switches, a little bit more complicated, but once we decide which distribution to use, the rest is the same. We're going to generate the world by using one of these distributions, assume here.	so this be just the different design of switch , a little bit more complicated , but once we decide which distribution to use , the rest be the same . we be go to generate the world by use one of these distribution , assume here .
1cc2d7fa-3d11-49fa-b979-ef5e9442466f	2020-11-02 22:56:18.772855	125	00:04:46.17	00:04:56.48	OK, so now let's look at this question about the like hold. So what's the probability of observing a word from such a distribution? What do you think?	ok , so now let 's look at this question about the like hold . so what be the probability of observe a word from such a distribution ? what do you think ?
1cc2d7fa-3d11-49fa-b979-ef5e9442466f	2020-11-02 22:56:18.772864	145	00:04:57.12	00:05:39.4	Now we've seen this problem many Times Now, and if you recall, it's generally a sum over all the different possibilities of generating the world. So let's first look at the how the world can be generated from the background model. The probability that the world is generated from the background model is Lambda multiplied by the probability of the world from the background model, right? Two things must happen. First, we have to have chosen the background model. And that's probability of Lambda sub B and then the second we must have actually obtained the world W from the background, and that's probability of W given sit out submit.	now we 've see this problem many time now , and if you recall , it be generally a sum over all the different possibility of generate the world . so let 's first look at the how the world can be generate from the background model . the probability that the world be generate from the background model be Lambda multiply by the probability of the world from the background model , right ? two thing must happen . first , we have to have choose the background model . and that be probability of Lambda sub B and then the second we must have actually obtain the world W from the background , and that be probability of w given sit out submit .
1cc2d7fa-3d11-49fa-b979-ef5e9442466f	2020-11-02 22:56:18.772875	169	00:05:40.09	00:06:30.9	OK, so similarly we can figure out the probability of observing the world from another topic. A topical theta sub k. Now notice that here's a product of three terms, and that's the cause. The choice of topic theta sub k. K only happens if two things happen. One is we decided not to talk about background, so that's probability 1 minus Lambda sub b. Second, we also have to actually choose. Set us up. K among these K topics. So that's probability of serious up cake or pie. And similarly, the probability of generating the water from the second topic and his first topic popular like what you're seeing here and then. So in the end, the probability of observing the world is just a sum of all these cases.	ok , so similarly we can figure out the probability of observe the world from another topic . a topical theta sub k. now notice that here be a product of three term , and that be the cause . the choice of topic theta sub k. K only happen if two thing happen . one be we decide not to talk about background , so that be probability 1 minus Lambda sub b. Second , we also have to actually choose . set we up . k among these K topic . so that be probability of serious up cake or pie . and similarly , the probability of generate the water from the second topic and his first topic popular like what you be see here and then . so in the end , the probability of observe the world be just a sum of all these case .
1cc2d7fa-3d11-49fa-b979-ef5e9442466f	2020-11-02 22:56:18.772879	176	00:06:32.38	00:06:47.59	And I have to stress again, this is a very important formula to know because. This is really key to know to for understanding all the topic models and indeed a lot of mixture models, so make sure that you really understand the probability.	and I have to stress again , this be a very important formula to know because . this be really key to know to for understand all the topic model and indeed a lot of mixture model , so make sure that you really understand the probability .
1cc2d7fa-3d11-49fa-b979-ef5e9442466f	2020-11-02 22:56:18.772879	177	00:06:48.29	00:06:52.86	Of W is indeed the some of these terms.	of w be indeed the some of these term .
1cc2d7fa-3d11-49fa-b979-ef5e9442466f	2020-11-02 22:56:18.772887	195	00:06:56.44	00:07:31.77	So next, once we have the likelihood function, we would be interested in knowing the parameters right? So to estimate the parameters. But first let's put all these together to have the complete likelihood function for PLSA. Now the first line shows the probability of a word as illustrated on the previous slide and this is an important formula as I said. And so let's take a closer look at this. After that contains all the important parameters. So first we see Lambda sub b here. This represents the percentage of background words.	so next , once we have the likelihood function , we would be interested in know the parameter right ? so to estimate the parameter . but first let 's put all these together to have the complete likelihood function for PLSA . now the first line show the probability of a word as illustrate on the previous slide and this be an important formula as I say . and so let 's take a close look at this . after that contain all the important parameter . so first we see Lambda sub b here . this represent the percentage of background word .
1cc2d7fa-3d11-49fa-b979-ef5e9442466f	2020-11-02 22:56:18.772888	198	00:07:32.42	00:07:39.39	That would believe exist in the text data and this can be unknown value that we set empirically.	that would believe exist in the text datum and this can be unknown value that we set empirically .
1cc2d7fa-3d11-49fa-b979-ef5e9442466f	2020-11-02 22:56:18.772891	205	00:07:41.09	00:07:51.95	second we see the background language model and typically we also assume this is known. We can use a large collection of text or use all the tests that we have available to estimate the water distribution.	second we see the background language model and typically we also assume this be know . we can use a large collection of text or use all the test that we have available to estimate the water distribution .
1cc2d7fa-3d11-49fa-b979-ef5e9442466f	2020-11-02 22:56:18.772891	206	00:07:52.7	00:07:55.29	Now next in the rest of this formula.	now next in the rest of this formula .
1cc2d7fa-3d11-49fa-b979-ef5e9442466f	2020-11-02 22:56:18.772894	212	00:07:56.39	00:08:10.46	Excuse me, you see two interesting kinds of parameters. Those are the most important parameters that we are asked, so one is pies and these are the coverage of topic in the document.	excuse I , you see two interesting kind of parameter . those be the most important parameter that we be ask , so one be pie and these be the coverage of topic in the document .
1cc2d7fa-3d11-49fa-b979-ef5e9442466f	2020-11-02 22:56:18.772895	214	00:08:11.16	00:08:15.41	And the other is the word distributions that characterize all the topics.	and the other be the word distribution that characterize all the topic .
1cc2d7fa-3d11-49fa-b979-ef5e9442466f	2020-11-02 22:56:18.7729	227	00:08:17.93	00:08:48.91	So the next line then is simply to plug this in to calculate the probability of document. This is again of the familiar form where you have some and you have account of world in the document and then log of a probability. Now it's a little bit more complicated than the two component because now we have more components. So the sum involves more terms and then this line is just the like holder for the whole collection and it's very similar.	so the next line then be simply to plug this in to calculate the probability of document . this be again of the familiar form where you have some and you have account of world in the document and then log of a probability . now it be a little bit more complicated than the two component because now we have more component . so the sum involve more term and then this line be just the like holder for the whole collection and it be very similar .
1cc2d7fa-3d11-49fa-b979-ef5e9442466f	2020-11-02 22:56:18.772901	229	00:08:49	00:08:51.23	Just accounting for more documents in the collection.	just account for more document in the collection .
1cc2d7fa-3d11-49fa-b979-ef5e9442466f	2020-11-02 22:56:18.772902	233	00:08:52.37	00:08:58.35	So what are the unknown primers? I already said there are two kinds on his coverage. One is awarded distributions.	so what be the unknown primer ? I already say there be two kind on his coverage . one be award distribution .
1cc2d7fa-3d11-49fa-b979-ef5e9442466f	2020-11-02 22:56:18.772903	236	00:08:59.04	00:09:04.92	Again, it's a useful exercise for you to figure out exactly how many premise there on here.	again , it be a useful exercise for you to figure out exactly how many premise there on here .
1cc2d7fa-3d11-49fa-b979-ef5e9442466f	2020-11-02 22:56:18.772907	244	00:09:05.66	00:09:22.609999	How many unknown parameters are there? Now trying to figure out that question would help you understand the model in more detail, and it would also allow you to understand what would be the output that we generate when we use PLSA to analyze text data, and these are precisely the unknown parameters.	how many unknown parameter be there ? now try to figure out that question would help you understand the model in more detail , and it would also allow you to understand what would be the output that we generate when we use PLSA to analyze text datum , and these be precisely the unknown parameter .
1cc2d7fa-3d11-49fa-b979-ef5e9442466f	2020-11-02 22:56:18.772908	248	00:09:24.38	00:09:30.99	So after we have obtained the likelihood function shown here, the next is to worry about parameter estimation.	so after we have obtain the likelihood function show here , the next be to worry about parameter estimation .
1cc2d7fa-3d11-49fa-b979-ef5e9442466f	2020-11-02 22:56:18.772913	260	00:09:31.93	00:09:55.8	And we can do the usual thing. Maximum likelihood estimator. So again, it's a constrained optimization problem like what we have seen before, only that we have a collection of text and we have more parameters to estimate and we still have two constraints, different constraint, two kinds of constraints. One is awarded distributions. All the words must have probabilities that sum to 141 distribution.	and we can do the usual thing . maximum likelihood estimator . so again , it be a constrained optimization problem like what we have see before , only that we have a collection of text and we have more parameter to estimate and we still have two constraint , different constraint , two kind of constraint . one be award distribution . all the word must have probability that sum to 141 distribution .
1cc2d7fa-3d11-49fa-b979-ef5e9442466f	2020-11-02 22:56:18.772915	266	00:09:56.38	00:10:08.11	The other is the topic coverage distribution. Anna Document will have to cover precisely these K topics, so the probability of covering each topical would have to sum to one.	the other be the topic coverage distribution . Anna Document will have to cover precisely these K topic , so the probability of cover each topical would have to sum to one .
20703c3c-ced6-4410-ace1-139baa46505c	2020-11-02 23:10:04.613494	8	00:00:00.3	00:00:17.9	So I just showed you that empirically the likelihood will converge, but theoretically it can also be proved that EM algorithm with converge to a local maximum. So here is just the illustration of what happened an A detailed explanation this.	so I just show you that empirically the likelihood will converge , but theoretically it can also be prove that EM algorithm with converge to a local maximum . so here be just the illustration of what happen an A detailed explanation this .
20703c3c-ced6-4410-ace1-139baa46505c	2020-11-02 23:10:04.613495	9	00:00:26.36	00:00:29.39	Require more.	require more .
20703c3c-ced6-4410-ace1-139baa46505c	2020-11-02 23:10:04.613496	12	00:00:30.62	00:00:37.1	Knowledge about some of the inequalities that we haven't really covered yet.	knowledge about some of the inequality that we have n't really cover yet .
20703c3c-ced6-4410-ace1-139baa46505c	2020-11-02 23:10:04.613499	18	00:00:38.17	00:00:48.83	So here what you see is on the X dimension. We have set up value. This is the parameter that we left on the Y axis. We see the likelihood function.	so here what you see be on the x dimension . we have set up value . this be the parameter that we leave on the Y axis . we see the likelihood function .
20703c3c-ced6-4410-ace1-139baa46505c	2020-11-02 23:10:04.613501	21	00:00:50.7	00:00:57.41	So this curve is reaching or like roller function, right? So this one.	so this curve be reach or like roller function , right ? so this one .
20703c3c-ced6-4410-ace1-139baa46505c	2020-11-02 23:10:04.613502	24	00:00:58.04	00:01:05.7	And this is the one that we hope to maximize an we hope to find a set of value at this point to maximize this.	and this be the one that we hope to maximize an we hope to find a set of value at this point to maximize this .
20703c3c-ced6-4410-ace1-139baa46505c	2020-11-02 23:10:04.61351	43	00:01:06.55	00:01:49.25	But in the case of mixture model, we cannot easily find the analytical solution to the problem. So we have to resolve a numerical algorithm. An EM algorithm is such an algorithm. It's a Hill climb algorithm that would mean you start with some random guess. Let's say you start from here. That's your starting point and then you try to improve this by moving this to another point where you can have a higher like recorder. So that's the idea of Hill climbing. Any in the MRI was the way we achieve this is to do two things. First will fix a lower bound of likelihood function, so this is the lower bound you can see here.	but in the case of mixture model , we can not easily find the analytical solution to the problem . so we have to resolve a numerical algorithm . an EM algorithm be such an algorithm . it be a Hill climb algorithm that would mean you start with some random guess . let 's say you start from here . that be your starting point and then you try to improve this by move this to another point where you can have a high like recorder . so that be the idea of Hill climbing . any in the mri be the way we achieve this be to do two thing . first will fix a lower bind of likelihood function , so this be the lower bind you can see here .
20703c3c-ced6-4410-ace1-139baa46505c	2020-11-02 23:10:04.613513	51	00:01:51.15	00:02:12.14	An once we fit the lower bound we can then maximise the lower bound and of course the reason why this works is because the lower bound is much easier to optimize so we know our current gas is here an by maximizing the lower bound will move this point to the top two here.	an once we fit the lower bind we can then maximise the lower bind and of course the reason why this work be because the lower bind be much easy to optimize so we know our current gas be here an by maximize the lower bind will move this point to the top two here .
20703c3c-ced6-4410-ace1-139baa46505c	2020-11-02 23:10:04.613513	52	00:02:13.12	00:02:13.73	I.	I.
20703c3c-ced6-4410-ace1-139baa46505c	2020-11-02 23:10:04.613515	55	00:02:14.54	00:02:19.49	And that we can then map to the original like role function. We find this point.	and that we can then map to the original like role function . we find this point .
20703c3c-ced6-4410-ace1-139baa46505c	2020-11-02 23:10:04.613515	57	00:02:20.06	00:02:24.62	Be cause it's a lower bound, we are guaranteed to improve this gas.	be cause it be a lower bind , we be guarantee to improve this gas .
20703c3c-ced6-4410-ace1-139baa46505c	2020-11-02 23:10:04.613519	66	00:02:25.26	00:02:45.689999	Right, because we improve our lower bound and then the original lighter Holder curve which is above this lower bound will definitely be improved as well. I so we already know it's improving the lower bound, so we definitely improve this original like record function which is above this lower bound.	right , because we improve our lower bind and then the original light Holder curve which be above this lower bind will definitely be improve as well . I so we already know it be improve the lower bind , so we definitely improve this original like record function which be above this lower bind .
20703c3c-ced6-4410-ace1-139baa46505c	2020-11-02 23:10:04.613523	75	00:02:47.42	00:03:08.15	So in our example, the current gas is parameter value given by the current generation and then the next guest is the RE estimated parameter values. From this illustration you can see the next gas is always better than the current gas unless it has reached the maximum where it would be stuck there. So the two would be equal.	so in our example , the current gas be parameter value give by the current generation and then the next guest be the RE estimate parameter value . from this illustration you can see the next gas be always well than the current gas unless it have reach the maximum where it would be stick there . so the two would be equal .
20703c3c-ced6-4410-ace1-139baa46505c	2020-11-02 23:10:04.613523	77	00:03:09.73	00:03:16.38	So the E step is basically to compute this lower bound.	so the e step be basically to compute this lower bind .
20703c3c-ced6-4410-ace1-139baa46505c	2020-11-02 23:10:04.613528	88	00:03:17.5	00:03:36.78	And we don't direct it, just computed this likely or function, but we computed the latent variable values and. These are basically part of this lower bound. This helps determine the lower bound the M step on the other hand, is to maximize the lower bound. It allows us to move parameters to a new point.	and we do n't direct it , just compute this likely or function , but we compute the latent variable value and . these be basically part of this lower bind . this help determine the lower bind the M step on the other hand , be to maximize the lower bind . it allow we to move parameter to a new point .
20703c3c-ced6-4410-ace1-139baa46505c	2020-11-02 23:10:04.613528	90	00:03:37.37	00:03:41.64	And that's why EML is gone. The little converge to a local maximum.	and that be why EML be go . the little converge to a local maximum .
20703c3c-ced6-4410-ace1-139baa46505c	2020-11-02 23:10:04.613535	108	00:03:42.37	00:04:18.07	Now, as you can imagine, when we have many local Maxima, we also have to repeat the EML with multiple times in order to figure out which one is the actual global maximum. And this actually in general is a difficult problem in numerical optimization. So here for example, how do we start from here? Then we gradually just climb up to this top, so that's not optimal, and we'd like to climb up all the way to here. So the only way to climb up to this here. This will start from somewhere here or here. Right so.	now , as you can imagine , when we have many local Maxima , we also have to repeat the EML with multiple time in order to figure out which one be the actual global maximum . and this actually in general be a difficult problem in numerical optimization . so here for example , how do we start from here ? then we gradually just climb up to this top , so that be not optimal , and we 'd like to climb up all the way to here . so the only way to climb up to this here . this will start from somewhere here or here . right so .
20703c3c-ced6-4410-ace1-139baa46505c	2020-11-02 23:10:04.613536	112	00:04:19.63	00:04:28.1	In the EM algorithm, we generally would have to start from different points or have some other way to determine a good initial starting point.	in the EM algorithm , we generally would have to start from different point or have some other way to determine a good initial starting point .
20703c3c-ced6-4410-ace1-139baa46505c	2020-11-02 23:10:04.613539	122	00:04:29.13	00:04:47.74	To summarize, in this lecture we introduce the EM algorithm. This is a general algorithm for computing. Maximum regular is made of all kinds of mixture models. So not just for our simple mixture model and so here climbing algorithm so can only converge it or local maximum, and it would depend on initial points.	to summarize , in this lecture we introduce the EM algorithm . this be a general algorithm for compute . maximum regular be make of all kind of mixture model . so not just for our simple mixture model and so here climb algorithm so can only converge it or local maximum , and it would depend on initial point .
20703c3c-ced6-4410-ace1-139baa46505c	2020-11-02 23:10:04.613543	132	00:04:49.67	00:05:10.16	The general idea is that we will have two steps to improve the estimate of parameters in the E step. We roughly all augmenting our data by predicting values of useful hidden variables that we would use to simplify the estimation. In our case, this is the distribution that has been used to generate the world.	the general idea be that we will have two step to improve the estimate of parameter in the e step . we roughly all augment our datum by predict value of useful hidden variable that we would use to simplify the estimation . in our case , this be the distribution that have be use to generate the world .
20703c3c-ced6-4410-ace1-139baa46505c	2020-11-02 23:10:04.613548	146	00:05:11.03	00:05:44.66	In the end step, then would exploit such augmented data, which would make it easier to estimate the distribution to improve the estimate of parameters. Here improve is guaranteed in terms of the likelihood function. Note that it's not necessary that we will have a stable converged parameter values, even though the likelihood function is insured to increase. There are some properties that have to be satisfied in order for the parameters also too. Convert it to some stable value.	in the end step , then would exploit such augmented datum , which would make it easy to estimate the distribution to improve the estimate of parameter . here improve be guarantee in term of the likelihood function . note that it be not necessary that we will have a stable converge parameter value , even though the likelihood function be insure to increase . there be some property that have to be satisfied in order for the parameter also too . convert it to some stable value .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.24159	2	00:00:00.3	00:00:03.84	This lecture is about the syntagmatic relation discovery.	this lecture be about the syntagmatic relation discovery .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241591	3	00:00:10.68	00:00:11.62	An entropy.	an entropy .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-24 06:51:48.921223	8	00:00:13.26	00:00:21.65	In this lecture, we're going to continue talking about word Association mning. In particular, we can talk about how to discover syntagmatic relations.	in this lecture , we be go to continue talk about word Association mning . in particular , we can talk about how to discover syntagmatic relation .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241596	12	00:00:22.3	00:00:30.21	And we're going to start with the introduction of entropy, which is the basis for designing some measures for discovering such relations.	and we be go to start with the introduction of entropy , which be the basis for design some measure for discover such relation .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241601	22	00:00:32.36	00:00:59.97	By definition, Syntagmatic relations hold between words that have correlated Co occurrences. That means when we see one word occurs in the context, we tend to see the occurrence of the other word. So take a more specific example, here we can ask the question whenever eats occurs, but other words also tend to occur.	by definition , syntagmatic relation hold between word that have correlate co occurrence . that mean when we see one word occur in the context , we tend to see the occurrence of the other word . so take a more specific example , here we can ask the question whenever eat occur , but other word also tend to occur .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241606	31	00:01:01.02	00:01:18.97	Now looking at the sentence is on the left. We see some words that might occur together with eats like a cat, dog or fish is right. But if I take them out and if you look at the right side where we only show eats and some other words.	now look at the sentence be on the left . we see some word that might occur together with eat like a cat , dog or fish be right . but if I take they out and if you look at the right side where we only show eat and some other word .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241608	34	00:01:19.68	00:01:27.2	The question that is, can you predict what other words occur? To the left or to the right.	the question that is , can you predict what other word occur ? to the left or to the right .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-24 06:58:07.045891	39	00:01:28.17	00:01:37.64	Right, so this would force us to think about what other words are associated with eats. If they are associated with eats, they tend to occur in the context of eats.	right , so this would force we to think about what other word be associate with eat . if they be associate with eat , they tend to occur in the context of eat .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241614	45	00:01:38.32	00:01:53.07	So more specifically, our prediction problem is to take any text segment, which can be a sentence, paragraph or a document, and then I asked the question is a particular word present or absent in this segment.	so more specifically , our prediction problem be to take any text segment , which can be a sentence , paragraph or a document , and then I ask the question be a particular word present or absent in this segment .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241615	48	00:01:54.42	00:02:00.43	Right here we can ask the question about the world W is present or absent in this segment.	right here we can ask the question about the world W be present or absent in this segment .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241617	51	00:02:02.28	00:02:08.41	Now, what's interesting is that some words are actually easier for it, in other words.	now , what be interesting be that some word be actually easy for it , in other word .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-24 06:58:57.916768	53	00:02:09.71	00:02:14.98	If you take a look at the three words shown here, meet, the and Unicorn.	if you take a look at the three word show here , meet , the and Unicorn .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241619	55	00:02:16.08	00:02:18.35	Which one do you think it is easier to predict?	which one do you think it be easy to predict ?
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.24162	57	00:02:20.51	00:02:23.77	Now, if you think about it for a moment, you might conclude that.	now , if you think about it for a moment , you might conclude that .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-24 06:59:28.13274	60	00:02:24.36	00:02:30.94	The is easier to predict because it tends to occur everywhere, so I can just say with the in the semtence.	the be easy to predict because it tend to occur everywhere , so I can just say with the in the semtence .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241624	64	00:02:31.79	00:02:41.66	Unicorn is also relatively easy. Because Unicorn is rare, is very rare. And I can bet that it doesn't occur in this sentence.	Unicorn be also relatively easy . because Unicorn be rare , be very rare . and I can bet that it do n't occur in this sentence .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241627	69	00:02:42.44	00:02:52.65	But meat is somewhere in between in terms of frequency, and it makes it hard to predict because it's possible that it occurs in the sentence or the segment more accurately.	but meat be somewhere in between in term of frequency , and it make it hard to predict because it be possible that it occur in the sentence or the segment more accurately .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241627	71	00:02:53.71	00:02:56.71	But it may also not occur in the segment.	but it may also not occur in the segment .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241629	73	00:02:57.88	00:03:01.13	So now let's start this problem more formally.	so now let 's start this problem more formally .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241632	80	00:03:02.41	00:03:17.52	Alright, so the problem can be formally defined as predicting the value of a binary random variable. Here we denoted by X sub w, w denotes a word. So this random variable is associated with precisely one word.	alright , so the problem can be formally define as predict the value of a binary random variable . here we denote by X sub w , w denote a word . so this random variable be associate with precisely one word .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241636	88	00:03:18.26	00:03:36.37	When the value of the variable is 1, it means this word is present. When it's zero, it means the word is absent, and naturally the probabilities for one and zero should sum to 1. Because a word is either present or absent in the segment. There's no other choice.	when the value of the variable be 1 , it mean this word be present . when it be zero , it mean the word be absent , and naturally the probability for one and zero should sum to 1 . because a word be either present or absent in the segment . there be no other choice .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241639	93	00:03:38.18	00:03:48.64	So the intuition we discussed earlier can be formally stated as follows. The more random this random variable is, the more difficult the prediction would be.	so the intuition we discuss early can be formally state as follow . the more random this random variable be , the more difficult the prediction would be .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241641	98	00:03:49.6	00:04:00.98	Now the question is, how does one quantitatively measure the randomness of a random variable like X sub w, how in general, can we quantify the randomness of a variable?	now the question be , how do one quantitatively measure the randomness of a random variable like X sub w , how in general , can we quantify the randomness of a variable ?
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241644	103	00:04:01.58	00:04:09.89	And that's why we need a measure called entropy. And this is a measure introduced in information theory to measure the randomness of X.	and that be why we need a measure call entropy . and this be a measure introduce in information theory to measure the randomness of X.
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241646	106	00:04:10.46	00:04:15.65	There is also some connection with the information here, but that's beyond the scope of this course.	there be also some connection with the information here , but that be beyond the scope of this course .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241652	121	00:04:17.35	00:04:52.24	So for our purpose we just treat the entropy function as a function defined on a random variable. In this case it's a binary random variable, although the definition can be easily generalized for a random variable with multiple values. Now the function form looks like this. There's a sum over all the possible values for this random variable inside the sum, for each value we have a product of the probability that the random variable equals this value and log of this probability.	so for our purpose we just treat the entropy function as a function define on a random variable . in this case it be a binary random variable , although the definition can be easily generalize for a random variable with multiple value . now the function form look like this . there be a sum over all the possible value for this random variable inside the sum , for each value we have a product of the probability that the random variable equal this value and log of this probability .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241653	123	00:04:53.25	00:04:55.46	And note that there is also an negative sign there.	and note that there be also an negative sign there .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-24 07:04:19.44929	125	00:04:56.1	00:05:01.63	Now, entropy in general is not negative and that can be mathematically proved.	now , entropy in general be not negative and that can be mathematically prove .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241656	129	00:05:02.43	00:05:14.79	So if we expand this sum will see the equation looks like a second one I explicitly plugged in the two values zero and one.	so if we expand this sum will see the equation look like a second one I explicitly plug in the two value zero and one .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241658	132	00:05:15.99	00:05:26.15	And sometimes when we have 0 log of 0, we would generally find that as zero because log of 0 is undefined.	and sometimes when we have 0 log of 0 , we would generally find that as zero because log of 0 be undefined .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241659	136	00:05:28.35	00:05:35.85	So this is the entropy function and this function will give a different value for different distributions of this random variable.	so this be the entropy function and this function will give a different value for different distribution of this random variable .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-24 07:05:58.802322	143	00:05:37.12	00:05:59.21	And this clear it clearly depends on the probability that the random variable taking a value of one or zero. If we plotted his function against the probability that the random variable is equal to 1 and then the function looks like this.	and this clear it clearly depend on the probability that the random variable take a value of one or zero . if we plot his function against the probability that the random variable be equal to 1 and then the function look like this .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-24 07:06:42.43906	144	00:06:00.47	00:06:02.85	At the two ends,	at the two end ,
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241666	149	00:06:03.49	00:06:18.55	That means when the probability of X = 1 is very small or very large, then the entropy function has a lower value when it's .5 in the middle that it reaches the maximum.	that mean when the probability of x = 1 be very small or very large , then the entropy function have a low value when it be .5 in the middle that it reach the maximum .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241668	154	00:06:20.04	00:06:34.6	Now, if we plot the function against the probability that the X is taking a value of 0 and it the function would show exactly the same curve here.	now , if we plot the function against the probability that the x be take a value of 0 and it the function would show exactly the same curve here .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-24 07:07:37.698689	155	00:06:35.27	00:06:38.18	And you can imagine why.	and you can imagine why .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-24 07:08:12.383179	157	00:06:39.11	00:06:44.43	that's because the two probabilities are symmetric.	that be because the two probability be symmetric .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-24 07:08:30.034754	158	00:06:45.45	00:06:46.97	and completely symmetric.	and completely symmetric .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-24 07:09:48.371496	169	00:06:48.62	00:07:14.22	So an interesting question. You could think about in general here is for what kind of X? Does the entropy reached maximum or minimum and we can in particular think about some special cases. For example, in one case we might have a random variable that always takes the value of what the the probability is. one.	so an interesting question . you could think about in general here be for what kind of x ? do the entropy reach maximum or minimum and we can in particular think about some special case . for example , in one case we might have a random variable that always take the value of what the the probability be . one .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-24 07:10:03.03898	170	00:07:15.94	00:07:18.9	or there is a random variable that	or there be a random variable that
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241676	174	00:07:19.77	00:07:28.96	Is equally likely taking a value of 1 or 0. In this case, the probability that X = 1 is .5.	be equally likely take a value of 1 or 0 . in this case , the probability that x = 1 be .5 .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241677	175	00:07:30.54	00:07:32.47	Now, which one has a higher entropy?	now , which one have a high entropy ?
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241678	177	00:07:34.49	00:07:38.97	It's easier to look at the problem by thinking of simple example.	it be easy to look at the problem by think of simple example .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-24 07:11:02.408936	181	00:07:40.19	00:07:51.18	Using coin tossing, so when we think about the random experiment like a tossing a coin it gives us a random variable that.	use coin tossing , so when we think about the random experiment like a toss a coin it give we a random variable that .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-24 07:11:15.197838	182	00:07:52.4	00:07:54.97	can represent the result.	can represent the result .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-24 07:11:38.719132	187	00:07:55.62	00:08:08.75	It can be head or tail, so we can define a random variable X sub coin so that it's one when the coin shows up as head, it's zero when the coin shows up as tail.	it can be head or tail , so we can define a random variable x sub coin so that it be one when the coin show up as head , it be zero when the coin show up as tail .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241684	192	00:08:09.63	00:08:23.11	So now we can compute the entropy of this random variable, and this entropy indicates how difficult it is to predict the outcome of a coin for coin tossing.	so now we can compute the entropy of this random variable , and this entropy indicate how difficult it be to predict the outcome of a coin for coin tossing .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-24 07:12:14.19828	198	00:08:25.32	00:08:36.41	So we can think about the two cases. One is a fair coin, it's completely fair. The coin shows up as head hotel equally likely, so the two probabilities would be,	so we can think about the two case . one be a fair coin , it be completely fair . the coin show up as head hotel equally likely , so the two probability would be ,
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241688	200	00:08:37.91	00:08:42.98	1/2 right so both will have both equal to 1/2.	1/2 right so both will have both equal to 1/2 .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241689	204	00:08:44.55	00:08:52.94	Another extreme case is completely biased coin, where the coin always shows up as head, so it's a completely biased coin.	another extreme case be completely biased coin , where the coin always show up as head , so it be a completely biased coin .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241692	210	00:08:54.5	00:09:09.69	Now let's think about the entropies in the two cases, and if you plug in these values you can see the entropies, would be as follows for a fair coin we see the entropy reaches its maximum, that's one.	now let 's think about the entropy in the two case , and if you plug in these value you can see the entropy , would be as follow for a fair coin we see the entropy reach its maximum , that be one .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241697	221	00:09:11.12	00:09:39.57	For the completely biased coin we see is 0 and that intuitively makes a lot of sense because a fair coin is most difficult to predict whereas a completely biased coin is very easy to predict that we can always say it's a head because it is a head all the time so they can be shown on the curve as follows. So the fair coin corresponds to the middle point, or it's very uncertain.	for the completely biased coin we see be 0 and that intuitively make a lot of sense because a fair coin be most difficult to predict whereas a completely biased coin be very easy to predict that we can always say it be a head because it be a head all the time so they can be show on the curve as follow . so the fair coin correspond to the middle point , or it be very uncertain .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241699	225	00:09:40.12	00:09:48.27	The completely biased coin corresponds to the end point. We have a probability of 1.0 and the entropy is 0.	the completely biased coin correspond to the end point . we have a probability of 1.0 and the entropy be 0 .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-24 07:14:55.00681	234	00:09:50.01	00:10:05.55	So now let's see how we can use entropy for word prediction. Now the problem, let's think about our problem right, still predicted whether W is present or absolutely in this segment. Again, think about the three words. Particularly, think about their entropies.	so now let 's see how we can use entropy for word prediction . now the problem , let 's think about our problem right , still predict whether w be present or absolutely in this segment . again , think about the three word . particularly , think about their entropy .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241703	236	00:10:06.41	00:10:10.38	Now we can assume high entropy words are harder to predict.	now we can assume high entropy word be hard to predict .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-02 23:15:52.241705	239	00:10:11.26	00:10:18.4	And so we will now have quantitative way to tell us which word is harder to predict.	and so we will now have quantitative way to tell we which word be hard to predict .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-24 07:15:24.615617	241	00:10:20.73	00:10:24.26	Now if you look at the three words, meat, the and Unicorn again.	now if you look at the three word , meat , the and Unicorn again .
2736e0b3-cd3e-4760-b07e-e9aadcc588e2	2020-11-24 07:15:42.141724	246	00:10:25.24	00:10:36.13	An we clearly would expect the meat to have a high entropy, then the OR Unicorn. In fact, if you look at the entropy of the,	an we clearly would expect the meat to have a high entropy , then the OR Unicorn . in fact , if you look at the entropy of the ,
27d06808-2624-4922-a079-04dccb301dde	2020-11-02 23:15:04.997082	2	00:00:00.3	00:00:03.33	This lecture is about mixture model estimation.	this lecture be about mixture model estimation .
27d06808-2624-4922-a079-04dccb301dde	2020-11-02 23:15:04.997083	8	00:00:12.13	00:00:20.56	In this lecture, we're going to continue discussing probabilistic topic models. In particular, we're going to talk about how to estimate the parameters of a mixture model.	in this lecture , we be go to continue discuss probabilistic topic model . in particular , we be go to talk about how to estimate the parameter of a mixture model .
27d06808-2624-4922-a079-04dccb301dde	2020-12-01 03:57:05.779664	12	00:00:22.91	00:00:32.1	So let's first look at our motivation for using a mixture model and we hope to factor out the background words from the topic word distribution.	so let 's first look at our motivation for use a mixture model and we hope to factor out the background word from the topic word distribution .
27d06808-2624-4922-a079-04dccb301dde	2020-11-02 23:15:04.997085	16	00:00:33.28	00:00:44.39	So the idea is to assume that the text data actually contain two kinds of words. One kind is from the background here.	so the idea be to assume that the text datum actually contain two kind of word . one kind be from the background here .
27d06808-2624-4922-a079-04dccb301dde	2020-11-02 23:15:04.997085	19	00:00:45.7	00:00:54.01	So the is away etc and the other kind is from our topic word distribution that we're interested in.	so the be away etc and the other kind be from our topic word distribution that we be interested in .
27d06808-2624-4922-a079-04dccb301dde	2020-12-01 04:00:37.667915	28	00:00:56.23	00:01:19.3	So in order to solve this problem of factoring out background words, we can set up our mixture model as follows. We're going to assume that we already know the parameters of all the values for all the parameters in the mixture model except for the word distribution of theta sub d, which is our target. So this is the case of customizing a probalistic model so that we embed the unknown variables that we are interested in, but we are going to simplify other things.	so in order to solve this problem of factor out background word , we can set up our mixture model as follow . we be go to assume that we already know the parameter of all the value for all the parameter in the mixture model except for the word distribution of theta sub d , which be our target . so this be the case of customize a probalistic model so that we embed the unknown variable that we be interested in , but we be go to simplify other thing .
27d06808-2624-4922-a079-04dccb301dde	2020-12-01 04:03:02.573201	43	00:01:34.06	00:02:07.48	And this is a powerful way of customizing a model for a particular need. Now you can imagine we could have assumed that we also don't know the background word distribution, but in this case our goal is factor out precisely those high probability background words. So we assume the background model is already fixed. And the problem here is how can we adjust theta sub D in order to maximize the probability of the observed document here and we assume all the other parameters are known.	and this be a powerful way of customize a model for a particular need . now you can imagine we could have assume that we also do n't know the background word distribution , but in this case our goal be factor out precisely those high probability background word . so we assume the background model be already fix . and the problem here be how can we adjust theta sub d in order to maximize the probability of the observed document here and we assume all the other parameter be know .
27d06808-2624-4922-a079-04dccb301dde	2020-11-02 23:15:04.99709	46	00:02:09.34	00:02:14.72	Now, although we designed the model heuristically to try to factor out this background words.	now , although we design the model heuristically to try to factor out this background word .
27d06808-2624-4922-a079-04dccb301dde	2020-11-02 23:15:04.997092	53	00:02:16.62	00:02:31.86	It's unclear whether if we use maximum likelihood estimator we will actually end up having order distribution where the common words like the will be indeed having smaller probabilities than before. So now.	it be unclear whether if we use maximum likelihood estimator we will actually end up have order distribution where the common word like the will be indeed have small probability than before . so now .
27d06808-2624-4922-a079-04dccb301dde	2020-11-02 23:15:04.997093	61	00:02:32.54	00:02:52.74	In this case, it turns out that the answer is yes, and when we set up the problem is volume. this way when we use maximum likelihood estimator we will end up having a word distribution that where the common words will be factored out via the use of the background distribution.	in this case , it turn out that the answer be yes , and when we set up the problem be volume . this way when we use maximum likelihood estimator we will end up have a word distribution that where the common word will be factor out via the use of the background distribution .
27d06808-2624-4922-a079-04dccb301dde	2020-11-02 23:15:04.997095	74	00:02:53.48	00:03:23.759999	So to understand why this is so, it's useful to examine the behavior of a mixture model. So we're going to look at a very, very simple case in order to understand some interesting behaviors of a mixture model the observed patterns here actually are generalizable to mixture model in general, but it's much easier to understand this behavior when we use a very simple case like what we're seeing here. So specifically in this case.	so to understand why this be so , it be useful to examine the behavior of a mixture model . so we be go to look at a very , very simple case in order to understand some interesting behavior of a mixture model the observe pattern here actually be generalizable to mixture model in general , but it be much easy to understand this behavior when we use a very simple case like what we be see here . so specifically in this case .
27d06808-2624-4922-a079-04dccb301dde	2020-11-02 23:15:04.997096	79	00:03:24.1	00:03:33.47	Let's assume that the probability of choosing each of the two models is exactly the same, so we're going to flip fair coin to decide which model to use.	let 's assume that the probability of choose each of the two model be exactly the same , so we be go to flip fair coin to decide which model to use .
27d06808-2624-4922-a079-04dccb301dde	2020-11-02 23:15:04.997098	87	00:03:34.35	00:03:52.17	Furthermore, we are going to assume there are precisely two words: the and text. Obviously this is a very naive oversimplification of the actual text, but again it is useful to examine the behavior in such a special case.	furthermore , we be go to assume there be precisely two word : the and text . obviously this be a very naive oversimplification of the actual text , but again it be useful to examine the behavior in such a special case .
27d06808-2624-4922-a079-04dccb301dde	2020-12-01 04:06:32.344725	91	00:03:53.6	00:04:02.15	So we further assume that the background model gives probability of point nine to the word the and text .1.	so we far assume that the background model give probability of point nine to the word the and text .1 .
27d06808-2624-4922-a079-04dccb301dde	2020-12-01 04:06:54.359114	95	00:04:03.03	00:04:09.21	Now, let's also assume that our data is extremely simple. The document has just the two words text and the.	now , let 's also assume that our data be extremely simple . the document have just the two word text and the .
27d06808-2624-4922-a079-04dccb301dde	2020-12-01 04:07:23.221187	99	00:04:10.34	00:04:18.6	So now let's write down the likelihood function in such a case. First, what's the probability of text and what's the probability of the?	so now let 's write down the likelihood function in such a case . first , what be the probability of text and what be the probability of the ?
27d06808-2624-4922-a079-04dccb301dde	2020-11-02 23:15:04.997101	101	00:04:19.45	00:04:22.55	I hope by this point and you will be able to write it down.	I hope by this point and you will be able to write it down .
27d06808-2624-4922-a079-04dccb301dde	2020-11-02 23:15:04.997101	105	00:04:23.65	00:04:33.62	So the probability of text is basically the sum over 2 cases, where each case corresponds to each of the word distribution.	so the probability of text be basically the sum over 2 case , where each case correspond to each of the word distribution .
27d06808-2624-4922-a079-04dccb301dde	2020-11-02 23:15:04.997102	107	00:04:34.37	00:04:38.15	And it accounts for the two ways of generating text.	and it account for the two way of generate text .
27d06808-2624-4922-a079-04dccb301dde	2020-11-02 23:15:04.997103	111	00:04:39.39	00:04:49.62	An inside each case we have the probability of choosing the model which is .5 multiplied by the probability of observing text from that model.	an inside each case we have the probability of choose the model which be .5 multiply by the probability of observe text from that model .
27d06808-2624-4922-a079-04dccb301dde	2020-11-02 23:15:04.997103	114	00:04:50.26	00:04:57.56	Similarly, the would have a probability of the same form, just with different exact probabilities.	similarly , the would have a probability of the same form , just with different exact probability .
27d06808-2624-4922-a079-04dccb301dde	2020-11-02 23:15:04.997104	117	00:04:58.3	00:05:06.93	So naturally our likelihood function is just the product of the two, so it's very easy to see that.	so naturally our likelihood function be just the product of the two , so it be very easy to see that .
27d06808-2624-4922-a079-04dccb301dde	2020-11-02 23:15:04.997105	123	00:05:08.01	00:05:18.88	Once you understand what's the probability of each word, which is also why it's so important to understand what exactly the probability of observing each word from such a mixture model.	once you understand what be the probability of each word , which be also why it be so important to understand what exactly the probability of observe each word from such a mixture model .
27d06808-2624-4922-a079-04dccb301dde	2020-11-02 23:15:04.997108	134	00:05:19.55	00:05:39.8	Now the interesting question now is, how can we then optimize this likelihood? Well, you will notice that there were only two variables. They are precisely the two probabilities of the two words text and the given by theta sub D. And this is because we have assumed all the other parameters are known.	now the interesting question now be , how can we then optimize this likelihood ? well , you will notice that there be only two variable . they be precisely the two probability of the two word text and the give by theta sub D. and this be because we have assume all the other parameter be know .
27d06808-2624-4922-a079-04dccb301dde	2020-11-02 23:15:04.997109	140	00:05:41.14	00:05:53.37	So now the question is a very simple algebra question, right? So we have a simple expression with two variables and we hope to choose the values of these two variables to maximize this function.	so now the question be a very simple algebra question , right ? so we have a simple expression with two variable and we hope to choose the value of these two variable to maximize this function .
27d06808-2624-4922-a079-04dccb301dde	2020-11-02 23:15:04.99711	144	00:05:54.17	00:06:04.9	And the exercise that we have seen some simple algebra problems and note that the two probabilities must sum to one. So there's some constraint.	and the exercise that we have see some simple algebra problem and note that the two probability must sum to one . so there be some constraint .
27d06808-2624-4922-a079-04dccb301dde	2020-11-02 23:15:04.997111	151	00:06:06.23	00:06:20.45	If there were no constraint, of course we would set both probabilities to their maximum value, which would be one to maximize this. But we can't do that because text and the must sum to one. We can't give both a probability of 1.	if there be no constraint , of course we would set both probability to their maximum value , which would be one to maximize this . but we ca n't do that because text and the must sum to one . we ca n't give both a probability of 1 .
27d06808-2624-4922-a079-04dccb301dde	2020-11-02 23:15:04.997113	160	00:06:21.73	00:06:40.05	So now the question is how should we allocate the probability mass between the two words? What do you think? Now it would be useful to look at this formula for moment and to see what intuitively what we do in order to set these probabilities to maximize the value of this function.	so now the question be how should we allocate the probability mass between the two word ? what do you think ? now it would be useful to look at this formula for moment and to see what intuitively what we do in order to set these probability to maximize the value of this function .
27d06808-2624-4922-a079-04dccb301dde	2020-11-02 23:15:04.997115	170	00:06:42.25	00:07:04.68	OK, if we look into this further then we'll see some interesting behavior of the two component models in that there will be collaborating to maximize the probability of the observed data which is dictated by the maximum likelihood estimator. But there are also competing in someway an in particular they will be competing on the words.	ok , if we look into this far then we 'll see some interesting behavior of the two component model in that there will be collaborate to maximize the probability of the observe datum which be dictate by the maximum likelihood estimator . but there be also compete in someway an in particular they will be compete on the word .
27d06808-2624-4922-a079-04dccb301dde	2020-11-02 23:15:04.997117	178	00:07:05.24	00:07:20.3	And they will tend to bet high probabilities on different words to avoid this competition in some sense. Or to gain advantage in this competition. So again, looking at this objective function and we have a constraint. On the two probabilities.	and they will tend to bet high probability on different word to avoid this competition in some sense . or to gain advantage in this competition . so again , look at this objective function and we have a constraint . on the two probability .
27d06808-2624-4922-a079-04dccb301dde	2020-11-02 23:15:04.997117	179	00:07:21.25	00:07:21.92	Now.	now .
27d06808-2624-4922-a079-04dccb301dde	2020-11-02 23:15:04.997118	183	00:07:23.29	00:07:31.22	If you look at the formula intuitively, you might feel that you want to set the probability of text to be somewhat larger than the.	if you look at the formula intuitively , you might feel that you want to set the probability of text to be somewhat large than the .
27d06808-2624-4922-a079-04dccb301dde	2020-11-02 23:15:04.99712	193	00:07:32.03	00:07:55.27	And this intuition can be well supported by a mathematical fact, which is when the sum of two variables is a constant. Then the product of them, which is maximum when they are equal and this is a fact that we know from algebra. Now if we plug that in, it will mean would mean that we have to make the two probabilities equal.	and this intuition can be well support by a mathematical fact , which be when the sum of two variable be a constant . then the product of they , which be maximum when they be equal and this be a fact that we know from algebra . now if we plug that in , it will mean would mean that we have to make the two probability equal .
27d06808-2624-4922-a079-04dccb301dde	2020-11-02 23:15:04.997123	211	00:07:56.08	00:08:33.44	And when we make them equal an, if we consider the constraint that we can easy to solve this problem and the solution is the probability of text would be point nine and probability of the is point one. And as you can see, indeed the probability of text is now much larger than probability of the. This is not the case when we have just one distribution and this is clearly because of the use of the background model which assigns a very high probability to the and low probability to text. And if you look at the equation, you will see obviously some interaction of the two distributions here.	and when we make they equal an , if we consider the constraint that we can easy to solve this problem and the solution be the probability of text would be point nine and probability of the be point one . and as you can see , indeed the probability of text be now much large than probability of the . this be not the case when we have just one distribution and this be clearly because of the use of the background model which assign a very high probability to the and low probability to text . and if you look at the equation , you will see obviously some interaction of the two distribution here .
27d06808-2624-4922-a079-04dccb301dde	2020-11-02 23:15:04.997124	217	00:08:34.45	00:08:50.62	In particular, you will see in order to make them equal and then the probability assigned by theta sub D must be higher for a word that has a smaller probability given by the background.	in particular , you will see in order to make they equal and then the probability assign by theta sub d must be high for a word that have a small probability give by the background .
27d06808-2624-4922-a079-04dccb301dde	2020-11-02 23:15:04.997127	234	00:08:51.83	00:09:33.98	And, this is obvious from examining this equation because the background part is weak for text it's small. So in order to compensate for that we must make the probability of text given by theta sub D somewhat larger so that the two sides can be balanced. So this is in fact a very general behavior of this mixture model, and that is if one distribution assigns a high probability to one word than another, then the other distribution. Would tend to do the opposite. Basically it would discourage other distributions to do the same, and this is to balance them out so that we can account for all kinds of words.	and , this be obvious from examine this equation because the background part be weak for text it be small . so in order to compensate for that we must make the probability of text give by theta sub d somewhat large so that the two side can be balance . so this be in fact a very general behavior of this mixture model , and that be if one distribution assign a high probability to one word than another , then the other distribution . would tend to do the opposite . basically it would discourage other distribution to do the same , and this be to balance they out so that we can account for all kind of word .
27d06808-2624-4922-a079-04dccb301dde	2020-11-02 23:15:04.997131	244	00:09:34.56	00:09:57.52	And this also means that by using a background model that is fixed to assign high probabilities to background words, we can indeed encourage the unknown topic world distribution to assign smaller probabilities for such common words, instead put more probability mass on the content words that cannot be explained well by the background model.	and this also mean that by use a background model that be fix to assign high probability to background word , we can indeed encourage the unknown topic world distribution to assign small probability for such common word , instead put more probability mass on the content word that can not be explain well by the background model .
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-11-02 22:55:42.980868	20	00:00:00.29	00:00:54.99	So, as we explained different textual representation tends to enable different analysis. In particular, we can gradually add more and more deeper analysis results to represent text data, and that would open up more interesting representation opportunities and also analysis capacities. So this table summarizes what we have just seen. So the first column shows the text recognition, the second visualizes the generality of such representation, meaning whether we can do this kind of representation accurate before all the text data, or only some of them, and third column shows the enabled analysis techniques.	so , as we explain different textual representation tend to enable different analysis . in particular , we can gradually add more and more deep analysis result to represent text datum , and that would open up more interesting representation opportunity and also analysis capacity . so this table summarize what we have just see . so the first column show the text recognition , the second visualize the generality of such representation , mean whether we can do this kind of representation accurate before all the text datum , or only some of they , and third column show the enable analysis technique .
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-11-02 22:55:42.980877	35	00:00:55.92	00:01:27.47	And the final column shows some examples of application that can be achieved through this level of representation. So let's take a look at them. So as a string text can only be processed by using stream processing algorithms, but it's very robust, it's general. And there are still some interesting applications that can be done at this level. For example, compressing of text doesn't necessarily need to know the word boundaries. Although knowing word boundaries might actually also help.	and the final column show some example of application that can be achieve through this level of representation . so let 's take a look at they . so as a string text can only be process by use stream processing algorithm , but it be very robust , it be general . and there be still some interesting application that can be do at this level . for example , compress of text do n't necessarily need to know the word boundary . although know word boundary might actually also help .
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-11-02 22:55:42.980879	39	00:01:28.44	00:01:35.64	Word based representation is very important level of representation. It's quite general and relatively robust.	word base representation be very important level of representation . it be quite general and relatively robust .
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-11-02 22:55:42.980886	52	00:01:36.39	00:02:07.47	It can enable a lot of analysis techniques such as word relation analysis, topic analysis and sentiment analysis, and there are many applications that can be enabled by this kind of analysis. For example, Thesaurus discovery has to do with discovering related words and topic and opinion related applications are abundant, and there are for example, and people might be interested in knowing the major topics covered in the collection of text.	it can enable a lot of analysis technique such as word relation analysis , topic analysis and sentiment analysis , and there be many application that can be enable by this kind of analysis . for example , Thesaurus discovery have to do with discover related word and topic and opinion relate application be abundant , and there be for example , and people might be interested in know the major topic cover in the collection of text .
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-11-02 22:55:42.980887	53	00:02:08.09	00:02:09.76	And this can be the case.	and this can be the case .
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-12-14 08:17:12.901345	61	00:02:10.98	00:02:28.5	In research literature, a scientist want to know what are the most important research topics today or customer service people might want to know know what are the major complaints from their customers about by mining their email messages.	in research literature , a scientist want to know what be the most important research topic today or customer service people might want to know know what be the major complaint from their customer about by mine their email message .
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-11-02 22:55:42.980893	67	00:02:29.33	00:02:42.32	And business intelligence people might be interested in understanding consumers opinions about their products and competitors products to figure out the what are the winning features of their products.	and business intelligence people might be interested in understand consumer opinion about their product and competitor product to figure out the what be the win feature of their product .
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-11-02 22:55:42.980894	70	00:02:43.02	00:02:51.45	And in general there are many applications that can be enabled by the representation at this level.	and in general there be many application that can be enable by the representation at this level .
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-12-14 08:18:31.969896	77	00:02:53.6	00:03:06.84	Now moving down, we'll see we can gradually add additional representation by adding syntactic structures we can enable, Of course, syntactic graph analysis. We can use graph mining algorithms to analyze	now move down , we 'll see we can gradually add additional representation by add syntactic structure we can enable , of course , syntactic graph analysis . we can use graph mining algorithm to analyze
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-11-02 22:55:42.980898	78	00:03:07.45	00:03:08.59	Syntactic graphs.	syntactic graph .
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-11-02 22:55:42.9809	83	00:03:09.35	00:03:18.53	And some applications are related to this kind of representation. For example, stylistic analysis generally requires syntactical representation.	and some application be relate to this kind of representation . for example , stylistic analysis generally require syntactical representation .
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-11-02 22:55:42.980902	88	00:03:19.4	00:03:32.87	"Syntactical structure We can also generate the structure based feature features and those are features that might help us classify text objects into different categories.	" syntactical structure we can also generate the structure base feature feature and those be feature that might help we classify text object into different category .
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-12-14 08:20:32.313704	96	00:03:33.49	00:03:52.36	By looking at the structures, sometimes the classification can be more accurate. For example, if you want to classify articles into different category categories corresponding to different authors want to figure out which of the	by look at the structure , sometimes the classification can be more accurate . for example , if you want to classify article into different category category correspond to different author want to figure out which of the
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-11-02 22:55:42.980908	100	00:03:53.45	00:04:01.46	K authors has actually written this article. Then you generally need to look at the syntactic structures.	k author have actually write this article . then you generally need to look at the syntactic structure .
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-11-02 22:55:42.980912	109	00:04:03.23	00:04:27.63	When we add entities and relations, then we can enable lot of techniques such as knowledge graph analysis or information network analysis in general and this analysis would enable applications about entities, for example, discovery of all the knowledge and opinions about the real world energy entity.	when we add entity and relation , then we can enable lot of technique such as knowledge graph analysis or information network analysis in general and this analysis would enable application about entity , for example , discovery of all the knowledge and opinion about the real world energy entity .
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-11-02 22:55:42.980913	113	00:04:28.27	00:04:35.52	You can also use this level representation to integrate everything about entity from scattered sources.	you can also use this level representation to integrate everything about entity from scatter source .
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-11-02 22:55:42.980916	118	00:04:36.66	00:04:48.31	Finally, when we add logic predicates then we would enable logic inference ofcourse, and this can be very useful for integrative analysis of scattered knowledge.	finally , when we add logic predicate then we would enable logic inference ofcourse , and this can be very useful for integrative analysis of scatter knowledge .
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-11-02 22:55:42.980917	121	00:04:49.64	00:04:58.13	For example, we can also add ontology on top of the extracted information from text to make inferences.	for example , we can also add ontology on top of the extract information from text to make inference .
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-11-02 22:55:42.980919	125	00:04:59.7	00:05:07.94	A good example of application in this enabled by this level of representation is a intelligent knowledge assistant for biologists.	a good example of application in this enable by this level of representation be a intelligent knowledge assistant for biologist .
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-11-02 22:55:42.980926	144	00:05:08.57	00:05:56.26	And this is intended program that can help biologists manage all the relevant knowledge from literature about the research problem, such as understanding functions of genes. And the computer can make inferences about some of the hypothesis that biologist might be interesting, for example, whether a gene has a certain function and then the intelligent program can read the literature to extract the relevant facts. Doing by doing information extraction and then using a logical system to actually track that's the answers to researchers questioning about what genes are related to what functions.	and this be intend program that can help biologist manage all the relevant knowledge from literature about the research problem , such as understanding function of gene . and the computer can make inference about some of the hypothesis that biologist might be interesting , for example , whether a gene have a certain function and then the intelligent program can read the literature to extract the relevant fact . do by do information extraction and then use a logical system to actually track that be the answer to researcher question about what gene be relate to what function .
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-11-02 22:55:42.980928	150	00:05:57.86	00:06:10.82	So in order to support this level of application, we need to go as far as logical representation. Now this course is covering techniques mainly based on word based representation.	so in order to support this level of application , we need to go as far as logical representation . now this course be cover technique mainly base on word base representation .
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-11-02 22:55:42.98093	153	00:06:11.94	00:06:19.66	These techniques are general and robust and thus are more widely used in various applications.	these technique be general and robust and thus be more widely use in various application .
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-11-02 22:55:42.980932	158	00:06:20.77	00:06:33.05	In fact, in virtually all the text mining applications you need this level of representation and the techniques that support analysis of texting this level.	in fact , in virtually all the text mining application you need this level of representation and the technique that support analysis of texte this level .
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-11-02 22:55:42.980935	166	00:06:35.14	00:06:52.02	But obviously all these other levels can be combined and should be combined in order to support sophisticated applications. So to summarize, here are the major takeaway points. Text representation determines what kind of mining algorithms can be applied.	but obviously all these other level can be combine and should be combine in order to support sophisticated application . so to summarize , here be the major takeaway point . text representation determine what kind of mining algorithm can be apply .
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-11-02 22:55:42.980937	170	00:06:53.5	00:07:03.83	And there are multiple ways to represent text - strings, words, syntactic structures and the relation graphs, logical predicates, etc.	and there be multiple way to represent text - string , word , syntactic structure and the relation graph , logical predicate , etc .
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-11-02 22:55:42.980939	174	00:07:04.4	00:07:12.32	"And these different representations should in general be combined in real applications to the extent we can.	" and these different representation should in general be combine in real application to the extent we can .
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-11-02 22:55:42.980942	183	00:07:13.09	00:07:32.79	For example, if even if we cannot do accurately, this application of syntactic structures we can stick at partial structures extracted and if we can recognize some entities and that would be great. So in general we want to do as much as we can.	for example , if even if we can not do accurately , this application of syntactic structure we can stick at partial structure extract and if we can recognize some entity and that would be great . so in general we want to do as much as we can .
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-11-02 22:55:42.980944	187	00:07:34.37	00:07:41.57	And when different levels are combined together, we can enable richer analysis. More powerful analysis.	and when different level be combine together , we can enable rich analysis . more powerful analysis .
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-11-02 22:55:42.980949	196	00:07:42.26	00:08:02.16	This course, however, focuses on word based representation. Such techniques have also several advantages. First, they are general and robust, so they are applicable to any natural language. That's a big advantage over other approaches that rely on more fragile natural language processing techniques.	this course , however , focus on word base representation . such technique have also several advantage . first , they be general and robust , so they be applicable to any natural language . that be a big advantage over other approach that rely on more fragile natural language processing technique .
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-11-02 22:55:42.98095	199	00:08:03.38	00:08:10.77	Secondly, it does not require much manual effort or sometimes it does not require any manual effort.	secondly , it do not require much manual effort or sometimes it do not require any manual effort .
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-11-02 22:55:42.980951	202	00:08:11.39	00:08:18.5	So that's again important benefit, because that means you can apply directly to any application.	so that be again important benefit , because that mean you can apply directly to any application .
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-12-14 08:27:36.670121	205	00:08:20.61	00:08:27.93	Third, these techniques are actually surprisingly powerful and effective for many applications.	third , these technique be actually surprisingly powerful and effective for many application .
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-11-02 22:55:42.980953	207	00:08:29.05	00:08:32.32	Although not all, of course, as I just explained.	although not all , of course , as I just explain .
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-11-02 22:55:42.980954	211	00:08:34.19	00:08:44.82	Now they are very effective, partly because the words are invented by humans as basic units for communications.	now they be very effective , partly because the word be invent by human as basic unit for communication .
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-11-02 22:55:42.980955	214	00:08:45.48	00:08:51.13	So they are actually quite sufficient for representing all kinds of semantics.	so they be actually quite sufficient for represent all kind of semantic .
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-12-14 08:28:14.37352	216	00:08:53.57	00:08:59.04	So that makes this kind of word based representation also powerful.	so that make this kind of word base representation also powerful .
2997c717-2552-411d-9dc4-7e648e16bbf0	2020-11-02 22:55:42.980957	221	00:09:00.16	00:09:11.72	And finally such a word based representation and the techniques enabled by such a representation can be combined with many other sophisticated approaches.	and finally such a word base representation and the technique enable by such a representation can be combine with many other sophisticated approach .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-02 23:10:07.96869	7	00:00:00.3	00:00:13.64	This lecture is about the ordinal logistic regression for sentiment analysis. So this is our problem set up for a typical sentiment classification problem, or more specifically, rating prediction.	this lecture be about the ordinal logistic regression for sentiment analysis . so this be our problem set up for a typical sentiment classification problem , or more specifically , rating prediction .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-02 23:10:07.968698	25	00:00:21.38	00:01:07.63	We have an opinionated text document D as input an we want to generate as output already in the range of one through K, so it's discrete rating and thus this is a categorization problem. We have K categories here. Now we can use a regular text for categorization technique to solve this problem, but such a solution would not consider the order and dependency of the categories. Intuitively, the features that can distinguish Category 2 from 1 or rather rating 2 from 1 may be similar to those that can distinguish K from K - 1. For example, positive words generally suggest a higher rating.	we have an opinionate text document d as input an we want to generate as output already in the range of one through k , so it be discrete rating and thus this be a categorization problem . we have k category here . now we can use a regular text for categorization technique to solve this problem , but such a solution would not consider the order and dependency of the category . intuitively , the feature that can distinguish Category 2 from 1 or rather rate 2 from 1 may be similar to those that can distinguish K from K - 1 . for example , positive word generally suggest a high rating .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-02 23:10:07.968699	28	00:01:08.22	00:01:16.76	Now when we train categorisation program by treating these categories as independent, we would not capture this.	now when we train categorisation program by treat these category as independent , we would not capture this .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-02 23:10:07.968706	47	00:01:17.6	00:01:55.53	So what's the solution? In general, we can add order to classify and there are many different approaches, and here we are going to talk about one of them is called the ordinal logistic regression. Now let's first think about how we use logistic regression for binary setting categorization problem. So suppose we just want to distinguish it positive from negative and then it's just a two category categorization problem. So the predictors are represented as X and these are the features and there are M features altogether, which feature value is a real number, and this can be representation of a text document.	so what be the solution ? in general , we can add order to classify and there be many different approach , and here we be go to talk about one of they be call the ordinal logistic regression . now let 's first think about how we use logistic regression for binary set categorization problem . so suppose we just want to distinguish it positive from negative and then it be just a two category categorization problem . so the predictor be represent as X and these be the feature and there be M feature altogether , which feature value be a real number , and this can be representation of a text document .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-02 23:10:07.96871	59	00:01:56.93	00:02:27.82	And y has two values, binary response variable {0,1}. 1 means X is positive, 0 means X is negative. And then of course, this is a standard two category categorization problem. We can apply logistical regression. You may recall that in logistic regression we assume the log of probability that Y is equal to 1 is assumed to be a linear function of these features as shown here.	and y have two value , binary response variable { 0,1 } . 1 mean x be positive , 0 mean x be negative . and then of course , this be a standard two category categorization problem . we can apply logistical regression . you may recall that in logistic regression we assume the log of probability that Y be equal to 1 be assume to be a linear function of these feature as show here .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-02 23:10:07.968714	67	00:02:28.37	00:02:56.57	So this would allow us to also write the probability of y = 1 given X in this equation that you are seeing on the bottom, and so that's logistical function and you can see it relates this probability to probaility that y = 1 to the feature values.	so this would allow we to also write the probability of y = 1 give x in this equation that you be see on the bottom , and so that be logistical function and you can see it relate this probability to probaility that y = 1 to the feature value .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-02 23:10:07.968714	69	00:02:57.84	00:03:00.62	And of course, B_i is our parameters here.	and of course , B_i be our parameter here .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-02 23:10:07.968716	72	00:03:02.17	00:03:07.58	So this is just a direct application of logistical regression for binary categorization.	so this be just a direct application of logistical regression for binary categorization .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-28 08:31:48.368061	86	00:03:08.71	00:03:44.26	What if we have multiple categories, multiple levels? We actually use such a binary logistic regression program to solve this multi level rating prediction. And the idea is we can introduce multiple binary classifiers and each case we ask the classifier to predict whether the rating is J or above all the ratings lower than J. So when Y_j is equal to 1, it means rating is J or above. When it's zero, that means the rating is lower than J.	what if we have multiple category , multiple level ? we actually use such a binary logistic regression program to solve this multi level rating prediction . and the idea be we can introduce multiple binary classifier and each case we ask the classifier to predict whether the rating be J or above all the rating low than J. So when Y_j be equal to 1 , it mean rating be J or above . when it be zero , that mean the rating be low than J.
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-27 22:24:02.68394	96	00:03:45.24	00:04:12.169999	So basically, if we want to predict rating in the range of one through K, we first have one classifier to distinguish K versus others. And that's our classifier one, and then we're going to have another classifier to distinguish K - 1 from the rest. That's classifier two, and in the end we need a classifier to distinguish two and one	so basically , if we want to predict rating in the range of one through k , we first have one classifier to distinguish K versus other . and that be our classifier one , and then we be go to have another classifier to distinguish K - 1 from the rest . that be classifier two , and in the end we need a classifier to distinguish two and one
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-02 23:10:07.968725	98	00:04:13	00:04:16.56	So altogether we'll have K - 1 classifiers.	so altogether we 'll have K - 1 classifier .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-02 23:10:07.968729	106	00:04:17.72	00:04:37.67	Now if we do that of course, then we can also solve this problem, and the logistical regression program would be also very straightforward as you have just seen on the previous slide. Only that here we have more parameters because for each classify we need a different set of parameters.	now if we do that of course , then we can also solve this problem , and the logistical regression program would be also very straightforward as you have just see on the previous slide . only that here we have more parameter because for each classify we need a different set of parameter .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-02 23:10:07.96873	109	00:04:38.35	00:04:44.96	So now the logistic regression classifiers indexed by J, which corresponds to a reading level.	so now the logistic regression classifier index by J , which correspond to a reading level .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-27 22:24:57.390171	114	00:04:46.09	00:05:00.91	And I have also used offer subject to replace beta 0. And this is to make the notation more consistent with what we can show in the ordinal logistic regression.	and I have also use offer subject to replace beta 0 . and this be to make the notation more consistent with what we can show in the ordinal logistic regression .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-02 23:10:07.968739	132	00:05:02.1	00:05:51.15	So anyway, so here we now have basically K - 1 regular logistic regression classifiers. Each has its own set of parameters. So now with this approach we can now do rating prediction as follows. After we have trained these K - 1 logistic regression classifiers, separately of course, then we can take a new instance and then invoke a classifier sequentially to make the decision. So first let's look at the classifier that corresponds to level of rating K. So this classifier will tell us whether this object should have a rating of K or above.	so anyway , so here we now have basically K - 1 regular logistic regression classifier . each have its own set of parameter . so now with this approach we can now do rating prediction as follow . after we have train these K - 1 logistic regression classifier , separately of course , then we can take a new instance and then invoke a classifier sequentially to make the decision . so first let 's look at the classifier that correspond to level of rating K. so this classifier will tell we whether this object should have a rating of K or above .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-28 08:33:44.089034	136	00:05:53.38	00:06:01.13	And if its probability according to this logistical regression classifier is larger than .5, we're going to say yes, the rating is K.	and if its probability accord to this logistical regression classifier be large than .5 , we be go to say yes , the rating be K.
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-28 08:33:51.89061	142	00:06:02.3	00:06:17.77	Now, what if it's not as large as .5? Well, that means the reading is Below K, right? So now we need to invoke the next class file, which tells us whether it's above K - 1.	now , what if it be not as large as .5 ? well , that mean the reading be below K , right ? so now we need to invoke the next class file , which tell we whether it be above K - 1 .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-28 08:34:05.818126	146	00:06:18.46	00:06:25.31	It's at least K - 1 and if the probability is larger than .5 then will say well, then it's K -- 1.	it be at least K - 1 and if the probability be large than .5 then will say well , then it be k -- 1 .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-02 23:10:07.968747	151	00:06:26.32	00:06:36.35	What if it says no? Well, that means the rating will be even below K minus one, and so we're going to just keep invoking these classifiers until we hit the end.	what if it say no ? well , that mean the rating will be even below K minus one , and so we be go to just keep invoke these classifier until we hit the end .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-02 23:10:07.96875	161	00:06:37.07	00:06:59.55	When we need to decide whether it's two or one, so this will help us solve the problem, right? So we can have a classifier that would actually give us a prediction of rating in the range of one through K, unfortunately, such a strategy is not the optimal way of solving this problem, and specifically there are two problems with this approach.	when we need to decide whether it be two or one , so this will help we solve the problem , right ? so we can have a classifier that would actually give we a prediction of rating in the range of one through k , unfortunately , such a strategy be not the optimal way of solve this problem , and specifically there be two problem with this approach .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-02 23:10:07.96875	163	00:07:00.54	00:07:05.32	So these equations are the same as you have seen before.	so these equation be the same as you have see before .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-28 08:34:47.780009	173	00:07:06.04	00:07:27.22	Now the first problem is that there are just too many parameters. There are many parameters. Now can you count how many parameters we have exactly here? Now this may be interesting exercise. To do so you might want to just pause the video and try to figure out the solution how many premises we have for each classifier?	now the first problem be that there be just too many parameter . there be many parameter . now can you count how many parameter we have exactly here ? now this may be interesting exercise . to do so you might want to just pause the video and try to figure out the solution how many premise we have for each classifier ?
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-02 23:10:07.968756	174	00:07:28.47	00:07:30.4	And how many classifiers do we have?	and how many classifier do we have ?
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-02 23:10:07.96876	186	00:07:31.76	00:08:03.4	You can see the answer is that for each classifier we have N + 1 parameters. And we have K - 1 classifiers altogether, so the total number of premises (K - 1) * (M + 1). That's alot alot of parameters. So when the classifier has a lot of parameters would in general need a lot of data to actually help us training data to help us decide the optimal parameters of the this such a complex model?	you can see the answer be that for each classifier we have n + 1 parameter . and we have K - 1 classifier altogether , so the total number of premise ( k - 1 ) * ( M + 1 ) . that be alot alot of parameter . so when the classifier have a lot of parameter would in general need a lot of datum to actually help we training datum to help we decide the optimal parameter of the this such a complex model ?
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-02 23:10:07.968761	187	00:08:04.35	00:08:06.17	So that's not the idea.	so that be not the idea .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-02 23:10:07.968762	191	00:08:07.1	00:08:17.43	The second problem is that these problems these K - 1 classifiers are not really independent. These problems are actually dependent.	the second problem be that these problem these K - 1 classifier be not really independent . these problem be actually dependent .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-27 22:28:04.71173	204	00:08:18.27	00:08:56.77	In general, words that are positive would make the rating higher and for any of these classifiers, for all these classifiers. So we should be able to take advantage of this factor. Now the idea of ordinal logistic regression is precisely that A key idea is just the improvement over the K -1 independent logistical regression classifiers, and that idea is to tie these beta parameters and that means we are going to assume the	in general , word that be positive would make the rating higher and for any of these classifier , for all these classifier . so we should be able to take advantage of this factor . now the idea of ordinal logistic regression be precisely that a key idea be just the improvement over the K -1 independent logistical regression classifier , and that idea be to tie these beta parameter and that mean we be go to assume the
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-02 23:10:07.968768	207	00:08:57.52	00:09:04.53	Beta parameters these are the parameters that indicate the influence of those weights.	beta parameter these be the parameter that indicate the influence of those weight .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-02 23:10:07.96877	213	00:09:05.17	00:09:18.12	And we're going to assume these better values are the same for all the K - 1 premise, and this just encodes our intuition that positive words in general would make a higher rating more likely.	and we be go to assume these well value be the same for all the K - 1 premise , and this just encode our intuition that positive word in general would make a high rating more likely .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-02 23:10:07.968772	218	00:09:19.45	00:09:27.49	So this is intuitively appealing assumption. It's reasonable for our problem set up when we have this order in these categories.	so this be intuitively appeal assumption . it be reasonable for our problem set up when we have this order in these category .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-02 23:10:07.968773	222	00:09:28.5	00:09:37.66	Now, in fact, this would allow us to have two positive benefit of one is it's going to reduce the number of parameters significantly.	now , in fact , this would allow we to have two positive benefit of one be it be go to reduce the number of parameter significantly .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-02 23:10:07.968775	227	00:09:38.64	00:09:49.34	And the other is to allow us to share the training data, because all these parameters are assumed to be equal. So these training data for different classifiers.	and the other be to allow we to share the training datum , because all these parameter be assume to be equal . so these training datum for different classifier .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-02 23:10:07.968776	229	00:09:50.17	00:09:55.43	Can then be shared to help us set the optimal value for beta.	can then be share to help we set the optimal value for beta .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-27 22:29:01.05784	231	00:09:56.19	00:10:00.12	So we have more data to help us choose a good beta value.	so we have more datum to help we choose a good beta value .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-27 22:29:15.291212	238	00:10:01.72	00:10:17.97	So what's the consequence? The formula would look very similar to what you have seen before, only that now the beta parameter has just one index that correspond to the feature and no longer has the other index that corresponds to the level of rating.	so what be the consequence ? the formula would look very similar to what you have see before , only that now the beta parameter have just one index that correspond to the feature and no long have the other index that correspond to the level of rating .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-02 23:10:07.968779	241	00:10:19.16	00:10:25.63	So that means we tie them together and there's only one set of beta values for all the classifiers.	so that mean we tie they together and there be only one set of beta value for all the classifier .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-28 08:37:11.481927	261	00:10:26.21	00:11:13.85	However, each classifiers there has a distinct Alpha value, the Alpha parameter, the except it's different and this is of course needed to predict the different levels of ratings. So apha subject is different. It depends on J. Different J has a different alpha, but the rest of the parameters of beta are the same. So now you can also ask the question, how many parameters do we have now? Again, that's an interesting question to think about. So if you think about it for a moment and you will see now the plan that we have far fewer parameters. Specifically, we have N + K - 1 because we have M beta values and plus K minus one alpha values.	however , each classifier there have a distinct alpha value , the Alpha parameter , the except it be different and this be of course need to predict the different level of rating . so apha subject be different . it depend on J. Different J have a different alpha , but the rest of the parameter of beta be the same . so now you can also ask the question , how many parameter do we have now ? again , that be an interesting question to think about . so if you think about it for a moment and you will see now the plan that we have far few parameter . specifically , we have N + K - 1 because we have M beta value and plus K minus one alpha value .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-02 23:10:07.968786	264	00:11:15.4	00:11:21.79	So that's just the basically that's basically the main idea of ordinal logistic regression.	so that be just the basically that be basically the main idea of ordinal logistic regression .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-02 23:10:07.968787	267	00:11:24.48	00:11:33.39	So now let's see how we can use such a method to actually assign ratings. It turns out that with this.	so now let 's see how we can use such a method to actually assign rating . it turn out that with this .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-02 23:10:07.96879	275	00:11:34.87	00:11:58.58	Idea of tying all the parameters, the beta values. we also end up having a simpler way to make decisions, and more specifically now the criteria whether the predicted probabilities above is or at least .5 above and now is equivalent to whether the score of the object that is.	idea of tie all the parameter , the beta value . we also end up have a simple way to make decision , and more specifically now the criterion whether the predict probability above be or at least .5 above and now be equivalent to whether the score of the object that be .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-27 22:30:53.137913	280	00:11:59.9	00:12:14.5	Larger than or equal to negative of alpha_j as shown here. Now the scoring function now is just taking linear combination of all the features weighted by beta values.	large than or equal to negative of  alpha_j as show here . now the scoring function now be just take linear combination of all the feature weight by beta value .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-27 22:31:02.339878	284	00:12:15.15	00:12:26.78	So this means now we can simply make a desicion of rating by looking at the value of this scoring function and see which bracket it falls into.	so this mean now we can simply make a desicion of rating by look at the value of this scoring function and see which bracket it fall into .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-02 23:10:07.968794	290	00:12:27.76	00:12:46.61	Now you can see the General Decision rule is thus when the score is in the particular range of our values, then we will assign the corresponding rating to that text object.	now you can see the General Decision rule be thus when the score be in the particular range of our value , then we will assign the corresponding rating to that text object .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-02 23:10:07.968794	292	00:12:49.87	00:12:53.93	So in sum, in this approach we're going to score the object.	so in sum , in this approach we be go to score the object .
2d0e46c7-df4e-48b3-9550-dac3fec3062d	2020-11-27 22:31:28.211224	294	00:12:54.64	00:12:58.85	By using the features and the parameter values, beta values.	by use the feature and the parameter value , beta value .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133686	4	00:00:00.3	00:00:09	[Intro] This vector is about a specific technique for contextual text mining called contextual probabilistic latent semantic analysis.	[ intro ] this vector be about a specific technique for contextual text mining call contextual probabilistic latent semantic analysis .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133688	12	00:00:19.28	00:00:32.78	In this lecture, we're going to continue discussing contextual text mining. And we're going to introduce contextual probabilistic latent semantic analysis As an extension of PLSA for doing contextual text mining.	in this lecture , we be go to continue discuss contextual text mining . and we be go to introduce contextual probabilistic latent semantic analysis as an extension of PLSA for do contextual text mining .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133688	14	00:00:34.27	00:00:39.389999	Recall that in contextual text mining we hope to analyze topics in text.	recall that in contextual text mining we hope to analyze topic in text .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-12-09 01:41:33.798086	21	00:00:40.03	00:00:54.33	In consideration of context so that we can associate the topics with appropriate context that we're interested in. So in this approach contextual probabilistic latent semantic analysis or CPLSA	in consideration of context so that we can associate the topic with appropriate context that we be interested in . so in this approach contextual probabilistic latent semantic analysis or CPLSA
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133693	32	00:00:55.03	00:01:22.67	The main idea is to explicitly add interesting context variables into a generated model. Recall that before when we generate the text, we generally assume we will start with some topics and then sample words from some topics. But here we are going to add context variables so that the coverage of topics and also the content of topics will be tight little context.	the main idea be to explicitly add interesting context variable into a generate model . recall that before when we generate the text , we generally assume we will start with some topic and then sample word from some topic . but here we be go to add context variable so that the coverage of topic and also the content of topic will be tight little context .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133694	35	00:01:23.35	00:01:29.24	Or in other words, we can do let the context influence both coverage and content of a topic.	or in other word , we can do let the context influence both coverage and content of a topic .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133696	43	00:01:31.01	00:01:50.75	The consequences that this would enable us to discover contextualized topics make the topics more interesting, more meaningful, because we can then have topics that can be interpreted as specific to a particular context that we're interested in. For example, a particular time period.	the consequence that this would enable we to discover contextualized topic make the topic more interesting , more meaningful , because we can then have topic that can be interpret as specific to a particular context that we be interested in . for example , a particular time period .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133697	47	00:01:51.91	00:02:05.85	As extension of PLSA model, CPLSA mainly does the following changes. Firstly it would model the conditional likelihood of text given context.	as extension of PLSA model , CPLSA mainly do the follow change . firstly it would model the conditional likelihood of text give context .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133698	51	00:02:06.73	00:02:16.77	That clearly suggests that the generation of text would then depend on context, and that allows us to bring context into the generative model.	that clearly suggest that the generation of text would then depend on context , and that allow we to bring context into the generative model .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133701	66	00:02:18.12	00:02:53.22	Secondly, it makes 2 specific assumptions about the dependency of topics on context. One is to assume that depending on the context depending on different time periods or different locations, we assume that there are different views of the topic or different versions of word distributions that characterize a topic, and this assumption allows us to discover different variations of the same topic in different context. The other is that we assume. The topic coverage also depends on the context.	secondly , it make 2 specific assumption about the dependency of topic on context . one be to assume that depend on the context depend on different time period or different location , we assume that there be different view of the topic or different version of word distribution that characterize a topic , and this assumption allow we to discover different variation of the same topic in different context . the other be that we assume . the topic coverage also depend on the context .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-12-12 18:25:10.316031	73	00:02:54.24	00:03:07.78	And that means depending on the time or location, we might cover topics differently. And then again this dependency would then allow us to capture the association of topics with specific context.	and that mean depend on the time or location , we might cover topic differently . and then again this dependency would then allow we to capture the association of topic with specific context .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133704	76	00:03:09.38	00:03:14.76	We can still use the EM algorithm to solve the problem of parameter estimation.	we can still use the EM algorithm to solve the problem of parameter estimation .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133706	81	00:03:15.44	00:03:28.45	And in this case, the estimate premise would naturally contain context variables, and in particular a lot of conditional probabilities of topics given certain context.	and in this case , the estimate premise would naturally contain context variable , and in particular a lot of conditional probability of topic give certain context .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133706	83	00:03:29.83	00:03:32.44	And this would allow us to do contextual text mining.	and this would allow we to do contextual text mining .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133706	84	00:03:33	00:03:34.69	So this is the basic idea.	so this be the basic idea .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133709	93	00:03:35.64	00:04:00.49	Now we don't have time to introduce this model in detail, but there are references here that you can look into to know more detail here. I just want to explain the high level ideas in more detail, particularly willing to explain the generation process of text data that has context associated in such a model.	now we do n't have time to introduce this model in detail , but there be reference here that you can look into to know more detail here . I just want to explain the high level idea in more detail , particularly willing to explain the generation process of text datum that have context associate in such a model .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-12-12 18:26:37.670317	101	00:04:01.23	00:04:20.779999	So as you see here, we can assume there are still multiple topics. For example, some topics might represent the themes like a government response donation or the city of New Orleans. Now this example is in the context of Hurricane Katrina and that hit New Orleans.	so as you see here , we can assume there be still multiple topic . for example , some topic might represent the theme like a government response donation or the city of New Orleans . now this example be in the context of Hurricane Katrina and that hit New Orleans .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133712	104	00:04:22.97	00:04:31.6	Now, as you can see, we assume there are different views associated with the each of the topics.	now , as you can see , we assume there be different view associate with the each of the topic .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133712	108	00:04:32.3	00:04:40.69	And these are shown as view one, view tow and view three view three Each view is a different version of word distributions.	and these be show as view one , view tow and view three view three each view be a different version of word distribution .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133714	113	00:04:41.37	00:04:54.55	And these views are tide to some context variables. For example, type to the location Texas or the time July 2005 or the occupation of the other being sociologist.	and these view be tide to some context variable . for example , type to the location Texas or the time July 2005 or the occupation of the other be sociologist .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133715	121	00:04:55.95	00:05:13.37	Now on the right side you see we assume the document has contact information, so the time is known to be July 2005, location is Texes, etc. Now such context information is what we hope to model as well. So we're not going to just model the text.	now on the right side you see we assume the document have contact information , so the time be know to be July 2005 , location be Texes , etc . now such context information be what we hope to model as well . so we be not go to just model the text .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133716	126	00:05:15	00:05:26.08	And so one idea here is to model the variations of topic content in different context and this gives us different views of the world distributions.	and so one idea here be to model the variation of topic content in different context and this give we different view of the world distribution .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133717	129	00:05:27.6	00:05:33.92	Now on the bottom you will see the theme coverage or topic coverage might also vary according to these context.	now on the bottom you will see the theme coverage or topic coverage might also vary accord to these context .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133718	130	00:05:34.87	00:05:36.68	Because in the.	because in the .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-12-12 18:30:46.745045	138	00:05:38.38	00:06:01.35	In the case of location like Texas, people might want to cover the red topics more at the new audience, as visualized here. But in a certain time period, maybe particular topic like donation will be covered more so this variation is also considered in CPLSA.	in the case of location like Texas , people might want to cover the red topic more at the new audience , as visualize here . but in a certain time period , maybe particular topic like donation will be cover more so this variation be also consider in CPLSA .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.13372	140	00:06:02.11	00:06:07.94	So to generate such a document with context, we first also choose a view.	so to generate such a document with context , we first also choose a view .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.13372	144	00:06:08.59	00:06:17.62	And this view of course now could be from any of these contexts. Let's say we have taken this view. That depends on the time in the middle.	and this view of course now could be from any of these context . let 's say we have take this view . that depend on the time in the middle .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133721	148	00:06:18.2	00:06:25.21	So now we have a specific version of word distributions. Now you can see some probabilities of words for each topic.	so now we have a specific version of word distribution . now you can see some probability of word for each topic .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133723	154	00:06:26.57	00:06:38.99	Now, once we have chosen a view, now the situation will be very similar to what happened in standard PLSA. We assume we have got a word distribution associated with each topic, right?	now , once we have choose a view , now the situation will be very similar to what happen in standard PLSA . we assume we have get a word distribution associate with each topic , right ?
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133724	162	00:06:39.75	00:06:57.64	And then next to we choose a coverage from the bottom. So we're going to choose particular coverage and that coverage. Before is fixed in PLSA and it's hard to a particular document. Each document has just one coverage distribution.	and then next to we choose a coverage from the bottom . so we be go to choose particular coverage and that coverage . before be fix in PLSA and it be hard to a particular document . each document have just one coverage distribution .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133726	170	00:06:58.77	00:07:14.54	Now here, because we consider context so the distribution of topics or the coverage of topics can vary depending on the context that has influenced the coverage. So, for example, we might pick a particular coverage, let's say in this case.	now here , because we consider context so the distribution of topic or the coverage of topic can vary depend on the context that have influence the coverage . so , for example , we might pick a particular coverage , let 's say in this case .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133728	182	00:07:15.29	00:07:43.71	We pick We've picked the document specifically coverage now with the coverage and these word distributions, we can generate the document in exactly the same way as in PLSA. So what it means we're going to use the coverage to choose a topic to choose one of these three topics. Let's say we have picked up, let's say, the yellow topic, then withdraw a word from this particular topic on the top.	we pick we 've pick the document specifically coverage now with the coverage and these word distribution , we can generate the document in exactly the same way as in PLSA . so what it mean we be go to use the coverage to choose a topic to choose one of these three topic . let 's say we have pick up , let 's say , the yellow topic , then withdraw a word from this particular topic on the top .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-12-12 18:40:39.052208	189	00:07:44.72	00:07:58.76	So we might get the word like government. And then next time we might choose a different topic, an will get donate, etc right until we generate all the words and this is basically the same process as in PLSA.	so we might get the word like government . and then next time we might choose a different topic , an will get donate , etc right until we generate all the word and this be basically the same process as in PLSA .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133734	207	00:07:59.47	00:08:39.05	Now, so the main difference is when we obtain the coverage and the word distributions, we let the context influence our choice. So in other words, we have extra switches that are tied to this context that would control the choices of different views of topics and choices of coverage. And naturally, the model will have more parameters to estimate, but once we can estimate those parameters that involve the context, then we will be able to understand the context of specific views of topics or context specific coverages of topics. And this is precisely what we want in contextual text mining.	now , so the main difference be when we obtain the coverage and the word distribution , we let the context influence our choice . so in other word , we have extra switch that be tie to this context that would control the choice of different view of topic and choice of coverage . and naturally , the model will have more parameter to estimate , but once we can estimate those parameter that involve the context , then we will be able to understand the context of specific view of topic or context specific coverage of topic . and this be precisely what we want in contextual text mining .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133737	214	00:08:40.29	00:08:55.17	So here are some sample results from using such a model. Not necessary exactly the same model, but similar models. So on this slide you see some sample results of comparing news articles about Iraq war and Afghanistan war.	so here be some sample result from use such a model . not necessary exactly the same model , but similar model . so on this slide you see some sample result of compare news article about Iraq war and Afghanistan war .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133737	215	00:08:56.21	00:08:57.85	Now we have about 30 articles.	now we have about 30 article .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133738	218	00:08:58.98	00:09:04.89	On Iraq war and 26 articles on Afghanistan war. Now, in this case, the goal is to.	on Iraq war and 26 article on Afghanistan war . now , in this case , the goal be to .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133739	222	00:09:06.46	00:09:17.45	To review the common topics covered in both sets, articles and the differences or variations of the topic in each of the two collections.	to review the common topic cover in both set , article and the difference or variation of the topic in each of the two collection .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.13374	225	00:09:18.53	00:09:23.55	So in this case, the context that is explicitly specified by the topical collection.	so in this case , the context that be explicitly specify by the topical collection .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.13374	226	00:09:24.93	00:09:27.84	And we see the results here show that.	and we see the result here show that .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133741	229	00:09:28.88	00:09:34.97	There is a common theme that's corresponding to cluster around here in this column.	there be a common theme that be correspond to cluster around here in this column .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133743	235	00:09:35.95	00:09:49.87	That there is a common theme indicating that United Nations is involved in both wars is a common topic covered in both sets of articles, and that's indicated by the high probability words showmn here United Nations.	that there be a common theme indicate that United Nations be involve in both war be a common topic cover in both set of article , and that be indicate by the high probability word showmn here United Nations .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-12-12 18:42:43.173107	239	00:09:51.05	00:09:59.35	Now if you the background, of course this is not surprising and this is. This topic is indeed very relevant, to both wars.	now if you the background , of course this be not surprising and this be . this topic be indeed very relevant , to both war .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133748	251	00:10:00.22	00:10:29.089999	If you look at the column further and what's interesting is that the next two cells of word distributions actually tell US collection specific variations of the topic of United Nations. So it indicates that in Iraq war, United Nations was more involved in weapon inspections, whereas in Afghanistan war it was more involved in maybe aid to Northern Alliance as a different variation of the topic of United Nations.	if you look at the column far and what be interesting be that the next two cell of word distribution actually tell US collection specific variation of the topic of United Nations . so it indicate that in Iraq war , United Nations be more involved in weapon inspection , whereas in Afghanistan war it be more involved in maybe aid to Northern Alliance as a different variation of the topic of United Nations .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-12-12 18:43:51.62413	258	00:10:30.01	00:10:45.34	So this shows that by bringing the context, in this case, different wards are different collections of text. We can have topic variations, tied to these contexts to review the differences of coverage of United Nations in the two wars.	so this show that by bring the context , in this case , different ward be different collection of text . we can have topic variation , tie to these context to review the difference of coverage of United Nations in the two war .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.13375	260	00:10:46.19	00:10:48.57	Similarly, if you look at the second cluster.	similarly , if you look at the second cluster .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133752	265	00:10:49.48	00:10:58.53	Cluster 2 has to do with the killing of people and again. Not surprising if you know the background about wars or the wars involved killing of people.	Cluster 2 have to do with the killing of people and again . not surprising if you know the background about war or the war involve killing of people .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133755	277	00:10:59.1	00:11:26.3	But imagine if you are not familiar with the text collections or have a lot of text articles and such a technique can review the common topics covered in both sets of articles. It can be used to review common topics in multiple sets of articles as well. If you look down, of course in that column of cluster 2 you see variations of killing of people and that correspond to in different contexts.	but imagine if you be not familiar with the text collection or have a lot of text article and such a technique can review the common topic cover in both set of article . it can be use to review common topic in multiple set of article as well . if you look down , of course in that column of cluster 2 you see variation of killing of people and that correspond to in different context .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133756	278	00:11:28.39	00:11:31.94	And here is another example of results.	and here be another example of result .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133757	280	00:11:33.36	00:11:36.62	Obtain the front block articles about the Hurricane Katrina.	obtain the front block article about the Hurricane Katrina .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133757	282	00:11:37.35	00:11:43.01	Now in this case, what you see here is visualization of the.	now in this case , what you see here be visualization of the .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-12-12 18:47:00.407329	283	00:11:44.16	00:11:46.28	trends of topics overtime.	trend of topic overtime .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-12-12 18:47:15.091476	286	00:11:47.12	00:11:55.36	And the top one shows just the temporal chains of two topics. One is oil price and one is.	and the top one show just the temporal chain of two topic . one be oil price and one be .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133759	291	00:11:56.02	00:12:06.51	about the flooding of the city. New Orleans. This these topics are obtained from block articles about the Hurricane Katrina.	about the flooding of the city . New Orleans . this these topic be obtain from block article about the Hurricane Katrina .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.13376	292	00:12:07.18	00:12:09.43	And people talked about these topics.	and people talk about these topic .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.13376	293	00:12:10.08	00:12:11.72	And in addition to some other topics.	and in addition to some other topic .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133761	297	00:12:12.29	00:12:18.93	But the visualization shows that with this technique that we can have conditional distribution of time given a topic.	but the visualization show that with this technique that we can have conditional distribution of time give a topic .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-12-12 18:48:56.755089	304	00:12:19.56	00:12:40.54	So this allows us to plot this conditional probability. General curves like what you're seeing here. We see that initially the two curves tracked each other very well. But later anf	so this allow we to plot this conditional probability . general curve like what you be see here . we see that initially the two curve track each other very well . but later anf
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133763	309	00:12:41.87	00:12:52.62	This turns out to be the time period when another Hurricane Hurricane Rita hit the region that apparently tricked more discussion about the flooding of the city.	this turn out to be the time period when another Hurricane Hurricane Rita hit the region that apparently trick more discussion about the flooding of the city .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133765	314	00:12:54.7	00:13:08.72	The bottom curve shows the coverage of this topic about the flooding of the city by block articles in different locations and also shows some shift of coverage.	the bottom curve show the coverage of this topic about the flooding of the city by block article in different location and also show some shift of coverage .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133765	317	00:13:09.42	00:13:19.35	That might be related to peoples migrating from the state of Louisiana to Texas, for example.	that might be relate to people migrate from the state of Louisiana to Texas , for example .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133766	320	00:13:20.47	00:13:26.22	So in this case we can see the time can be used as context to reveal trends of topics.	so in this case we can see the time can be use as context to reveal trend of topic .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133767	324	00:13:27.61	00:13:36.93	This is some additional result on special patterns and this. In this case it's about the topic of government response.	this be some additional result on special pattern and this . in this case it be about the topic of government response .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133769	334	00:13:37.73	00:14:02.87	And there was some criticism about the slow response of government in the case of Hurricane Katrina, and discussion now is covered in different locations and these visualizations show the coverage in different weeks of the event, and initially it's covered mostly in the victim states in the South, but then gradually it's spreading to other.	and there be some criticism about the slow response of government in the case of Hurricane Katrina , and discussion now be cover in different location and these visualization show the coverage in different week of the event , and initially it be cover mostly in the victim state in the South , but then gradually it be spread to other .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-12-12 18:50:35.949552	340	00:14:04.36	00:14:17.97	Locations, but in week four, which is shown on the bottom on the left, we see a pattern that's very similar to the very first week on the top left, and that's why again the hurricane without hit the region.	location , but in week four , which be show on the bottom on the left , we see a pattern that be very similar to the very first week on the top left , and that be why again the hurricane without hit the region .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133772	347	00:14:18.58	00:14:32.86	So such a technique would allow us to use location as context to examine variations of topics. And of course, the model is completely general, so you can apply this to any other collections of text to reveal spatial temporal patterns.	so such a technique would allow we to use location as context to examine variation of topic . and of course , the model be completely general , so you can apply this to any other collection of text to reveal spatial temporal pattern .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133772	350	00:14:34.35	00:14:42.01	Is yet another application of this kind of model where we look at the use of the model for event impact analysis.	be yet another application of this kind of model where we look at the use of the model for event impact analysis .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-12-12 18:50:57.971326	353	00:14:43.2	00:14:48.64	So here we are looking at the research articles in information retrieval, IR, particularly SIGIR papers.	so here we be look at the research article in information retrieval , IR , particularly sigir paper .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133774	358	00:14:49.37	00:14:58.63	And the topic we focus on is about the retrieval models and you can see the top word top words with high probability is about this model on the left.	and the topic we focus on be about the retrieval model and you can see the top word top word with high probability be about this model on the left .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133776	368	00:14:59.46	00:15:22.95	And then we hope to examine the impact of two events. One is the start of TREC for text retrieval conference. This is a major evaluation effort sponsored by US government and was launched in 1992 or around that time and that is known to have made an impact on the topics of research information retrieval.	and then we hope to examine the impact of two event . one be the start of TREC for text retrieval conference . this be a major evaluation effort sponsor by US government and be launch in 1992 or around that time and that be know to have make an impact on the topic of research information retrieval .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-12-12 18:53:58.605722	398	00:15:23.72	00:16:26.69	The other is the publication of a Seminal paper by craft and pond, and this is about the language modeling approach to information retrieval. It's also known to have made a high impact on information retrieval research, so we hope to use this kind of model, understand impact, and the idea here is simply to use the time as context an use these events to divide the time periods into a period before the event an another after this event, and then we can compare the differences of. The topics, the coverage and variations, etc. So in this case the results show I've seen that before TREC the study of retrieval models was mostly a vector space model, Boolean model, etc. But the after TREC. Apparently the study of retrieval models have involved a lot of other words that seem to suggest some different retrieval tasks though. So for example email was used in the enterprise search tasks and subtropical retrieval, with another task introduced later by TREC.	the other be the publication of a seminal paper by craft and pond , and this be about the language modeling approach to information retrieval . it be also know to have make a high impact on information retrieval research , so we hope to use this kind of model , understand impact , and the idea here be simply to use the time as context an use these event to divide the time period into a period before the event an another after this event , and then we can compare the difference of . the topic , the coverage and variation , etc . so in this case the result show I 've see that before TREC the study of retrieval model be mostly a vector space model , boolean model , etc . but the after TREC . apparently the study of retrieval model have involve a lot of other word that seem to suggest some different retrieval task though . so for example email be use in the enterprise search task and subtropical retrieval , with another task introduce later by TREC .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133786	417	00:16:28.09	00:17:10.29	On the bottom we see the variations that are correlated with the publication of the language model paper. Before we have those classical probabilistic model logic model, Boolean model etc. But after 1998 that we see clear dominance of language model as probabilistic models and we see words like a language model, estimation of parameters etc. So this technique here can use event as context. To understand the impact of event again, the technique is general so you can use this to analyze the impact of any event. Here are some suggested readings.	on the bottom we see the variation that be correlate with the publication of the language model paper . before we have those classical probabilistic model logic model , boolean model etc . but after 1998 that we see clear dominance of language model as probabilistic model and we see word like a language model , estimation of parameter etc . so this technique here can use event as context . to understand the impact of event again , the technique be general so you can use this to analyze the impact of any event . here be some suggest reading .
3103be2f-681e-41cf-b0f7-21cf6ba56616	2020-11-02 23:06:11.133789	428	00:17:11.66	00:17:37.62	The first is paper about simple extension of PSA to enable cross collection comparison. It's to perform comparative text mining to allow us to extract the common topics shared by multiple collections and their variations in each collection. The second one is the main paper about the CPLSA model with a discussion of a lot of applications.	the first be paper about simple extension of PSA to enable cross collection comparison . it be to perform comparative text mining to allow we to extract the common topic share by multiple collection and their variation in each collection . the second one be the main paper about the cplsa model with a discussion of a lot of application .
380a7417-6702-4df8-9818-5aceba7cde2b	2020-11-24 17:56:08.425843	2	00:00:17.67	00:00:22.73	This lecture is about topic mining and analysis. "We	this lecture be   about topic mining and analysis . " we
380a7417-6702-4df8-9818-5aceba7cde2b	2020-11-02 23:01:30.940284	7	00:00:23.68	00:00:38.38	"In this v "about the language namely discovery of word associations such as paradigmatic relations relations and syntagmatic relations.	" in this v " about the language namely discovery of word association such as paradigmatic relation relation and syntagmatic relation .
380a7417-6702-4df8-9818-5aceba7cde2b	2020-11-02 23:01:30.940287	12	00:00:39.07	00:00:50.32	Now, starting from this lecture, we're going to talk about mining another kind of knowledge, which is content mining and trying to discover knowledge about.	now , start from this lecture , we be go to talk about mine another kind of knowledge , which be content mining and try to discover knowledge about .
380a7417-6702-4df8-9818-5aceba7cde2b	2020-11-02 23:01:30.940288	13	00:00:51.37	00:00:53.3	The main topics.	the main topic .
380a7417-6702-4df8-9818-5aceba7cde2b	2020-11-02 23:01:30.940288	14	00:00:54.02	00:00:55.12	In the text.	in the text .
380a7417-6702-4df8-9818-5aceba7cde2b	2020-11-02 23:01:30.940289	16	00:00:56.04	00:00:58.88	And we call that topic mining and analysis.	and we call that topic mining and analysis .
380a7417-6702-4df8-9818-5aceba7cde2b	2020-11-02 23:01:30.940298	32	00:00:59.79	00:01:30.91	In this lecture we're going to talk about its motivation and the task definition. So first, let's look at the concept of topic. So topic is something that we all understand, I think, but it's actually not that easy to formally define it. Roughly speaking, topic is the main idea discussed in text data, and you can think of this as a theme or subject of discussion or conversation. It can also have different granularities. For example, we can talk about the topic of a sentence.	in this lecture we be go to talk about its motivation and the task definition . so first , let 's look at the concept of topic . so topic be something that we all understand , I think , but it be actually not that easy to formally define it . roughly speak , topic be the main idea discuss in text datum , and you can think of this as a theme or subject of discussion or conversation . it can also have different granularity . for example , we can talk about the topic of a sentence .
380a7417-6702-4df8-9818-5aceba7cde2b	2020-11-02 23:01:30.9403	36	00:01:31.17	00:01:39.63	A topic of article topic of a paragraph, or the topics of all the research articles in the digital library.	a topic of article topic of a paragraph , or the topic of all the research article in the digital library .
380a7417-6702-4df8-9818-5aceba7cde2b	2020-11-02 23:01:30.940301	38	00:01:40.27	00:01:45.74	So different granularities of topics obviously have different applications.	so different granularity of topic obviously have different application .
380a7417-6702-4df8-9818-5aceba7cde2b	2020-11-02 23:01:30.940315	66	00:01:46.47	00:02:51.98	Indeed, there are many applications that require discovery of topics in text and then analyze them. Here are some examples. For example, we might be interested in knowing what are Twitter users talking about today? Are they talking about NBA sports or talking about some international events, etc. Or we are interested in knowing about the research topics. For example, one might be interested in knowing what are the current research topics in data mining and how are they different from those five years ago. Now this involves discovery of topics in data mining, literatures and also we want to discover topics in today's literature and those in the past. And then we can make a comparison. We might be also interested in knowing what do people like about some product like iPhone 6 an what do they dislike? And this involves discovering topics in positive opinions about iPhone 6 and also negative reviews about it.	indeed , there be many application that require discovery of topic in text and then analyze they . here be some example . for example , we might be interested in know what be Twitter user talk about today ? be they talk about NBA sport or talk about some international event , etc . or we be interested in know about the research topic . for example , one might be interested in know what be the current research topic in data mining and how be they different from those five year ago . now this involve discovery of topic in data mining , literature and also we want to discover topic in today 's literature and those in the past . and then we can make a comparison . we might be also interested in know what do people like about some product like iPhone 6 an what do they dislike ? and this involve discover topic in positive opinion about iPhone 6 and also negative review about it .
380a7417-6702-4df8-9818-5aceba7cde2b	2020-11-02 23:01:30.940317	69	00:02:52.41	00:02:58.82	Or perhaps we're interested in knowing what are the major topics debated in 2012 presidential election?	or perhaps we be interested in know what be the major topic debate in 2012 presidential election ?
380a7417-6702-4df8-9818-5aceba7cde2b	2020-11-02 23:01:30.940319	73	00:02:59.66	00:03:07.94	And all these have to do is discovering topics in texts and analyzing them, and we're going to talk about a lot of techniques for doing this.	and all these have to do be discover topic in text and analyze they , and we be go to talk about a lot of technique for do this .
380a7417-6702-4df8-9818-5aceba7cde2b	2020-11-02 23:01:30.940323	82	00:03:08.51	00:03:28.41	In general, we can view topic as some knowledge about the world. So from text that we expected to discover a number of topics and then this topic generally provide the description about the world and it tells us something about the world, about the product, about the person, etc.	in general , we can view topic as some knowledge about the world . so from text that we expect to discover a number of topic and then this topic generally provide the description about the world and it tell we something about the world , about the product , about the person , etc .
380a7417-6702-4df8-9818-5aceba7cde2b	2020-11-02 23:01:30.940332	100	00:03:29.24	00:04:14.45	Now when we have some non-text data then we can have more context for analyzing the topics. For example, we might know the time associated with the text data or locations where the text data will produced or the authors of text or the sources of the text etc. All such meta data or context variables can be associated with the topics that we discover. And then we can use these context variables to help us analyze patterns of topics. For example, looking at topics overtime, we would be able to discover whether there's a trending topic or some topics might be fading away.	now when we have some non - text datum then we can have more context for analyze the topic . for example , we might know the time associate with the text datum or location where the text datum will produce or the author of text or the source of the text etc . all such meta datum or context variable can be associate with the topic that we discover . and then we can use these context variable to help we analyze pattern of topic . for example , look at topic overtime , we would be able to discover whether there be a trending topic or some topic might be fade away .
380a7417-6702-4df8-9818-5aceba7cde2b	2020-11-02 23:01:30.940334	104	00:04:15.06	00:04:23.919999	Similar looking at the topics in different locations, we might know some insights about peoples' opinions in different locations.	similar look at the topic in different location , we might know some insight about people ' opinion in different location .
380a7417-6702-4df8-9818-5aceba7cde2b	2020-11-02 23:01:30.940337	111	00:04:24.96	00:04:39.06	So that's why mining topics is very important. Now let's look at the tasks of topic mining and analysis. In general, it would involve first discovering a lot of topics. In this case K topics.	so that be why mining topic be very important . now let 's look at the task of topic mining and analysis . in general , it would involve first discover a lot of topic . in this case k topic .
380a7417-6702-4df8-9818-5aceba7cde2b	2020-11-02 23:01:30.940339	115	00:04:40.69	00:04:47.93	And then we also would like to know which topics are covered in which documents, to what extent. So for example in.	and then we also would like to know which topic be cover in which document , to what extent . so for example in .
380a7417-6702-4df8-9818-5aceba7cde2b	2020-11-02 23:01:30.940341	119	00:04:48.51	00:04:57.5	Document one we might see that topic one is covered a lot, topic two and topic k are covered with a small portion.	document one we might see that topic one be cover a lot , topic two and topic k be cover with a small portion .
380a7417-6702-4df8-9818-5aceba7cde2b	2020-11-02 23:01:30.940342	121	00:04:58.77	00:05:00.87	And other topics perhaps are not covered.	and other topic perhaps be not cover .
380a7417-6702-4df8-9818-5aceba7cde2b	2020-11-02 23:01:30.940344	126	00:05:02.57	00:05:13.48	Document 2, on the other hand, covered topic 2 very well, but it did not cover topic 1 at all and also covers topic key to some extent.	document 2 , on the other hand , cover topic 2 very well , but it do not cover topic 1 at all and also cover topic key to some extent .
380a7417-6702-4df8-9818-5aceba7cde2b	2020-11-02 23:01:30.940357	152	00:05:14.3	00:06:12.47	etc., right? So now you can see there are generally two different tasks or subtasks. The first is to discover K topics from a collection of text data. What are these K topics? OK, major topics in the text data. The second task is to figure out which documents cover which topics to what extent. So more formally, we can define the problem as follows. First, we have as input a collection of N text documents. Here we can denote that text connection. as C. And denote a text article as d i an we generally also need to have an input the number of topics K. But there may be techniques that can automatically suggest a number of topics, but in the techniques that we will discuss which are also the most useful techniques, we often need to specify a number of topics.	etc . , right ? so now you can see there be generally two different task or subtask . the first be to discover k topic from a collection of text datum . what be these k topic ? ok , major topic in the text datum . the second task be to figure out which document cover which topic to what extent . so more formally , we can define the problem as follow . first , we have as input a collection of n text document . here we can denote that text connection . as C. and denote a text article as d I an we generally also need to have an input the number of topic K. but there may be technique that can automatically suggest a number of topic , but in the technique that we will discuss which be also the most useful technique , we often need to specify a number of topic .
380a7417-6702-4df8-9818-5aceba7cde2b	2020-11-02 23:01:30.940364	164	00:06:13.94	00:06:44.7	Now the output would then be the K topics that we would like to discover denoted as theater sub one through theta sub k. Also we want to generate the coverage of topics in each document di and this is denoted by  sub i j and " sub i j document d sub i covering topic theta sub j. So obviously for each document we have a set of such values indicate.	now the output would then be the k topic that we would like to discover denote as theater sub one through theta sub k. also we want to generate the coverage of topic in each document di and this be denote by  sub I j and "  sub I j document d sub I cover topic theta sub j. So obviously for each document we have a set of such value indicate .
380a7417-6702-4df8-9818-5aceba7cde2b	2020-11-02 23:01:30.940365	166	00:06:45.37	00:06:47.94	To what extent did the document covers each topic.	to what extent do the document cover each topic .
380a7417-6702-4df8-9818-5aceba7cde2b	2020-11-24 18:15:20.403842	177	00:06:48.82	00:07:15.34	An we can assume that these probabilities sum to one, because a document won't be able to cover other topics outside the topics that we discussed we discovered. So now the question is how do we define \theta sub i? How do we define the topic now? This problem has not been completely defined until we define what is exactly theta.	an we can assume that these probability sum to one , because a document wo n't be able to cover other topic outside the topic that we discuss we discover . so now the question be how do we define \theta sub I ? how do we define the topic now ? this problem have not be completely define until we define what be exactly theta .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748783	2	00:00:00.3	00:00:03.68	This lecture is about the evaluation of taxable categorization.	this lecture be about the evaluation of taxable categorization .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748787	9	00:00:11.74	00:00:28.35	So we've talked about many different methods for taxi categorisation, but how do you which method works better? And for a particular application, how do you this is the best way of solving your problem. To understand these will have to.	so we 've talk about many different method for taxi categorisation , but how do you which method work well ? and for a particular application , how do you this be the good way of solve your problem . to understand these will have to .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748788	11	00:00:29.02	00:00:33.99	How to we have to know how to evaluate categorisation results?	how to we have to know how to evaluate categorisation result ?
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748794	27	00:00:34.76	00:01:10.74	So first some general thoughts about the evaluation in general for evaluation of this kind of empirical tasks such as categorisation, we use methodology that was developed in 1960s by information retrieval researchers called Cranfield Evaluation Methodology. The basic idea is to help humans to create test collection. Where we already every document is tagged with the desired categories, or in the case of search for which query, which documents should have been retrieved and this is called ground truth.	so first some general thought about the evaluation in general for evaluation of this kind of empirical task such as categorisation , we use methodology that be develop in 1960s by information retrieval researcher call Cranfield Evaluation Methodology . the basic idea be to help human to create test collection . where we already every document be tag with the desire category , or in the case of search for which query , which document should have be retrieve and this be call ground truth .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748798	34	00:01:11.96	00:01:27.71	Now with this ground truth test collection, we can then reduce the collection to test many different systems and compare different systems. We can also turn off some component in system to see what's going to happen. Basically it provides.	now with this ground truth test collection , we can then reduce the collection to test many different system and compare different system . we can also turn off some component in system to see what be go to happen . basically it provide .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748798	36	00:01:29.19	00:01:34.83	A way to do controlled experiments to compare different methods.	a way to do control experiment to compare different method .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748799	39	00:01:35.79	00:01:43.87	So this methodology has been virtually used for all the tasks that involve empirically defined problems.	so this methodology have be virtually use for all the task that involve empirically define problem .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748801	43	00:01:45.85	00:01:55.65	So in our case, then we're going to compare our systems categorization results with the categorisation ground truth created by humans.	so in our case , then we be go to compare our system categorization result with the categorisation ground truth create by human .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748803	46	00:01:56.63	00:02:04.77	And we're going to compare our systems decisions on which documents should get which category with what.	and we be go to compare our system decision on which document should get which category with what .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748805	50	00:02:06.05	00:02:13.44	Categories have been assigned to those documents by humans and we want to quantify the similarity of these decisions.	category have be assign to those document by human and we want to quantify the similarity of these decision .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748806	54	00:02:14.17	00:02:23.79	Or equivalently, to measure the difference between the system output and desired ideal output generated by the humans?	or equivalently , to measure the difference between the system output and desire ideal output generate by the human ?
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748809	58	00:02:24.9	00:02:32.85	So obviously the higher similarity is, the better the results are. The similarity can be measured in different ways.	so obviously the high similarity be , the well the result be . the similarity can be measure in different way .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748813	68	00:02:33.69	00:02:53.2	And that would lead to different measures, and sometimes it's desirable also to measure the similarity from different perspectives just to have a better understanding of the results in detail. For example, it might be also interested in knowing which category performs better, which category is easy to categorize, etc.	and that would lead to different measure , and sometimes it be desirable also to measure the similarity from different perspective just to have a well understanding of the result in detail . for example , it might be also interested in know which category perform well , which category be easy to categorize , etc .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748817	79	00:02:54.63	00:03:18.67	In general, different categorization mistakes, however, have different costs for a specific application, so some errors might be more serious than others. So ideally we would like to model such differences. But if you read many papers in texture catalyzation, you will see that they don't generally do that, and instead they will use a simplified measure.	in general , different categorization mistake , however , have different cost for a specific application , so some error might be more serious than other . so ideally we would like to model such difference . but if you read many paper in texture catalyzation , you will see that they do n't generally do that , and instead they will use a simplified measure .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748821	89	00:03:19.38	00:03:39.87	And that's the cause. It's often OK not to consider such a cost variation when we compare different methods. An we when we are interested in knowing the relative difference of these methods. So it's OK to introduce some bias as long as the bias is not correlated with a particular method.	and that be the cause . it be often ok not to consider such a cost variation when we compare different method . an we when we be interested in know the relative difference of these method . so it be ok to introduce some bias as long as the bias be not correlate with a particular method .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748823	93	00:03:40.68	00:03:50.65	And then we should still expect the more effective method to perform better than a less effective one, even though the measure is not perfect.	and then we should still expect the more effective method to perform well than a less effective one , even though the measure be not perfect .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748828	104	00:03:53.01	00:04:15.16	So the first measure that we will introduce is called classification accuracy, and this is basically to measure the percentage of corrective decisions. So here you show that here you see that there are K categories denoted by C1 through CK and there are N documents in order by D1 through DN an for each pair of a category on the document that we can then look at the situation.	so the first measure that we will introduce be call classification accuracy , and this be basically to measure the percentage of corrective decision . so here you show that here you see that there be K category denote by C1 through CK and there be n document in order by D1 through DN an for each pair of a category on the document that we can then look at the situation .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748838	133	00:04:16.68	00:05:17.439999	And see if the system has said yes to despair. Basically has assigned this category to this document or no, so this is denoted by Y or N. That's the system to decision. And similarly we can look at the humans decision. Also, if the human has assigned a category to the document, there will be a plus sign here. That's just that just means a human would think this assignment is correct an if the incorrect, and then there's a minus. So we will see. All combinations of these ends yes and Nos with minus and plus. So there are four combinations in total and two of them are correct and when we have Y plus or minus and then there are also two kinds of errors. So the measure of classification accuracy is similar to count how many of these decisions are correct and normalize that by the total number of decisions we have made. So we know that the total number of decisions is.	and see if the system have say yes to despair . basically have assign this category to this document or no , so this be denote by Y or N. that be the system to decision . and similarly we can look at the human decision . also , if the human have assign a category to the document , there will be a plus sign here . that be just that just mean a human would think this assignment be correct an if the incorrect , and then there be a minus . so we will see . all combination of these end yes and Nos with minus and plus . so there be four combination in total and two of they be correct and when we have Y plus or minus and then there be also two kind of error . so the measure of classification accuracy be similar to count how many of these decision be correct and normalize that by the total number of decision we have make . so we know that the total number of decision be .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748838	134	00:05:18.01	00:05:19.52	In multiplied by K.	in multiply by K.
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748842	143	00:05:20.19	00:05:39.77	And the number of characters decisions obviously are basically of two kinds. One is why pluses and the other is N minus is and we just put together the account. Now this is a very convenient measure that will give us a one number to characterize performance of method and the higher the better of course.	and the number of character decision obviously be basically of two kind . one be why plus and the other be N minus be and we just put together the account . now this be a very convenient measure that will give we a one number to characterize performance of method and the high the well of course .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748849	157	00:05:40.37	00:06:09.76	But the method I also had some problems. First it has treated all the decisions equally so, but in reality there's some decision errors are more serious than others. For example, it may be more important to get the decisions right on some documents than others, and or maybe more important to get the divisions right on some categories than others, and this would call for some detailed evaluation of this results to understand.	but the method I also have some problem . first it have treat all the decision equally so , but in reality there be some decision error be more serious than other . for example , it may be more important to get the decision right on some document than other , and or maybe more important to get the division right on some category than other , and this would call for some detailed evaluation of this result to understand .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748849	159	00:06:10.72	00:06:13.6	The strengths and weaknesses of different methods.	the strength and weakness of different method .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.74885	161	00:06:14.88	00:06:19.21	And to understand the performance of these methods in detail.	and to understand the performance of these method in detail .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.74885	162	00:06:21.38	00:06:24.43	In APA category or per document basis?	in APA category or per document basis ?
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748857	180	00:06:25.36	00:07:07.62	One example that shows clearly the desicion errors are having different causes, spam filtering that could be retrieved as a two category categorization problem. Missing a legitimate email is all is 1 type of error. But letting us ma'am to come into your folder is another type of error. The two types of errors are clearly very different because it's very important not to miss a legitimate email. It's OK to occasionally let us spam email to come into your inbox, so the error of the first missing a legitimate email is very high cost. It's very serious mistake.	one example that show clearly the desicion error be have different cause , spam filtering that could be retrieve as a two category categorization problem . miss a legitimate email be all be 1 type of error . but let we ma'am to come into your folder be another type of error . the two type of error be clearly very different because it be very important not to miss a legitimate email . it be ok to occasionally let we spam email to come into your inbox , so the error of the first miss a legitimate email be very high cost . it be very serious mistake .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748858	182	00:07:08.42	00:07:13.34	And classification error classification accuracy does not address this issue.	and classification error classification accuracy do not address this issue .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748863	193	00:07:14.23	00:07:35.51	There's also another problem with imbalanced tests at the Imagine there's a skew. The test set where most instances are in category one. And 98% of instances are in category one only 2% are in category Two. In such a case, we can have a very simple baseline that actually performs very, and the baseline would Simply put all instances in the major category.	there be also another problem with imbalanced test at the Imagine there be a skew . the test set where most instance be in category one . and 98 % of instance be in category one only 2 % be in category two . in such a case , we can have a very simple baseline that actually perform very , and the baseline would simply put all instance in the major category .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748865	198	00:07:36.22	00:07:45.69	That would give us 98% accuracy. In this case, it's going to be appearing to be very effective, but in reality this is obviously not a good result.	that would give we 98 % accuracy . in this case , it be go to be appear to be very effective , but in reality this be obviously not a good result .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748867	202	00:07:46.38	00:07:53.43	And so, in general, when we use classification accuracy as a measure, we want to ensure that the classes are balanced.	and so , in general , when we use classification accuracy as a measure , we want to ensure that the class be balanced .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748869	208	00:07:54.72	00:08:05.92	And we wonder about equal number of instances. For example, in each class the minority categories or classes tend to be overlooked in the evaluation of classification accuracy.	and we wonder about equal number of instance . for example , in each class the minority category or class tend to be overlook in the evaluation of classification accuracy .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748873	219	00:08:06.79	00:08:28.81	How to address these problems? We of course would like to also evaluate the results in other ways and in different ways. As I said, it's beneficial to look at the actual must multiple perspectives. So for example, we can look at the perspective from each document perspective based on each document. So the question here is, how could other divisions on this document?	how to address these problem ? we of course would like to also evaluate the result in other way and in different way . as I say , it be beneficial to look at the actual must multiple perspective . so for example , we can look at the perspective from each document perspective base on each document . so the question here be , how could other division on this document ?
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748875	222	00:08:29.78	00:08:37.81	Now, as in the general cases of all decisions, we can think about four combinations of possibilities.	now , as in the general case of all decision , we can think about four combination of possibility .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748881	235	00:08:38.39	00:09:06.96	Depending on whether the system has said yes, and depending on whether the human has said it correctly or incorrectly, or say yes or no, and so the four combinations are first. When both the human system said yes and that's true positives when the system says yes, it's actually positive. So when the system says yes, it's a positive. But when the human confirmed that it is indeed correct, that becomes true positive.	depend on whether the system have say yes , and depend on whether the human have say it correctly or incorrectly , or say yes or no , and so the four combination be first . when both the human system say yes and that be true positive when the system say yes , it be actually positive . so when the system say yes , it be a positive . but when the human confirm that it be indeed correct , that become true positive .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748884	242	00:09:07.79	00:09:21.55	When the system says yes, but human says no, that's incorrect. That's a false positive FP. And when the system says no, but the human says yes, then it's a false negative. We missed one assignment.	when the system say yes , but human say no , that be incorrect . that be a false positive fp . and when the system say no , but the human say yes , then it be a false negative . we miss one assignment .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748885	245	00:09:22.12	00:09:28.72	When does the system and human said no? Then that's also corrected vision. That's true negatives.	when do the system and human say no ? then that be also correct vision . that be true negative .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748891	261	00:09:29.42	00:10:01.61	Alright, so then we can have some meshes to just better characterize the performance by using these phone numbers and so 2 popular measures of precision and recall. And these are also proposed by information retrieval researchers in 19, six days for evaluating searching results. But now they have become a standard measure used everywhere. So when the system says yes, we can ask the question how many are correct? What's the percentage of correct decisions when the system says yes? That's called precision.	alright , so then we can have some mesh to just well characterize the performance by use these phone number and so 2 popular measure of precision and recall . and these be also propose by information retrieval researcher in 19 , six day for evaluate search result . but now they have become a standard measure use everywhere . so when the system say yes , we can ask the question how many be correct ? what be the percentage of correct decision when the system say yes ? that be call precision .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748891	264	00:10:02.21	00:10:07.47	It's a true positive divided by all the cases when the system says yes all the positives.	it be a true positive divide by all the case when the system say yes all the positive .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748892	266	00:10:09.16	00:10:12.67	The other recall the other meshes called Recall an this measures.	the other recall the other mesh call Recall an this measure .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748897	280	00:10:14.09	00:10:44.61	Whether the document that has called all the categories it should have. So in this case it's divided the true positive by true positives and false negatives. So these are all the cases where this human says the document should have this category. So this represents the old categories that it should have got an. So recall tells us whether the system has actually indeed assigned all the categories that it should have to this document.	whether the document that have call all the category it should have . so in this case it be divide the true positive by true positive and false negative . so these be all the case where this human say the document should have this category . so this represent the old category that it should have get an . so recall tell we whether the system have actually indeed assign all the category that it should have to this document .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748901	297	00:10:45.53	00:11:17.34	This gives us a detailed view of the decision on each document. Then we can aggregate them later. And if you're interested in some documents and this would tell us how well we did that those documents a subset of them might be more interesting than others. For example, and this allows us to analyze errors in more detail as well. We can separate the documents of certain characteristic from others and then look at the errors. You might see a pattern here for this kind of documents along documents it doesn't do as well as. For short documents.	this give we a detailed view of the decision on each document . then we can aggregate they later . and if you be interested in some document and this would tell we how well we do that those document a subset of they might be more interesting than other . for example , and this allow we to analyze error in more detail as well . we can separate the document of certain characteristic from other and then look at the error . you might see a pattern here for this kind of document along document it do n't do as well as . for short document .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748902	299	00:11:18.8	00:11:21.4	And this gives you some insight for improving the better.	and this give you some insight for improve the well .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748905	309	00:11:22.23	00:11:38.89	Similarly, we can look at the popular category valuation. This. In this case we're going to look at the how good are the decision on a particular category. And as in the previous case, we can define precision and recall and it will just basically answer the questions from a different perspective.	similarly , we can look at the popular category valuation . this . in this case we be go to look at the how good be the decision on a particular category . and as in the previous case , we can define precision and recall and it will just basically answer the question from a different perspective .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748907	318	00:11:39.53	00:11:58.839999	I saw when the system says yes, how many are corrected that means looking at this category to see if all the documents that are assigned with this category are indeed in this category. An recall would tell us has the category being actually assigned to all the documents that should have this category.	I see when the system say yes , how many be correct that mean look at this category to see if all the document that be assign with this category be indeed in this category . an recall would tell we have the category be actually assign to all the document that should have this category .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748909	325	00:12:00.62	00:12:12.66	Is sometimes also useful to combine precision and recall as one measure, and this is often done by using if mesh. And this is just the harmonic mean of precision and recall defined on this slide.	be sometimes also useful to combine precision and recall as one measure , and this be often do by use if mesh . and this be just the harmonic mean of precision and recall define on this slide .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.74891	326	00:12:13.29	00:12:13.96	Ann	ann
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748915	344	00:12:14.52	00:12:55.3	It's also controlled by a parameter beta two to indicate the weather precision is more important, or recall is more important when beta is set to one, we have a measure called F1, and in this case we just take a equal weight on both precision and recall. If one is very often used as a measure for categorisation. Now, as in all cases when we combine results, you always should think about the best way of combining them. So in this case I don't know if you have thought about it and we could have combining them just with the arithmetic mean, right? So that would still give it the same range of values.	it be also control by a parameter beta two to indicate the weather precision be more important , or recall be more important when beta be set to one , we have a measure call F1 , and in this case we just take a equal weight on both precision and recall . if one be very often use as a measure for categorisation . now , as in all case when we combine result , you always should think about the good way of combine they . so in this case I do n't know if you have think about it and we could have combine they just with the arithmetic mean , right ? so that would still give it the same range of value .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748917	352	00:12:56.26	00:13:09.52	But obviously there's a reason why we didn't do that and why. If one is more popular and it's actually useful to think about difference. And if you think about that, you will see that there is indeed some difference and sum.	but obviously there be a reason why we do n't do that and why . if one be more popular and it be actually useful to think about difference . and if you think about that , you will see that there be indeed some difference and sum .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748918	354	00:13:10.21	00:13:12.76	Undesirable property of this arithmetic mean.	undesirable property of this arithmetic mean .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.74892	361	00:13:13.4	00:13:27.53	Basically, it would be obvious to you if you think about a case when the system says yes for all the category and nothing appears. And even tried to compute the precision and recall in that case and see what would happen.	basically , it would be obvious to you if you think about a case when the system say yes for all the category and nothing appear . and even try to compute the precision and recall in that case and see what would happen .
3956403f-f159-448a-9514-5dc69f314c5a	2020-11-02 22:58:06.748921	366	00:13:28.28	00:13:41.19	I basically this kind of measure will not the arithmetic mean is not going to be as reasonable FF1, which tends to prefer a tradeoff between precision and recall.	I basically this kind of measure will not the arithmetic mean be not go to be as reasonable FF1 , which tend to prefer a tradeoff between precision and recall .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.23134	2	00:00:00.3	00:00:03.23	This lecture is about the text categorization.	this lecture be about the text categorization .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.23134	4	00:00:11.24	00:00:15.45	In this lecture we're going to talk about the text categorization.	in this lecture we be go to talk about the text categorization .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231341	6	00:00:16.31	00:00:21.36	This is a very important technique for a text, data mining and analytics.	this be a very important technique for a text , datum mining and analytic .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231343	11	00:00:22.35	00:00:32.6	It is relevant to discovery of various different kinds of knowledge as shown here. First is related to topic mining analysis.	it be relevant to discovery of various different kind of knowledge as show here . first be relate to topic mining analysis .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231345	19	00:00:33.27	00:00:50.83	And that's because it has to do with analyzing text data based on some predefined topics. Secondly, it's also related to opinion mining and sentiment analysis, which has to do with discovering knowledge about the observer that the human sensor.	and that be because it have to do with analyze text datum base on some predefined topic . secondly , it be also related to opinion mining and sentiment analysis , which have to do with discover knowledge about the observer that the human sensor .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231346	22	00:00:51.83	00:01:00.77	Because we can categorize the authors, for example, based on the content of the articles that they have written.	because we can categorize the author , for example , base on the content of the article that they have write .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231348	27	00:01:01.56	00:01:15.73	We can in general categorize the observer based on the content. That they produce. Finally, it's also related to text based prediction.	we can in general categorize the observer base on the content . that they produce . finally , it be also related to text base prediction .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231351	31	00:01:16.58	00:01:26.34	Because we can often use text categorization techniques to predict some variables in the real world that are only remotely related to text data.	because we can often use text categorization technique to predict some variable in the real world that be only remotely related to text datum .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231352	33	00:01:27.11	00:01:32.65	And so this is a very important technique for text data mining.	and so this be a very important technique for text datum mining .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231356	47	00:01:34.75	00:01:59.33	This is the overall plan for covering the topic. First we're going to talk about what is text categorization and why we are interested in doing that in this lecture. And then we're going to talk about how to do text categorisation followed by how to evaluate the categorisation results so. The problem of texture categorisation is defined as follows. We're given a set of predefined categories.	this be the overall plan for cover the topic . first we be go to talk about what be text categorization and why we be interested in do that in this lecture . and then we be go to talk about how to do text categorisation follow by how to evaluate the categorisation result so . the problem of texture categorisation be define as follow . we be give a set of predefined category .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231356	48	00:02:00.18	00:02:02.84	Possibly forming a hierarchy so.	possibly form a hierarchy so .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231396	51	00:02:03.54	00:02:10.69	And often also a set of training examples or training set of labeled text objects.	and often also a set of training example or training set of label text object .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231398	56	00:02:11.39	00:02:25.65	Which means that text objects have already been labeled with known categories, and then the task is to classify any tax object into one or more of these predefined categories.	which mean that text object have already be label with know category , and then the task be to classify any tax object into one or more of these predefined category .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231399	58	00:02:26.23	00:02:29.2	So the picture on the slide shows what happens.	so the picture on the slide show what happen .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.2314	61	00:02:30.09	00:02:36.64	When we do text categorization, we have a lot of text objects to be processed by a categorisation system.	when we do text categorization , we have a lot of text object to be process by a categorisation system .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231403	73	00:02:37.28	00:03:10.49	And the system will in general assign categories to these documents as shown on the right. And the categorisation results. And we often assume the availability of training examples, and these are the documents that are tagged with known categories, and these examples are very important for helping the system to learn patterns in different categories, and this would further help the system then learn how to recognize.	and the system will in general assign category to these document as show on the right . and the categorisation result . and we often assume the availability of training example , and these be the document that be tag with know category , and these example be very important for help the system to learn pattern in different category , and this would far help the system then learn how to recognize .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231404	75	00:03:11.18	00:03:15.78	The categories of new tax objects that it has not seen.	the category of new tax object that it have not see .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-26 06:06:19.436439	77	00:03:16.45	00:03:21.47	So here are some specific examples of text categorization and	so here be some specific example of text categorization and
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231406	79	00:03:22.5	00:03:26.26	In fact, there are many examples. Here are just a few.	in fact , there be many example . here be just a few .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.23141	93	00:03:27.11	00:03:57.69	So first text objects can vary, so we can categorize a document. Or a passage or sentence or collections of text, as in the case of clustering the units to be analyzed can vary a lot, so this creates a lot of possibilities. Secondly, categories can also vary, and we can generally distinguish two kinds of categories. One is internal categories. These are categories that characterize content of text object. For example, topic categories.	so first text object can vary , so we can categorize a document . or a passage or sentence or collection of text , as in the case of cluster the unit to be analyze can vary a lot , so this create a lot of possibility . secondly , category can also vary , and we can generally distinguish two kind of category . one be internal category . these be category that characterize content of text object . for example , topic category .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231412	97	00:03:58.66	00:04:07.03	Or sentiment categories and they generally have to do with the content of the tax objects direct Characterization of the content.	or sentiment category and they generally have to do with the content of the tax object direct Characterization of the content .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-26 06:09:05.875494	107	00:04:08.06	00:04:31.83	The other kind is external categories that can characterize the entity associated with the text object. For example, authors or entities associated with the content that they produce. And so we can use their content, determine which author has written which part, for example, and that's called author attribution.	the other kind be external category that can characterize the entity associate with the text object . for example , author or entity associate with the content that they produce . and so we can use their content , determine which author have write which part , for example , and that be call author attribution .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231416	110	00:04:33.41	00:04:40.19	Or we can have any other meaningful categories associated with text data, as long as.	or we can have any other meaningful category associate with text datum , as long as .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231416	111	00:04:41.47	00:04:42.46	There is a.	there be a.
39d13817-de51-4195-a33a-985b0b54e64d	2020-12-09 01:45:19.753027	116	00:04:43.53	00:04:51.68	There are, there's a meaningful connection between the entity and text data. For example, we might collect a lot of reviews about a restaurant.	there be , there be a meaningful connection between the entity and text datum . for example , we might collect a lot of review about a restaurant .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-26 06:10:26.98264	120	00:04:52.29	00:05:03.73	Or a lot of reviews about the product. And then these text data can help us infer properties of product or a restaurant.	or a lot of review about the product . and then these text datum can help we infer property of product or a restaurant .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231422	127	00:05:04.63	00:05:16.1	In that case, we can treat this as a categorization problem. We can categorize restaurants or categorize products based on their corresponding reviews. So this is example of external category.	in that case , we can treat this as a categorization problem . we can categorize restaurant or categorize product base on their corresponding review . so this be example of external category .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231425	135	00:05:17.42	00:05:34.47	Here are some specific examples of applications. News categorization is very common, has been stuided. A lot. News agencies would like to assign predefined categories to categorize news generated every day.	here be some specific example of application . news categorization be very common , have be stuide . a lot . news agency would like to assign predefined category to categorize news generate every day .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231428	143	00:05:35.85	00:05:52.65	And literature article categorizations another important task, for example, in biomedical domain, Is this mesh annotations , mesh stands for medical subject heading. And this is ontology of terms characterize content of literature articles in detail.	and literature article categorization another important task , for example , in biomedical domain , be this mesh annotation , mesh stand for medical subject heading . and this be ontology of term characterize content of literature article in detail .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.23143	149	00:05:54.47	00:06:13.36	Another example of application spam, email detection or filtering right? So we often have a spam filter to help us distinguish spam from legitimate emails, and this is clearly a binary classification problem.	another example of application spam , email detection or filtering right ? so we often have a spam filter to help we distinguish spam from legitimate email , and this be clearly a binary classification problem .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231433	155	00:06:14.24	00:06:26.48	Sentiment categorization of product reviews or tweets is yet another kind of applications where we can categorize content into positive or negative or positive and negative or neutral.	sentiment categorization of product review or tweet be yet another kind of application where we can categorize content into positive or negative or positive and negative or neutral .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-26 06:14:26.710291	158	00:06:27.23	00:06:32.94	so you can have the same sentiment categories assigned. to text content.	so you can have the same sentiment category assign . to text content .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231437	165	00:06:35.43	00:06:47.5	Another application is automatically email routing or sorting, so you might want to automatically sort your emails into different folders, and that's one application of text categorization, where each folder is a category.	another application be automatically email routing or sorting , so you might want to automatically sort your email into different folder , and that be one application of text categorization , where each folder be a category .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231444	182	00:06:48.16	00:07:29.55	There is also another important kind of applications of routing emails to the right person to handle. So in helpdesk email messages generally routed to a particular person to handle different people attempt to handle different kinds of requests and in many cases a person will manually assign the messages to the right people. But you can imagine you can build automatic text categorization system to help routing a request. And this is to classify the incoming request in to one of the categories where each category actually corresponds to a person to handle the request.	there be also another important kind of application of route email to the right person to handle . so in helpdesk email message generally route to a particular person to handle different people attempt to handle different kind of request and in many case a person will manually assign the message to the right people . but you can imagine you can build automatic text categorization system to help route   a request . and this be to classify the incoming request in to one of the category where each category actually correspond to a person to handle the request .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231449	195	00:07:31.08	00:08:00.72	And finally, author Attribution. As I just mentioned, is yet another application, and it's another example of using text to actually infer properties of some other entities. And there are also many variants of the problem formulation and so first we have the simplest case, which is a binary categorization where there are only two categories and there are many examples like that information retrieval or search engine applications would want to.	and finally , author Attribution . as I just mention , be yet another application , and it be another example of use text to actually infer property of some other entity . and there be also many variant of the problem formulation and so first we have the simple case , which be a binary categorization where there be only two category and there be many example like that information retrieval or search engine application would want to .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231453	204	00:08:01.09	00:08:17.95	Distinguish it relevant documents from non relevant documents for a particular query. Spam filter is interesting. Distinguishing spams from non spam. So also two categories. Sometimes classification of opinions can be in two categories together with positive and negative.	distinguish it relevant document from non relevant document for a particular query . spam filter be interesting . distinguish spam from non spam . so also two category . sometimes classification of opinion can be in two category together with positive and negative .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231459	218	00:08:19.01	00:08:48.11	A more general case would be K-category categorization and there are also many applications like that. There could be more than two categories, so topical categorisation is often such example where you can have multiple topics. Email routing would be another example when you may have multiple folders, or if you route the email to the right person to handle it, then there are multiple people, to clasify so in all these cases there are more than two kinds of categories.	a more general case would be k - category categorization and there be also many application like that . there could be more than two category , so topical categorisation be often such example where you can have multiple topic . email routing would be another example when you may have multiple folder , or if you route the email to the right person to handle it , then there be multiple people , to clasify so in all these case there be more than two kind of category .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-26 06:22:33.554328	223	00:08:49.13	00:08:56.83	And another variation to have hierarchical categorization, where categories form hierarchy, again, topical hierarchy is very common. Yet another categorization is joint categorization. That's when you have multiple categorization tasks that are related. And then you hope to kind of join the categorization	and another variation to have hierarchical categorization , where category form hierarchy , again , topical hierarchy be very common . yet another categorization be joint categorization . that be when you have multiple categorization task that be relate . and then you hope to kind of join the categorization
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231465	232	00:09:15.09	00:09:38.76	Now among all these, binary categorization is most fundamental and partly also because it's simple and partly it's cause it can actually be used to perform all the other categorization tasks. For example, K category categorisation task can be actually performed by using binary categorization.	now among all these , binary categorization be most fundamental and partly also because it be simple and partly it be cause it can actually be use to perform all the other categorization task . for example , K category categorisation task can be actually perform by use binary categorization .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231467	237	00:09:39.49	00:09:52.06	And basically we can look at each category separately and then the binary categorization problem is whether object is in this category or not. Meaning in other categories.	and basically we can look at each category separately and then the binary categorization problem be whether object be in this category or not . mean in other category .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-26 06:28:33.04948	263	00:09:53.31	00:10:52.559999	And the hierarchical category categorisation can also be done by progressively doing flat categorisation at each level. So we can first categorize all the objects in tune. It's a small number of high level categories an inside each category. We can further categorize into sub categories etc. So why is text categories important well, I already showed you several applications, but in general there are several reasons. One is text Categorization helps us enrich text representation, and that's to achieve more understanding of text data that's always useful for text analysis. So now with categorisation, text can be represented in multiple levels, meaning keyword bag of words representation has often used for a lot of text processing tasks. But we can also add categories and they provide 2 levels of representation.	and the hierarchical category categorisation can also be do by progressively do flat categorisation at each level . so we can first categorize all the object in tune . it be a small number of high level category an inside each category . we can far categorize into sub category etc . so why be text category important well , I already show you several application , but in general there be several reason . one be text categorization help we enrich text representation , and that be to achieve more understanding of text datum that be always useful for text analysis . so now with categorisation , text can be represent in multiple level , mean keyword bag of word representation have often use for a lot of text processing task . but we can also add category and they provide 2 level of representation .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.23148	269	00:10:54.16	00:11:07.66	Semantic categories assigned can also be directly or indirectly useful for application. So for example, sentiment categories could be already very useful, or author Attribution might be directly useful.	semantic category assign can also be directly or indirectly useful for application . so for example , sentiment category could be already very useful , or author Attribution might be directly useful .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231481	270	00:11:10.75	00:11:11.46	And.	and .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231483	274	00:11:12.41	00:11:21.25	Another example is when semantic categories can facilitate aggregation of tax content, and this is another case of.	another example be when semantic category can facilitate aggregation of tax content , and this be another case of .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231483	275	00:11:22.67	00:11:24.93	Applications of text categorisation.	application of text categorisation .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231488	287	00:11:25.76	00:11:59	For example, we if we want to know the overall opinions about the product, we could first categorize the opinions in each individual review as positive or negative, and then that would allow us to easily aggregate all the sentiments and it will tell us about 70% of the views positive and 30% are negative, etc. So without doing categorization it will be much harder to aggregate such opinions.	for example , we if we want to know the overall opinion about the product , we could first categorize the opinion in each individual review as positive or negative , and then that would allow we to easily aggregate all the sentiment and it will tell we about 70 % of the view positive and 30 % be negative , etc . so without do categorization it will be much hard to aggregate such opinion .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231489	290	00:12:00.31	00:12:06.96	So it provides a concise way of coding text in some sense based on our vocabulary.	so it provide a concise way of code text in some sense base on our vocabulary .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231491	294	00:12:07.64	00:12:16.46	And sometimes you miss seeing some applications, text or categorization is called a text coding encoding with some controller vocabulary.	and sometimes you miss see some application , text or categorization be call a text code encoding with some controller vocabulary .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231492	297	00:12:18.64	00:12:27.86	The second kind of reasons is to use text categorization to infer properties of entities.	the second kind of reason be to use text categorization to infer property of entity .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231496	307	00:12:28.97	00:12:52.8	And text categorisation allows us to infer the properties of such entities that are associated with text data. So this means we can use text categorization to discover knowledge about the world in general, as long as we can associate the entity with text data, we can always use the text data to help categorize the corresponding entities.	and text categorisation allow we to infer the property of such entity that be associate with text datum . so this mean we can use text categorization to discover knowledge about the world in general , as long as we can associate the entity with text datum , we can always use the text datum to help categorize the correspond entity .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231508	337	00:12:53.48	00:13:57.2	So it's useful to think about the information network that will connect the other entities with text data. The obvious entities that can be directly connected are authors, but you can also imagine the authors affiliations or the authors ages and other things can be actually connected to text data indirectly. Once we can make the connection, then we can make predictions about those values. So this is a general way to allow us to use text mining tool. Sorry, text categorization to discover knowledge about the world. Very useful, especially in big text data. Analytics, where we are often interested in using text data as extra sensor data collected from humans to infer certain desicion factors. Often together with non text data specifically to text. For example, we can also think of examples of inferring properties of entities. For example discovery of non native speakers of a language and this can be done by categorizing the content of.	so it be useful to think about the information network that will connect the other entity with text datum . the obvious entity that can be directly connect be author , but you can also imagine the author affiliation or the author age and other thing can be actually connect to text datum indirectly . once we can make the connection , then we can make prediction about those value . so this be a general way to allow we to use text mining tool . sorry , text categorization to discover knowledge about the world . very useful , especially in big text datum . analytic , where we be often interested in use text datum as extra sensor datum collect from human to infer certain desicion factor . often together with non text datum specifically to text . for example , we can also think of example of infer property of entity . for example discovery of non native speaker of a language and this can be do by categorize the content of .
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231509	338	00:13:58.63	00:13:59.52	Speakers	speaker
39d13817-de51-4195-a33a-985b0b54e64d	2020-11-02 23:19:19.231511	343	00:14:00.57	00:14:14.04	Another example is to predict the party affiliation of a politician based on the political speech at this is again example of using text data to infer some knowledge about real world.	another example be to predict the party affiliation of a politician base on the political speech at this be again example of use text datum to infer some knowledge about real world .
4453a049-7597-4df4-9b9b-67c2d124a116	2020-11-02 23:07:32.448985	2	00:00:00.3	00:00:04.19	We can compute this maximum regular estimated by using the EM algorithm.	we can compute this maximum regular estimate by use the EM algorithm .
4453a049-7597-4df4-9b9b-67c2d124a116	2020-12-14 08:02:51.462906	22	00:00:12.92	00:01:01.78	So in the E-step, we now have to introduce more hidden variables because we have more topics. So our hidden variable Z now, which is a topic indicator, can take more than two values. Specifically, will take a K plus one values with B denoting the background and one through K to denote all the K topics. So now the E step as you can recall is augmented data and by predicting the values of the hidden variable. So we're going to predict for word whether the word has come from one of these K+1 distributions. This equation allows us to predict the probability that the word W in Document "D is generated from topic theta sub j	so in the e - step , we now have to introduce more hidden variable because we have more topic . so our hidden variable Z now , which be a topic indicator , can take more than two value . specifically , will take a k plus one value with B denote the background and one through K to denote all the K topic . so now the e step as you can recall be augment datum and by predict the value of the hidden variable . so we be go to predict for word whether the word have come from one of these k+1 distribution . this equation allow we to predict the probability that the word W in Document " d be generate from topic theta sub j
4453a049-7597-4df4-9b9b-67c2d124a116	2020-11-02 23:07:32.448993	25	00:01:02.89	00:01:07.9	And the bottom one is the predicted probability that this word has been generated from the background.	and the bottom one be the predict probability that this word have be generate from the background .
4453a049-7597-4df4-9b9b-67c2d124a116	2020-11-02 23:07:32.448996	38	00:01:08.6	00:01:36.94	Note that we use Document D here to index the word. Why? Because Whether a word is from a particular topic, actually depends on the document. Can you see why? Well, it's through the pi. The pis are tied to each document. Each document can have a potentially different pis, right? The pis will then affect our prediction, so the pis are here, and this depends on the document.	note that we use document d here to index the word . why ? because whether a word be from a particular topic , actually depend on the document . can you see why ? well , it be through the pi . the pis be tie to each document . each document can have a potentially different pis , right ? the pis will then affect our prediction , so the pis be here , and this depend on the document .
4453a049-7597-4df4-9b9b-67c2d124a116	2020-11-02 23:07:32.448997	41	00:01:38.41	00:01:45	And that might give a different guess of word for word in different documents, and that's desirable.	and that might give a different guess of word for word in different document , and that be desirable .
4453a049-7597-4df4-9b9b-67c2d124a116	2020-12-14 08:03:53.159707	45	00:01:46.2	00:01:56.29	In both cases we are using the bayes rule as I explained, basically assessing the likelihood of generating word in from each distribution and is normalized.	in both case we be use the baye rule as I explain , basically assess the likelihood of generate word in from each distribution and be normalize .
4453a049-7597-4df4-9b9b-67c2d124a116	2020-11-02 23:07:32.449001	55	00:01:57.69	00:02:21.72	What about the M-step? Well, we may recall the M step is to take advantage of the inferred Z values to split the counts and then collect the right counts to re estimate parameters. So in this case we can re estimate our coverage probability and this is re estimated based on collecting all the words in the document.	what about the m - step ? well , we may recall the M step be to take advantage of the infer Z value to split the count and then collect the right count to re estimate parameter . so in this case we can re estimate our coverage probability and this be re estimate base on collect all the word in the document .
4453a049-7597-4df4-9b9b-67c2d124a116	2020-11-02 23:07:32.449002	58	00:02:22.51	00:02:28.4	And that's why we have the count of the word in document and sum over all the words.	and that be why we have the count of the word in document and sum over all the word .
4453a049-7597-4df4-9b9b-67c2d124a116	2020-12-14 08:04:54.058532	62	00:02:29.01	00:02:39.45	And then we're going to look at the to what extent this word belongs to the topic theta sub-j, and this part is our guess from E-step.	and then we be go to look at the to what extent this word belong to the topic theta sub - j , and this part be our guess from e - step .
4453a049-7597-4df4-9b9b-67c2d124a116	2020-11-02 23:07:32.449005	70	00:02:40.21	00:02:57.84	This tells us how likely this word is actually from theta sub-j, and when we multiply them together we get the discounted count that's allocated for topic theta sub-j and we normalize this over all the topics we get the distribution over all the topics to indicate the coverage.	this tell we how likely this word be actually from theta sub - j , and when we multiply they together we get the discount count that be allocate for topic theta sub - j and we normalize this over all the topic we get the distribution over all the topic to indicate the coverage .
4453a049-7597-4df4-9b9b-67c2d124a116	2020-11-02 23:07:32.44901	84	00:02:58.68	00:03:26.4	And similarly, the bottom one is to re-estimate the probability of word for topic. In this case we're using exactly the same count. You can see this is the same discounted count, it tells us to what extent we should allocate this word to , topic theta sub-j. But the normalization is different because in this case we are interested in the word distribution. So we simply normalize this over all the words.	and similarly , the bottom one be to re - estimate the probability of word for topic . in this case we be use exactly the same count . you can see this be the same discount count , it tell we to what extent we should allocate this word to , topic theta sub - j. but the normalization be different because in this case we be interested in the word distribution . so we simply normalize this over all the word .
4453a049-7597-4df4-9b9b-67c2d124a116	2020-11-02 23:07:32.44901	85	00:03:27.26	00:03:28.31	This is different.	this be different .
4453a049-7597-4df4-9b9b-67c2d124a116	2020-11-02 23:07:32.44901	87	00:03:29.12	00:03:32.58	In contrast, here we normalized among all the topics.	in contrast , here we normalize among all the topic .
4453a049-7597-4df4-9b9b-67c2d124a116	2020-11-02 23:07:32.449011	89	00:03:33.34	00:03:35.87	It would be useful to take a comparison between the two.	it would be useful to take a comparison between the two .
4453a049-7597-4df4-9b9b-67c2d124a116	2020-12-14 08:06:36.455257	92	00:03:36.9	00:03:45.01	This gives us different distributions and these tells us how to improve the parameters?	this give we different distribution and these tell we how to improve the parameter ?
4453a049-7597-4df4-9b9b-67c2d124a116	2020-11-08 19:27:19.842848	105	00:03:47.04	00:04:16.15	And as I just explained in both E step formulas, we have a maximum likelihood estimator based on the allocated word "counts to "topic theta sub-j. Now this phenomena is actually general phenomenon in all the EM algorithms in the M step, you generate expected count of event based on the E step result and then you just collect the relevant counts for a particular parameter. and re-estimate with normalizing.	and as I just explain in both e step formula , we have a maximum likelihood estimator base on the allocate word " count to " topic theta sub - j. now this phenomena be actually general phenomenon in all the EM algorithm in the M step , you generate expect count of event base on the e step result and then you just collect the relevant count for a particular parameter . and re - estimate with normalize .
4453a049-7597-4df4-9b9b-67c2d124a116	2020-11-02 23:07:32.449017	106	00:04:17.61	00:04:18.14	Typically.	typically .
4453a049-7597-4df4-9b9b-67c2d124a116	2020-11-02 23:07:32.449018	108	00:04:20.16	00:04:25.36	So in terms of computation of the EM algorithm, we can.	so in term of computation of the EM algorithm , we can .
4453a049-7597-4df4-9b9b-67c2d124a116	2020-11-02 23:07:32.449021	120	00:04:26.9	00:04:48.84	Actually, just keep counting various events and then normalize them. And when we think in this way, we also have a more concise way of presenting the EM algorithm. It actually helps us better understand the formulas. So I'm going to go over this in some detail. So as the algorithm, we first initialize all the unknown parameters randomly.	actually , just keep count various event and then normalize they . and when we think in this way , we also have a more concise way of present the EM algorithm . it actually help we well understand the formula . so I be go to go over this in some detail . so as the algorithm , we first initialize all the unknown parameter randomly .
4453a049-7597-4df4-9b9b-67c2d124a116	2020-12-04 08:15:18.501408	123	00:04:49.54	00:04:56.61	In our case we are interested in all those coverage parameters-- pis--and word distributions, thetas.	in our case we be interested in all those coverage parameters-- pis -- and word distribution , theta .
4453a049-7597-4df4-9b9b-67c2d124a116	2020-11-02 23:07:32.449024	133	00:04:57.58	00:05:18.14	And we just randomly normalize them. This is the initialization step, and then we will repeat until likelihood converges. Now how do we know whether likelihood converges we're going to compute likelihood at each step and compare the current likelihood with the previous likelihood if it doesn't change much and we're going to say stop right?	and we just randomly normalize they . this be the initialization step , and then we will repeat until likelihood converge . now how do we know whether likelihood converge we be go to compute likelihood at each step and compare the current likelihood with the previous likelihood if it do n't change much and we be go to say stop right ?
4453a049-7597-4df4-9b9b-67c2d124a116	2020-11-02 23:07:32.449027	141	00:05:19.36	00:05:42.74	So in each step we can do E step and M step in the E step we're going to augment the data by predicting the hidden variable values. In this case the hidden variable Z sub DW indicates whether word in W in D is from topic or background, an if it's from a topic which topic?	so in each step we can do e step and M step in the e step we be go to augment the datum by predict the hide variable value . in this case the hide variable z sub DW indicate whether word in W in d be from topic or background , an if it be from a topic which topic ?
4453a049-7597-4df4-9b9b-67c2d124a116	2020-11-02 23:07:32.449033	163	00:05:43.68	00:06:34.44	So if you look at the E step formulas essentially we're actually normalizing these counts. At all, sorry, these are probabilities of observing the word from each distribution, so you can see basically the prediction of word from topical theTAsubject is based on the probability of selecting that theta sub-j as a word distribution to begin to generate the world multiplied by the probability of observing the word from that distribution. And I said it's proportional to this because in completing the implementation of EM algorithm you can just keep count counter for this quantity and then in the end you just normalize it. So the normalization here is over all the topics and then you will get a probability.	so if you look at the e step formula essentially we be actually normalize these count . at all , sorry , these be probability of observe the word from each distribution , so you can see basically the prediction of word from topical thetasubject be base on the probability of select that theta sub - j as a word distribution to begin to generate the world multiply by the probability of observe the word from that distribution . and I say it be proportional to this because in complete the implementation of EM algorithm you can just keep count counter for this quantity and then in the end you just normalize it . so the normalization here be over all the topic and then you will get a probability .
4453a049-7597-4df4-9b9b-67c2d124a116	2020-11-02 23:07:32.449033	165	00:06:36.25	00:06:42.06	Now in the M step we do the same and we are going to collect these.	now in the M step we do the same and we be go to collect these .
4453a049-7597-4df4-9b9b-67c2d124a116	2020-12-04 08:18:30.27202	166	00:06:43.93	00:06:46.51	Allocated counts for each topic.	allocate count for each topic .
4453a049-7597-4df4-9b9b-67c2d124a116	2020-11-02 23:07:32.449033	167	00:06:47.64	00:06:49.73	And we split words among the topics.	and we split word among the topic .
4453a049-7597-4df4-9b9b-67c2d124a116	2020-11-02 23:07:32.449037	181	00:06:50.83	00:07:22.49	And then we're going to normalize them in different ways to obtain the re-estimate. So, for example, we can normalize among all the topics to get re estimate of Pi the coverage. Or we can renormalize based on the. For all the words and that would give us a word distribution. So it's useful to think of the algorithm in this way, because when you implement, you can just use. Variables to keep track of these quantities in each case.	and then we be go to normalize they in different way to obtain the re - estimate . so , for example , we can normalize among all the topic to get re estimate of Pi the coverage . or we can renormalize base on the . for all the word and that would give we a word distribution . so it be useful to think of the algorithm in this way , because when you implement , you can just use . variable to keep track of these quantity in each case .
4453a049-7597-4df4-9b9b-67c2d124a116	2020-11-02 23:07:32.449038	183	00:07:23.68	00:07:31.41	And then you just normalize these variables to make them a distribution.	and then you just normalize these variable to make they a distribution .
4453a049-7597-4df4-9b9b-67c2d124a116	2020-12-14 08:16:05.755859	191	00:07:32.08	00:07:49.98	Now I did not put the constraint for this one and I intentionally leave this as exercise for you and you can see what's the normalizer for this one. It's of a slightly different form, but it's essentially the same as the one that you have seen here. Namely this one.	now I do not put the constraint for this one and I intentionally leave this as exercise for you and you can see what be the normalizer for this one . it be of a slightly different form , but it be essentially the same as the one that you have see here . namely this one .
4453a049-7597-4df4-9b9b-67c2d124a116	2020-11-02 23:07:32.44904	195	00:07:50.6	00:07:59.63	So in general, in the implementation of EM algorithm with you will see you accumulated counts various comes and then you normalize them.	so in general , in the implementation of EM algorithm with you will see you accumulate count various come and then you normalize they .
4453a049-7597-4df4-9b9b-67c2d124a116	2020-11-02 23:07:32.449041	199	00:08:01.58	00:08:10.77	So to summarize, we introduced the PLSA model, which is a mixture model with K unigram language models representing K topics.	so to summarize , we introduce the PLSA model , which be a mixture model with K unigram language model represent K topic .
4453a049-7597-4df4-9b9b-67c2d124a116	2020-11-02 23:07:32.449042	204	00:08:11.7	00:08:22.5	And we also added a predetermined background language model to help discover discriminating topics. Because this background language model can help attract the common terms.	and we also add a predetermine background language model to help discover discriminate topic . because this background language model can help attract the common term .
4453a049-7597-4df4-9b9b-67c2d124a116	2020-11-02 23:07:32.449047	223	00:08:23.66	00:09:01.85	And, We show that with maximum likelihood estimator we can discover topical knowledge from text data. In this case PLSA allows us to discover two things. One is k-word distributions, each representing a topic and the other is the proportion of each topic in each document. And such detailed characterization of coverage of topics in documents can enable a lot of further analysis. For example, we can aggregate the documents in the particular time period to assess the coverage of a particular topic in a time period that would allow us to generate the temporal chains of topics.	and , we show that with maximum likelihood estimator we can discover topical knowledge from text datum . in this case PLSA allow we to discover two thing . one be k - word distribution , each represent a topic and the other be the proportion of each topic in each document . and such detailed characterization of coverage of topic in document can enable a lot of further analysis . for example , we can aggregate the document in the particular time period to assess the coverage of a particular topic in a time period that would allow we to generate the temporal chain of topic .
4453a049-7597-4df4-9b9b-67c2d124a116	2020-11-02 23:07:32.449048	227	00:09:02.45	00:09:13.37	We can also aggregate topics covered in documents associated with a particular author, and then we can characterize the topics written by this author, etc.	we can also aggregate topic cover in document associate with a particular author , and then we can characterize the topic write by this author , etc .
4453a049-7597-4df4-9b9b-67c2d124a116	2020-11-02 23:07:32.449049	235	00:09:14.32	00:09:31.41	And in addition to this, we can also cluster terms and cast documents. In fact, each topic can be regarded as a cluster, so we already have term clusters. And the higher probability words can be regarded as in belonging to one cluster.	and in addition to this , we can also cluster term and cast document . in fact , each topic can be regard as a cluster , so we already have term cluster . and the high probability word can be regard as in belong to one cluster .
4453a049-7597-4df4-9b9b-67c2d124a116	2020-12-04 08:22:30.22186	246	00:09:32.62	00:09:55.7	Represented by the topic. Similarly documents can be clustered in the same way. We can assign a document to the topic cluster that's covered most in the document. So remember pid indicate to what extent each topic is covered in the document. We can assign the document to the topic cluster that has the highest pi.	represent by the topic . similarly document can be cluster in the same way . we can assign a document to the topic cluster that be cover most in the document . so remember pid indicate to what extent each topic be cover in the document . we can assign the document to the topic cluster that have the high pi .
44df41bc-04d3-41ca-ac51-dbd22dc98305	2020-11-02 23:11:30.206971	3	00:00:00.29	00:00:08.25	In general, we can use the empirical counts of events in the observed data to estimate probabilities.	in general , we can use the empirical count of event in the observed datum to estimate probability .
44df41bc-04d3-41ca-ac51-dbd22dc98305	2020-11-02 23:11:30.206972	6	00:00:15.21	00:00:21.86	and a commonly used technique is called a maximum likelihood estimate, where we simply normalize the observed accounts.	and a commonly use technique be call a maximum likelihood estimate , where we simply normalize the observe account .
44df41bc-04d3-41ca-ac51-dbd22dc98305	2020-11-02 23:11:30.206973	14	00:00:22.45	00:00:46.36	So if we do that, we can see we can compute these probabilities as follows for estimating the probability that we see a word occurring in segment, we simply normalize the counts of segments that contain this word. So let's first take a look at the data here.	so if we do that , we can see we can compute these probability as follow for estimate the probability that we see a word occur in segment , we simply normalize the count of segment that contain this word . so let 's first take a look at the datum here .
44df41bc-04d3-41ca-ac51-dbd22dc98305	2020-11-02 23:11:30.206975	21	00:00:47.25	00:01:00.9	On the right side you see I listed some hypothesizes that data these are segments. And in some segments you see both words occur. Their indicator as once for both columns.	on the right side you see I list some hypothesize that datum these be segment . and in some segment you see both word occur . their indicator as once for both column .
44df41bc-04d3-41ca-ac51-dbd22dc98305	2020-11-02 23:11:30.206976	27	00:01:01.49	00:01:12.94	In some other cases, only one word occurs, so only that column has one and the other column has zero. And of course in some other cases, none of the words occur, so they are both zeros.	in some other case , only one word occur , so only that column have one and the other column have zero . and of course in some other case , none of the word occur , so they be both zero .
44df41bc-04d3-41ca-ac51-dbd22dc98305	2020-12-04 06:04:44.8691	28	00:01:13.78	00:01:14.48	And	and
44df41bc-04d3-41ca-ac51-dbd22dc98305	2020-11-02 23:11:30.206977	31	00:01:15.13	00:01:19.37	For estimating these probabilities, we simply need to collect the three counts.	for estimate these probability , we simply need to collect the three count .
44df41bc-04d3-41ca-ac51-dbd22dc98305	2020-11-02 23:11:30.206979	41	00:01:20.23	00:01:39.699999	So the three counts of 1st, the count of W. 1 and that's the total number of segments that contain world W one. It's just the ones in the column of W one we can just count how many ones we have seen there. The second counter is for word 2 and we just count the ones in the second column.	so the three count of 1st , the count of W. 1 and that be the total number of segment that contain world W one . it be just the one in the column of W one we can just count how many one we have see there . the second counter be for word 2 and we just count the one in the second column .
44df41bc-04d3-41ca-ac51-dbd22dc98305	2020-11-02 23:11:30.206979	43	00:01:40.33	00:01:44.73	And these this would give us a total number of segments that contain W2.	and these this would give we a total number of segment that contain W2 .
44df41bc-04d3-41ca-ac51-dbd22dc98305	2020-12-04 06:07:11.118764	47	00:01:45.39	00:01:55.43	The third account is when both words occurred, so this is time we're going to count the segments where both columns have ones.	the third account be when both word occur , so this be time we be go to count the segment where both column have one .
44df41bc-04d3-41ca-ac51-dbd22dc98305	2020-11-02 23:11:30.206982	56	00:01:56.05	00:02:14.86	And then so this would give us the total number of segments where we have seen both W and W2. Once we have these counts, we can just normalize. These counts by n, which is the total number of segments and this will give us the probabilities that we need to compute mutual information.	and then so this would give we the total number of segment where we have see both W and W2 . once we have these count , we can just normalize . these count by n , which be the total number of segment and this will give we the probability that we need to compute mutual information .
44df41bc-04d3-41ca-ac51-dbd22dc98305	2020-11-02 23:11:30.206984	68	00:02:15.81	00:02:47.069999	Now there is a small problem. When we have zero counts sometimes and in this case we don't want a zero probability because our data maybe a small sample and in general we would believe that it's potentially possible for award to occur in any context. So to address this problem we can use a technique called smoothing and that's basically to add some small constant to discounts and then so that we don't get a zero probability in any case.	now there be a small problem . when we have zero count sometimes and in this case we do n't want a zero probability because our datum maybe a small sample and in general we would believe that it be potentially possible for award to occur in any context . so to address this problem we can use a technique call smoothing and that be basically to add some small constant to discount and then so that we do n't get a zero probability in any case .
44df41bc-04d3-41ca-ac51-dbd22dc98305	2020-11-02 23:11:30.206986	79	00:02:48.38	00:03:14.09	Now, the best way to understand the smoothing is imagine that we actually. Observed more data than we actually have. We will pretend we observe some pseudo segments that are illustrated on the top on the right side of the slide and these pseudo segments would contribute additional counts of these words so that no event will have zero probability probability.	now , the good way to understand the smoothing be imagine that we actually . observe more datum than we actually have . we will pretend we observe some pseudo segment that be illustrate on the top on the right side of the slide and these pseudo segment would contribute additional count of these word so that no event will have zero probability probability .
44df41bc-04d3-41ca-ac51-dbd22dc98305	2020-11-02 23:11:30.206987	82	00:03:15.06	00:03:20.34	Now, in particular, we introduce the four pseudo segments. Each is weighted 1/4.	now , in particular , we introduce the four pseudo segment . each be weight 1/4 .
44df41bc-04d3-41ca-ac51-dbd22dc98305	2020-11-02 23:11:30.206989	92	00:03:21.08	00:03:42.15	And these represent the four different combinations of occurrences of these words. So now each event, each combination will have at least one count or at least non zero counter. From these pseudo segment. So in the actual segments that we observed, it's OK if we haven't observed all the combinations.	and these represent the four different combination of occurrence of these word . so now each event , each combination will have at least one count or at least non zero counter . from these pseudo segment . so in the actual segment that we observe , it be ok if we have n't observe all the combination .
44df41bc-04d3-41ca-ac51-dbd22dc98305	2020-11-02 23:11:30.20699	98	00:03:44.15	00:03:57.57	So more specifically, you can see the point of five. Here actually comes from the two ones in the two pseudo segments, because each is weighted 1/4, we added them up. We get .5.	so more specifically , you can see the point of five . here actually come from the two one in the two pseudo segment , because each be weight 1/4 , we add they up . we get .5 .
44df41bc-04d3-41ca-ac51-dbd22dc98305	2020-11-02 23:11:30.206991	101	00:03:59.41	00:04:08.4	And similarly this .05 comes from one single pseudo segment that indicates the two words occur together.	and similarly this .05 come from one single pseudo segment that indicate the two word occur together .
44df41bc-04d3-41ca-ac51-dbd22dc98305	2020-11-02 23:11:30.206993	110	00:04:09.32	00:04:24.34	And of course, in the denominator we add the total number of pseudo segments that we added. In this case we added a 4th through the segments. Each is weighted 1/4, so the total the sum is actually one. So that's why in the denominator you still want there.	and of course , in the denominator we add the total number of pseudo segment that we add . in this case we add a 4th through the segment . each be weight 1/4 , so the total the sum be actually one . so that be why in the denominator you still want there .
44df41bc-04d3-41ca-ac51-dbd22dc98305	2020-11-02 23:11:30.206993	114	00:04:25.87	00:04:33.99	So this basically concludes the discussion of how to compute the mutual information, how to use this for syntagmatic relation discovery.	so this basically conclude the discussion of how to compute the mutual information , how to use this for syntagmatic relation discovery .
44df41bc-04d3-41ca-ac51-dbd22dc98305	2020-11-02 23:11:30.206996	127	00:04:35.48	00:05:03.44	No. So, to summarize, select the cinematic relation can generally be discovered by measuring correlations between occurrences of two words. We introduce the three concepts from information theory, entropy, which meshes uncertainly over random variable X conditional entropy, which measures the entropy of X. Given we know why. And mutual information of X&Y which matches the entropy reduction of X.	no . so , to summarize , select the cinematic relation can generally be discover by measure correlation between occurrence of two word . we introduce the three concept from information theory , entropy , which mesh uncertainly over random variable x conditional entropy , which measure the entropy of X. give we know why . and mutual information of X&Y which match the entropy reduction of X.
44df41bc-04d3-41ca-ac51-dbd22dc98305	2020-11-02 23:11:30.206996	129	00:05:04.44	00:05:10.54	Due to knowing why or entropy reduction of why do too knowing eggs?	due to know why or entropy reduction of why do too know egg ?
44df41bc-04d3-41ca-ac51-dbd22dc98305	2020-11-02 23:11:30.207002	160	00:05:11.14	00:06:23.86	They are the same, so these three concepts are actually very useful for other applications as well. That's why we spend some time to explain this in detail, but in particular there also very useful for discovering syntagmatic relations. In particular, mutual information is a principled way for discovering such a relation. It allows us to have values computer on different pairs of words that are comfortable, and so we can rank these pairs and discover the strongest cinematical relationship from collection of documents. Now note that there is some relation between syntactic medical relation discovery and paradigmatically relation discovery. So we already discussed the possibility of using BM 25 to achieve waiting for terms in the context to potentially also suggest the candidates that have seen like medical relations with the candidate word. But here, once we use mutual information to discover Syntagmatic relations, we can also represent the context with this mutual information as weights.	they be the same , so these three concept be actually very useful for other application as well . that be why we spend some time to explain this in detail , but in particular there also very useful for discover syntagmatic relation . in particular , mutual information be a principled way for discover such a relation . it allow we to have value computer on different pair of word that be comfortable , and so we can rank these pair and discover the strong cinematical relationship from collection of document . now note that there be some relation between syntactic medical relation discovery and paradigmatically relation discovery . so we already discuss the possibility of use BM 25 to achieve wait for term in the context to potentially also suggest the candidate that have see like medical relation with the candidate word . but here , once we use mutual information to discover syntagmatic relation , we can also represent the context with this mutual information as weight .
44df41bc-04d3-41ca-ac51-dbd22dc98305	2020-11-02 23:11:30.207003	162	00:06:24.42	00:06:29.65	So this would give us another way to represent the context.	so this would give we another way to represent the context .
44df41bc-04d3-41ca-ac51-dbd22dc98305	2020-11-02 23:11:30.207004	170	00:06:30.2	00:06:48.06	Of a word like a cat, and if we do the same for all the words, then we can cluster these words or computer similarity between these words based on their context similarity. So this provides yet another way to do term waiting for paradigmatic. A relation discovery an.	of a word like a cat , and if we do the same for all the word , then we can cluster these word or computer similarity between these word base on their context similarity . so this provide yet another way to do term wait for paradigmatic . a relation discovery an .
44df41bc-04d3-41ca-ac51-dbd22dc98305	2020-11-02 23:11:30.207007	183	00:06:52.12	00:07:19.8	So to summarize, this whole part about word Association mining, we introduce the two basic associations, called Paradigmatic and Syntagmatic relations. These are fairly general. They can be applied to any items in any language, so the units don't have to be worse than they can be phrases or entities. Are we introduced multiple statistical approaches for discovering them? Then it showing that pure statistical approaches are visible?	so to summarize , this whole part about word Association mining , we introduce the two basic association , call paradigmatic and syntagmatic relation . these be fairly general . they can be apply to any item in any language , so the unit do n't have to be bad than they can be phrase or entity . be we introduce multiple statistical approach for discover they ? then it show that pure statistical approach be visible ?
44df41bc-04d3-41ca-ac51-dbd22dc98305	2020-11-02 23:11:30.207008	189	00:07:21.3	00:07:33.96	Available for discovering both kinds of relations, and they can be combined to perform. Join the analysis as well. These approaches can be applied to any text with no helmet human effort.	available for discover both kind of relation , and they can be combine to perform . join the analysis as well . these approach can be apply to any text with no helmet human effort .
44df41bc-04d3-41ca-ac51-dbd22dc98305	2020-11-02 23:11:30.207008	191	00:07:34.68	00:07:38.92	And mostly becausw. They are based on counting of words.	and mostly becausw . they be base on counting of word .
44df41bc-04d3-41ca-ac51-dbd22dc98305	2020-11-02 23:11:30.207008	193	00:07:39.48	00:07:42.75	Yet they can actually discover interesting relations of words.	yet they can actually discover interesting relation of word .
44df41bc-04d3-41ca-ac51-dbd22dc98305	2020-11-02 23:11:30.20701	203	00:07:43.4	00:08:03.9	We can also use different ways to define context and segment and this would lead to some interesting variations of applications. For example, the context can be very narrow, like a few words around a word or sentence or maybe paragraphs and using different contexts, which allows you to discover different flavors of paradigmatic relations.	we can also use different way to define context and segment and this would lead to some interesting variation of application . for example , the context can be very narrow , like a few word around a word or sentence or maybe paragraph and use different context , which allow you to discover different flavor of paradigmatic relation .
44df41bc-04d3-41ca-ac51-dbd22dc98305	2020-11-02 23:11:30.207012	211	00:08:05.17	00:08:25.73	And similarly, counting Co occurrences using, let's say mutual information to discover syntagmatic relations, we also have to define the segment and the segment can be defined as an arrow, text window or longer text article and this would give us different kinds of associations.	and similarly , count co occurrence use , let 's say mutual information to discover syntagmatic relation , we also have to define the segment and the segment can be define as an arrow , text window or long text article and this would give we different kind of association .
44df41bc-04d3-41ca-ac51-dbd22dc98305	2020-11-02 23:11:30.207013	216	00:08:26.57	00:08:35.13	These discovery associations can support them. Any other applications in both information retrieval and text data mining.	these discovery association can support they . any other application in both information retrieval and text datum mining .
44df41bc-04d3-41ca-ac51-dbd22dc98305	2020-11-02 23:11:30.207013	217	00:08:37.78	00:08:40.24	So here are some recommended readings.	so here be some recommend reading .
44df41bc-04d3-41ca-ac51-dbd22dc98305	2020-11-02 23:11:30.207014	221	00:08:41.25	00:08:49.43	If you want to know more about the topic, the 1st is a book with a chapter on locations which is quite relevant to the topic of these lectures.	if you want to know more about the topic , the 1st be a book with a chapter on location which be quite relevant to the topic of these lecture .
44df41bc-04d3-41ca-ac51-dbd22dc98305	2020-11-02 23:11:30.207015	228	00:08:50.71	00:09:07.81	The second is the article about the using various statistical measures to discover lexical atoms. Those are phrases that are non composition compositional or for example hot dog is not really a dog that's hot.	the second be the article about the use various statistical measure to discover lexical atom . those be phrase that be non composition compositional or for example hot dog be not really a dog that be hot .
44df41bc-04d3-41ca-ac51-dbd22dc98305	2020-11-02 23:11:30.207016	232	00:09:08.42	00:09:16.21	Blue chip is not a chip that's blue, and the paper has a discussion about to some techniques for discovering such phrases.	blue chip be not a chip that be blue , and the paper have a discussion about to some technique for discover such phrase .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-12-10 18:21:55.314849	7	00:00:00.3	00:00:14.69	So now let's talk about the problem a little bit more and specifically, let's talk about the two different ways of estimating parameters. One is called maximum likelihood estimate that I already just mentioned. The other is Bayesian estimation.	so now let 's talk about the problem a little bit more and specifically , let 's talk about the two different way of estimate parameter . one be call maximum likelihood estimate that I already just mention . the other be bayesian estimation .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-12-10 18:24:04.890439	14	00:00:22.08	00:00:40.06	So in Maximum Likelihood estimation, we define best as meaning the data likelihood has reached the maximum, so formally it's given by this expression here. Where we define the estimate as Arg arg max.	so in maximum Likelihood estimation , we define well as mean the datum likelihood have reach the maximum , so formally it be give by this expression here . where we define the estimate as Arg arg max .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-12-10 18:24:14.19996	15	00:00:41.39	00:00:45.36	of the probability of X given theater.	of the probability of X give theater .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-12-10 18:24:43.062112	17	00:00:46	00:00:52.78	Andso arg max here just means it's actually a function that would return.	Andso arg max here just mean it be actually a function that would return .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-11-02 23:06:46.902989	31	00:00:53.47	00:01:26.27	the argument that gives the function maximum value as the value, so the value of arg max is not value of this function, but rather the argument that has made the function reach maximum. So in this case the value of argmax is Theta. It's the theater that makes the probability of X given Theta reaches maximum, so estimate. also makes sense, and it's often very useful, and it seeks the parameters that best explain the data.	the argument that give the function maximum value as the value , so the value of arg max be not value of this function , but rather the argument that have make the function reach maximum . so in this case the value of argmax be Theta . it be the theater that make the probability of x give Theta reach maximum , so estimate . also make sense , and it be often very useful , and it seek the parameter that well explain the datum .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-12-10 18:48:19.406439	63	00:01:27	00:02:46.86	But it has a problem when the data is too small, because when the data points are too small, there are very few data points. The sample is small, then if we trust data entirely and try to fit the data and then we will be biased. So in the case of text data, let's say our observed 100 words did not contain another word related to text mining, then our maximum likelihood estimator would give that word zero probability. Because giving a non zero probability would take away probability mass from some observed world which obviously is not optimal in terms of maximizing the likelihood of the observed data. But this zero probability for all the unseen words may not be reasonable sometimes, especially if we want the distribution to characterize the topic of text mining. So one way to address this problem is actually to use Bayesian estimation, where we actually would look at both the data and all our prior knowledge about the parameters. We assume that we have some prior belief about the parameters. Now in this case, of course, so we are.	but it have a problem when the datum be too small , because when the datum point be too small , there be very few data point . the sample be small , then if we trust datum entirely and try to fit the datum and then we will be bias . so in the case of text datum , let 's say our observed 100 word do not contain another word relate to text mining , then our maximum likelihood estimator would give that word zero probability . because give a non zero probability would take away probability mass from some observe world which obviously be not optimal in term of maximize the likelihood of the observed datum . but this zero probability for all the unseen word may not be reasonable sometimes , especially if we want the distribution to characterize the topic of text mining . so one way to address this problem be actually to use bayesian estimation , where we actually would look at both the datum and all our prior knowledge about the parameter . we assume that we have some prior belief about the parameter . now in this case , of course , so we be .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-11-02 23:06:46.903004	66	00:02:47.13	00:02:57.95	not going to look at just the data, but also look at the prior so the prior here is defined by P of Theta.	not go to look at just the datum , but also look at the prior so the prior here be define by p of Theta .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-11-02 23:06:46.903006	69	00:02:59.23	00:03:05.9	And this means we will impose some preference on certain Thetas of others.	and this mean we will impose some preference on certain Thetas of other .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-12-10 23:28:52.446404	71	00:03:06.75	00:03:10.86	And by using Bayes rule that I have shown here,	and by use Bayes rule that I have show here ,
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-12-10 18:49:40.639059	73	00:03:12.51	00:03:19.63	We can then combine the likelihood function with the prior.	we can then combine the likelihood function with the prior .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-12-10 18:49:48.272212	74	00:03:20.2	00:03:22.33	to give us this.	to give we this .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-12-10 18:50:05.954337	76	00:03:23.38	00:03:28.34	posterior probability of the parameter.	posterior probability of the parameter .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-11-02 23:06:46.903014	84	00:03:29.02	00:03:47.76	Now a full explanation of Bayes, and some of these things related to Bayesian reasoning would be outside the scope of this course, but I just give a brief introduction because this is a general knowledge that might be useful for you, so the Bayes rule is basically defined here.	now a full explanation of Bayes , and some of these thing relate to Bayesian reasoning would be outside the scope of this course , but I just give a brief introduction because this be a general knowledge that might be useful for you , so the Bayes rule be basically define here .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-12-10 18:53:37.743917	97	00:03:48.68	00:04:29.93	And allows us to write down one conditional probability of X given Y in terms of the conditional probability of Y given X. And you can see the two probabilities are two conditional probabilities are different in the order of the two variables, but often the rule is used for making inferences of, of a variable. So let's take a look at again, we can assume that P of X encodes our prior belief about the X.	and allow we to write down one conditional probability of x give Y in term of the conditional probability of Y give X. and you can see the two probability be two conditional probability be different in the order of the two variable , but often the rule be use for make inference of , of a variable . so let 's take a look at again , we can assume that p of X encode our prior belief about the x.
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-11-02 23:06:46.903022	101	00:04:30.61	00:04:39.38	That means before we observe any other data, that's our belief about X, what we believe some X values have higher probability than others.	that mean before we observe any other datum , that be our belief about x , what we believe some x value have high probability than other .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-12-10 18:54:56.208883	111	00:04:40.63	00:05:08.56	And this probability of X given Y is a conditional probability, and this is our posterior belief about X, because this is our belief about X values after we have observed Y. Given that we have observed Y now? what do we believe about X now, do we believe some values have high probabilities than others?	and this probability of x give Y be a conditional probability , and this be our posterior belief about x , because this be our belief about x value after we have observe Y. give that we have observe Y now ? what do we believe about X now , do we believe some value have high probability than other ?
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-12-10 18:56:07.663955	116	00:05:09.85	00:05:25.85	Now, the two probabilities of related through this can be regarded as the probability of the observed evidence why. here given a particular acts.	now , the two probability of related through this can be regard as the probability of the observe evidence why . here give a particular act .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-11-02 23:06:46.903029	118	00:05:26.6	00:05:29.28	So you can think about X as our hypothesis.	so you can think about x as our hypothesis .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-12-10 18:57:40.503163	126	00:05:30.31	00:06:00.79	And we have some prior belief about which hypothesis to choose and after we have observed Y, we will update our belief and this updating formula is based on the combination of our prior here and the likelihood of observing this Y if X is indeed true. So much for a detour about Bayes Rule.	and we have some prior belief about which hypothesis to choose and after we have observe Y , we will update our belief and this update formula be base on the combination of our prior here and the likelihood of observe this Y if X be indeed true . so much for a detour about Bayes Rule .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-11-02 23:06:46.903035	129	00:06:02.04	00:06:09.66	So in our case, what we're interested in is inferring the theta values so we have a prior here.	so in our case , what we be interested in be infer the theta value so we have a prior here .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-11-02 23:06:46.903036	131	00:06:10.87	00:06:14.69	That includes our prior knowledge about the parameters.	that include our prior knowledge about the parameter .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-11-02 23:06:46.903038	136	00:06:15.52	00:06:29.11	And then we have the data likelihood here that would tell us which parameter value can explain the data well. The posterior probability combines both of them.	and then we have the datum likelihood here that would tell we which parameter value can explain the datum well . the posterior probability combine both of they .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-11-02 23:06:46.903039	138	00:06:30.14	00:06:33.65	So it represents a compromise of the two preferences.	so it represent a compromise of the two preference .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-12-10 18:59:09.398143	141	00:06:34.28	00:06:44.57	And in such a case, we can maximize this posterior probability to find a theta that would maximize.	and in such a case , we can maximize this posterior probability to find a theta that would maximize .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-12-10 19:00:17.471432	144	00:06:45.16	00:06:54.44	this posterior probability. And this is estimator is called a maximum a Posteriori or MAP estimate.	this posterior probability . and this be estimator be call a maximum a Posteriori or MAP estimate .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-11-02 23:06:46.903045	156	00:06:55.38	00:07:23.68	And this estimate is a more general estimate than the maximum likelihood estimate. Because what if we define our prior as a noninformative prior meaning that it's uniform over all theta values? no preference, then we basically would go back to the maximum likelihood estimator because in such a case it's mainly going to be determined by this likelihood value here.	and this estimate be a more general estimate than the maximum likelihood estimate . because what if we define our prior as a noninformative prior meaning that it be uniform over all theta value ? no preference , then we basically would go back to the maximum likelihood estimator because in such a case it be mainly go to be determine by this likelihood value here .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-11-02 23:06:46.903045	157	00:07:24.34	00:07:25.61	The same as here.	the same as here .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-11-02 23:06:46.903048	163	00:07:27.45	00:07:43.48	OK, but if we have some informative prior, some bias towards certain values, then MAP estimate can allow us to incorporate that, but the problem here of course is how to define the prior.	ok , but if we have some informative prior , some bias towards certain value , then MAP estimate can allow we to incorporate that , but the problem here of course be how to define the prior .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-12-10 19:04:43.956551	171	00:07:44.06	00:07:59.6	There's no free lunch, and if you want to solve the problem with more knowledge, we have to have that knowledge and that knowledge ideally should be reliable. Otherwise your estimate may not necessarily be more accurate than maximum likelihood estimate.	there be no free lunch , and if you want to solve the problem with more knowledge , we have to have that knowledge and that knowledge ideally should be reliable . otherwise your estimate may not necessarily be more accurate than maximum likelihood estimate .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-11-02 23:06:46.903053	178	00:08:00.51	00:08:23.24	Now let's look at the Bayesian estimation in more detail. OK, so I show the theta values as just one dimension value and that's a simplification of course. So we're interested in which value of data is optimal.	now let 's look at the bayesian estimation in more detail . ok , so I show the theta value as just one dimension value and that be a simplification of course . so we be interested in which value of datum be optimal .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-11-02 23:06:46.903056	185	00:08:24.5	00:08:41.02	So now first we have the prior. The prior tells us some theta values are more likely than others. We believe, for example, these values are more likely than the values like here or here or other places.	so now first we have the prior . the prior tell we some theta value be more likely than other . we believe , for example , these value be more likely than the value like here or here or other place .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-11-02 23:06:46.903056	186	00:08:42.01	00:08:44.98	So this is our prior.	so this be our prior .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-11-02 23:06:46.903056	187	00:08:46.04	00:08:50.39	And then we have our data likelihood.	and then we have our data likelihood .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-12-10 19:06:51.96549	191	00:08:51.35	00:08:59.94	In this case, the data also tells us which values of theta are more likely and that just means those theta values can best explain our data.	in this case , the datum also tell we which value of theta be more likely and that just mean those theta value can well explain our datum .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-11-02 23:06:46.903062	202	00:09:01.75	00:09:24.31	And then when we combine the two, we get the posterior distribution and that's just a compromise of the two. It would say that it is somewhere in between, so we can now look at some interesting point estimates of theta. Now this point represents the mode of prior. That means the most likely parameter value according to our prior before we observe any data.	and then when we combine the two , we get the posterior distribution and that be just a compromise of the two . it would say that it be somewhere in between , so we can now look at some interesting point estimate of theta . now this point represent the mode of prior . that mean the most likely parameter value accord to our prior before we observe any datum .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-11-02 23:06:46.903063	205	00:09:25.11	00:09:31.65	This point is the maximum Riker is estimate that represents the city that gives the data the maximum probability.	this point be the maximum Riker be estimate that represent the city that give the datum the maximum probability .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-11-02 23:06:46.903063	207	00:09:32.25	00:09:36.54	Now this point is interesting. It's the posterior mode, it's the.	now this point be interesting . it be the posterior mode , it be the .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-12-10 19:09:31.30706	212	00:09:37.09	00:09:48.19	It's the most likely value of theta given by the posterior distribution, and it represents a good compromise of the prior mode and the maximum likehood estimate.	it be the most likely value of theta give by the posterior distribution , and it represent a good compromise of the prior mode and the maximum likehood estimate .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-12-10 19:10:02.027354	218	00:09:51.37	00:10:07.91	In general, in Bayesian inference we are interested in the distribution of all these parameter values. As you see, here is there's a distribution over Theta values that you can see here P of theta given X.	in general , in Bayesian inference we be interested in the distribution of all these parameter value . as you see , here be there be a distribution over theta value that you can see here p of theta give X.
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-11-02 23:06:46.903079	247	00:10:09.03	00:11:23.17	So the problem of Bayesian inference is to infer this posterior distribution and also to infer other interesting quantities that might depend on Theta. So I showed F of Theta here as an interesting variable that we want to compute. But in order to compute this value, we need to know the value of Theta. In Bayesian inference, we treat data as uncertain variable. So we think about all the possible values of Theta. Therefore we can estimate the value of this function F as the expected value of F according to the posterior distribution of data given the observed evidence X. As a special case, we can assume F of Theta is just equal to 0. In this case we get the expected value of Theta. That's basically the posterior mean that gives us also one point of Theta. And it's sometimes the same as posterior mode, but it's not always the same, so it gives us another way to estimate the parameters.	so the problem of bayesian inference be to infer this posterior distribution and also to infer other interesting quantity that might depend on Theta . so I show F of Theta here as an interesting variable that we want to compute . but in order to compute this value , we need to know the value of Theta . in Bayesian inference , we treat datum as uncertain variable . so we think about all the possible value of Theta . therefore we can estimate the value of this function F as the expect value of F accord to the posterior distribution of datum give the observe evidence X. as a special case , we can assume F of Theta be just equal to 0 . in this case we get the expect value of Theta . that be basically the posterior mean that give we also one point of Theta . and it be sometimes the same as posterior mode , but it be not always the same , so it give we another way to estimate the parameter .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-11-02 23:06:46.90308	250	00:11:24.34	00:11:28.5	So this is a general illustration of Bayesian estimation and Bayesian inference.	so this be a general illustration of bayesian estimation and Bayesian inference .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-12-10 19:14:11.146807	254	00:11:29.11	00:11:38.74	And later you will see this can be useful for topic mining where we want to inject some prior knowledge about the topics.	and later you will see this can be useful for topic mining where we want to inject some prior knowledge about the topic .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-12-10 19:14:38.774869	263	00:11:39.41	00:11:53.52	So to summarize, we introduced the language model which is basically probability distribution over text. It's also called a generative model for text data. The simplest language model is unigram language model. It's basically a word distribution.	so to summarize , we introduce the language model which be basically probability distribution over text . it be also call a generative model for text datum . the simple language model be unigram language model . it be basically a word distribution .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-11-02 23:06:46.903086	266	00:11:54.62	00:12:01.03	We introduced the concept of likelihood function which is the probability of data given some model.	we introduce the concept of likelihood function which be the probability of datum give some model .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-11-02 23:06:46.903086	267	00:12:02.16	00:12:03.96	And this function is very important.	and this function be very important .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-12-10 19:15:33.835344	271	00:12:05.22	00:12:13.23	Given a particular set of parameter values, this function can tell us which X, which data point has a higher likelihood, higher probability.	give a particular set of parameter value , this function can tell we which X , which datum point have a high likelihood , high probability .
48b37a2f-5ca3-4b7b-9bfc-d841da37c566	2020-12-10 19:16:00.39928	277	00:12:14.2	00:12:29.8	Given a data point, sorry, given a data sample X, we can use this function to determine which parameter values would maximize the probability of the observed data, and this is the maximum likelihood estimate.	give a data point , sorry , give a data sample x , we can use this function to determine which parameter value would maximize the probability of the observed datum , and this be the maximum likelihood estimate .
4a54f790-991c-44bb-ab62-713cbef84ad1	2020-11-02 23:17:00.521613	2	00:00:00.3	00:00:03.28	This lecture is about the sentiment classification.	this lecture be about the sentiment classification .
4a54f790-991c-44bb-ab62-713cbef84ad1	2020-11-02 23:17:00.521616	11	00:00:10.98	00:00:31.57	If we assume that most of the elements in the opinion representation are already known, then our only task maybe just the sentiment classification as shown in this case. So suppose we know who is the opinion holder and what's the opinion target and also know the content and context of the opinion.	if we assume that most of the element in the opinion representation be already know , then our only task maybe just the sentiment classification as show in this case . so suppose we know who be the opinion holder and what be the opinion target and also know the content and context of the opinion .
4a54f790-991c-44bb-ab62-713cbef84ad1	2020-11-02 23:17:00.521618	16	00:00:32.13	00:00:45.71	Then we mainly need to decide the opinion sentiment of the review. So this is a case of just using sentiment classification for understanding opinion.	then we mainly need to decide the opinion sentiment of the review . so this be a case of just use sentiment classification for understand opinion .
4a54f790-991c-44bb-ab62-713cbef84ad1	2020-11-02 23:17:00.521619	21	00:00:46.5	00:00:54.51	Sentiment classification can be defined more specifically as follows: The input is opinionated text object.	Sentiment classification can be define more specifically as follow : the input be opinionate text object .
4a54f790-991c-44bb-ab62-713cbef84ad1	2020-11-02 23:17:00.521621	27	00:00:55.45	00:01:07.7	The output is typically, a sentiment label or sentiment tag, and that can be designed in two ways. One is polarity analysis where we have categories such as positive, negative or neutral.	the output be typically , a sentiment label or sentiment tag , and that can be design in two way . one be polarity analysis where we have category such as positive , negative or neutral .
4a54f790-991c-44bb-ab62-713cbef84ad1	2020-11-02 23:17:00.521621	28	00:01:08.72	00:01:12.27	The other is emotion analysis.	the other be emotion analysis .
4a54f790-991c-44bb-ab62-713cbef84ad1	2020-12-04 06:12:49.920399	31	00:01:13.14	00:01:20.65	That can go beyond polarity to characterize the feeling of the opinion holder.	that can go beyond polarity to characterize the feeling of the opinion holder .
4a54f790-991c-44bb-ab62-713cbef84ad1	2020-11-02 23:17:00.521623	35	00:01:21.47	00:01:29.67	In the case of polarity analysis, we sometimes also have numerical ratings, as you often see in some reviews on the web.	in the case of polarity analysis , we sometimes also have numerical rating , as you often see in some review on the web .
4a54f790-991c-44bb-ab62-713cbef84ad1	2020-11-02 23:17:00.521625	40	00:01:30.42	00:01:42.47	Five might denote the most positive and one maybe at most negative, for example. In general you have just discrete categories to characterize the sentiment.	five might denote the most positive and one maybe at most negative , for example . in general you have just discrete category to characterize the sentiment .
4a54f790-991c-44bb-ab62-713cbef84ad1	2020-12-04 06:14:20.643379	46	00:01:43.6	00:01:57.26	In emotion analysis, of course, there are also different ways to design the categories. The six most frequently used categories are happy, sad, fearful, angry, surpised and disgusted.	in emotion analysis , of course , there be also different way to design the category . the six most frequently use category be happy , sad , fearful , angry , surpised and disgusted .
4a54f790-991c-44bb-ab62-713cbef84ad1	2020-11-02 23:17:00.521631	54	00:01:59.15	00:02:14.23	So as you can see, the task is essentially a classification task or categorisation task. As we've seen before, so it's a special case of text categorization. This also means any text categorization method can be used to do sentiment classification.	so as you can see , the task be essentially a classification task or categorisation task . as we 've see before , so it be a special case of text categorization . this also mean any text categorization method can be use to do sentiment classification .
4a54f790-991c-44bb-ab62-713cbef84ad1	2020-12-04 06:15:54.123395	66	00:02:15.21	00:02:40.23	Now, of course, if you just do that, the accuracy may not be good because sentiment classification does require some improvement over regular text categorization technique or simple text categorization technique. In particular, it needs two kinds of improvements. One is to use more sophisticated features that may be more appropriate for sentiment tagging, as I will discuss more in a moment.	now , of course , if you just do that , the accuracy may not be good because sentiment classification do require some improvement over regular text categorization technique or simple text categorization technique . in particular , it need two kind of improvement . one be to use more sophisticated feature that may be more appropriate for sentiment tagging , as I will discuss more in a moment .
4a54f790-991c-44bb-ab62-713cbef84ad1	2020-12-04 06:17:35.255344	89	00:02:41.3	00:03:31.5	The other is to consider the order of these categories. An especially polarity analysis, very clear that order here and so these categories are not all that independent. There is order among them, and so it's useful to consider the order. For example, we could use ordinal regression to do, and that's something that will talk more about later. So now let's talk about some features that often very useful for text categorization and text mining in general, but some of them are especially also needed for sentiment analysis. So let's start from the simplest one, which is character n-grams. You can just have a sequence of characters as a unit, and they can be mixed with different n(s) and different lengths.	the other be to consider the order of these category . an especially polarity analysis , very clear that order here and so these category be not all that independent . there be order among they , and so it be useful to consider the order . for example , we could use ordinal regression to do , and that be something that will talk more about later . so now let 's talk about some feature that often very useful for text categorization and text mining in general , but some of they be especially also need for sentiment analysis . so let 's start from the simple one , which be character n - gram . you can just have a sequence of character as a unit , and they can be mix with different n(s ) and different length .
4a54f790-991c-44bb-ab62-713cbef84ad1	2020-11-02 23:17:00.521643	93	00:03:32.13	00:03:40.75	And this is a very general way, and a very robust way to represent the text data. You could do that for any language pretty much.	and this be a very general way , and a very robust way to represent the text datum . you could do that for any language pretty much .
4a54f790-991c-44bb-ab62-713cbef84ad1	2020-12-04 06:23:03.987588	103	00:03:42.15	00:04:03.9	And this is also robust to spelling errors or recognition errors, right? So if you misspelled the word by 1 character and this representation actually would allow you to match this word when it occurs in the text correctly. So misspelled word and the correct form can be matched because they contain some common n-grams of characters.	and this be also robust to spell error or recognition error , right ? so if you misspell the word by 1 character and this representation actually would allow you to match this word when it occur in the text correctly . so misspelled word and the correct form can be match because they contain some common n - gram of character .
4a54f790-991c-44bb-ab62-713cbef84ad1	2020-11-02 23:17:00.521646	106	00:04:04.58	00:04:08.94	But of course such a representation would not be as discriminative as words.	but of course such a representation would not be as discriminative as word .
4a54f790-991c-44bb-ab62-713cbef84ad1	2020-11-02 23:17:00.521654	134	00:04:09.99	00:05:10.63	So next we have word n-grams, a sequence of words and again we can mix them with different lengths. Uni Grams are actually often very effective for a lot of text processing tasks and that's mostly because words are well designed features by humans for communication, and so they often good enough for many tasks, but it's not good or not sufficient for sentiment analysis clearly. For example, we might see a sentence like it's not good or it's not as good as something else. So in such a case, if you just take a good and that would suggest positive, it's not good, so it's not accurate, but if you take the bigram, not good together, and then it's more accurate, so longer n-grams are generally more discriminative and they are more specific. If you match it and it says a lot and it's accurate. It's unlikely, very ambiguous.	so next we have word n - gram , a sequence of word and again we can mix they with different length . Uni Grams be actually often very effective for a lot of text processing task and that be mostly because word be well design feature by human for communication , and so they often good enough for many task , but it be not good or not sufficient for sentiment analysis clearly . for example , we might see a sentence like it be not good or it be not as good as something else . so in such a case , if you just take a good and that would suggest positive , it be not good , so it be not accurate , but if you take the bigram , not good together , and then it be more accurate , so long n - gram be generally more discriminative and they be more specific . if you match it and it say a lot and it be accurate . it be unlikely , very ambiguous .
4a54f790-991c-44bb-ab62-713cbef84ad1	2020-12-04 06:29:47.92514	156	00:05:11.19	00:06:01.83	But it may cause overfitting because with such very unique features the machine learning program can easily pick up such features from the training set and to rely on such unique features to distinguish categories. An obviously that kind of classifier won't generalize to future data when such discriminating features will not necessarily occur. So that's a problem of overfitting. That's not desirable. We can also consider part of speech tag n-grams if we can do part of speech tagging and for example, adjective, noun could form a pair. We can also mix N grams of words and N grams of part of speech tags. For example, the word great might be followed by a non and this could become a feature, a hybrid feature.	but it may cause overfitting because with such very unique feature the machine learn program can easily pick up such feature from the training set and to rely on such unique feature to distinguish category . an obviously that kind of classifier wo n't generalize to future datum when such discriminate feature will not necessarily occur . so that be a problem of overfitting . that be not desirable . we can also consider part of speech tag n - gram if we can do part of speech tagging and for example , adjective , noun could form a pair . we can also mix n gram of word and n gram of part of speech tag . for example , the word great might be follow by a non and this could become a feature , a hybrid feature .
4a54f790-991c-44bb-ab62-713cbef84ad1	2020-11-02 23:17:00.521661	158	00:06:02.42	00:06:05.19	That could be useful for sentiment analysis.	that could be useful for sentiment analysis .
4a54f790-991c-44bb-ab62-713cbef84ad1	2020-11-02 23:17:00.521662	161	00:06:06.71	00:06:13.65	So next we can also have word classes so these classes can be syntactic like a part of speech tags.	so next we can also have word class so these class can be syntactic like a part of speech tag .
4a54f790-991c-44bb-ab62-713cbef84ad1	2020-11-02 23:17:00.521663	164	00:06:14.2	00:06:19.97	Or could be semantic and they might represent concepts in thethesaurus or ontology like word net.	or could be semantic and they might represent concept in the  thesaurus or ontology like word net .
4a54f790-991c-44bb-ab62-713cbef84ad1	2020-11-02 23:17:00.521669	186	00:06:20.85	00:07:07.31	Or they can be recognized the named entities like people or place and these categories can be used to enrich the representation as additional features. We can also learn word clusters empirically, for example we talk about mining associations of words and so we can have cluster of paradigmatically related words or sementically related words. And these clusters can be features to supplement the word based representation. Furthermore, we can also have frequent pattern syntax and these could be frequent word set. The words that formed a pattern do not necessarily occur together or next to each other. But we also have locations where the words might occur more closely together.	or they can be recognize the name entity like people or place and these category can be use to enrich the representation as additional feature . we can also learn word cluster empirically , for example we talk about mining association of word and so we can have cluster of paradigmatically relate word or sementically relate word . and these cluster can be feature to supplement the word base representation . furthermore , we can also have frequent pattern syntax and these could be frequent word set . the word that form a pattern do not necessarily occur together or next to each other . but we also have location where the word might occur more closely together .
4a54f790-991c-44bb-ab62-713cbef84ad1	2020-12-04 08:23:06.530694	203	00:07:08.46	00:07:50.29	And such patterns provide a more discriminative features than words, obviously, and they may also generalize better than just the regular n-grams because they are frequent, so you can expect them to occur also in test data so they have a lot of advantages, but they might still face the problem of overfitting as the features become more complex. This is the problem in general, and the same is true for parse tree based features where you can use a parse tree to derive features such as frequent subtrees or paths, and those are even more discriminating, but they also are more likely to cause overfitting.	and such pattern provide a more discriminative feature than word , obviously , and they may also generalize well than just the regular n - gram because they be frequent , so you can expect they to occur also in test datum so they have a lot of advantage , but they might still face the problem of overfitting as the feature become more complex . this be the problem in general , and the same be true for parse tree base feature where you can use a parse tree to derive feature such as frequent subtree or path , and those be even more discriminating , but they also be more likely to cause overfitting .
4a54f790-991c-44bb-ab62-713cbef84ad1	2020-12-04 06:38:37.948362	216	00:07:51.03	00:08:20.37	And in General, Patton discovery algorithms are very useful for feature construction, because they allow us to search in a larger space of possible features that are more complex than words that are sometimes useful. So in general, natural language processing is very important to derive complex features. They can enrich text representation. So for example, this is a simple sentence that I showed you long time ago, and in another lecture.	and in General , Patton discovery algorithm be very useful for feature construction , because they allow we to search in a large space of possible feature that be more complex than word that be sometimes useful . so in general , natural language processing be very important to derive complex feature . they can enrich text representation . so for example , this be a simple sentence that I show you long time ago , and in another lecture .
4a54f790-991c-44bb-ab62-713cbef84ad1	2020-12-04 06:40:25.135109	230	00:08:21.04	00:08:54.29	So from these words we can only derive simple world n-grams representations or character n-grams. But with NLP we can enrich the representation with a lot of other information such as part of speech tags, parse trees or entities, or even speech act. Now with such enriched information, of course, then we can generate a lot of other features, more complex features, like a mixed grams of word and part of speech tags. Or even a part of parse tree.	so from these word we can only derive simple world n - gram representation or character n - gram . but with nlp we can enrich the representation with a lot of other information such as part of speech tag , parse tree or entity , or even speech act . now with such enriched information , of course , then we can generate a lot of other feature , more complex feature , like a mixed gram of word and part of speech tag . or even a part of parse tree .
4a54f790-991c-44bb-ab62-713cbef84ad1	2020-12-04 06:40:55.025489	239	00:08:55.76	00:09:14.99	So in general, feature design actually affects categorization accuracy significantly, and it's a very important part of any machine learning application. In general I think it would be most effective if you can combine machine learning, error analysis and domain knowledge in designing features.	so in general , feature design actually affect categorization accuracy significantly , and it be a very important part of any machine learning application . in general I think it would be most effective if you can combine machine learning , error analysis and domain knowledge in designing feature .
4a54f790-991c-44bb-ab62-713cbef84ad1	2020-12-04 06:42:01.589154	242	00:09:15.64	00:09:21.2	So first you want to use domain knowledge and your understanding of the problem to design seed features.	so first you want to use domain knowledge and your understanding of the problem to design seed feature .
4a54f790-991c-44bb-ab62-713cbef84ad1	2020-11-02 23:17:00.521688	250	00:09:22.09	00:09:36.68	And you can also define a basic feature space with a lot of possible features for the Machine learning program to work on. And machine learning can be applied to select the most effective features or construct the new features that feature learning.	and you can also define a basic feature space with a lot of possible feature for the Machine learn program to work on . and machine learning can be apply to select the most effective feature or construct the new feature that feature learn .
4a54f790-991c-44bb-ab62-713cbef84ad1	2020-11-02 23:17:00.521689	253	00:09:37.25	00:09:42.86	And these features can then be further analyzed by humans through error analysis.	and these feature can then be far analyze by human through error analysis .
4a54f790-991c-44bb-ab62-713cbef84ad1	2020-12-04 06:44:26.531557	277	00:09:43.45	00:10:36.91	And you can look at the categorization errors and then further analyze what features can help you recover from those errors or what features cause overfitting and cause those errors, and so this can lead to feature validation that would revise the feature set and then you can iterate and we might consider using a different feature space. So NLP enriches text representation. As I just said and because it enriches the feature space. It allows much larger search space of features. And there are also many meaningful features that can be very useful for a lot of tasks. But be careful not to use a lot of complicated features because it can cause overfitting or otherwise you have to do the training carefully, not to let overfeeding happen?	and you can look at the categorization error and then far analyze what feature can help you recover from those error or what feature cause overfitting and cause those error , and so this can lead to feature validation that would revise the feature set and then you can iterate and we might consider use a different feature space . so nlp enriche text representation . as I just say and because it enrich the feature space . it allow much large search space of feature . and there be also many meaningful feature that can be very useful for a lot of task . but be careful not to use a lot of complicated feature because it can cause overfitting or otherwise you have to do the training carefully , not to let overfeed happen ?
4a54f790-991c-44bb-ab62-713cbef84ad1	2020-11-02 23:17:00.521696	281	00:10:37.69	00:10:46.59	So a main challenge in designing features a common challenge is to optimize the tradeoff between exhaustive activity and specificity.	so a main challenge in designing feature a common challenge be to optimize the tradeoff between exhaustive activity and specificity .
4a54f790-991c-44bb-ab62-713cbef84ad1	2020-11-02 23:17:00.521698	286	00:10:48.28	00:10:59.45	And this trade off, it turns out to be very difficult. Now, exhaustivity means we want that features to actually have high coverage of a lot of documents.	and this trade off , it turn out to be very difficult . now , exhaustivity mean we want that feature to actually have high coverage of a lot of document .
4a54f790-991c-44bb-ab62-713cbef84ad1	2020-11-02 23:17:00.521698	288	00:11:00.21	00:11:03.64	And so in that sense, you wanted features to be frequent.	and so in that sense , you want feature to be frequent .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-12-04 23:57:31.896129	2	00:00:00.3	00:00:05.72	So this is indeed a general idea of the expectation maximization, or EM algorithm.	so this be indeed a general idea of the expectation maximization , or EM algorithm .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-11-02 23:01:52.790312	8	00:00:14.53	00:00:26.47	So in all the EM algorithms, we introduce a hidden variable to help us solve the problem more easily. In our case, the hidden variable is a binary variable for each occurrence of word.	so in all the EM algorithm , we introduce a hidden variable to help we solve the problem more easily . in our case , the hidden variable be a binary variable for each occurrence of word .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-12-04 23:59:16.632284	12	00:00:27.27	00:00:34.38	And this binary variable would indicate whether the world has been generated from theta on Sunday or theater Super B.	and this binary variable would indicate whether the world have be generate from theta on Sunday or theater Super B.
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-12-05 00:00:17.805247	18	00:00:35.14	00:00:52.29	And here we show some possible values of these variables. For example for the it's from Background, Z value is 1 and text on the other hand is from the topic. Then it's 0 for Z etc.	and here we show some possible value of these variable . for example for the it be from Background , z value be 1 and text on the other hand be from the topic . then it be 0 for Z etc .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-12-05 00:00:39.324916	22	00:00:53.15	00:01:01.95	Now, of course we don't observe those Z values. We just imagine there are such a social values of Z attached to all the words.	now , of course we do n't observe those z value . we just imagine there be such a social value of z attach to all the word .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-11-02 23:01:52.790326	29	00:01:02.81	00:01:14.48	And that's why we call these hidden variables. Now the idea that we talked about before for predicting the word distribution that has been used with the general the world is it'll predict this.	and that be why we call these hide variable . now the idea that we talk about before for predict the word distribution that have be use with the general the world be it 'll predict this .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-11-02 23:01:52.790328	32	00:01:16.36	00:01:20.48	The value of this hidden variable. And So.	the value of this hidden variable . and so .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-12-05 02:31:34.922157	48	00:01:21.17	00:01:58.9	The algorithm, the EM algorithm then would work as follows. First will initialize all the parameters with random values. In our case the parameters are mainly the probability of a word given by status update. So this is the initialization stage. It is initialized values would allow us to use Bayes rule to take a guess of these Z values. So will guess these values we can say for sure whether taxes from background or not, but we can have our guesses. This is given by this formula. It's called E-step.	the algorithm , the EM algorithm then would work as follow . first will initialize all the parameter with random value . in our case the parameter be mainly the probability of a word give by status update . so this be the initialization stage . it be initialize value would allow we to use Bayes rule to take a guess of these z value . so will guess these value we can say for sure whether taxis from background or not , but we can have our guess . this be give by this formula . it be call e - step .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-12-05 02:32:14.60053	53	00:01:59.58	00:02:11.1	And so the algorithm would then try to use the E Step 2 gas. These Z values. After that it would then invoke another spec step called M-step.	and so the algorithm would then try to use the e Step 2 gas . these z value . after that it would then invoke another spec step call M - step .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-11-02 23:01:52.790344	58	00:02:12.07	00:02:26.52	In this step we simply take advantage of the inferred values and then just group words that are in the same distribution like this from background, including this as well.	in this step we simply take advantage of the infer value and then just group word that be in the same distribution like this from background , include this as well .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-11-02 23:01:52.790345	61	00:02:27.18	00:02:35.49	We can then normalize the count to estimate the probabilities or to revise our estimate of the parameters.	we can then normalize the count to estimate the probability or to revise our estimate of the parameter .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-11-02 23:01:52.790348	66	00:02:36.48	00:02:50.18	So let me also illustrate we can group the words that are believed to have come from Cedar sub D and as text mining algorithm for example and clustering.	so let I also illustrate we can group the word that be believe to have come from Cedar sub d and as text mining algorithm for example and clustering .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-11-02 23:01:52.790349	67	00:02:51.04	00:02:52.79	And we had group them together.	and we have group they together .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-11-02 23:01:52.79035	68	00:02:54.16	00:02:57.73	To help us re estimate the parameters.	to help we re estimate the parameter .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-11-02 23:01:52.790353	74	00:02:59.15	00:03:14.7	That were interested in so these will help us re estimate these parameters. But note that before we just set these parameter values randomly, But with this guess we will have a somewhat improved estimate of this.	that be interested in so these will help we re estimate these parameter . but note that before we just set these parameter value randomly , but with this guess we will have a somewhat improve estimate of this .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-11-02 23:01:52.790357	81	00:03:15.55	00:03:30.9	Of course, we don't know exactly whether it's zero or one, so we're not going to really do the split in hardware, but rather we can do those soft split and this is what happened here. So we're going to adjust the count.	of course , we do n't know exactly whether it be zero or one , so we be not go to really do the split in hardware , but rather we can do those soft split and this be what happen here . so we be go to adjust the count .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-12-05 02:37:11.045318	84	00:03:31.72	00:03:38.63	By the probability that we believe this would has been generated by using the theta sub d.	by the probability that we believe this would have be generate by use the theta sub d.
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-11-02 23:01:52.790364	94	00:03:39.73	00:04:02.71	And you can see this. Where does this come from? Well, this has come from here right from the E step. So the EM algorithm with iteratively improve our initial estimate of parameters by using E-step first and then M step. the E step is to augment the data with additional information like Z.	and you can see this . where do this come from ? well , this have come from here right from the e step . so the EM algorithm with iteratively improve our initial estimate of parameter by use e - step first and then M step . the e step be to augment the datum with additional information like Z.
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-11-02 23:01:52.790366	98	00:04:03.32	00:04:13.22	And the M step is to take advantage of the additional information to separate the data to split the data accounts and then collect the right data counts.	and the M step be to take advantage of the additional information to separate the datum to split the datum account and then collect the right data count .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-12-05 02:38:11.128733	99	00:04:15.08	00:04:16.96	re estimate our parameters.	re estimate our parameter .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-11-02 23:01:52.790372	107	00:04:17.78	00:04:33.629999	And then once we have a new generation of parameters, we're going to repeat this. We're going to use the E-step again to improve our estimate of the hidden variables, and then that would lead to another generation of re estimate the parameters.	and then once we have a new generation of parameter , we be go to repeat this . we be go to use the e - step again to improve our estimate of the hidden variable , and then that would lead to another generation of re estimate the parameter .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-11-02 23:01:52.790374	109	00:04:34.18	00:04:37.79	For the word distribution that we're interested in.	for the word distribution that we be interested in .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-12-05 02:39:42.971096	114	00:04:39.5	00:04:55.45	OK, so as I said, the bridge between the two is really variable Z hidden variable, which indicates how likely this world is from the topic word distributions theta sub d.	ok , so as I say , the bridge between the two be really variable z hide variable , which indicate how likely this world be from the topic word distribution theta sub d.
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-11-02 23:01:52.79038	120	00:04:56.74	00:05:11.78	So this slide has a lot of content and you may need to pause the video to digest it, but this basically captured the essence of EM algorithm. Start with initial values that are often randomly set.	so this slide have a lot of content and you may need to pause the video to digest it , but this basically capture the essence of EM algorithm . start with initial value that be often randomly set .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-12-05 02:40:41.952094	129	00:05:12.36	00:05:33.57	And then we invoke E step followed by M step to get an improved setting of parameters, and then we repeat this. So this is a hill climbing algorithm that would gradually improve the estimate of parameters and as I will explain later, there's some guarantee for reaching a local maximum of the likelihood function.	and then we invoke e step follow by M step to get an improved setting of parameter , and then we repeat this . so this be a hill climb algorithm that would gradually improve the estimate of parameter and as I will explain later , there be some guarantee for reach a local maximum of the likelihood function .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-11-02 23:01:52.790388	135	00:05:35.21	00:05:52.95	So let's take a look at the computation for specific case. So these formulas are the EM formulas that you see before, and you can also see there are superscripts here N to indicate the generation of parameters.	so let 's take a look at the computation for specific case . so these formula be the EM formula that you see before , and you can also see there be superscript here N to indicate the generation of parameter .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-12-05 02:43:02.006924	140	00:05:53.61	00:06:03.16	I go here. For example, we have N + 1. That means we have improved parameters from here to. here we have improvement.	I go here . for example , we have N + 1 . that mean we have improve parameter from here to . here we have improvement .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-11-02 23:01:52.790393	145	00:06:04.12	00:06:13.46	So in this setting we have assumed that the two models have equal probabilities and the background model is known. So what are the relevant statistics? Well, these are the word counts.	so in this setting we have assume that the two model have equal probability and the background model be know . so what be the relevant statistic ? well , these be the word count .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-11-02 23:01:52.790395	149	00:06:14.03	00:06:23.61	So assume we have just 4 words and their counts are like this and this is our background model that assigns high probabilities to common words like the.	so assume we have just 4 word and their count be like this and this be our background model that assign high probability to common word like the .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-11-02 23:01:52.790399	157	00:06:25.81	00:06:38.92	An in the first iteration you can picture what would happen. Well, we first we initialize all the values. So here this probability that we're interested in is normalized into an uniform distribution over all the words.	an in the first iteration you can picture what would happen . well , we first we initialize all the value . so here this probability that we be interested in be normalize into an uniform distribution over all the word .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-11-02 23:01:52.7904	159	00:06:39.58	00:06:42.14	And then the E step would give us a guess.	and then the e step would give we a guess .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-11-02 23:01:52.790407	174	00:06:43.32	00:07:14.39	Of the distribution that has been used to generate each word, we can see we have different probabilities for different words. Why that's be cause these words have different probabilities in the background. So even though the two distributions are equally likely, and then our initialization says uniform distribution because of the difference in the background world distribution, we have different guest probabilities. So these words are believed to be more likely from the topic.	of the distribution that have be use to generate each word , we can see we have different probability for different word . why that be be cause these word have different probability in the background . so even though the two distribution be equally likely , and then our initialization say uniform distribution because of the difference in the background world distribution , we have different guest probability . so these word be believe to be more likely from the topic .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-11-02 23:01:52.790408	176	00:07:15.42	00:07:18.93	These, on the other hand, are less likely probably from background.	these , on the other hand , be less likely probably from background .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-11-02 23:01:52.790411	182	00:07:20.14	00:07:38.02	So once we have the Z values, we know in the E step these probabilities would be used to adjust the counts. So 4 must be multiplied by this point three three in order to get the allocated counts toward the topic.	so once we have the z value , we know in the e step these probability would be use to adjust the count . so 4 must be multiply by this point three three in order to get the allocate count toward the topic .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-11-02 23:01:52.790412	184	00:07:39.43	00:07:42.94	And this is done by this multiplication.	and this be do by this multiplication .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-12-05 03:15:02.580491	186	00:07:43.64	00:07:49.949999	Note that if our guess says this is 100%. If this is 1.0	note that if our guess say this be 100 % . if this be 1.0
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-11-02 23:01:52.790415	193	00:07:52.22	00:08:08.79	Then we just get the full Council of this word for this topic. But in general, as I said, it's not going to be 1.0, so we're going to just get some percentage of the counts toward this topic, and then we simply normalize these counts.	then we just get the full Council of this word for this topic . but in general , as I say , it be not go to be 1.0 , so we be go to just get some percentage of the count toward this topic , and then we simply normalize these count .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-11-02 23:01:52.790416	196	00:08:09.44	00:08:16.77	To have a new generation of practice mate so you can see, compare this with the old one which is here.	to have a new generation of practice mate so you can see , compare this with the old one which be here .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-11-02 23:01:52.790419	203	00:08:17.35	00:08:30.62	So compare this with this one and will see at the probability is different. Not only that, we also see some words that are believed to have come from the topic. We have high probability like this one text.	so compare this with this one and will see at the probability be different . not only that , we also see some word that be believe to have come from the topic . we have high probability like this one text .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-11-02 23:01:52.790422	210	00:08:32.41	00:08:49.76	And of course, this new generation of parameters would allow us to further adjust the infer the latent variable or hidden variable values. So we have a new generation of values because of the E step based on the new generation of parameters.	and of course , this new generation of parameter would allow we to far adjust the infer the latent variable or hide variable value . so we have a new generation of value because of the e step base on the new generation of parameter .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-11-02 23:01:52.790424	214	00:08:51.04	00:09:02.41	And this these new in further values of these will give us then another generation of the estimate of probabilities of the words.	and this these new in further value of these will give we then another generation of the estimate of probability of the word .
4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	2020-11-02 23:01:52.790436	246	00:09:03.26	00:10:14.73	And so on so forth. So this is what would actually happen when we compute these probabilities using the EM algorithm. And as you can see in the last rule where we showed the log like code and the likelihood is increasing as we do the iteration. And note that these log likelihood is negative becausw the probability is between zero and one when you take logarithm, it becomes a negative value. What's also interesting is do not last column, and these are the inferred word split, and these are the probabilities that a word is believed to have come from one distribution. In this case the topic distribution, and you might wonder whether this would be also useful because our main goal is to estimate these word distribution right? So this is our primary goal. We hope to have a more discriminating world distribution. But the last column is also by product and this actually can also be very useful and you can think about that. And one use is to. For example, is made to what extent this document has covered background words.	and so on so forth . so this be what would actually happen when we compute these probability use the EM algorithm . and as you can see in the last rule where we show the log like code and the likelihood be increase as we do the iteration . and note that these log likelihood be negative becausw the probability be between zero and one when you take logarithm , it become a negative value . what be also interesting be do not last column , and these be the infer word split , and these be the probability that a word be believe to have come from one distribution . in this case the topic distribution , and you might wonder whether this would be also useful because our main goal be to estimate these word distribution right ? so this be our primary goal . we hope to have a more discriminating world distribution . but the last column be also by product and this actually can also be very useful and you can think about that . and one use be to . for example , be make to what extent this document have cover background word .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377422	3	00:00:00.3	00:00:05.38	This lecture is about the discriminative classifiers for text categorization.	this lecture be about the discriminative classifier for text categorization .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377436	23	00:00:12.85	00:00:55.85	In this lecture, we're going to continue talking about how to do text categorization and cover discriminative approaches. This is a slide that you have seen from the discussion of Naive Bayes classifier, where we have shown that although naive Bayes classifier tries to model the generation of text data from each categories, we can actually use bayes rule and to eventually rewrite the scoring function as you see on this slide and this scoring function is basically a weighted combination of a lot of word features where the feature values are word count and the feature weights are the log of probability ratios of the word given by two distributions here.	in this lecture , we be go to continue talk about how to do text categorization and cover discriminative approach . this be a slide that you have see from the discussion of Naive Bayes classifier , where we have show that although naive Bayes classifier try to model the generation of text datum from each category , we can actually use baye rule and to eventually rewrite the scoring function as you see on this slide and this scoring function be basically a weight combination of a lot of word feature where the feature value be word count and the feature weight be the log of probability ratio of the word give by two distribution here .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377441	30	00:00:57.14	00:01:15.43	Now this kind of scoring function can be actually a general scoring function where we can in general represent text data as a feature vector. Of course the features don't have to be all the words and their features can be other signals that we want to use.	now this kind of scoring function can be actually a general scoring function where we can in general represent text datum as a feature vector . of course the feature do n't have to be all the word and their feature can be other signal that we want to use .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-12-07 19:09:54.814283	44	00:01:16.13	00:01:51.92	And we mentioned that this is precisely similar to logistic regression. So in this lecture we're going to introduce some discriminative classifiers. They try to model the conditional distribution of labels given the data directly rather than using Bayes rule to compute that indirectly. As we have seen in naive bayes. So the general idea of logistical regression is to model the dependency of the binary response variable Y here,	and we mention that this be precisely similar to logistic regression . so in this lecture we be go to introduce some discriminative classifier . they try to model the conditional distribution of label give the datum directly rather than use Bayes rule to compute that indirectly . as we have see in naive baye . so the general idea of logistical regression be to model the dependency of the binary response variable Y here ,
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.37745	45	00:01:52.27	00:01:53.68	On some predictors.	on some predictor .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-12-07 19:10:04.5975	48	00:01:54.24	00:02:02.8	That are denoted as X. So here we have also changed the notation to X	that be denote as X. So here we have also change the notation to X
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-12-07 19:10:17.488282	51	00:02:04.71	00:02:12.87	For feature values you may recall in the previous slide we have used Fi to represent the feature values.	for feature value you may recall in the previous slide we have use Fi to represent the feature value .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377456	56	00:02:13.75	00:02:30.44	An here we use the notation of X vector, which is more common when we. Introduce such machine learning algorithms, so X is our input, it's a vector.	an here we use the notation of X vector , which be more common when we . introduce such machine learn algorithm , so X be our input , it be a vector .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377456	57	00:02:31.96	00:02:34.5	And with M features.	and with M feature .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-12-08 06:25:03.726965	67	00:02:35.05	00:02:59.24	And each feature has a value exhibi here and our goal is model the dependency of this binary response variable on all these features. So in our categorization problem we have two categories, lets say theta 1 and theta 2, and we can use the Y value to denote the two categories. And when Y is1 it means the category of the documents first class theta 1	and each feature have a value exhibi here and our goal be model the dependency of this binary response variable on all these feature . so in our categorization problem we have two category , let say theta 1 and theta 2 , and we can use the Y value to denote the two category . and when Y is1 it mean the category of the document first class theta 1
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377465	72	00:03:00.85	00:03:12.55	Now the goal here is to model the conditional probability of Y given X directly as opposed to model the generation of X&Y as in the case of Naive Bayes.	now the goal here be to model the conditional probability of Y give x directly as oppose to model the generation of X&Y as in the case of Naive Bayes .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377471	81	00:03:13.33	00:03:29.83	And another advantage of this kind of approach is that it would allow many other features than words to be used in this vector. Since we're not modeling the generation of this vector and we can plug in any signals that we want, so this is potentially advantages for doing text categorization.	and another advantage of this kind of approach be that it would allow many other feature than word to be use in this vector . since we be not model the generation of this vector and we can plug in any signal that we want , so this be potentially advantage for do text categorization .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377477	90	00:03:31.27	00:03:56.34	So most specifically, in logistic regression the assumed functional form of y depending on X is the following, and this is very closed, closely related to the log or log odds that I introduced in the naive bayes or log of probability ratio of the two categories that you have seen on the previous slide.	so most specifically , in logistic regression the assume functional form of y depend on X be the following , and this be very closed , closely related to the log or log odd that I introduce in the naive baye or log of probability ratio of the two category that you have see on the previous slide .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377481	96	00:03:57.11	00:04:10.78	So that this is what I meant, right? So in the case of Naive Bayes, we compute this by using bayes rule and eventually we have reached a formula that look like this. That looks like this.	so that this be what I mean , right ? so in the case of Naive Bayes , we compute this by use baye rule and eventually we have reach a formula that look like this . that look like this .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-12-07 19:14:13.181244	98	00:04:12.87	00:04:22.35	But here we actually would assume explicitly that we would model our	but here we actually would assume explicitly that we would model our
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377482	99	00:04:24.04	00:04:27.4	Probability of Y given X.	probability of Y give X.
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377483	101	00:04:28.42	00:04:36.48	As directly as a function of these features.	as directly as a function of these feature .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377486	105	00:04:37.37	00:04:51.51	So most specifically, we assume that log of the ratio of probability of y = 1 and the probability of y = 0. Is a function of X.	so most specifically , we assume that log of the ratio of probability of y = 1 and the probability of y = 0 . be a function of X.
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-12-08 06:27:04.459777	108	00:04:54.35	00:05:00.94	And so it's a function of X, and it's a linear combination of these feature values, controlled by beta values.	and so it be a function of x , and it be a linear combination of these feature value , control by beta value .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.37749	112	00:05:02.24	00:05:14.4	And since we know that probability of y = 0 is 1 minus probability of y = 1, and this can be also written in this way.	and since we know that probability of y = 0 be 1 minus probability of y = 1 , and this can be also write in this way .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377492	114	00:05:15.89	00:05:20.29	So this is a log odds ratio. Here.	so this be a log odd ratio . here .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377495	119	00:05:21.89	00:05:33.89	And so in logistic regression, we basically assume that the probability of y = 1 given X is dependent on this linear combination of all these features.	and so in logistic regression , we basically assume that the probability of y = 1   give x be dependent on this linear combination of all these feature .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377498	124	00:05:34.45	00:05:45.84	So it's just one of the many possible ways of assuming that the dependency, But this particular form has been quite useful, and it has also has some nice properties.	so it be just one of the many possible way of assume that the dependency , but this particular form have be quite useful , and it have also have some nice property .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377502	131	00:05:46.6	00:06:04.59	So if we rewrite this question to actually express the probability of Y given X in terms of X by taking by getting rid of the logarithm and we get this functional form and this is called a logistical function, it's a transformation of X into Y.	so if we rewrite this question to actually express the probability of Y give x in term of x by take by getting rid of the logarithm and we get this functional form and this be call a logistical function , it be a transformation of x into Y.
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377503	132	00:06:06.33	00:06:07.22	As you see.	as you see .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-12-08 06:28:15.108847	133	00:06:08.71	00:06:09.78	on the right side here.	on the right side here .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377505	135	00:06:10.42	00:06:17.51	So that the Xs will be mapped into a range of values from zero to 1.0.	so that the Xs will be map into a range of value from zero to 1.0 .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377507	138	00:06:18.42	00:06:23.36	You can see, and that's precisely what we want. Since we have a probability here.	you can see , and that be precisely what we want . since we have a probability here .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377507	139	00:06:24.21	00:06:26.73	And the function form looks like this.	and the function form look like this .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-12-08 06:28:45.850338	144	00:06:27.82	00:06:39.08	So this is the basic idea of logistic regression, and it's a very useful classifier that can be used to do a lot of classification tasks, including text categorization.	so this be the basic idea of logistic regression , and it be a very useful classifier that can be use to do a lot of classification task , include text categorization .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377514	150	00:06:41.62	00:06:52.57	So as in all cases of model, we would be interested in estimating the parameters and in fact in all the machine learning programs. Once you set up the model set of objective function.	so as in all case of model , we would be interested in estimate the parameter and in fact in all the machine learn program . once you set up the model set of objective function .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377521	163	00:06:53.14	00:07:22.84	To model the classifier, then the next step is to compute the parameter values. In general, we're going to adjust these parameter values, optimize the performance of classifier on the training data. So in our case, let's assume we have training data. The training data here, X i and Y i and each pair is basically feature vector of X and a known label for that X Y, either one or zero.	to model the classifier , then the next step be to compute the parameter value . in general , we be go to adjust these parameter value , optimize the performance of classifier on the training datum . so in our case , let 's assume we have training datum . the training datum here , x I and Y I and each pair be basically feature vector of X and a known label for that X Y , either one or zero .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377523	165	00:07:23.79	00:07:29.97	So in our case we are interested in maximizing this conditional likelihood.	so in our case we be interested in maximize this conditional likelihood .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377524	168	00:07:30.69	00:07:39.67	The condition likelihood here is basically to model y given the observed X.	the condition likelihood here be basically to model y give the observe x.
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377525	169	00:07:41.14	00:07:43.35	So it's not like a.	so it be not like a.
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377526	171	00:07:44.21	00:07:48.53	It's not like a modeling X, but rather we're going to model this.	it be not like a modeling x , but rather we be go to model this .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377527	173	00:07:50.94	00:07:55.19	Note that this is a conditional probability of Y given X.	note that this be a conditional probability of Y give X.
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.37753	178	00:07:56.4	00:08:05.01	And this is also precisely what we want for classification. Now, so the likelihood function would be just a product over all the training cases.	and this be also precisely what we want for classification . now , so the likelihood function would be just a product over all the training case .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377537	188	00:08:06.62	00:08:32.14	And in each case, this is the modeled probability of observing this particular training case. So given a particular XI, how likely will we are going to observe the corresponding why? of course, why I could be one or zero and in fact the function form here would vary depending on whether Y sub I is one or zero. If it's one will be taking this form.	and in each case , this be the modeled probability of observe this particular training case . so give a particular xi , how likely will we be go to observe the corresponding why ? of course , why I could be one or zero and in fact the function form here would vary depend on whether Y sub I be one or zero . if it be one will be take this form .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.37754	193	00:08:33.56	00:08:45.39	And that's basically the logistical regression function. But what about this if it's 0? Well, if it's zero then we have to use a different form and that's this one.	and that be basically the logistical regression function . but what about this if it be 0 ? well , if it be zero then we have to use a different form and that be this one .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377542	196	00:08:48.48	00:08:54.97	Now how do we get this one? That's just one minus the probability of y = 1, right?	now how do we get this one ? that be just one minus the probability of y = 1 , right ?
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377545	201	00:08:55.85	00:09:08.359999	And you can easily see this now. The key point here is that the function form here depends on the observed. Y I if it's one, it has a different form than when it's 0.	and you can easily see this now . the key point here be that the function form here depend on the observed . Y I if it be one , it have a different form than when it be 0 .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.37755	208	00:09:09.22	00:09:25.35	And if you think about when we want to maximize this probability we will basically going to want this probability to be as high as possible when the label is one. That means the document is in topic one.	and if you think about when we want to maximize this probability we will basically go to want this probability to be as high as possible when the label be one . that mean the document be in topic one .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-12-07 19:21:03.391819	214	00:09:26.68	00:09:42.49	But if the document is not we are going to maximize this value, and what's going to happen is actually to make this value as small as possible. Because they sum to one. When I maximize this one.	but if the document be not we be go to maximize this value , and what be go to happen be actually to make this value as small as possible . because they sum to one . when I maximize this one .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377554	215	00:09:43.13	00:09:45.75	It's equivalent to minimize this one.	it be equivalent to minimize this one .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377557	220	00:09:47.93	00:09:58.45	So you can see basically the if we maximize the conditional likelihood we're going to basically try to make the prediction on the training data as accurate as possible.	so you can see basically the if we maximize the conditional likelihood we be go to basically try to make the prediction on the training datum as accurate as possible .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.37756	225	00:10:01.11	00:10:11.2	So, as in other cases, when compute the maximum likelihood estimator Basically lets go find a beta value, a set of beta values that will maximize this conditional likelihood.	so , as in other case , when compute the maximum likelihood estimator basically lets go find a beta value , a set of beta value that will maximize this conditional likelihood .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377569	239	00:10:12.04	00:10:41.51	And this again then gives us a standard optimization problem. In this case, it can be also solved in many ways. Newtons method is a popular way to solve this problem. There are other methods as well, but in the end will we're going to get the set of beta values once we have the beta values, then we have a well defined the scoring function to help us classify a document right? So what's the function? Well, it's this one.	and this again then give we a standard optimization problem . in this case , it can be also solve in many way . newton method be a popular way to solve this problem . there be other method as well , but in the end will we be go to get the set of beta value once we have the beta value , then we have a well define the scoring function to help we classify a document right ? so what be the function ? well , it be this one .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-12-07 19:22:59.240948	243	00:10:42.65	00:10:50.51	If we have all the betavalues already known, all we need is to compute The Xi's for that document.	if we have all the betavalue already know , all we need be to compute the Xi 's for that document .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377573	247	00:10:51.1	00:10:58.16	And then plugging those values that will give us a estimate. The probability that the document is in category one.	and then plug those value that will give we a estimate . the probability that the document be in category one .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377577	253	00:10:59.05	00:11:11.98	OK, so much for logistical regression. Let's also introduce another discriminative classifier called K nearest neighbors. Now in general, I should say there are many such approaches.	ok , so much for logistical regression . let 's also introduce another discriminative classifier call K near neighbor . now in general , I should say there be many such approach .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377595	282	00:11:12.53	00:12:11.73	And thorough introduction to all of them is clearly beyond the scope of this course and you should take a machine learning course or read more about machine learning to know about them. Here, just want to include the basic introduction to some of the most commonly used classifiers, since you might use them often for text categorization. So the second classifier, is called k nearest neighbors. In this approach, we're going to also estimate the conditional probability of label. Given data, but in a very different way. So the idea is to keep all the training examples and then once we see a text object that we want to classify, we're going to find the K examples in the training set and that are most similar to this text object. Basically this is to find the neighbors of this text object in the training data set. So once we found we found the neighborhood and found the objects that are close to the.	and thorough introduction to all of they be clearly beyond the scope of this course and you should take a machine learn course or read more about machine learn to know about they . here , just want to include the basic introduction to some of the most commonly use classifier , since you might use they often for text categorization . so the second classifier , be call k near neighbor . in this approach , we be go to also estimate the conditional probability of label . give datum , but in a very different way . so the idea be to keep all the training example and then once we see a text object that we want to classify , we be go to find the k example in the training set and that be most similar to this text object . basically this be to find the neighbor of this text object in the training datum set . so once we find we find the neighborhood and find the object that be close to the .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377599	290	00:12:12.45	00:12:25.45	The object we're interested in classifying and say we have found the K nearest neighbors. That's why this method is called K nearest neighbors. And then we're going to assign the category that's most common in these neighbors.	the object we be interested in classify and say we have find the k near neighbor . that be why this method be call K near neighbor . and then we be go to assign the category that be most common in these neighbor .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377601	294	00:12:26.13	00:12:32.16	Basically, we're going to allow these neighbors to vote for the category of the object that we're interested in classifying.	basically , we be go to allow these neighbor to vote for the category of the object that we be interested in classify .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377604	298	00:12:33.36	00:12:41.97	Now that means if most of them have a particular category, lets say category 1 then we're going to say this current object will have category one.	now that mean if most of they have a particular category , lets say category 1 then we be go to say this current object will have category one .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.37761	308	00:12:42.97	00:13:04.7	This approach, can also be improved by considering the distance of a neighbor and the current object. Basically, we can assume a close neighbor will have more saying about the category of this object, so we can have we can give such a neighbor more influence on the vote and we can take weighted sum of their votes based on the distances.	this approach , can also be improve by consider the distance of a neighbor and the current object . basically , we can assume a close neighbor will have more saying about the category of this object , so we can have we can give such a neighbor more influence on the vote and we can take weighted sum of their vote base on the distance .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377612	312	00:13:06.01	00:13:12.52	But the general idea is to look at the neighborhood and then try to assess the category based on the categories of the neighbors.	but the general idea be to look at the neighborhood and then try to assess the category base on the category of the neighbor .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377616	317	00:13:13.11	00:13:26.95	Intuitively, this makes a lot of sense. But mathematically, this can also be regarded as a way to directly estimate the conditional probability of label given data that is P of Y given X.	intuitively , this make a lot of sense . but mathematically , this can also be regard as a way to directly estimate the conditional probability of label give datum that be p of Y give X.
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377619	322	00:13:28.04	00:13:39.65	Now I'm going to explain this intuition in the moment, but before we proceed, let me emphasize that we do need a similarity function here in order for this work.	now I be go to explain this intuition in the moment , but before we proceed , let I emphasize that we do need a similarity function here in order for this work .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.37762	324	00:13:40.2	00:13:43.99	I note that in naive base classifier we did not need a similarity function.	I note that in naive base classifier we do not need a similarity function .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377623	329	00:13:44.65	00:13:51.7	An in logistical regression, we did not talk about the similarity function either. But here we explicitly requires a similarity function.	an in logistical regression , we do not talk about the similarity function either . but here we explicitly require a similarity function .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377633	345	00:13:52.38	00:14:31.67	Now this similarity function. Actually is a good opportunity for us to inject any of our insights about features. Basically, effective features are those that would make the objects that are in the same category look more similar, but distinguishing objects in different categories. So the design of this similarity function is closely tied to the design of the features in logistic regression. and other classifiers, so let's illustrate how K-NN works. Suppose we have a lot of training instances here.	now this similarity function . actually be a good opportunity for we to inject any of our insight about feature . basically , effective feature be those that would make the object that be in the same category look more similar , but distinguish object in different category . so the design of this similarity function be closely tie to the design of the feature in logistic regression . and other classifier , so let 's illustrate how K - NN work . suppose we have a lot of training instance here .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377636	349	00:14:32.23	00:14:42.99	And I've colored them differently and to show just different categories. Now suppose we have a new object in the center that we want to classify.	and I 've color they differently and to show just different category . now suppose we have a new object in the center that we want to classify .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377641	354	00:14:43.55	00:14:51.66	So according to this approach we're going to find neighbors. And let's first think of a special case of finding just one neighbor, the closest neighbor.	so accord to this approach we be go to find neighbor . and let 's first think of a special case of find just one neighbor , the close neighbor .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-12-07 19:28:41.145865	363	00:14:52.95	00:15:15.14	Now in this case, if the, let's assume the closest neighbor is the box filled with diamonds and so then we're going to say. Well, since this is in, this object is in category of diamonds. Let's say then we're going to say, well, we're going to assign the same Category to our text object.	now in this case , if the , let 's assume the close neighbor be the box fill with diamond and so then we be go to say . well , since this be in , this object be in category of diamond . let 's say then we be go to say , well , we be go to assign the same Category to our text object .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.37765	371	00:15:17.13	00:15:31.35	But let's also look at the another possibility of finding a larger neighborhood. So let's think about the four neighbors. In this case, we're going to include a lot of other solid field boxes in red or pink.	but let 's also look at the another possibility of find a large neighborhood . so let 's think about the four neighbor . in this case , we be go to include a lot of other solid field box in red or pink .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377658	385	00:15:32.1	00:16:02.29	So in this case now we are going to notice that among the four neighbors there are actually three neighbors in a different category. So if we take a vote, then we'll conclude the object is actually of a different category. So this both illustrates how K nearest neighbor works and also illustrates some potential problems of this classifier. Basically the results might depend on the K and indeed K is an important parameter to optimize.	so in this case now we   be go to notice that among the four neighbor there be actually three neighbor in a different category . so if we take a vote , then we 'll conclude the object be actually of a different category . so this both illustrate how K near neighbor work and also illustrate some potential problem of this classifier . basically the result might depend on the k and indeed K be an important parameter to optimize .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-12-07 19:30:43.505066	414	00:16:03.88	00:17:05.95	Now you can intuitively imagine if we have a lot of neighbors around this object and then we'll be OK because we have a lot of neighbors for help us decide categories. But if we have only a few, then the decision may not be reliable. So on the one hand we want to find more neighbors right? And then we have more votes, but on the other hand as we try to find the more neighbors, we actually could risk on getting neighbors that are not really similar to this instance, they might be actually far away as you try to get more neighbors, so although you get more neighbors to vote, but those neighbors aren't necessary so helpful because they are not very similar to the object. So the parameter as there has to be set empirically and typically you can optimize such a parameter by using cross validation. Basically, you're going to separate your training data into two parts and then you're going to use one part to actually help you choose. The	now you can intuitively imagine if we have a lot of neighbor around this object and then we 'll be ok because we have a lot of neighbor for help we decide category . but if we have only a few , then the decision may not be reliable . so on the one hand we want to find more neighbor right ? and then we have more vote , but on the other hand as we try to find the more neighbor , we actually could risk on get neighbor that be not really similar to this instance , they might be actually far away as you try to get more neighbor , so although you get more neighbor to vote , but those neighbor be n't necessary so helpful because they be not very similar to the object . so the parameter as there have to be set empirically and typically you can optimize such a parameter by use cross validation . basically , you be go to separate your training datum into two part and then you be go to use one part to actually help you choose . the
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377675	420	00:17:06.74	00:17:20.86	The parameter K here or some other parameters in other classifiers, and then you're going to assume this number that works well on your training set would be actually the best for your future data.	the parameter k here or some other parameter in other classifier , and then you be go to assume this number that work well on your training set would be actually the good for your future datum .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377678	425	00:17:23.16	00:17:33.75	So as I mentioned that KNN can be actually regarded as estimate of conditional probability of Y given X, and that's why we put this in the category of discriminative approaches.	so as I mention that KNN can be actually regard as estimate of conditional probability of Y give x , and that be why we put this in the category of discriminative approach .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-12-07 19:31:42.077145	436	00:17:34.46	00:18:00.76	So the key assumption that we made in this approach is that the distribution of the label given the document or probability of a category given document. For example, probability of theta I given document D is locally smoothed and that just means we're going to assume that this probability is the same for all the documents in this region. R here.	so the key assumption that we make in this approach be that the distribution of the label give the document or probability of a category give document . for example , probability of theta I give document d be locally smoothed and that just mean we be go to assume that this probability be the same for all the document in this region . r   here .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377689	448	00:18:01.44	00:18:26.59	And suppose we draw a neighborhood and we're going to assume in this neighborhood, since the data instances are very similar, we're going to assume that the conditional distribution of the label, given the data, would be roughly the same. If D is not different, very different than we're going to assume that the probability of theta given D would be also similar, and so that's a very key assumption, and that that's.	and suppose we draw a neighborhood and we be go to assume in this neighborhood , since the data instance  be very similar , we be go to assume that the conditional distribution of the label , give the datum , would be roughly the same . if d be not different , very different than we be go to assume that the probability of theta give d would be also similar , and so that be a very key assumption , and that that be .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377695	464	00:18:27.88	00:18:56.94	That's actually important assumption that would allow us to do a lot of machine learning, but in reality, whether this is true of course would depend on how we define similarity, because the neighborhood is largely determined by our similarity function. If our similarity function captures objects that do follow similar distributions, then this assumption is OK. But if our similarity function could not capture that. Obviously the assumption would be a problem, and then the classifier would not be accurate.	that be actually important assumption that would allow we to do a lot of machine learning , but in reality , whether this be true of course would depend on how we define similarity , because the neighborhood be largely determine by our similarity function . if our similarity function capture object that do follow similar distribution , then this assumption be ok . but if our similarity function could not capture that . obviously the assumption would be a problem , and then the classifier would not be accurate .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.377699	473	00:18:59.27	00:19:20.41	Let's proceed with this assumption. Then what we are saying is that in order to estimate the probability of a category given a document, we can try to estimate the probability of the category given that entire region. Now this has the benefit of course, of bringing additional data points to help us estimate this probability.	let 's proceed with this assumption . then what we be say be that in order to estimate the probability of a category give a document , we can try to estimate the probability of the category give that entire region . now this have the benefit of course , of bring additional datum point to help we estimate this probability .
4da6283d-6903-4be9-8bfc-ad5d330343c6	2020-11-02 23:08:52.3777	477	00:19:21.13	00:19:31.7	And so this is precise idea of KNN. Basically now we can use the known categories of all the documents in this region to estimate this probability.	and so this be precise idea of KNN . basically now we can use the know category of all the document in this region to estimate this probability .
5190e288-54f7-4021-9083-8e8ceac11345	2020-11-02 23:02:33.660018	2	00:00:00.3	00:00:04.29	This lecture is about the latent Dirichlet allocation or LDA.	this lecture be about the latent Dirichlet allocation or LDA .
5190e288-54f7-4021-9083-8e8ceac11345	2020-11-02 23:02:33.660022	9	00:00:13.7	00:00:27.91	In this lecture, we're going to continue talking about topic models. In particular, we are going to talk about some extension of PLSA, and one of them is LDA or latent Dirichlet allocation.	in this lecture , we be go to continue talk about topic model . in particular , we be go to talk about some extension of PLSA , and one of they be LDA or latent Dirichlet allocation .
5190e288-54f7-4021-9083-8e8ceac11345	2020-12-14 08:19:54.132781	20	00:00:30.67	00:01:00.4	So the plan for this lecture is to cover two things. One is to extend the PLSA with prior knowledge that would allow us to have in some sense a user controlled PLSA, so it doesn't blindly just listen to data but also would listen to our needs. The second is to extend the PLSA as a generative model fully generated model. This has led to the development of latent dirichlet allocation or LDA.	so the plan for this lecture be to cover two thing . one be to extend the PLSA with prior knowledge that would allow we to have in some sense a user control PLSA , so it do n't blindly just listen to datum but also would listen to our need . the second be to extend the PLSA as a generative model fully generate model . this have lead to the development of latent dirichlet allocation or LDA .
5190e288-54f7-4021-9083-8e8ceac11345	2020-11-02 23:02:33.660029	26	00:01:01.12	00:01:13.89	So first let's talk about the PLSA with prior knowledge. In practice, when we apply PLSA to analyze text data, we might have additional knowledge that we want to inject to guide the analysis.	so first let 's talk about the PLSA with prior knowledge . in practice , when we apply PLSA to analyze text datum , we might have additional knowledge that we want to inject to guide the analysis .
5190e288-54f7-4021-9083-8e8ceac11345	2020-11-02 23:02:33.660035	43	00:01:14.62	00:01:49.36	The standard PLSA is going to blindly listen to the data by using maximum likelihood estimator. We are going to just fit data as much as we can and get some insight about data. This is also very useful, but sometimes a user might have some expectations about which topics to analyze. For example, we might expect to see retrieval models as a topic in information retrieval. We also may be interested in certain aspects such as battery and memory when looking at the opinions about the laptop, because the user is particularly interested in these aspects.	the standard PLSA be go to blindly listen to the datum by use maximum likelihood estimator . we be go to just fit datum as much as we can and get some insight about datum . this be also very useful , but sometimes a user might have some expectation about which topic to analyze . for example , we might expect to see retrieval model as a topic in information retrieval . we also may be interested in certain aspect such as battery and memory when look at the opinion about the laptop , because the user be particularly interested in these aspect .
5190e288-54f7-4021-9083-8e8ceac11345	2020-11-02 23:02:33.660037	45	00:01:50.84	00:01:53.78	Now, a user may also have knowledge about the topic coverage.	now , a user may also have knowledge about the topic coverage .
5190e288-54f7-4021-9083-8e8ceac11345	2020-11-02 23:02:33.660048	70	00:01:54.51	00:02:43.77	And we may know which topic is definitely not covered in which document or is covered in the document. For example, we might have seen those tags topic tags assigned to documents. And those tag could be treated as topics if we do that, then a document that can only be generated using topics corresponding to the tags already assigned to the document. If the document is not assigned to a tag, we're going to say there's no way for using that topic to generate the document. The document must be generated by using the topics corresponding to the assigned tags. So the question is, how can we incorporate such knowledge into PLSA? It turns out that there's a. A very elegant way of doing that, and that's all incorporated such knowledge as priors on the models.	and we may know which topic be definitely not cover in which document or be cover in the document . for example , we might have see those tag topic tag assign to document . and those tag could be treat as topic if we do that , then a document that can only be generate use topic correspond to the tag already assign to the document . if the document be not assign to a tag , we be go to say there be no way for use that topic to generate the document . the document must be generate by use the topic correspond to the assign tag . so the question be , how can we incorporate such knowledge into PLSA ? it turn out that there be a. a very elegant way of do that , and that be all incorporate such knowledge as prior on the model .
5190e288-54f7-4021-9083-8e8ceac11345	2020-11-02 23:02:33.660049	74	00:02:44.39	00:02:53.3	And you may recall in Bayesian inference we use prior together with data to estimate parameters, and this is precisely what will happen.	and you may recall in Bayesian inference we use prior together with datum to estimate parameter , and this be precisely what will happen .
5190e288-54f7-4021-9083-8e8ceac11345	2020-11-02 23:02:33.660057	93	00:02:54.16	00:03:35.73	So in this case we can use maximum a posteriori estimate, also called map estimate, and the formula is given here. Basically is to maximize the posterior distribution probability and this is a combination of the likelihood of data and the prior. So what would happen is that we're going to have an estimate that listens to the data and also listens to our prior preferences. We can use this prior, which is denoted as P of Lambda to encode. All kinds of preferences and constraints. So for example, we can use this to encode the need of having precisely 1 background the topic.	so in this case we can use maximum a posteriori estimate , also call map estimate , and the formula be give here . basically be to maximize the posterior distribution probability and this be a combination of the likelihood of datum and the prior . so what would happen be that we be go to have an estimate that listen to the datum and also listen to our prior preference . we can use this prior , which be denote as p of Lambda to encode . all kind of preference and constraint . so for example , we can use this to encode the need of have precisely 1 background the topic .
5190e288-54f7-4021-9083-8e8ceac11345	2020-11-02 23:02:33.660062	105	00:03:36.34	00:04:05.79	Now this can be encoded as a prior because we can say the prior for the parameters is only a non zero if the plan does contain one topic that's equivalent to the background language model. In other words, in other cases if it's not like that, we're going to say supplier says it's impossible. So the probability of that kind of model setting would be 0 according to our prior.	now this can be encode as a prior because we can say the prior for the parameter be only a non zero if the plan do contain one topic that be equivalent to the background language model . in other word , in other case if it be not like that , we be go to say supplier say it be impossible . so the probability of that kind of model setting would be 0 accord to our prior .
5190e288-54f7-4021-9083-8e8ceac11345	2020-11-02 23:02:33.660065	112	00:04:07.92	00:04:26.949999	So now we can also, for example use the prior to force particular choice of topic to have a probability of a certain number. For example, we can force the document D to choose topic one with probability of 1/2.	so now we can also , for example use the prior to force particular choice of topic to have a probability of a certain number . for example , we can force the document d to choose topic one with probability of 1/2 .
5190e288-54f7-4021-9083-8e8ceac11345	2020-11-02 23:02:33.660067	118	00:04:27.5	00:04:40.67	Or we can prevent a topic from being used in generated document. So we can say the third topic should not be user generated. Document D will set to the Pi value to 0 for that topic.	or we can prevent a topic from be use in generate document . so we can say the third topic should not be user generate . document D will set to the Pi value to 0 for that topic .
5190e288-54f7-4021-9083-8e8ceac11345	2020-11-02 23:02:33.66007	126	00:04:41.68	00:04:54.67	We can also use the prior to favor set of parameters with topics that assign high probabilities to some particular words. In this case, we're not going to say it's impossible, but we're going to just strongly favor certain kind of distributions.	we can also use the prior to favor set of parameter with topic that assign high probability to some particular word . in this case , we be not go to say it be impossible , but we be go to just strongly favor certain kind of distribution .
5190e288-54f7-4021-9083-8e8ceac11345	2020-11-02 23:02:33.660073	133	00:04:55.54	00:05:09.56	And you will see example later. The map can be computed using a similar EM algorithm as we have used for that maximum likelihood estimator with just some modification to smallest parameters reflect the prior preferences.	and you will see example later . the map can be compute use a similar em algorithm as we have use for that maximum likelihood estimator with just some modification to small parameter reflect the prior preference .
5190e288-54f7-4021-9083-8e8ceac11345	2020-11-02 23:02:33.660077	147	00:05:10.26	00:05:40.71	And in such a estimate, if we use a special form of the prior called conjugate prior, then the functional form of the prior will be similar to the data. As a result, we can combine the two and the consequences that you can basically convert the influence of the prior into the influence of having additional pseudo data because the two functional forms are the same and they can be combined. So the effect is as if we had more data.	and in such a estimate , if we use a special form of the prior call conjugate prior , then the functional form of the prior will be similar to the datum . as a result , we can combine the two and the consequence that you can basically convert the influence of the prior into the influence of have additional pseudo datum because the two functional form be the same and they can be combine . so the effect be as if we have more datum .
5190e288-54f7-4021-9083-8e8ceac11345	2020-12-11 04:17:21.420994	150	00:05:41.16	00:05:48.48	And this is convenient for computation. It doesn't mean conjugate prior is the best way to define the prior.	and this be convenient for computation . it do n't mean conjugate prior be the good way to define the prior .
5190e288-54f7-4021-9083-8e8ceac11345	2020-11-02 23:02:33.660081	159	00:05:50.03	00:06:08.659999	So now let's look at the specific example. Suppose the user is particularly interested in battery life of a laptop, and we're analyzing reviews. So the prior says that the distribution should contain one distribution that would assign high probabilities to battery, and life.	so now let 's look at the specific example . suppose the user be particularly interested in battery life of a laptop , and we be analyze review . so the prior say that the distribution should contain one distribution that would assign high probability to battery , and life .
5190e288-54f7-4021-9083-8e8ceac11345	2020-11-02 23:02:33.660099	209	00:06:09.47	00:08:06.49	So we could do say there's a distribution that's entirely concentrated on battery life and we all priors is that one of your distributions should be very similar to this. Now if we use map estimator with the conjugated prior, which is Dirichlet prior Dirichlet distribution based on this preference, then the only difference in the EM algorithm is in the M step. When we re estimate word distributions, we are going to add. Additional counts to reflect our prior right? So here you can see the pseudocounts are defined the based on the probability of words in our prior. So battery obviously will have a high pseudocounts similar life would have also high pseudocounts or the other words. We have 0 pseudocounts because their probability is zero in the prior and when you see this is also controlled by a parameter mu and We're going to add mu multiplied by the probability of W given our prior distribution to the connected counts. When we re estimate the when we re estimate the this world distribution right? So this is the only step that changed and the changes happened here and before we just collect the counts of words that we believe have been generated from this topic. But now we force this distribution. To give more probabilities to these words by adding them to the pseudocounts so to artificially in effect, we artificially inflated their probabilities and to make this distribution we also need to add this many pseudocounts to the denominator. This is the total sum of all the pseudocounts we have added for all the words. This would make this again a distribution.	so we could do say there be a distribution that be entirely concentrated on battery life and we all prior be that one of your distribution should be very similar to this . now if we use map estimator with the conjugate prior , which be Dirichlet prior dirichlet distribution base on this preference , then the only difference in the EM algorithm be in the M step . when we re estimate word distribution , we be go to add . additional count to reflect our prior right ? so here you can see the pseudocount be define the base on the probability of word in our prior . so battery obviously will have a high pseudocount similar life would have also high pseudocount or the other word . we have 0 pseudocount because their probability be zero in the prior and when you see this be also control by a parameter mu and we be go to add mu multiply by the probability of w give our prior distribution to the connect count . when we re estimate the when we re estimate the this world distribution right ? so this be the only step that change and the change happen here and before we just collect the count of word that we believe have be generate from this topic . but now we force this distribution . to give more probability to these word by add they to the pseudocount so to artificially in effect , we artificially inflate their probability and to make this distribution we also need to add this many pseudocount to the denominator . this be the total sum of all the pseudocount we have add for all the word . this would make this again a distribution .
5190e288-54f7-4021-9083-8e8ceac11345	2020-11-02 23:02:33.660101	214	00:08:07.25	00:08:16.94	Now, this is a intuitively very reasonable way of modifying the EM algorithm and theoretically speaking, this deal works, and it computes the map estimator.	now , this be a intuitively very reasonable way of modify the EM algorithm and theoretically speak , this deal work , and it compute the map estimator .
5190e288-54f7-4021-9083-8e8ceac11345	2020-11-02 23:02:33.660104	222	00:08:17.6	00:08:34.51	It's useful to think about two specific extreme cases of Mu. Now can you picture. Think about what would happen if we set Mu to Zero. Well, that's essentially to remove this prior, so mu in some sense indicates our strength on prior.	it be useful to think about two specific extreme case of Mu . now can you picture . think about what would happen if we set Mu to Zero . well , that be essentially to remove this prior , so mu in some sense indicate our strength on prior .
5190e288-54f7-4021-9083-8e8ceac11345	2020-11-02 23:02:33.660115	252	00:08:35.18	00:09:37.56	Now what would happen if we set Mu to positive Infinity? Well, that's to say this price is so strong that we're not going to listen to the data at all. So in the end you can see in this case we can do make one distribution fixed to the prior. You see why? When mu is Infinity, we basically let this one dominate. In fact, we are going to set this one. to precise this distribution, so in this case it is this distribution, and that's why we said the background language model is in fact a way to enforce a prior, because with false one distribution to be exactly the same as what we give, that's the background distribution. So in this case we can even force the distribution to entirely focused on battery life. But of course this won't work well 'cause it cannot attract other words, it would affect the accuracy of counting. Topics about the battery life so in practice mu is set somewhere in between, of course.	now what would happen if we set Mu to positive Infinity ? well , that be to say this price be so strong that we be not go to listen to the datum at all . so in the end you can see in this case we can do make one distribution fix to the prior . you see why ? when mu be Infinity , we basically let this one dominate . in fact , we be go to set this one . to precise this distribution , so in this case it be this distribution , and that be why we say the background language model be in fact a way to enforce a prior , because with false one distribution to be exactly the same as what we give , that be the background distribution . so in this case we can even force the distribution to entirely focus on battery life . but of course this wo n't work well 'cause it can not attract other word , it would affect the accuracy of counting . topic about the battery life so in practice mu be set somewhere in between , of course .
5190e288-54f7-4021-9083-8e8ceac11345	2020-11-08 19:42:05.504526	259	00:09:38.73	00:09:53.06	So this is one way to impose our prior. We can also impose some other constraints. For example, we can set any parameters for constraints, including zero as needed. For example, we may want to set one of the pis to 0.	so this be one way to impose our prior . we can also impose some other constraint . for example , we can set any parameter for constraint , include zero as need . for example , we may want to set one of the pis to 0 .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459459	3	00:00:00.29	00:00:05.4	This lecture is about the syntagmatic relation discovery and conditional entropy.	this lecture be about the syntagmatic relation discovery and conditional entropy .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459461	10	00:00:12.26	00:00:24.95	In this lecture, we're going to continue the discussion of word association mining an analysis. We're going to talk about the conditional entropy, which is useful for discovering syntagmatic relations.	in this lecture , we be go to continue the discussion of word association mine an analysis . we be go to talk about the conditional entropy , which be useful for discover syntagmatic relation .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459463	13	00:00:25.55	00:00:33.25	Earlier we talked about using entropy to capture how easy it is to predict the presence or absence of a word.	early we talk about use entropy to capture how easy it be to predict the presence or absence of a word .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459467	31	00:00:34.02	00:01:22.61	Now we address the different scenario where we assume that we know something about the text segment. So now the question is, suppose we know eats occured in the segment, how would that help us predict the presence or absence of a word? like meat, and in particular we want to know whether the presence of eats has helped us predict the presence of meat. And if we frame this using entropy, that would mean we are interested in knowing whether knowing the presence of eats could reduce uncertainty about the meat or reduce the entropy of the random variable corresponding to the presence or absence of meat.	now we address the different scenario where we assume that we know something about the text segment . so now the question be , suppose we know eat occur in the segment , how would that help we predict the presence or absence of a word ? like meat , and in particular we want to know whether the presence of eat have help we predict the presence of meat . and if we frame this use entropy , that would mean we be interested in know whether know the presence of eat could reduce uncertainty about the meat or reduce the entropy of the random variable correspond to the presence or absence of meat .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459468	35	00:01:23.27	00:01:33.25	We can also ask the question, what if we know of the absence of eats? Would that also help us predict the presence or absence of meat.	we can also ask the question , what if we know of the absence of eat ? would that also help we predict the presence or absence of meat .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459469	37	00:01:34.48	00:01:38.48	So these questions can be addressed by using.	so these question can be address by use .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459472	49	00:01:39.75	00:02:02.8	Another concept, called the conditional entropy. So to explain this concept, let's first look at the scenario we had before where we know nothing about the segment segment. So we have these probabilities indicating whether a word like meat occurs or does not occur in the segment, and we have the entropy function that looks like what you see on the slide.	another concept , call the conditional entropy . so to explain this concept , let 's first look at the scenario we have before where we know nothing about the segment segment . so we have these probability indicate whether a word like meat occur or do not occur in the segment , and we have the entropy function that look like what you see on the slide .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459473	52	00:02:03.71	00:02:11.41	I suppose we know eats is present, so now know the value of another random variable that denotes eats.	I suppose we know eat be present , so now know the value of another random variable that denote eat .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459503	58	00:02:12.54	00:02:25.79	Now that would change all these probabilities to conditional probabilities where we look at the presence or absence of meat. Given that we know eats occured in the context.	now that would change all these probability to conditional probability where we look at the presence or absence of meat . give that we know eat occur in the context .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459505	63	00:02:26.41	00:02:36.539999	So as a result, if we replace these probabilities with their corresponding conditional probabilities in the entropy function, we will get the conditional entropy.	so as a result , if we replace these probability with their correspond conditional probability in the entropy function , we will get the conditional entropy .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459505	64	00:02:37.49	00:02:41.79	So this equation now here.	so this equation now here .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459505	65	00:02:42.47	00:02:43.76	Would be.	would be .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459506	67	00:02:45.31	00:02:49.2	The conditional entropy conditioned on the presence of eats.	the conditional entropy condition on the presence of eat .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-12-10 07:07:37.977427	68	00:02:50.19	00:02:50.63	Right.	right .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459507	72	00:02:52.01	00:03:02.02	So you can see this is essentially the same entropy function as you have seen before, except that we all the probabilities now have a condition.	so you can see this be essentially the same entropy function as you have see before , except that we all the probability now have a condition .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459508	75	00:03:04.28	00:03:13.33	And this then tells us the entropy of meat after we have known eats occurring in the segment.	and this then tell we the entropy of meat after we have know eat occur in the segment .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.45951	83	00:03:14.22	00:03:30.92	And of course, we can also define this conditional entropy for the scenario where we don't see eats. So if we know eats did not occur in the segment, then this conditional entropy would capture the uncertainty of meat in that content in that condition.	and of course , we can also define this conditional entropy for the scenario where we do n't see eat . so if we know eat do not occur in the segment , then this conditional entropy would capture the uncertainty of meat in that content in that condition .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459511	87	00:03:31.75	00:03:37.69	So now putting different scenarios together, we have the complete definition of conditional entropy as follows.	so now put different scenario together , we have the complete definition of conditional entropy as follow .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459513	96	00:03:39.13	00:04:04.53	Basically. We're going to consider both scenarios of the value of eats zero or one, and this gives us the probability that eats is equal to 0 or 1. Basically, whether eats is present or absent, and this of course is the entropy conditional entropy of meat in that particular scenario.	basically . we be go to consider both scenario of the value of eat zero or one , and this give we the probability that eat be equal to 0 or 1 . basically , whether eat be present or absent , and this of course be the entropy conditional entropy of meat in that particular scenario .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459513	98	00:04:05.35	00:04:14.42	So if you expand this entropy, then you have the following equation.	so if you expand this entropy , then you have the follow equation .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459525	100	00:04:15.62	00:04:19.47	Where you see the involvement of those conditional probabilities.	where you see the involvement of those conditional probability .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459526	102	00:04:21.4	00:04:26.48	Now in general, for any discrete random variables X&Y we have.	now in general , for any discrete random variable X&Y we have .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459549	113	00:04:27.78	00:04:57.81	The conditional entropy is no larger than the entropy of the variable X, so basically this is upper bound for the conditional entropy. That means by knowing more information about the segment, we won't be able to increase the uncertainty. We can only reduce uncertainty, and that intuitively makes sense because as we know more information, it should always help us.	the conditional entropy be no large than the entropy of the variable x , so basically this be upper bind for the conditional entropy . that mean by know more information about the segment , we wo n't be able to increase the uncertainty . we can only reduce uncertainty , and that intuitively make sense because as we know more information , it should always help we .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.45955	115	00:04:58.03	00:05:03.84	Make the prediction and it cannot hurt the prediction in any case.	make the prediction and it can not hurt the prediction in any case .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459551	120	00:05:05.3	00:05:16.37	Now what's interesting here is also to think about what's the minimum possible value of this conditional entropy. Now we know that the maximum value is the entropy of X.	now what be interesting here be also to think about what be the minimum possible value of this conditional entropy . now we know that the maximum value be the entropy of x.
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459552	122	00:05:17.79	00:05:20.52	But what about the minimum? So what do you think?	but what about the minimum ? so what do you think ?
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459553	130	00:05:22.76	00:05:38.39	I hope you can reach the conclusion that the minimum possible value would be 0 and it will be interesting to think about and in what situation will achieve this. So let's see how we can use conditional entropy to capture syntagmatic relations.	I hope you can reach the conclusion that the minimum possible value would be 0 and it will be interesting to think about and in what situation will achieve this . so let 's see how we can use conditional entropy to capture syntagmatic relation .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-27 22:59:51.690761	133	00:05:39.27	00:05:47.41	Now, of course this conditional entropy gives us directly one way to measure the association of two words.	now , of course this conditional entropy give we directly one way to measure the association of two word .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459555	137	00:05:48.17	00:05:57.56	Because it tells us to what extent we can predict the one word given that we know the presence or absence of another word.	because it tell we to what extent we can predict the one word give that we know the presence or absence of another word .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-27 23:00:32.300792	144	00:05:58.59	00:06:17.38	Now before we look at the intuition of conditional entropy in capturing syntagmatic relations, it's useful to think of a very special case listed here, that is, the conditional entropy of the word given itself.	now before we look at the intuition of conditional entropy in capture syntagmatic relation , it be useful to think of a very special case list here , that is , the conditional entropy of the word give itself .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-27 23:00:40.788297	146	00:06:18.81	00:06:24.25	So, here we listed the this.	so , here we list the this .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-27 23:00:51.804042	147	00:06:26.63	00:06:28.55	conditional entropy in the middle.	conditional entropy in the middle .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459558	148	00:06:30.11	00:06:31.08	So it's here.	so it be here .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459559	149	00:06:33.43	00:06:35.11	So what is the value of this?	so what be the value of this ?
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459559	150	00:06:36.25	00:06:36.95	Now.	now .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459562	162	00:06:38.54	00:07:03.66	This means we know whether meat occurs in the sentence and we hope to predict whether the meat occurs in the sentence. Now of course this is zero 'cause there's no uncertain there anymore Once we know whether the word occurs in the segment will already know the answer for the prediction. So this is 0. And that's also when this conditional entropy reaches the minimum.	this mean we know whether meat occur in the sentence and we hope to predict whether the meat occur in the sentence . now of course this be zero 'cause there be no uncertain there anymore once we know whether the word occur in the segment will already know the answer for the prediction . so this be 0 . and that be also when this conditional entropy reach the minimum .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459562	165	00:07:06.15	00:07:11.42	So now let's look at some other cases. So this is a case of.	so now let 's look at some other case . so this be a case of .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459564	171	00:07:12.25	00:07:27.96	Knowing the and trying to predict the meat and this is the case of knowing eats and trying to predict the meat. Which one do you think is smaller? Note that a smaller entropy means easier for prediction.	know the and try to predict the meat and this be the case of know eat and try to predict the meat . which one do you think be small ? note that a small entropy mean easy for prediction .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459564	173	00:07:31.55	00:07:35.08	Which one do you think is higher? Which one is smaller?	which one do you think be high ? which one be small ?
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459566	180	00:07:36.67	00:07:56.64	If you look at the uncertainty, then in the first case the doesn't really tell us much about the meat, so knowing the occurrence of the doesn't really help us reduce the entropy that match, so it stays as fairly close to the original entropy of meat.	if you look at the uncertainty , then in the first case the do n't really tell we much about the meat , so know the occurrence of the do n't really help we reduce the entropy that match , so it stay as fairly close to the original entropy of meat .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-27 23:05:04.645429	188	00:07:57.33	00:08:20.14	Whereas in the case of eats, eats is related to meet, so knowing presence of eats or absence of eats what help us predict weather meet occurs so it can help us reduce entropy of meat, so we should expect the second term, namely, this one to have a smaller entropy.	whereas in the case of eat , eat be relate to meet , so know presence of eat or absence of eat what help we predict weather meet occur so it can help we reduce entropy of meat , so we should expect the second term , namely , this one to have a small entropy .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-27 23:05:15.433956	190	00:08:21.49	00:08:25.9	And that means there is a stronger association between meat and eats.	and that mean there be a strong association between meat and eat .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459588	194	00:08:28.91	00:08:40.94	So we now also know when this is the same as this meat then the entropy conditional entropy would reach its minimum which is 0?	so we now also know when this be the same as this meat then the entropy conditional entropy would reach its minimum which be 0 ?
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-27 23:06:18.541406	198	00:08:41.9	00:08:49.96	And for what kind of words would it reach its maximum? Well, that's when this W is not really related to meat.	and for what kind of word would it reach its maximum ? well , that be when this w be not really relate to meat .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459593	201	00:08:51.29	00:08:58.64	like the, for example, it would be very close to the maximum, which is the entropy of meat itself.	like the , for example , it would be very close to the maximum , which be the entropy of meat itself .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459593	205	00:08:59.83	00:09:07.76	So this suggests that we can use conditional entropy for mining syntagmatic relations. The algorithm would look as follows.	so this suggest that we can use conditional entropy for mine syntagmatic relation . the algorithm would look as follow .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-27 23:07:28.37944	209	00:09:09.63	00:09:17.57	For each word w1, we're going to enumerate the overall other words W2, and then we can compute the conditional entropy	for each word w1 , we be go to enumerate the overall other word W2 , and then we can compute the conditional entropy
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-27 23:07:24.898468	210	00:09:18.95	00:09:21.42	Of W1 given W2.	of W1 give W2 .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-27 23:08:17.504097	219	00:09:22.03	00:09:40.73	And we thought all the candidate words in ascending order of the conditional entropy, because we want to favor a word that has a small entropy, meaning that it helps us predict the target word W1, and then we can take the top ranked the candidate words as words that have potential syntagmatic relations with W1.	and we think all the candidate word in ascend order of the conditional entropy , because we want to favor a word that have a small entropy , mean that it help we predict the target word W1 , and then we can take the top rank the candidate word as word that have potential syntagmatic relation with W1 .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.459598	224	00:09:41.77	00:09:54.62	Note that we need to use a threshold to find these words. The threshold can be the number of top candidates to take or absolute value for the conditional entropy.	note that we need to use a threshold to find these word . the threshold can be the number of top candidate to take or absolute value for the conditional entropy .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-27 23:09:08.691168	227	00:09:55.77	00:10:03.94	Now this would allow us to mine the most strongly correlated words with a particular word W1 here.	now this would allow we to mine the most strongly correlate word with a particular word W1 here .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.4596	234	00:10:05.06	00:10:21.69	But this algorithm does not help us mine the strongest that pay syntagmatic relations from entire collection. Because in order to do that, we have to ensure that these conditional entropies are comparable across different words.	but this algorithm do not help we mine the strong that pay syntagmatic relation from entire collection . because in order to do that , we have to ensure that these conditional entropy be   comparable across different word .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-27 23:10:27.09014	238	00:10:23.01	00:10:33.76	In this case of discovering Syntagmatic relations for a target word like W1, we only need to compare the conditional entropies	in this case of discover syntagmatic relation for a target word like W1 , we only need to compare the conditional entropy
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-27 23:10:25.878516	239	00:10:34.85	00:10:37.74	For W1 given different words.	for W1 give different word .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-12-10 07:12:58.112193	244	00:10:38.45	00:10:49.98	And in this case they all comparable right? So the conditional entropy of W1 given W2 and conditional entropy of W1 given W3 are comparable.	and in this case they all comparable right ? so the conditional entropy of W1 give W2 and conditional entropy of W1 give   W3 be comparable .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-27 23:11:21.703111	246	00:10:50.96	00:10:55.77	They all measure how hard it is to predict W1.	they all measure how hard it be to predict W1 .
51be74e8-eb10-47c2-a768-b688605de1e0	2020-11-02 22:57:07.45961	263	00:10:56.52	00:11:36.63	But if we think about the two pairs where we share W2 in the same condition and we try to predict the W1&W3, then the conditional entropies are actually not comperable. And you can think about this question, why? So Why are they not comfortable? Well, that was because they have a different upper bounds, right? So those upper bounds are precisely the entropy of W1 and the entropy of W3. And they have different upper bounds, so we cannot really compare them in this way. So how do we address this problem?	but if we think about the two pair where we share W2 in the same condition and we try to predict the W1&W3 , then the conditional entropy be actually not comperable . and you can think about this question , why ? so why be they not comfortable ? well , that be because they have a different upper bound , right ? so those upper bound be precisely the entropy of W1 and the entropy of W3 . and they have different upper bound , so we can not really compare they in this way . so how do we address this problem ?
5350ccd0-beab-48fc-8484-d8e6a38c4cbf	2020-12-08 04:13:47.315371	10	00:00:00.3	00:00:21.3	Now let's look at the another behavior of mixture model and in this case let's look at their response to the data frequencies. OK, So what you're seeing now is basically the likelihood function for the two word document, and we know in this case the solution is to give text a probability of 0.9 and the probability of 0.1.	now let 's look at the another behavior of mixture model and in this case let 's look at their response to the data frequency . ok , so what you be see now be basically the likelihood function for the two word document , and we know in this case the solution be to give text a probability of 0.9 and the probability of 0.1 .
5350ccd0-beab-48fc-8484-d8e6a38c4cbf	2020-12-08 04:14:19.680058	15	00:00:29.14	00:00:39.53	Now it's interesting to think about a scenario where we start adding more words to the document. So what would happen if we add many the's to the document?	now it be interesting to think about a scenario where we start add more word to the document . so what would happen if we add many the ' to the document ?
5350ccd0-beab-48fc-8484-d8e6a38c4cbf	2020-11-02 23:00:33.20213	16	00:00:41.23	00:00:43.6	Now this will change the game, right?	now this will change the game , right ?
5350ccd0-beab-48fc-8484-d8e6a38c4cbf	2020-11-02 23:00:33.202131	19	00:00:44.27	00:00:49.65	So how? Well, picture what would the likelihood function look like now?	so how ? well , picture what would the likelihood function look like now ?
5350ccd0-beab-48fc-8484-d8e6a38c4cbf	2020-11-02 23:00:33.202133	21	00:00:50.33	00:00:53.41	It started with the likelihood function for the two words.	it start with the likelihood function for the two word .
5350ccd0-beab-48fc-8484-d8e6a38c4cbf	2020-11-02 23:00:33.202136	29	00:00:54.19	00:01:10.8	As we add more words, we know that, we have to just multiply the likelihood function by additional terms to account for the additional occurrences of the. Since in this case all the additional terms are the, we're going to just multiply by this term for the probability of the.	as we add more word , we know that , we have to just multiply the likelihood function by additional term to account for the additional occurrence of the . since in this case all the additional term be the , we be go to just multiply by this term for the probability of the .
5350ccd0-beab-48fc-8484-d8e6a38c4cbf	2020-11-02 23:00:33.202138	34	00:01:12.33	00:01:26.03	An if we have another occurrence of the, we multiply again by the same term and so on, so forth until we add as many terms as the number of the's that we added to the document D prime.	an if we have another occurrence of the , we multiply again by the same term and so on , so forth until we add as many term as the number of the 's that we add to the document d prime .
5350ccd0-beab-48fc-8484-d8e6a38c4cbf	2020-11-02 23:00:33.202141	39	00:01:28.27	00:01:37.77	Now this obviously changes the likelihood function, so what's interesting is now to think about how would that change our solution. So what's the optimal solution now?	now this obviously change the likelihood function , so what be interesting be now to think about how would that change our solution . so what be the optimal solution now ?
5350ccd0-beab-48fc-8484-d8e6a38c4cbf	2020-11-02 23:00:33.202143	43	00:01:38.59	00:01:46.95	Intuitively, you would know the original solution. 0.9 and 0.1 will no longer be optimal for this new function, right?	intuitively , you would know the original solution . 0.9 and   0.1 will no long be optimal for this new function , right ?
5350ccd0-beab-48fc-8484-d8e6a38c4cbf	2020-11-02 23:00:33.202153	60	00:01:48.19	00:02:17.53	But the question is how should we change it? Well in general there's something one. So in order to change it, we must take away some probability mess from one word. An added the probability mass to the other word. The question is which word to have a reduced the probability and which word to have a larger probability? And in particular, let's think about the probability of the. Should it be increased to be more than 0.1 or should we decrease it to less than 0.1? What do you think?	but the question be how should we change it ? well in general there be something one . so in order to change it , we must take away some probability mess from one word . an add the probability mass to the other word . the question be which word to have a reduce the probability and which word to have a large probability ? and in particular , let 's think about the probability of the . should it be increase to be more than 0.1 or should we decrease it to less than   0.1 ? what do you think ?
5350ccd0-beab-48fc-8484-d8e6a38c4cbf	2020-11-02 23:00:33.202156	66	00:02:19.74	00:02:34.37	Now you might want to pause the video a moment to think more about this question, because this has to do with understanding of important behavior of a mixture model and indeed all the maximum likelihood estimator.	now you might want to pause the video a moment to think more about this question , because this have to do with understanding of important behavior of a mixture model and indeed all the maximum likelihood estimator .
5350ccd0-beab-48fc-8484-d8e6a38c4cbf	2020-11-02 23:00:33.202158	71	00:02:35.6	00:02:47.43	Now if you look at the formula for a moment then you will see. It seems that now the objective function is more influenced by the than text before each contributed one turn.	now if you look at the formula for a moment then you will see . it seem that now the objective function be more influence by the than text before each contribute one turn .
5350ccd0-beab-48fc-8484-d8e6a38c4cbf	2020-11-02 23:00:33.202163	81	00:02:48.4	00:03:15.11	So now, as you can imagine, it would make sense to actually assign a smaller probability for text and to make room for a larger probability for the. Why? Because the is repeated many times if we increase it a little bit, it will have more positive impact, whereas a slight decrease of text. We have relatively small impact because it occurs just once.	so now , as you can imagine , it would make sense to actually assign a small probability for text and to make room for a large probability for the . why ? because the be repeat many time if we increase it a little bit , it will have more positive impact , whereas a slight decrease of text . we have relatively small impact because it occur just once .
5350ccd0-beab-48fc-8484-d8e6a38c4cbf	2020-12-01 04:27:37.543543	103	00:03:16.71	00:04:05.17	Right, so this means there is another behavior that we observe here that is high frequency words generally how high probabilities from all the distributions. And this is no surprise at all, because after all we are maximizing the likelihood of the data. So all the more word occurs, then it's it makes more sense to give such a word a high probability because the impact would be more on the likelihood function. This is in fact a very general phenomenon of all the maximum likelihood estimator, but in this case we can see as we see more occurrences of term. It also encourages the unknown distribution theta sub d to assign somewhat higher probability to this word.	right , so this mean there be another behavior that we observe here that be high frequency word generally how high probability from all the distribution . and this be no surprise at all , because after all we be maximize the likelihood of the datum . so all the more word occur , then it be it make more sense to give such a word a high probability because the impact would be more on the likelihood function . this be in fact a very general phenomenon of all the maximum likelihood estimator , but in this case we can see as we see more occurrence of term . it also encourage the unknown distribution theta sub d to assign somewhat high probability to this word .
5350ccd0-beab-48fc-8484-d8e6a38c4cbf	2020-11-02 23:00:33.202175	108	00:04:07	00:04:15.6	Now it's also interesting to think about the impact of probability of theta sub B. The probability of choosing one of the two component models.	now it be also interesting to think about the impact of probability of theta sub B. the probability of choose one of the two component model .
5350ccd0-beab-48fc-8484-d8e6a38c4cbf	2020-11-02 23:00:33.202193	147	00:04:16.16	00:05:43.3	Now, we've being so far, assuming that each model is equally likely and that gives us 0.5, but you can again look at this like your function and try to picture what would happen if we increase the probability of choosing a background model. Now you will see these terms for the will have a different form where the probability of 'the' would be even larger because the background that has a high probability for the word and the coefficient in front of point nine which is now 0.5 would be even larger. When this is larger the overall result would be larger and that also makes them less important for thetasub D to increase the probability for the because it's already very large so the impact here of increasing the probability of the is somewhat regulated by this coefficient 0.5. If it's a larger on the background then it becomes less important to increase the value so. So. This means the behavior here, which is high frequency words tend to get a higher probabilities are affected or regularised somewhat by the probability of choosing each component. The more likely a component that is being chosen, it's more important than to have higher values for these frequent words. If you have a very small probability of being chosen, than the incentive is less.	now , we 've be so far , assume that each model be equally likely and that give we 0.5 , but you can again look at this like your function and try to picture what would happen if we increase the probability of choose a background model . now you will see these term for the will have a different form where the probability of ' the ' would be even large because the background that have a high probability for the word   and the coefficient in front of point nine which be now 0.5 would be even large . when this be large the overall result would be large and that also make they less important for thetasub d to increase the probability for the because it be already very large so the impact here of increase the probability of the be somewhat regulate by this coefficient 0.5 . if it be a large on the background then it become less important to increase the value so . so . this mean the behavior here , which be high frequency word tend to get a high probability be affect or regularise somewhat by the probability of choose each component . the more likely a component that be be choose , it be more important than to have high value for these frequent word . if you have a very small probability of be choose , than the incentive be less .
5350ccd0-beab-48fc-8484-d8e6a38c4cbf	2020-11-02 23:00:33.2022	166	00:05:44	00:06:32.25	So to summarize, we have just discussed the mixture model and we discussed the estimation problem of mixture model and in particular we discussed some general behavior of the estimate an that means we can expect the our estimator to capture these intuitions. 1st Every component component model attempts to assign high probabilities to high frequency words in the data. And this is tocollaboratively maximize the likelihood. Second, different component models tend to bet high probabilities on different words, and this is to avoid competition or waste of probability, and this would allow them to collaborate more efficiently to maximize the likelihood.	so to summarize , we have just discuss the mixture model and we discuss the estimation problem of mixture model and in particular we discuss some general behavior of the estimate an that mean we can expect the our estimator to capture these intuition . 1st every component component model attempt to assign high probability to high frequency word in the datum . and this be tocollaboratively maximize the likelihood . second , different component model tend to bet high probability on different word , and this be to avoid competition or waste of probability , and this would allow they to collaborate more efficiently to maximize the likelihood .
5350ccd0-beab-48fc-8484-d8e6a38c4cbf	2020-11-02 23:00:33.202203	174	00:06:33.53	00:06:50.71	3rd, the probability of choosing each component regulates the collaboration and competition between the component models. It would allow some component models to respond more to the change, for example of frequency of data point in the data.	3rd , the probability of choose each component regulate the collaboration and competition between the component model . it would allow some component model to respond more to the change , for example of frequency of datum point in the datum .
5350ccd0-beab-48fc-8484-d8e6a38c4cbf	2020-11-02 23:00:33.202208	189	00:06:53.04	00:07:22.78	We also talk about the special case of fixing one component to a background or distribution, and this distribution can be estimated by using a collection of documents. A large collection of English documents, by using just one distribution and then we'll just have normalized frequencies of terms to give us the probabilities of all these words. Now when we use such a specialized mixture model, we show that we can effectively get rid of background words in the other component.	we also talk about the special case of fix one component to a background or distribution , and this distribution can be estimate by use a collection of document . a large collection of english document , by use just one distribution and then we 'll just have normalize frequency of term to give we the probability of all these word . now when we use such a specialized mixture model , we show that we can effectively get rid of background word in the other component .
5350ccd0-beab-48fc-8484-d8e6a38c4cbf	2020-11-02 23:00:33.202209	191	00:07:23.84	00:07:26.88	And that would make it the discovered topic more discriminative.	and that would make it the discover topic more discriminative .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.72511	4	00:00:00.3	00:00:08.37	This lecture is about an overview of statistical language models which cover probabilistic topic models as special cases.	this lecture be about an overview of statistical language model which cover probabilistic topic model as special case .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.725113	10	00:00:16.1	00:00:27.23	In this lecture we're going to give an overview of statistical language models. These models are general models that cover probabilistic topic models as special cases.	in this lecture we be go to give an overview of statistical language model . these model be general model that cover probabilistic topic model as special case .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.725115	12	00:00:28.06	00:00:30.67	So first, what is the statistical language model?	so first , what be the statistical language model ?
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-24 06:03:38.528847	18	00:00:31.67	00:00:45.11	A statistical language model is basically the probability distribution over word sequences. So, for example, we might have a distribution that gives Today is Wednesday a probability of 0.001	a statistical language model be basically the probability distribution over word sequence . so , for example , we might have a distribution that give today be Wednesday a probability of 0.001
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.725121	22	00:00:46.53	00:00:53.73	It might give """Today Wednesday is"" which is a non" grammatical sentence very, very small probability as shown here.	it might give " " " today Wednesday be " " which be a non " grammatical sentence very , very small probability as show here .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-24 06:04:23.436448	25	00:00:54.47	00:01:01.02	"And similarly another sentence, ""The" "eigenvalue is positive"", might get a" probability of 0.00001	" and similarly another sentence , " " the " " eigenvalue be positive " " , might get a " probability of 0.00001
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.725124	27	00:01:02.17	00:01:06.39	So as you can see, such a distribution clearly is context dependent.	so as you can see , such a distribution clearly be context dependent .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.725126	31	00:01:07.04	00:01:13.55	It depends on the context of discussion. Some word sequences might have higher probabilities than others.	it depend on the context of discussion . some word sequence might have high probability than other .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.725128	34	00:01:14.32	00:01:19.14	But the same sequence of words might have a different probability in a different context.	but the same sequence of word might have a different probability in a different context .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.72513	37	00:01:20.4	00:01:24.96	And so this suggests that such a distribution can actually characterized topic.	and so this suggest that such a distribution can actually characterize topic .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.725132	40	00:01:26.87	00:01:32.6	Such a model can also be regarded as a probabilistic mechanism for generating text.	such a model can also be regard as a probabilistic mechanism for generate text .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-24 06:49:24.230396	45	00:01:33.75	00:01:46.68	And that just means we can view text data as data observed from such a model. For this reason, we also call such a model generative model.	and that just mean we can view text datum as datum observe from such a model . for this reason , we also call such a model generative model .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-24 06:05:33.80269	46	00:01:48	00:01:49.21	So now	so now
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.72514	55	00:01:50.43	00:02:09.34	Given a model, we can then sample sequences of words. So for example, based on the distribution that I have shown here on this slide, we might, let's say, sample a sequence like today is Wednesday because it has a relatively high probability, we might often get such a sequence.	give a model , we can then sample sequence of word . so for example , base on the distribution that I have show here on this slide , we might , let 's say , sample a sequence like today be Wednesday because it have a relatively high probability , we might often get such a sequence .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.725142	58	00:02:10.02	00:02:15.84	We might also get the eigenvalue is "positive"", sometimes with a smaller" probability.	we might also get the eigenvalue be " positive " " , sometimes with a small " probability .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.725144	62	00:02:16.72	00:02:23.2	Very, very occasionally, Might get today "Wednesday is"" because the probability is" so small.	very , very occasionally , might get today " Wednesday be " " because the probability be " so small .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.725149	70	00:02:24.1	00:02:41.22	So in general, in order to characterize such a distribution, we must specify probability values for all these different sequences of words. Obviously it's impossible to specify that, because it's impossible to enumerate all the possible sequences of words.	so in general , in order to characterize such a distribution , we must specify probability value for all these different sequence of word . obviously it be impossible to specify that , because it be impossible to enumerate all the possible sequence of word .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.725151	72	00:02:42.44	00:02:48.48	So in practice we will have to simplify the model in some way.	so in practice we will have to simplify the model in some way .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.725154	77	00:02:49.2	00:03:01.98	So the simplest language model is called a unigram language model. In such a case, we simply assume that text is generated by generating each word independently.	so the simple language model be call a unigram language model . in such a case , we simply assume that text be generate by generate each word independently .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.725157	82	00:03:02.88	00:03:11.17	Now, in general, the words may not be generated independently, but after we make this assumption, we can significantly simplify the language model.	now , in general , the word may not be generate independently , but after we make this assumption , we can significantly simplify the language model .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.72516	87	00:03:12.14	00:03:21.74	Basically now the probability of a sequence of words w_1 through w_n would be just a product of each. The probability of each word.	basically now the probability of a sequence of word w_1 through w_n would be just a product of each . the probability of each word .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-24 06:08:05.953945	93	00:03:24.74	00:03:37.55	So for such a model we have as many parameters as the number of words in our vocabulary. So here we assume we have N words, so we have N probabilities, one for each word, and they sum to one.	so for such a model we have as many parameter as the number of word in our vocabulary . so here we assume we have n word , so we have n probability , one for each word , and they sum to one .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.725165	96	00:03:39.98	00:03:45.51	So now we can assume our text is a sample drawn according to this word distribution.	so now we can assume our text be a sample draw accord to this word distribution .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-12-10 20:40:59.587484	99	00:03:46.14	00:03:52.41	That just means we're gonna draw a ward each time and then eventually we'll get a text.	that just mean we be gon na draw a ward each time and then eventually we 'll get a text .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.725167	100	00:03:53.61	00:03:55.97	So for example now again.	so for example now again .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-12-10 20:42:05.615737	121	00:03:57.69	00:04:48.94	We can try to sample words according to a distribution. We might get Wednesday often or today often and some other words like eigenvalue might have a small probability, etc. Now, with this we actually can also compute the probability of every sequence, even though our model only specifies the probabilities of words. This is because of the independence assumption. So specifically we can compute the "probability of ""today is Wednesday""." Because it's just a product of the probability of today, probability of is and probably Wednesday. For example, I showed some fake numbers here and we might then multiply these numbers together to get the probability "of ""today is Wednesday""."	we can try to sample word accord to a distribution . we might get Wednesday often or today often and some other word like eigenvalue might have a small probability , etc . now , with this we actually can also compute the probability of every sequence , even though our model only specify the probability of word . this be because of the independence assumption . so specifically we can compute the " probability of " " today be Wednesday " " . " because it be just a product of the probability of today , probability of be and probably Wednesday . for example , I show some fake number here and we might then multiply these number together to get the probability " of " " today be Wednesday " " . "
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.725186	135	00:04:49.54	00:05:23.82	So as you can see, with N probabilities, one for each word, we actually can characterize the probability distribution over all kinds of sequences of words, and so this is a very simple model. Ignore the word order, so it may not be effective for some problems such as speech recognition, where you may care about the order of words. But it turns out to be quite sufficient for many tasks that involve topic analysis, and that's also what we're interested in here.	so as you can see , with n probability , one for each word , we actually can characterize the probability distribution over all kind of sequence of word , and so this be a very simple model . ignore the word order , so it may not be effective for some problem such as speech recognition , where you may care about the order of word . but it turn out to be quite sufficient for many task that involve topic analysis , and that be also what we be interested in here .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.725188	141	00:05:24.48	00:05:37.77	So when we have a model, we generally have two problems that we can think about. One is given a model. How likely we'll observe certain kind of data points.	so when we have a model , we generally have two problem that we can think about . one be give a model . how likely we 'll observe certain kind of data point .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.725192	149	00:05:38.36	00:05:54.8	That is, we're interested in the sampling process. The other is the estimation process and that is to figure out the parameters of the model given some observed data, and we're going to talk about that in a moment. Let's first talk about the sampling.	that is , we be interested in the sample process . the other be the estimation process and that be to figure out the parameter of the model give some observe datum , and we be go to talk about that in a moment . let 's first talk about the sampling .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.725196	155	00:05:55.51	00:06:08.69	So here I show 2 two examples of word distributions or unigram language models. The first one has higher probabilities for words, text, mining, association, etc.	so here I show 2 two example of word distribution or unigram language model . the first one have high probability for word ,   text , mining , association , etc .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.725198	160	00:06:10	00:06:21.97	Now this signals a topic about text mining, because when we sample words from such a distribution we tend to see words that often occur in text mining context.	now this signal a topic about text mining , because when we sample word from such a distribution we tend to see word that often occur in text mining context .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.7252	165	00:06:23.62	00:06:37.4	So in this case, if we ask the question about what is the probability of generating a particular document, then we likely will see text that looks like a text mining paper of course.	so in this case , if we ask the question about what be the probability of generate a particular document , then we likely will see text that look like a text mining paper of course .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-24 06:11:53.071844	166	00:06:38.59	00:06:38.81	...	...
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.725206	178	00:06:39.48	00:07:06.6	The text that we generated by drawing words from this distribution is unlikely coherent, although the probability of generating a text mining paper publishing in the top conference is non zero. Assuming that no word has a zero probability in the distribution and that just means we can essentially generate all kinds of text documents, including very meaningful text documents.	the text that we generate by draw word from this distribution be unlikely coherent , although the probability of generate a text mining paper publishing in the top conference be non zero . assume that no word have a zero probability in the distribution and that just mean we can essentially generate all kind of text document , include very meaningful text document .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.725209	186	00:07:07.72	00:07:22.41	The second distribution show on the bottom has different words that with higher probability. Food, nutrition and healthy, diet etc. So this clearly indicates a different topic and in this case it's probably about health.	the second distribution show on the bottom have different word that with high probability . food , nutrition and healthy , diet etc . so this clearly indicate a different topic and in this case it be probably about health .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.725211	190	00:07:23.09	00:07:31.58	So if we sample words from such distribution, then the probability of observing a text mining paper would be very very small.	so if we sample word from such distribution , then the probability of observe a text mining paper would be very very small .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.725212	194	00:07:32.72	00:07:40.53	On the other hand, the probability of observing a text that looks like a food nutrition paper would be high, relatively higher.	on the other hand , the probability of observe a text that look like a food nutrition paper would be high , relatively high .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.725213	197	00:07:41.42	00:07:47.7	So that just means given a particular distribution, different text will have different probabilities.	so that just mean give a particular distribution , different text will have different probability .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.725218	209	00:07:48.36	00:08:11.09	Now let's look at the estimation problem. Now, in this case, we're going to assume that we have observed data. We know exactly what the text data looks like. In this case, let's assume we have a text mining paper. In fact, it's abstract of the paper, so the total number of words is 100, and I've shown some counts of individual words here.	now let 's look at the estimation problem . now , in this case , we be go to assume that we have observe datum . we know exactly what the text datum look like . in this case , let 's assume we have a text mining paper . in fact , it be abstract of the paper , so the total number of word be 100 , and I 've show some count of individual word here .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.725221	215	00:08:12.44	00:08:29.069999	If we ask the question, what is the most likely language model that has been used to generate this text data, assuming that the text is observed from some language model, what's our best guess of this language model?	if we ask the question , what be the most likely language model that have be use to generate this text datum , assume that the text be observe from some language model , what be our good guess of this language model ?
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.725222	218	00:08:30.63	00:08:36.6	OK, so the problem now is just the estimated probabilities of these words as I've shown here.	ok , so the problem now be just the estimate probability of these word as I 've show here .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-24 06:14:57.122441	220	00:08:37.47	00:08:39.67	So what do you think? What would be your guess?	so what do you think ? what would be your guess ?
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-12-10 20:45:59.72076	224	00:08:40.61	00:08:47.37	Would you guess text that has a very very small probability or relatively large probability?	would you guess text that have a very very small probability or relatively large probability ?
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.725224	225	00:08:48.28	00:08:49.61	What about the query?	what about the query ?
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.725225	228	00:08:50.19	00:08:57.18	Your guess probably will be dependent on how many times we have observed this word in the text data, right?	your guess probably will be dependent on how many time we have observe this word in the text datum , right ?
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.725228	234	00:08:58.34	00:09:15.58	And if you think about it for a moment, and if you like many others, you would have guessed that text has a probability of 10 out of 100. Because I've observed text 10 times in the text that has a total of 100 words.	and if you think about it for a moment , and if you like many other , you would have guess that text have a probability of 10 out of 100 . because I 've observe text 10 time in the text that have a total of 100 word .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-24 06:15:59.984768	239	00:09:16.91	00:09:26.5	And similarly, mining has five out of 100. And query as a relatively small probability, just observd once. So it's one out of 100.	and similarly , mining have five out of 100 . and query as a relatively small probability , just observd once . so it be one out of 100 .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.725231	244	00:09:27.98	00:09:36.56	Right, so that, intuitively, is a reasonable guess, but the question is Is this our best guess or best estimate of the parameters?	right , so that , intuitively , be a reasonable guess , but the question be be this our good guess or good estimate of the parameter ?
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-24 06:16:34.577271	247	00:09:37.72	00:09:42.96	Of course, in order to answer this question we have to define what we mean by best.	of course , in order to answer this question we have to define what we mean by good .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.725234	251	00:09:44.1	00:09:53.93	In this case, it turns out that our guesses are indeed the best in some sense, and this is called maximum likelihood estimate.	in this case , it turn out that our guess be indeed the good in some sense , and this be call maximum likelihood estimate .
54ab232c-85cb-4829-abd0-6cbaed5f3fc8	2020-11-02 23:04:24.725235	254	00:09:54.56	00:10:00.95	And it's the best in that it would give our observed data the maximum probability.	and it be the good in that it would give our observed datum the maximum probability .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.77364	3	00:00:00.3	00:00:04.74	This lecture is about the similarity based approaches to text for clustering.	this lecture be about the similarity base approach to text for clustering .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773641	6	00:00:13.16	00:00:17.72	In this lecture, we're going to continue the discussion of how to do a text clustering.	in this lecture , we be go to continue the discussion of how to do a text clustering .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773643	9	00:00:18.66	00:00:24.68	In particular, we're going to cover a different kind of approaches than generative models.	in particular , we be go to cover a different kind of approach than generative model .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773647	20	00:00:25.38	00:00:51.56	And that is similarity based approaches. So the general idea of similarity based clustering is to explicitly specify a similarity function to measure the similarity between 2:00 text objects. Now this is in contrast with a generative model where we implicitly define the clustering bias. By using a particular objective function like a likelihood function.	and that be similarity base approach . so the general idea of similarity base clustering be to explicitly specify a similarity function to measure the similarity between 2:00 text object . now this be in contrast with a generative model where we implicitly define the cluster bias . by use a particular objective function like a likelihood function .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773659	48	00:00:52.61	00:02:03.87	The whole process is driven by optimizing the likeable, but here we explicitly provide a review of what we think are similar, and this is often very useful because then it allows us to inject any particular view of similarity into the clustering program. So once we have a similarity function, we can then aim at optimally partitioning to partitioning the data into clusters or into different groups. Anne, try to maximize the intragroup similarity and minimize the intergroup similarity. That is, to ensure the objects that are put in the same group to be similar, but the objects that are put into different groups to be not similar, and these are the general goals of clustering. And there's often a tradeoff between achieving both goals. Now, there are many different methods for doing similarity based clustering. In general, I think we can distinguish two strategies at high level. One is to progressively construct the hierarchy of clusters.	the whole process be drive by optimize the likeable , but here we explicitly provide a review of what we think be similar , and this be often very useful because then it allow we to inject any particular view of similarity into the clustering program . so once we have a similarity function , we can then aim at optimally partition to partition the datum into cluster or into different group . Anne , try to maximize the intragroup similarity and minimize the intergroup similarity . that is , to ensure the object that be put in the same group to be similar , but the object that be put into different group to be not similar , and these be the general goal of clustering . and there be often a tradeoff between achieve both goal . now , there be many different method for do similarity base clustering . in general , I think we can distinguish two strategy at high level . one be to progressively construct the hierarchy of cluster .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773668	70	00:02:04.47	00:02:53.95	And so this often leads to hierarchical clustering an we can further distinguishes two ways to construct the hierarchy depending on whether we started with the collection to divide the collection or start with individual objects and gradually group them together. So one is bottom up that can be called agglomerative, where we gradually Group A similar object into larger and larger clusters until we group everything together. The other is top down or divisive. In this case we gradually partitioning the whole data set into smaller and smaller clusters. The other general strategy is to start with the initial tentative clustering and then iteratively improve it and this often leads to a flat clustering. One example is K means.	and so this often lead to hierarchical clustering an we can far distinguish two way to construct the hierarchy depend on whether we start with the collection to divide the collection or start with individual object and gradually group they together . so one be bottom up that can be call agglomerative , where we gradually Group a similar object into large and large cluster until we group everything together . the other be top down or divisive . in this case we gradually partition the whole datum set into small and small cluster . the other general strategy be to start with the initial tentative clustering and then iteratively improve it and this often lead to a flat clustering . one example be k mean .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773669	73	00:02:54.61	00:02:59.21	So as I just said, there are many different clustering methods available and.	so as I just say , there be many different clustering method available and .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.77367	76	00:03:00.39	00:03:05.95	A full coverage of all these custom methods would be beyond the scope of this course.	a full coverage of all these custom method would be beyond the scope of this course .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773671	78	00:03:06.74	00:03:11.08	But here we can talk about the two representative methods and.	but here we can talk about the two representative method and .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773674	84	00:03:12.12	00:03:23.88	In some detail. One is hierarchical agglomerative clustering or agency, the other is KMEANS So first let's look at the agglomerative hierarchical clustering.	in some detail . one be hierarchical agglomerative clustering or agency , the other be KMEANS so first let 's look at the agglomerative hierarchical clustering .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773677	93	00:03:24.67	00:03:43.39	In this case, we are giving a similarity function calls to measure similarity between two objects and then we can gradually group similar objects together in a bottom up profession to form larger and larger groups, and they also form a hierarchy and then we can stop when some stopping criterions that.	in this case , we be give a similarity function call to measure similarity between two object and then we can gradually group similar object together in a bottom up profession to form large and large group , and they also form a hierarchy and then we can stop when some stop criterion that .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773678	97	00:03:44.07	00:03:51.16	I could be either some number of classes has been achieved, or the threshold for similarity has been reached.	I could be either some number of class have be achieve , or the threshold for similarity have be reach .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773683	110	00:03:52.02	00:04:21.92	There are different variations here and there mainly differ in the ways to computer group similarity based on the individual object similarity. So let's illustrate how can induce a structure based on just similarity. So start with all the text objects and we can then measure the similarity between them. Of course based on the provider similarity function and then we can see which pair has the highest similarity and then just group them together.	there be different variation here and there mainly differ in the way to computer group similarity base on the individual object similarity . so let 's illustrate how can induce a structure base on just similarity . so start with all the text object and we can then measure the similarity between they . of course base on the provider similarity function and then we can see which pair have the high similarity and then just group they together .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773684	112	00:04:22.76	00:04:26.75	And then was going to see which pair is.	and then be go to see which pair be .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773684	113	00:04:28.17	00:04:29.96	The next one to group.	the next one to group .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773685	115	00:04:30.63	00:04:33.55	Maybe these two now have the highest similarity.	maybe these two now have the high similarity .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773687	122	00:04:34.61	00:04:48.88	And then we can gradually group them together in every time we're going to pick the highest similarity similarity pairs to group. This will give us a binary tree eventually to group everything together.	and then we can gradually group they together in every time we be go to pick the high similarity similarity pair to group . this will give we a binary tree eventually to group everything together .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773689	127	00:04:50.2	00:05:01.57	Now depending our applications, we can use the whole hierarchy as structure for browsing for example, or we can choose the cut off at say come here to get four clusters.	now depend our application , we can use the whole hierarchy as structure for browse for example , or we can choose the cut off at say come here to get four cluster .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.77369	131	00:05:02.86	00:05:12.15	Or we can use the threshold to cut or we can cut at this high level to get just the two clusters. So this is a general idea.	or we can use the threshold to cut or we can cut at this high level to get just the two cluster . so this be a general idea .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773694	141	00:05:13.14	00:05:32.98	Now, if you think about how to implement this algorithm, you will realize that we have everything specified except for how to compute the group similarity. We are only given the similarity function or two objects, but as we group groups together we also need to assess the similarity between two groups.	now , if you think about how to implement this algorithm , you will realize that we have everything specify except for how to compute the group similarity . we be only give the similarity function or two object , but as we group group together we also need to assess the similarity between two group .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773696	145	00:05:33.73	00:05:42.42	And there are also different ways to do that, and there's the three popular methods are single link complete link an average link?	and there be also different way to do that , and there be the three popular method be single link complete link an average link ?
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773699	152	00:05:43.21	00:05:58.98	So given two groups and singling algorithm is going to define the group similarity as the similarity of the closest repair of the two groups. Complete Link defines the similarity of two groups as the similarity of the father sister pair.	so give two group and single algorithm be go to define the group similarity as the similarity of the close repair of the two group . Complete Link define the similarity of two group as the similarity of the father sister pair .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.7737	155	00:05:59.62	00:06:05.66	Average link defines the similarity as average of similarity of all the pairs of the two groups.	average link define the similarity as average of similarity of all the pair of the two group .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773703	164	00:06:06.37	00:06:27.45	So it's much easier to understand these methods by illustrating them. So here are two groups G1 and G2 with some objects in each group, and we know how to compute the similarity between two objects. But the question now is, how can we compute the similarity between the two groups?	so it be much easy to understand these method by illustrate they . so here be two group G1 and G2 with some object in each group , and we know how to compute the similarity between two object . but the question now be , how can we compute the similarity between the two group ?
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773704	167	00:06:28.37	00:06:33.35	And then we can in general basis on the similarities of the objects in the two groups.	and then we can in general basis on the similarity of the object in the two group .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773706	172	00:06:35	00:06:45.74	So in terms of single link and we're just looking at the closest pair. So in this case these two pairs objects would define the similarity of the two groups.	so in term of single link and we be just look at the close pair . so in this case these two pair object would define the similarity of the two group .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773706	174	00:06:47.17	00:06:50.48	As long as they are very close orders, say the two groups are very.	as long as they be very close order , say the two group be very .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773707	176	00:06:51.63	00:06:56.26	Close so it's optimistic view of similarity.	close so it be optimistic view of similarity .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773708	181	00:06:57.8	00:07:11.07	The complete link, on the other hand, will be in some sense pessimistic and by taking the similarity of the two farthest appear as the similarity for the two groups.	the complete link , on the other hand , will be in some sense pessimistic and by take the similarity of the two farthest appear as the similarity for the two group .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773711	187	00:07:12.33	00:07:28.299999	So we're going to make sure that if the two groups are having a high similarity, then every pair of the two the objects in the group will be insured to have high similarity.	so we be go to make sure that if the two group be have a high similarity , then every pair of the two the object in the group will be insure to have high similarity .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773712	189	00:07:29.24	00:07:33.96	Every link is in between, so it takes average of all these pairs.	every link be in between , so it take average of all these pair .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773713	194	00:07:34.64	00:07:43.42	Now, these different ways of computing group similarities will need to different clustering algorithms, and they will generally give different results.	now , these different way of compute group similarity will need to different clustering algorithm , and they will generally give different result .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773714	197	00:07:45.22	00:07:51.23	Now, so it's useful to take a look at their differences and to make comparison.	now , so it be useful to take a look at their difference and to make comparison .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773714	198	00:07:53.52	00:07:55.79	Our first single link.	our first single link .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773716	204	00:07:56.64	00:08:07.93	Can be expected to generate the loose clusters. The reason is becausw. As long as two objects are very similar in the two groups, it would bring the two groups together.	can be expect to generate the loose cluster . the reason be becausw . as long as two object be very similar in the two group , it would bring the two group together .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773723	223	00:08:09.13	00:08:55.15	If you think about this is similar to having parties with people, then it just means two groups of two groups of people would be putting together as long as each group there is a person that is well connected with the other group. So the two leaders of the two groups can have a good relationship with each other and then they will bring together the two groups. In this case, the cluster is rules because there's no guarantee that other members of the two groups are actually very close to each other. Sometimes they may be very far away. Now in this case it's also based on individual decision, so it could be sensitive to outliers.	if you think about this be similar to have party with people , then it just mean two group of two group of people would be put together as long as each group there be a person that be well connected with the other group . so the two leader of the two group can have a good relationship with each other and then they will bring together the two group . in this case , the cluster be rule because there be no guarantee that other member of the two group be actually very close to each other . sometimes they may be very far away . now in this case it be also base on individual decision , so it could be sensitive to outlier .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773724	226	00:08:56.03	00:09:02.49	The complete linker is in the opposite situation where we can expect the clusters to be tight.	the complete linker be in the opposite situation where we can expect the cluster to be tight .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773727	234	00:09:03.27	00:09:19.61	Anne, it's also based on individual decision, so it can be sensitive to outliers. Again, to continue the analogy to having a party of people then complete the link would mean when two groups come together they want to ensure that even the.	Anne , it be also base on individual decision , so it can be sensitive to outlier . again , to continue the analogy to have a party of people then complete the link would mean when two group come together they want to ensure that even the .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773733	253	00:09:21.24	00:10:04.54	Even the people that are unlikely to talk to each other would be comfortable with talking to each other, so ensure the whole class to be coherent. The average link, of course is in between and group decision, so it's going to be insensitive to outliers. In practice, which one is the best? Well, this will depend on the application and sometimes you need a loose classes and to aggressively on cluster objects together. Then maybe simple English good. But other times you might need a tight clusters, then completely completely better, but in general you have to empirically evaluate these methods for your application to know which one is better.	even the people that be unlikely to talk to each other would be comfortable with talk to each other , so ensure the whole class to be coherent . the average link , of course be in between and group decision , so it be go to be insensitive to outlier . in practice , which one be the good ? well , this will depend on the application and sometimes you need a loose class and to aggressively on cluster object together . then maybe simple english good . but other time you might need a tight cluster , then completely completely well , but in general you have to empirically evaluate these method for your application to know which one be well .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773734	256	00:10:06.2	00:10:11.95	Now let's look at another example of method for similarity based classroom in this case.	now let 's look at another example of method for similarity base classroom in this case .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773735	260	00:10:14.47	00:10:24.05	Which is called K means clustering will represent each text object as a term vector and then assuming similarity function defined onto objects.	which be call K mean clustering will represent each text object as a term vector and then assume similarity function define onto object .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773738	266	00:10:25.61	00:10:42.62	Now we're going to start with some tentative clustering result by just selecting Kate randomly selected vectors as centroids of K clusters and treat them as sentence as they represent each cluster.	now we be go to start with some tentative clustering result by just select Kate randomly select vector as centroid of k cluster and treat they as sentence as they represent each cluster .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773739	269	00:10:43.29	00:10:46.39	So this is. This gives us the initial tentative classroom.	so this be . this give we the initial tentative classroom .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773744	287	00:10:47.04	00:11:27.55	And then we're going to iteratively improve it, and the process goes like this. And once we have these Central Eastside, we're going to assign a vector to the cluster hosts entry that is closest to the current vector. So basically we're going to measure the distance between this vector and each of the centroids, and see which one is closest to this one, and then just put this class this object into that cluster. Now this is to have tentative. Assignment of objects into clusters and we're going to partition or the objects into K clusters based on our tentative clustering centroids.	and then we be go to iteratively improve it , and the process go like this . and once we have these Central Eastside , we be go to assign a vector to the cluster host entry that be close to the current vector . so basically we be go to measure the distance between this vector and each of the centroid , and see which one be close to this one , and then just put this class this object into that cluster . now this be to have tentative . assignment of object into cluster and we be go to partition or the object into K cluster base on our tentative clustering centroid .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773745	290	00:11:28.51	00:11:34.81	And then we're going to recovery, compute the centroid based on the allocated objects in each cluster.	and then we be go to recovery , compute the centroid base on the allocate object in each cluster .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773745	291	00:11:35.46	00:11:36.6	And this is.	and this be .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.77375	307	00:11:37.2	00:12:12.99	To adjust the centroid and then we had repeated this process until the similarity based on objective function. In this case it's within cluster sum of squares converges an theoretically we can show that this process actually is going to minimize the within cluster sum of squares where define objective function. Given K clusters. So it can be also shown this process will converge to a local minimum. I think about this process for a moment. It might remind you the EM algorithm for mixture model.	to adjust the centroid and then we have repeat this process until the similarity base on objective function . in this case it be within cluster sum of square converge an theoretically we can show that this process actually be go to minimize the within cluster sum of square where define objective function . give K cluster . so it can be also show this process will converge to a local minimum . I think about this process for a moment . it might remind you the EM algorithm for mixture model .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773752	312	00:12:13.78	00:12:23.79	Indeed, this algorithm is very similar to the EM algorithm for the mixture model for clustering. So more specifically, we also initialize these.	indeed , this algorithm be very similar to the EM algorithm for the mixture model for clustering . so more specifically , we also initialize these .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773752	315	00:12:24.83	00:12:32.25	Predators in the EM algorithm, so the random inner inner initialization is similar.	predator in the EM algorithm , so the random inner inner initialization be similar .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.77376	341	00:12:34.08	00:13:36.049999	And then in the EML with them, you may recall that we're going to repeat eastep and M step to improved our primary destinations. In this case, we're going to improve the clustering result iteratively by also doing 2 steps, and in fact the two steps are very similar to EM algorithm. In that when we allocate vector into one of the clusters based on our tentative clustering, it's very similar to inferring the distribution that has been used with generally the document in the mixture model. So it's essentially similar to eastep. Also, what's the difference? While the differences here, we don't make a probabilistic on location as in the case of the step. But rather we make a choice. We're going to make a call if this data point is closest to cluster two that were going to say you are in class too. So there's no choice, and we're not going to say you are 70% belonging to class too.	and then in the EML with they , you may recall that we be go to repeat eastep and M step to improved our primary destination . in this case , we be go to improve the clustering result iteratively by also do 2 step , and in fact the two step be very similar to EM algorithm . in that when we allocate vector into one of the cluster base on our tentative clustering , it be very similar to infer the distribution that have be use with generally the document in the mixture model . so it be essentially similar to eastep . also , what be the difference ? while the difference here , we do n't make a probabilistic on location as in the case of the step . but rather we make a choice . we be go to make a call if this datum point be close to cluster two that be go to say you be in class too . so there be no choice , and we be not go to say you be 70 % belong to class too .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773762	345	00:13:37.08	00:13:44.61	And so we're not going to have a probability, but we're going to just put one object into precisely one cluster.	and so we be not go to have a probability , but we be go to just put one object into precisely one cluster .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773763	348	00:13:45.23	00:13:50.2	In the E step, however, we do a probabilistic location, so we split the counts.	in the e step , however , we do a probabilistic location , so we split the count .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773771	375	00:13:51	00:14:52.769999	And we're not going to say exactly which distribution has been used to generate the data point. Now the next we're going to adjust the centroid, and this is very similar to M step where we re estimate the parameters. That's when we'll have a better estimate of the parameter. So here we have a better clustering result by adjusting the centroid. And note that the central is adjusted based on the average of the vectors in the. A cluster, so this is also similar to the M step, where we do counts pull together counter and normalize them, or the difference of course is also because of the difference in the instep, and we're not going to consider probabilities when we count the points in this case, for K means we're going to only count the objects allocated to this cluster, and this is only a subset of data points. But in the EM algorithm, we in principle consider all the data points.	and we be not go to say exactly which distribution have be use to generate the data point . now the next we be go to adjust the centroid , and this be very similar to M step where we re estimate the parameter . that be when we 'll have a well estimate of the parameter . so here we have a well clustering result by adjust the centroid . and note that the central be adjust base on the average of the vector in the . a cluster , so this be also similar to the M step , where we do count pull together counter and normalize they , or the difference of course be also because of the difference in the instep , and we be not go to consider probability when we count the point in this case , for k mean we be go to only count the object allocate to this cluster , and this be only a subset of datum point . but in the EM algorithm , we in principle consider all the datum point .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773771	376	00:14:53.24	00:14:55.65	Based on probabilistic allocations.	base on probabilistic allocation .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773776	391	00:14:56.64	00:15:27.439999	But in nature they are very similar and that's why it's also maximizing where defined objective function and it's guaranteed to convert converted local minimum. So to summarize our discussion of clustering methods, we first discussed the model based approaches, mainly the mixture model. And here we use is implicitly similarity function. To define the clustering bias, there's no explicit definer similarity function. The model defines clustering bias.	but in nature they be very similar and that be why it be also maximize where define objective function and it be guarantee to convert convert local minimum . so to summarize our discussion of clustering method , we first discuss the model base approach , mainly the mixture model . and here we use be implicitly similarity function . to define the cluster bias , there be no explicit definer similarity function . the model define cluster bias .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773777	396	00:15:28.71	00:15:38.5	And the clustering structure is built into a generated model. That's why we can use potentially a different model to recover different instruction.	and the clustering structure be build into a generate model . that be why we can use potentially a different model to recover different instruction .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773779	402	00:15:39.6	00:15:53.51	Complex generative models can be used to discover complex clustering structures. We did not talk about it, but we can easily design generated model to generate a hierarchical clusters.	complex generative model can be use to discover complex clustering structure . we do not talk about it , but we can easily design generate model to generate a hierarchical cluster .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773782	410	00:15:54.69	00:16:10.7	We can also use prior to further customize clustering algorithm to for example, control the topic of 1 cluster or multiple clusters. However, one disadvantage of this approach is that there is no easy way to direct or control the similarity measure.	we can also use prior to far customize cluster algorithm to for example , control the topic of 1 cluster or multiple cluster . however , one disadvantage of this approach be that there be no easy way to direct or control the similarity measure .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773783	414	00:16:11.57	00:16:19.19	Sometimes we want to do that, but it's very hard to inject the such a explicit definition of similarity into such a model.	sometimes we want to do that , but it be very hard to inject the such a explicit definition of similarity into such a model .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773784	418	00:16:20.13	00:16:27.82	We also talked about the similarity based approaches. These approaches are more flexible. Directly specify similarity functions.	we also talk about the similarity base approach . these approach be more flexible . directly specify similarity function .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773787	427	00:16:28.8	00:16:47.17	But one potential disadvantage is that their object function is not always very clear. The K means algorithm has a clearly defined the objective function, but it's also very similar to a model based approach. The hierarchical clustering algorithm, on the other hand, is.	but one potential disadvantage be that their object function be not always very clear . the k mean algorithm have a clearly define the objective function , but it be also very similar to a model base approach . the hierarchical clustering algorithm , on the other hand , be .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773787	428	00:16:49.05	00:16:50.58	It's harder to.	it be hard to .
5bb813bd-6b7d-4f77-8156-21995f5944ad	2020-11-02 22:59:54.773788	431	00:16:51.27	00:16:59.6	To specify the objective function so it's not clear what exactly is being optimized.	to specify the objective function so it be not clear what exactly be be optimize .
6382e23f-d54e-4ece-a231-8df819983fb5	2020-11-02 22:56:24.364101	2	00:00:00.3	00:00:05.17	This lecture is continued discussion of evaluation of textual categorisation.	this lecture be continue discussion of evaluation of textual categorisation .
6382e23f-d54e-4ece-a231-8df819983fb5	2020-11-02 22:56:24.364105	18	00:00:12.56	00:00:47.81	Earlier we have introduced measures that can be used to compute the precision and recall for each category qnd each document. Now in this lecture We further examine how to combine the performance on these different categories or different documents. How do we aggregate them? How do we take average? You see on the title here, I indicated it's called a macro average and this is in contrast to micro average that will talk more about that later. So.	early we have introduce measure that can be use to compute the precision and recall for each category qnd each document . now in this lecture we far   examine how to combine the performance on these different category or different document . how do we aggregate they ? how do we take average ? you see on the title here , I indicate it be call a macro average and this be in contrast to micro average that will talk more about that later . so .
6382e23f-d54e-4ece-a231-8df819983fb5	2020-11-02 22:56:24.364114	46	00:00:49.67	00:01:49.97	Again, for each category, we can compute the precision recall and F1 so for example, for category C one. We have precision, P1 recall R1 and F value F1 and similarly we can do that for Category 2 and all the other categories. Once we compute that, then we can aggregate them. So for example, we can aggregate all the precision values for all the categories to compute the overall precision and this is often very useful. To summarize what we have seen in the whole data set and the aggregation can be done in many different ways. Again, as I said, in case one unit to aggregate different values. It's always good to think about what's the best way of doing the aggregation. For example, you can consider arithmetic mean, which is very commonly used. Or you can use geometric mean which would have different behavior depending on the way you aggregate. You might have got different conclusions.	again , for each category , we can compute the precision recall and F1 so for example , for category c one . we have precision , P1 recall R1 and F value F1 and similarly we can do that for category 2 and all the other category . once we compute that , then we can aggregate they . so for example , we can aggregate all the precision value for all the category to compute the overall precision and this be often very useful . to summarize what we have see in the whole datum set and the aggregation can be do in many different way . again , as I say , in case one unit to aggregate different value . it be always good to think about what be the good way of do the aggregation . for example , you can consider arithmetic mean , which be very commonly use . or you can use geometric mean which would have different behavior depend on the way you aggregate . you might have get different conclusion .
6382e23f-d54e-4ece-a231-8df819983fb5	2020-11-02 22:56:24.364117	59	00:01:50.42	00:02:20.86	In terms of which method works better, so it's important to consider these differences and choosing the right one or more suitable one for your task. So the difference, for example between arithmetic mean and geometric mean is that the arithmetic mean would be dominated by high values, whereas geometric mean would be more affected by low values, and so whether you want to emphasize low values or high values would be a question related to your application.	in term of which method work well , so it be important to consider these difference and choose the right one or more suitable one for your task . so the difference , for example between arithmetic mean and geometric mean be that the arithmetic mean would be dominate by high value , whereas geometric mean would be more affect by low value , and so whether you want to emphasize low value or high value would be a question relate to your application .
6382e23f-d54e-4ece-a231-8df819983fb5	2020-11-02 22:56:24.364118	63	00:02:21.42	00:02:29.62	And similar we can do that for recall and F score, so that's how we can then generate the overall precision, recall and F score.	and similar we can do that for recall and F score , so that be how we can then generate the overall precision , recall and F score .
6382e23f-d54e-4ece-a231-8df819983fb5	2020-11-02 22:56:24.364121	77	00:02:30.23	00:02:58.75	Now we can do the same for aggregation over all the documents, right? So it's exactly the same situation for each document or computer precision. Recall and F. And then after we have completed the computations for all these documents we were going to aggregate them to generate the overall precision, overall recall and overall F score. These are again examining the results from different angles and which one is more useful would depend on your application.	now we can do the same for aggregation over all the document , right ? so it be exactly the same situation for each document or computer precision . Recall and F. and then after we have complete the computation for all these document we be go to aggregate they to generate the overall precision , overall recall and overall F score . these be again examine the result from different angle and which one be more useful would depend on your application .
6382e23f-d54e-4ece-a231-8df819983fb5	2020-11-02 22:56:24.364123	88	00:03:00.32	00:03:25.28	In general, it's beneficial to look at the results from all these perspectives, and especially if you compare different methods in different dimensions. It might reveal which method is better, in which measure or in what situations, and this provides insight for understanding the strength of a method or weakness, and this provides further insight for improving them.	in general , it be beneficial to look at the result from all these perspective , and especially if you compare different method in different dimension . it might reveal which method be well , in which measure or in what situation , and this provide insight for understand the strength of a method or weakness , and this provide further insight for improve they .
6382e23f-d54e-4ece-a231-8df819983fb5	2020-11-02 22:56:24.364126	95	00:03:28.13	00:03:44.56	So as I mentioned, there is also micro averaging in contrast to the macro average that we talked about earlier. In this case, what we do is to pull together all the decisions. An then compute the precision and recall.	so as I mention , there be also micro average in contrast to the macro average that we talk about early . in this case , what we do be to pull together all the decision . an then compute the precision and recall .
6382e23f-d54e-4ece-a231-8df819983fb5	2020-12-02 09:37:54.433103	114	00:03:45.33	00:04:31.45	So we can compute the overall precision and recall by just counting how many cases are in true positive, how many cases in false positive, etc. Basically computing the values to fill in this contingency table and then we can compute precision recall just once. Now, in contrast, in macro averaging we're going to do that for each category 1st and then aggregate over these categories. Or we do that for each document and then aggregate over all the documents. But here we pulled them together. Now this will be very similar to the classification accuracy that we introduced earlier, and one problem here of course, is to treat all the instances, all the decisions equally.	so we can compute the overall precision and recall by just count how many case be in true positive , how many case in false positive , etc . basically compute the value to fill in this contingency table and then we can compute precision recall just once . now , in contrast , in macro average we be go to do that for each category 1st and then aggregate over these category . or we do that for each document and then aggregate over all the document . but here we pull they together . now this will be very similar to the classification accuracy that we introduce early , and one problem here of course , be to treat all the instance , all the decision equally .
6382e23f-d54e-4ece-a231-8df819983fb5	2020-12-02 09:38:02.570473	115	00:04:32.26	00:04:35.3	And, this may not be desirable.	and , this may not be desirable .
6382e23f-d54e-4ece-a231-8df819983fb5	2020-11-02 22:56:24.364132	119	00:04:36.12	00:04:44.84	But it may be appropriate for some applications, especially if we associate, for example, the cost for each combination.	but it may be appropriate for some application , especially if we associate , for example , the cost for each combination .
6382e23f-d54e-4ece-a231-8df819983fb5	2020-11-02 22:56:24.364133	124	00:04:45.42	00:04:55.3	Then we can actually compute, for example, weighted classification accuracy where you associate the different cost or utility for each specific decision.	then we can actually compute , for example , weight classification accuracy where you associate the different cost or utility for each specific decision .
6382e23f-d54e-4ece-a231-8df819983fb5	2020-11-02 22:56:24.364135	133	00:04:56.11	00:05:19.96	So there could be variations of these methods that would be more useful, but in general macro average tends to be more informative than micro averaging just because it might reflect the need for understanding performance on each category or performance on each document which are needed in many applications.	so there could be variation of these method that would be more useful , but in general macro average tend to be more informative than micro average just because it might reflect the need for understand performance on each category or performance on each document which be need in many application .
6382e23f-d54e-4ece-a231-8df819983fb5	2020-11-02 22:56:24.364137	140	00:05:20.54	00:05:39.39	But the macro averaging and micro averaging, they're both very common and you might see both reported in research papers on text categorisation. Also, sometimes categorisation results might actually be evaluated from ranking perspective.	but the macro averaging and micro averaging , they be both very common and you might see both report in research paper on text categorisation . also , sometimes categorisation result might actually be evaluate from rank perspective .
6382e23f-d54e-4ece-a231-8df819983fb5	2020-11-02 22:56:24.36414	158	00:05:40.27	00:06:24.99	Now this is because. Categorisation results are sometimes or often indeed passed to human for various purposes. For example, it might be passed to humans for further editing. For example, news articles can be tentatively categorized by using the system and then human editors would then correct them. And all the email messages might be routed to the right person for handling in the help desk, and in such a case the categorizations do help prioritizing the task for a particular customer service person. So in this case, the results have to be prioritized.	now this be because . categorisation result be sometimes or often indeed pass to human for various purpose . for example , it might be pass to human for far editing . for example , news article can be tentatively categorize by use the system and then human editor would then correct they . and all the email message might be route to the right person for handle in the help desk , and in such a case the categorization do help prioritize the task for a particular customer service person . so in this case , the result have to be prioritize .
6382e23f-d54e-4ece-a231-8df819983fb5	2020-11-02 22:56:24.364148	196	00:06:26.26	00:07:56.76	And if the system can give a score to the categorisation decision or confidence, then we can use the scores to rank these decisions and then evaluate the results as a ranked list, just as in search engine evaluation, where you rank the documents in response to the query. So for example, discovery of spam emails can be evaluated, based on ranking emails for the spam category and this is useful if you want people to verify whether this is really spam, right? The person would then take the ranked list to check one by one and then verify whether this is indeed a spam. So to reflect the utility for humans in such a task, it's better to evaluate the ranking accuracy, and this is basically similar to search again. And in such a case, often the problem can be better formulated as a ranking problem instead of categorization problem. So for example, ranking documents in the search engine can also be framed as a binary categorization problem, distinguishing relevant documents that are useful to users from those that are not useful. But typically we frame this as a ranking problem and we evaluated as a ranked list. That's be cause people tend to examine the results sequentially, so ranking evaluation more reflects the utility from users perspective.	and if the system can give a score to the categorisation decision or confidence , then we can use the score to rank these decision and then evaluate the result as a rank list , just as in search engine evaluation , where you rank the document in response to the query . so for example , discovery of spam email can be evaluate , base on rank email for the spam category and this be useful if you want people to verify whether this be really spam , right ? the person would then take the rank list to check one by one and then verify whether this be indeed a spam . so to reflect the utility for human in such a task , it be well to evaluate the ranking accuracy , and this be basically similar to search again . and in such a case , often the problem can be well formulate as a ranking problem instead of categorization problem . so for example , rank document in the search engine can also be frame as a binary categorization problem , distinguish relevant document that be useful to user from those that be not useful . but typically we frame this as a ranking problem and we evaluate as a rank list . that be be cause people tend to examine the result sequentially , so ranking evaluation more reflect the utility from user perspective .
6382e23f-d54e-4ece-a231-8df819983fb5	2020-11-02 22:56:24.36415	206	00:07:58.06	00:08:17.61	So, to summarize, categorization evaluation, first evaluation is always very important for all these tasks, so get it right. If you don't get it right, you might get misleading results an you might be misled to believe one method is better than the other, which is in fact not true. So it's very important to get it right.	so , to summarize , categorization evaluation , first evaluation be always very important for all these task , so get it right . if you do n't get it right , you might get misleading result an you might be mislead to believe one method be well than the other , which be in fact not true . so it be very important to get it right .
6382e23f-d54e-4ece-a231-8df819983fb5	2020-12-02 09:45:49.800129	212	00:08:18.5	00:08:29.5	Measures must also reflect the intended use of the results for particular application. For example, in spam filtering and news categorization results are used in maybe different ways.	measure must also reflect the intend use of the result for particular application . for example , in spam filtering and news categorization result be use in maybe  different way .
6382e23f-d54e-4ece-a231-8df819983fb5	2020-11-02 22:56:24.364152	215	00:08:30.56	00:08:35.65	So then we would need to consider the difference and design measures appropriately.	so then we would need to consider the difference and design measure appropriately .
6382e23f-d54e-4ece-a231-8df819983fb5	2020-12-02 09:46:26.44756	220	00:08:36.51	00:08:47.939999	We generally need to consider how will the results be further processed by a user and then think from a user's perspective what quality is important. What aspect of quality is important.	we generally need to consider how will the result be far process by a user and then think from a user 's perspective what quality be important . what aspect of quality be important .
6382e23f-d54e-4ece-a231-8df819983fb5	2020-11-02 22:56:24.364154	226	00:08:48.93	00:08:58.72	Sometimes there are tradeoffs between multiple aspects, like precision and recall, and then, so we need to know for this application is high recall more important or high precision is more important.	sometimes there be tradeoff between multiple aspect , like precision and recall , and then , so we need to know for this application be high recall more important or high precision be more important .
6382e23f-d54e-4ece-a231-8df819983fb5	2020-11-02 22:56:24.364155	230	00:08:59.77	00:09:06.93	Ideally we associate the different cost with each different decision error and this of course has to be designed in application specific away.	ideally we associate the different cost with each different decision error and this of course have to be design in application specific away .
6382e23f-d54e-4ece-a231-8df819983fb5	2020-12-02 09:59:40.316662	236	00:09:07.95	00:09:19.45	Some commonly used measures for relative comparison of different methods or the following classification accuracy is very commonly used for especially balanced tester set.	some commonly use measure for relative comparison of different method or the follow classification accuracy be very commonly use for especially balanced tester set .
6382e23f-d54e-4ece-a231-8df819983fb5	2020-11-02 22:56:24.36416	258	00:09:20	00:10:03.25	Precision, recall, and F scores are commonly reported to characterize the performances in different angles, and there are some also variations like per document based evaluation, per category evaluation and then take average of all of them in different ways. Micro versus macro averaging. In general, you want to look at the results from multiple perspectives and for particular application in some perspectives would be more important than others, but for diagnosis, analysis of categorization methods and it's generally useful to look at as many perspectives as possible to see subtle differences between methods or to see where a method might be weak, from which you can obtain insights for improving a method.	precision , recall , and F score be commonly report to characterize the performance in different angle , and there be some also variation like per document base evaluation , per category evaluation and then take average of all of they in different way . micro versus macro averaging . in general , you want to look at the result from multiple perspective and for particular application in some perspective would be more important than other , but for diagnosis , analysis of categorization method and it be generally useful to look at as many perspective as possible to see subtle difference between method or to see where a method might be weak , from which you can obtain insight for improve a method .
6382e23f-d54e-4ece-a231-8df819983fb5	2020-11-02 22:56:24.364162	265	00:10:04.58	00:10:16.57	Finally, sometimes ranking may be more appropriate, so be careful. Sometimes categorisation, task and maybe better frame as a ranking task and there are machine learning methods for optimizing ranking measures as well.	finally , sometimes ranking may be more appropriate , so be careful . sometimes categorisation , task and maybe well frame as a ranking task and there be machine learning method for optimize rank measure as well .
6382e23f-d54e-4ece-a231-8df819983fb5	2020-11-02 22:56:24.364162	269	00:10:17.38	00:10:26.34	So here are two suggested readings are one is some chapters of this book where you can find more discussion about evaluation measures.	so here be two suggest reading be one be some chapter of this book where you can find more discussion about evaluation measure .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.334949	3	00:00:00.3	00:00:06.13	This lecture is about how to use generative probabilistic models for text categorization.	this lecture be about how to use generative probabilistic model for text categorization .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.334952	10	00:00:14.26	00:00:28.93	There are in general are two kinds of approaches to text categorization by using machine learning. One is generative problem risk models, the other is discriminative approaches. In this lecture, we're going to talk about the generative models.	there be in general be two kind of approach to text categorization by use machine learning . one be generative problem risk model , the other be discriminative approach . in this lecture , we be go to talk about the generative model .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.334956	25	00:00:29.58	00:00:58.38	In the next lecture, we're going to talk about discriminative approaches. So the problem of text categorization is actually very similar to document clustering in that we assume that each document belongs to one category or one cluster. Main difference is that in clustering we don't really know what are the predefined categories or what are the clusters. In fact, that's the goal of text clustering. We want to find such clusters in the data.	in the next lecture , we be go to talk about discriminative approach . so the problem of text categorization be actually very similar to document clustering in that we assume that each document belong to one category or one cluster . main difference be that in clustering we do n't really know what be the predefined category or what be the cluster . in fact , that be the goal of text clustering . we want to find such cluster in the datum .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.334958	29	00:00:59.18	00:01:06.64	But in the case of categorization, we are given the categories. So we kind of have predefined categories and.	but in the case of categorization , we be give the category . so we kind of have predefine category and .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.334959	34	00:01:07.22	00:01:17.8	then based on these categories and training data, we would like to allocate a document to one of these categories, or sometimes multiple categories.	then base on these category and training datum , we would like to allocate a document to one of these category , or sometimes multiple category .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-12-07 18:28:46.054575	54	00:01:18.58	00:02:04.47	But because of the similarity of the two problems, we can actually adapt document clustering models for text categorization. Or we can understand how we can use generative models to do text categorization from the perspective of clustering. And so this is a slide that we've talked about before about text clustering, where we assume there are multiple topics represented by word distributions. Each topic is 1 cluster. So once we estimate such model, we faced the problem of deciding which cluster document d should belong to and this question boils down to decide which thtea i has been used to generate D.	but because of the similarity of the two problem , we can actually adapt document clustering model for text categorization . or we can understand how we can use generative model to do text categorization from the perspective of clustering . and so this be a slide that we 've talk about before about text clustering , where we assume there be multiple topic represent by word distribution . each topic be 1 cluster . so once we estimate such model , we face the problem of decide which cluster document d should belong to and this question boil down to decide which thtea I have be use to generate D.
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-12-07 18:29:01.48847	56	00:02:06.19	00:02:13.75	Suppose D has L words represent represent as Xi here.	suppose d have l word represent represent as Xi here .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.334971	62	00:02:14.32	00:02:31.2	Now, how can you compute the probability that particular topic word distributions theta i has been used to generate this document? In general, we use bayes rule to make this inference.	now , how can you compute the probability that particular topic word distribution theta I have be use to generate this document ? in general , we use baye rule to make this inference .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-12-07 18:29:25.049457	63	00:02:32.01	00:02:34.59	And you can see this	and you can see this
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.334974	69	00:02:35.84	00:02:51.77	Prior information here. That we need to consider if a topic or cluster has a higher prior then it's more likely that the document has been from this cluster, so we should favor such a cluster.	prior information here . that we need to consider if a topic or cluster have a high prior then it be more likely that the document have be from this cluster , so we should favor such a cluster .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.334976	74	00:02:52.32	00:03:01.62	The other is a likelihood part, that is this part. And this has to do with whether the topic word distribution can explain the content of this document well.	the other be a likelihood part , that be this part . and this have to do with whether the topic word distribution can explain the content of this document well .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.33498	84	00:03:02.22	00:03:25.99	And we want to pick a topic that's high by both values. So more specifically, we just multiply them together and then choose which topic has the highest product. So more rigorously, this is what we would be doing, so we're going to choose the topic that with the maximize this posterior probability of the topic given the document.	and we want to pick a topic that be high by both value . so more specifically , we just multiply they together and then choose which topic have the high product . so more rigorously , this be what we would be do , so we be go to choose the topic that with the maximize this posterior probability of the topic give the document .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.334981	88	00:03:26.72	00:03:38.81	Get posterior becausw this one P of Theta i is the prior, that's our belief about which topic is more likely. Before we observe any document.	get posterior becausw this one p of Theta I be the prior , that be our belief about which topic be more likely . before we observe any document .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-30 01:44:43.456735	89	00:03:39.37	00:03:41.73	But this conditional probability here	but this conditional probability here
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-12-07 18:30:57.994795	92	00:03:42.35	00:03:47.99	Is the posterior probability of the topic after we have observed the document d.	be the posterior probability of the topic after we have observe the document   d.
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-12-07 18:31:10.76163	95	00:03:49.22	00:03:55.81	And Bayes rule allows us to update this probability based on the prior and I shown the details.	and Bayes rule allow we to update this probability base on the prior and I show the detail .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-12-07 18:31:57.103666	108	00:03:56.46	00:04:30.48	Below here you can see how the prior here is related to the posterior on the left hand side. And this is related to how well this word distribution explains the document here, and the two are related in this way. So to find the topic that has the highest posterior probability here, it's equivalent to maximize this product as we have seen also multiple times in this course.	below here you can see how the prior here be relate to the posterior on the left hand side . and this be relate to how well this word distribution explain the document here , and the two be relate in this way . so to find the topic that have the high posterior probability here , it be equivalent to maximize this product as we have see also multiple time in this course .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.334991	116	00:04:32.16	00:04:49.74	An we can then change the probability of document in your product of the probability of each word and that's just because we've made the assumption about the independence in generating each word OK. So this is just something that you have seen in document clustering.	an we can then change the probability of document in your product of the probability of each word and that be just because we 've make the assumption about the independence in generate each word ok . so this be just something that you have see in document clustering .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-12-07 18:33:15.077191	130	00:04:50.53	00:05:24.28	An we now can see clearly how we can assign a documentary to a category based on the information about word distributions for these categories and the prior on these categories. So this idea can be directly adapted to do categorization and This is precisely what Naive Bayes classifier is doing, so here it's mostly the same information, except that we're looking at the categorization problem now, so we assume that if	an we now can see clearly how we can assign a documentary to a category base on the information about word distribution for these category and the prior on these category . so this idea can be directly adapt to do categorization and this be precisely what Naive Bayes classifier be do , so here it be mostly the same information , except that we be look at the categorization problem now , so we assume that if
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335002	142	00:05:25.5	00:05:55.15	Theta I represents category I accurately that means the word distribution characterizes the content of documents in category iii accurately. Then what we can do is precisely like what we did for text clustering. Namely, we are going to assign document D to the category that has the highest probability of generating this document. In other words, we're going to maximize this posterior probability as well.	theta I represent category I accurately that mean the word distribution characterize the content of document in category iii accurately . then what we can do be precisely like what we do for text clustering . namely , we be go to assign document d to the category that have the high probability of generate this document . in other word , we be go to maximize this posterior probability as well .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335003	145	00:05:56.46	00:06:03.52	And this is related to the prior and the likelihood an as you have seen on the previous slide.	and this be relate to the prior and the likelihood an as you have see on the previous slide .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335004	147	00:06:04.12	00:06:09.66	And so naturally, we can then decompose this likelihood into a product.	and so naturally , we can then decompose this likelihood into a product .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-12-07 18:34:39.828067	156	00:06:10.28	00:06:35.68	As you see here now here I changed the notation so that we will write down the product as product over all the words in the vocabulary and even if even though the document doesn't contain all the words and the product is there accurately representing the product of all the words in the document. because of this count here.	as you see here now here I change the notation so that we will write down the product as product over all the word in the vocabulary and even if even though the document do n't contain all the word and the product be there accurately represent the product of all the word in the document . because of this count here .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335012	167	00:06:36.72	00:06:55.77	when a word doesn't occur in the document. The count would be 0, so this time would just be zero. So effectively we're just have the product over all the words in the document. So basically with naive Bayes classifier, we're going to score each category for a document by this function.	when a word do n't occur in the document . the count would be 0 , so this time would just be zero . so effectively we be just have the product over all the word in the document . so basically with naive Bayes classifier , we be go to score each category for a document by this function .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.33502	189	00:06:56.71	00:07:46.38	Now you may notice that here It involves the product of a lot of small probabilities and this can cause underflow problem. So one way to solve the problem is to take logarithm of this function, which doesn't change the order of these categories, but would help us preserve precision and so this is often the. This is often the function that we actually use to score each category, and then we're going to choose the category that has the highest score by this function. So this is called a Naiyes Bayes classifier. Now the keyword Bayes is understandable because we are applying a Bayes rule here. When we go from the posterior probability of the topic to a product of the likelihood and the prior.	now you may notice that here it involve the product of a lot of small probability and this can cause underflow problem . so one way to solve the problem be to take logarithm of this function , which do n't change the order of these category , but would help we preserve precision and so this be often the . this be often the function that we actually use to score each category , and then we be go to choose the category that have the high score by this function . so this be call a Naiyes Bayes classifier . now the keyword Bayes be understandable because we be apply a Bayes rule here . when we go from the posterior probability of the topic to a product of the likelihood and the prior .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335026	201	00:07:47.41	00:08:13.98	Now it's also called a Naive because We've made an assumption that every word in the document is generated independently, and this is indeed a Naive assumption, because in reality they are not generated independently. Once you see some word and other words will more likely occur. For example, if you have seen a word like a text, then it makes categorisation or clustering more likely to appear And if you have not seen text.	now it be also call a Naive because we 've make an assumption that every word in the document be generate independently , and this be indeed a naive assumption , because in reality they be not generate independently . once you see some word and other word will more likely occur . for example , if you have see a word like a text , then it make categorisation or cluster more likely to appear and if you have not see text .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335028	205	00:08:15.37	00:08:22.16	But this assumption allows us to simplify the problem, and it's actually quite effective for many text categorization tasks.	but this assumption allow we to simplify the problem , and it be actually quite effective for many text categorization task .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335033	222	00:08:23.04	00:08:59.71	But you should know that this kind of model doesn't have to make this assumption. We could, for example, assume the words may be dependent on each other, so that would make it a bigram language model or trigram language model. And of course you can even use a mixture model to model what the document looks like in each category. So in nature they will be all using Bayes rule to do classification, but the actual generative model for documents in each category. Can vary, and here we just talk about a very simple case. Perhaps the simplest case.	but you should know that this kind of model do n't have to make this assumption . we could , for example , assume the word may be dependent on each other , so that would make it a bigram language model or trigram language model . and of course you can even use a mixture model to model what the document look like in each category . so in nature they will be all use Bayes rule to do classification , but the actual generative model for document in each category . can vary , and here we just talk about a very simple case . perhaps the simple case .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-30 01:53:03.41267	225	00:09:00.5	00:09:07.69	So now the question is, how can we make sure each theta i actually represents category i accurate?	so now the question be , how can we make sure each theta I actually represent category I accurate ?
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335041	242	00:09:09.3	00:09:53.58	Now, in clustering we learned this category i or the word distributions for category i from the data. But in our case what can we do to make sure this theta i represents indeed category i? If you think about the question and you're likely to come up with the idea of using the training data right. Indeed, in text categorization, we typically assume that there are training data available and those are the documents that are known to have been generated from which category. In other words, these are the documents with known categories assigned, and of course human experts must do that.	now , in clustering we learn this category I or the word distribution for category I from the datum . but in our case what can we do to make sure this theta I represent indeed category I ? if you think about the question and you be likely to come up with the idea of use the training datum right . indeed , in text categorization , we typically assume that there be train datum available and those be the document that be know to have be generate from which category . in other word , these be the document with know category assign , and of course human expert must do that .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335043	248	00:09:54.2	00:10:08.66	And here you see that T1 represents the set of documents that are known to have been generated from category one, and T2 represents the documents that are known to have been generated from category two, etc.	and here you see that T1 represent the set of document that be know to have be generate from category one , and T2 represent the document that be know to have be generate from category two , etc .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335046	258	00:10:10.56	00:10:29.96	Now if you look at this picture, you see that the model here is really a simplified unigram language model. It is no longer mixture model. Why? Because already know which distribution has been used to generate which documents. There's no uncertainty here. There's no mixing of different categories here.	now if you look at this picture , you see that the model here be really a simplified unigram language model . it be no long mixture model . why ? because already know which distribution have be use to generate which document . there be no uncertainty here . there be no mixing of different category here .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335049	266	00:10:30.87	00:10:47.95	So the estimation problem of course would be simplified, but in general you can imagine what we want to do is to estimate these probabilities that I marked here and what are the probabilities that we have to estimate in order to do categorization where there are two kinds.	so the estimation problem of course would be simplify , but in general you can imagine what we want to do be to estimate these probability that I mark here and what be the probability that we have to estimate in order to do categorization where there be two kind .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335051	271	00:10:48.63	00:11:01.75	So one is the prior. The probability of theta i and this indicates how popular each category is or how likely we would have observed the document in that category.	so one be the prior . the probability of theta I and this indicate how popular each category be or how likely we would have observe the document in that category .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335053	275	00:11:03.17	00:11:10.39	The other kind is word distributions and we want to know what words have high probabilities for each category.	the other kind be word distribution and we want to know what word have high probability for each category .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335054	278	00:11:11.56	00:11:17.8	So the idea then is to just use the observed training data to estimate these two probabilities.	so the idea then be to just use the observe training datum to estimate these two probability .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-30 01:55:58.63621	286	00:11:18.69	00:11:35.69	And in general we can do this separately for different categories. That's just because these documents are known to be generated from a specific category, so once we know that it's in some sense irrelevant what other categories we are also dealing with.	and in general we can do this separately for different category . that be just because these document be know to be generate from a specific category , so once we know that it be in some sense irrelevant what other category we be also deal with .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335059	293	00:11:37.34	00:11:49.64	So now this is statistical estimation problem. We have observed some data from some model and we want to guess the parameters of this model. We want to take our best guess of the parameters.	so now this be statistical estimation problem . we have observe some datum from some model and we want to guess the parameter of this model . we want to take our good guess of the parameter .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.33506	296	00:11:50.93	00:11:55.14	And this is the problem that you have seen. Also several times in this course.	and this be the problem that you have see . also several time in this course .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.33507	323	00:11:56.12	00:12:54.85	Now, if you haven't thought about that this problem, haven't seen naive Bayes classifier, it would be very useful for you to pause the video for a moment and to think about how to solve this problem. So let me state the problem again, so let's just think about category One. We know there is one word distribution that has been used to generate documents. And we generated each word in the document independently and we know that we have observed the set of N sub one documents in the set of T1. These documents have been all generated from category one, namely have been all generated using this same word distribution. Now the question is what will be your guess or estimate of the probability of each word in this distribution and what will be your guess of the prior probability of this category? Of course, this second probability depends on how likely that you will see documents in other categories.	now , if you have n't think about that this problem , have n't see   naive Bayes classifier , it would be very useful for you to pause the video for a moment and to think about how to solve this problem . so let I state the problem again , so let 's just think about category one . we know there be one word distribution that have be use to generate document . and we generate each word in the document independently and we know that we have observe the set of N sub one document in the set of T1 . these document have be all generate from category one , namely have be all generate use this same word distribution . now the question be what will be your guess or estimate of the probability of each word in this distribution and what will be your guess of the prior probability of this category ? of course , this second probability depend on how likely that you will see document in other category .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-12-07 18:41:18.852597	327	00:12:55.49	00:13:05.56	Right, so think for a moment that how do you use all these training data, including all these documents that are known to be in these K categories.	right , so think for a moment that how do you use all these training datum , include all these document that be know to be in these K category .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335073	335	00:13:06.31	00:13:22.44	To estimate all these parameters. Now if you spend some time to think about this and it would help you understand the following few slides. So do spend some time to make sure that you can try to solve this problem or do your best to solve the problem yourself.	to estimate all these parameter . now if you spend some time to think about this and it would help you understand the follow few slide . so do spend some time to make sure that you can try to solve this problem or do your good to solve the problem yourself .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-12-07 18:43:20.060395	349	00:13:23.16	00:13:55.91	Now, if you have thought about it and then you will realize the following intuition. First, what's the basis for estimating the prior or the probability of each category? Well, this has to do with whether you have observed a lot of documents from that category. Intuitively, if you have seen a lot of documents in sports and very few in medical science, then your guess is that the probability of sports category is larger or your prior on the category would be larger.	now , if you have think about it and then you will realize the follow intuition . first , what be the basis for estimate the prior or the probability of each category ? well , this have to do with whether you have observe a lot of document from that category . intuitively , if you have see a lot of document in sport and very few in medical science , then your guess be that the probability of sport category be large or your prior on the category would be large .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335084	367	00:13:56.99	00:14:36.58	And what about the basis for estimating the probability of word in each category? Well, the same and you'll be just assuming that words that are observed frequently in the documents that are known to be generated from a category. will likely have higher probability, and that's just the maximum likelihood estimator indeed, and that's what we could do. So to estimate the probability of each category. And to answer the question which category is most popular, then we can simply normalize the count of documents in each category. So here you see n sub I denotes the number of documents in each category.	and what about the basis for estimate the probability of word in each category ? well , the same and you 'll be just assume that word that be observe frequently in the document that be know to be generate from a category . will likely have high probability , and that be just the maximum likelihood estimator indeed , and that be what we could do . so to estimate the probability of each category . and to answer the question which category be most popular , then we can simply normalize the count of document in each category . so here you see n sub I denote the number of document in each category .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-30 02:14:36.410251	372	00:14:37.81	00:14:49.38	And we simply just normalize this count to make this a probability. In other words, we make this probability proportional to the size of training dataset in each category.	and we simply just normalize this count to make this a probability . in other word , we make this probability proportional to the size of training dataset in each category .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-30 02:14:43.797166	373	00:14:50.04	00:14:53.07	That's the size of the set T sub i.	that be the size of the set T sub i.
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-30 02:15:23.5132	384	00:14:55.11	00:15:19.37	Now, what about the word distribution? Well, we do the same again. This time we can do this for each category. So let's say we are considering category I or Theta i I. So which word has higher probability? Well, we simply count the word occurrences in the documents that are known to be generated from theta i.	now , what about the word distribution ? well , we do the same again . this time we can do this for each category . so let 's say we be consider category I or Theta I I. So which word have high probability ? well , we simply count the word occurrence in the document that be know to be generate from theta i.
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-30 02:15:32.25236	386	00:15:20.13	00:15:24.74	And then we put together all the all the counts of the same word in this set.	and then we put together all the all the count of the same word in this set .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-12-07 18:45:15.601489	390	00:15:25.53	00:15:34.74	And then we just normalize these counts to make this distribution of all the words make all the probabilities of all these words sum to one.	and then we just normalize these count to make this distribution of all the word make all the probability of all these word sum to one .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335093	396	00:15:35.53	00:15:48.5	So in this case you can see this is a proportional to the count of the word in the collection of training documents. T sub I and that's denoted by C of w and T sub I.	so in this case you can see this be a proportional to the count of the word in the collection of training document . t sub I and that be denote by C of w and T sub I.
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335097	405	00:15:49.56	00:16:10.73	Now you may notice that we often write down a probability estimate in the form of being proportional to certain number, and this is often sufficient. Becausw we have some constraints on these distributions and so the normalizer is dictated by the constraint.	now you may notice that we often write down a probability estimate in the form of be proportional to certain number , and this be often sufficient . Becausw we have some constraint on these distribution and so the normalizer be dictate by the constraint .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335098	409	00:16:11.35	00:16:17.63	So in this case it will be useful for you to think about what are the constraints on these two kinds of probabilities.	so in this case it will be useful for you to think about what be the constraint on these two kind of probability .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335111	440	00:16:19.81	00:17:24.68	So once you figure out the answer to this question and you will know how to normalize, this counts and so this is a good exercise to work on it if it's not obvious to you. There is another issue in Naive Bayes which is a smoothing. In fact the smoothing is a general problem in all the estimate of language models and this has to do with what would happen if you have observed a small amount of data. So smoothing is the important technique to address data sparseness. In our case the training data set can be small and one data set is small. When we use maximum likelihood estimator we often face the problem of zero probability. That means if the event is not observed. Then the estimated probability would be 0 in this case if we have not seen a word in the training documents for, let's say, category I, then our estimate would be 0 for the probability of this word in this category. And this is generally not accurate. So we have to do smoothing to make sure it's not zero probability.	so once you figure out the answer to this question and you will know how to normalize , this count and so this be a good exercise to work on it if it be not obvious to you . there be another issue in Naive Bayes which be a smoothing . in fact the smoothing be a general problem in all the estimate of language model and this have to do with what would happen if you have observe a small amount of datum . so smoothing be the important technique to address datum sparseness . in our case the training datum set can be small and one datum set be small . when we use maximum likelihood estimator we often face the problem of zero probability . that mean if the event be not observe . then the estimate probability would be 0 in this case if we have not see a word in the training document for , let 's say , category I , then our estimate would be 0 for the probability of this word in this category . and this be generally not accurate . so we have to do smoothing to make sure it be not zero probability .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-12-07 18:48:09.207104	452	00:17:25.26	00:17:53.859999	The other reason for smoothing is that this is a way to bring prior knowledge, and this is also generally true for a lot of situations of smoothing. When the data set is small, we tend to rely on some prior knowledge to to solve the problem. So in this case our prior knowledge says that no words should have zero probability, so smoothing allows us to inject this prior to make sure that no word has a zero probability.	the other reason for smoothing be that this be a way to bring prior knowledge , and this be also generally true for a lot of situation of smoothing . when the data set be small , we tend to rely on some prior knowledge to to solve the problem . so in this case our prior knowledge say that no word should have zero probability , so smoothing allow we to inject this prior to make sure that no word have a zero probability .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-30 02:19:12.598576	461	00:17:54.86	00:18:13.7	There is also a third reason, which is sometimes not very obvious, but we'll explain that in a moment and that is to help achieve discriminative waiting of terms. And this is also called IDF weighting inverse document frequency weighting that you have seen in mining word relations.	there be also a third reason , which be sometimes not very obvious , but we 'll explain that in a moment and that be to help achieve discriminative wait of term . and this be also call IDF weighting inverse document frequency weighting that you have see in mining word relation .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335127	466	00:18:14.6	00:18:21.69	So how do we do smoothing? Well in general we added pseudo counts to these events. We'll make sure that no event has zero count.	so how do we do smooth ? well in general we add pseudo count to these event . we 'll make sure that no event have zero count .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335135	483	00:18:22.54	00:19:04.57	So one possible way of smoothing the probability of category is to simply add small nonnegative constant Delta to the count. We pretend that every category has actually some extra number of documents represented by Delta. And in the denominator we also add K multiplied by Delta because we want the probability to sum to one. So in total we've added Delta K Times because we have K categories. Therefore in the sum we have to also add K multiplied by Delta as a total pseudo counts that we add to the estimate.	so one possible way of smooth the probability of category be to simply add small nonnegative constant Delta to the count . we pretend that every category have actually some extra number of document represent by Delta . and in the denominator we also add K multiply by Delta because we want the probability to sum to one . so in total we 've add Delta K Times because we have k category . therefore in the sum we have to also add K multiply by Delta as a total pseudo count that we add to the estimate .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335143	500	00:19:06.28	00:19:43.4	Now it's interesting to think about the influence of delta. Obvious Delta is a smoothing parameter here, meaning that the larger delta is and the more we will do smoothing and that means we'll more rely on pseudo counts and we might indeed ignore the actual counts if delta is set to Infinity. Imagine what would happen if delta approaches positive Infinity? Well, we're going to say every word has infinity amount of sorry, not every word every category has. infinity amount of documents, and then there's no distinction between them, so it becomes just a uniform.	now it be interesting to think about the influence of delta . Obvious Delta be a smooth parameter here , mean that the large delta be and the more we will do smoothing and that mean we 'll more rely on pseudo count and we might indeed ignore the actual count if delta be set to Infinity . imagine what would happen if delta approach positive Infinity ? well , we be go to say every word have infinity amount of sorry , not every word every category have . infinity amount of document , and then there be no distinction between they , so it become just a uniform .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335149	514	00:19:44.09	00:20:13.28	What if Delta is zero? Well we just go back to the original estimate based on the observed training data to estimate the probability of each category. Now we can do the same for the word distribution, but in this case we sometimes we find it useful to use a non-uniform pseudo counts for the words. So here you see we'll add pseudocounts to each word and that's mu multiplied by the probability of the world given by a background language model.	what if Delta be zero ? well we just go back to the original estimate base on the observed training datum to estimate the probability of each category . now we can do the same for the word distribution , but in this case we sometimes we find it useful to use a non - uniform pseudo count for the word . so here you see we 'll add pseudocount to each word and that be mu multiply by the probability of the world give by a background language model .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-30 02:22:01.955687	515	00:20:14.12	00:20:14.85	Theta sub b	theta sub b
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335154	524	00:20:15.92	00:20:35.1	Now that background model in general can be estimated by using a large collection of text, or in this case we can use the whole set of all the training data to estimate this background language model. But if we don't have to use this one, we can use larger text data that are available from somewhere else.	now that background model in general can be estimate by use a large collection of text , or in this case we can use the whole set of all the training datum to estimate this background language model . but if we do n't have to use this one , we can use large text datum that be available from somewhere else .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-12-07 18:51:45.306272	535	00:20:36.02	00:20:56.58	Now if we use such a background language model to add pseudocounts, we find that some words will receive more pseudocounts. So what are those words? Well those are the common words. Because they get higher probability by the background language model so the pseudocounts added for such words would be higher, rare words on the other hand will have smaller pseudocounts.	now if we use such a background language model to add pseudocount , we find that some word will receive more pseudocount . so what be those word ? well those be the common word . because they get high probability by the background language model so the pseudocount add for such word would be high , rare word on the other hand will have small pseudocount .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-12-07 18:56:07.650777	545	00:20:58.57	00:21:20.48	Now, this addition of background model would cause nonuniform smoothing of this word distributions were going to bring the probability of those common words, or to a higher level because of the background model. Now this helps make the difference of the probability of such words smaller across categories.	now , this addition of background model would cause nonuniform smoothing of this word distribution be go to bring the probability of those common word , or to a high level because of the background model . now this help make the difference of the probability of such word small across category .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335173	564	00:21:21.4	00:22:04.49	Because every category has some help from their background for words, like the, a which have high probabilities. Therefore it's no longer so important that each category has documents that contain such a lot of occurrences of such word, or the estimate is more influenced by the background model and the consequences that when we do categorization, such words tend not to influence the decision that much as words that have small probabilities. From the background language model, those words don't get some help from the background language model, so the difference would be primarily because of the differences of the occurrences in the training documents in different categories.	because every category have some help from their background for word , like the , a which have high probability . therefore it be no long so important that each category have document that contain such a lot of occurrence of such word , or the estimate be more influence by the background model and the consequence that when we do categorization , such word tend not to influence the decision that much as word that have small probability . from the background language model , those word do n't get some help from the background language model , so the difference would be primarily because of the difference of the occurrence in the training document in different category .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335175	568	00:22:05.28	00:22:13.66	You also see another smoothing parameter mu here, which controls the amount of smoothing, just like delta does for the other probability.	you also see another smoothing parameter mu here , which control the amount of smoothing , just like delta do for the other probability .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335176	572	00:22:14.26	00:22:23.38	And you can easy to understand why we add mu to the denominator because that represents the sum of all the pseudo counts that we add for all the words.	and you can easy to understand why we add mu to the denominator because that represent the sum of all the pseudo count that we add for all the word .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335182	585	00:22:26.08	00:22:52.45	So mu is also non-negative constant and it's empirically set to control smoothing. There are some interesting special cases to think about as well. First, let's think about when mu approaches Infinity. What would happen? Or in this case, the estimate will approach to the background language model will tend to the background language model, so we would bring every word distribution to the same background language model.	so mu be also non - negative constant and it be empirically set to control smoothing . there be some interesting special case to think about as well . first , let 's think about when mu approach Infinity . what would happen ? or in this case , the estimate will approach to the background language model will tend to the background language model , so we would bring every word distribution to the same background language model .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335183	588	00:22:53.16	00:22:57.96	And that essentially removes the difference between these categories. Obviously we don't want to do that.	and that essentially remove the difference between these category . obviously we do n't want to do that .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335185	594	00:22:58.56	00:23:11.69	The other special cases we think about the background model an suppose we actually set the two uniform distribution and let's say one over the size of the vocabulary. So each word has the same probability.	the other special case we think about the background model an suppose we actually set the two uniform distribution and let 's say one over the size of the vocabulary . so each word have the same probability .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335188	599	00:23:14.22	00:23:25.27	Then this smoothing formula is going to be very similar to the one on the top. When we add Delta because we're going to add a constant pseudo count to every word.	then this smooth formula be go to be very similar to the one on the top . when we add Delta because we be go to add a constant pseudo count to every word .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-12-07 18:59:06.307298	607	00:23:29.43	00:23:47.31	So in general, in naiyes bayes categorization we have to do such smoothing and once we have these probabilities, then we can compute the score for each category for a document and then choose the category with the highest score as we discussed earlier.	so in general , in naiye baye categorization we have to do such smoothing and once we have these probability , then we can compute the score for each category for a document and then choose the category with the high score as we discuss early .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335192	611	00:23:49.1	00:23:59.38	Now it's useful to further understand whether the naive Bayes scoring function actually makes sense, so to understand that.	now it be useful to far understand whether the naive Bayes scoring function actually make sense , so to understand that .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335194	615	00:24:00.09	00:24:09.06	And also to understand why adding a background language model will actually achieve the effect of idea of IDF weighting and to penalize common words.	and also to understand why add a background language model will actually achieve the effect of idea of IDF weighting and to penalize common word .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-30 02:26:31.49143	618	00:24:10.64	00:24:15.04	Right, so it's suppose we have just two categories and we're going to score based on their	right , so it be suppose we have just two category and we be go to score base on their
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-12-07 18:59:42.234832	619	00:24:16.47	00:24:21.57	Ratio of probability, so this is	ratio of probability , so this be
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335197	622	00:24:23.29	00:24:30.84	Ann let's say this is our scoring function for two categories.	Ann let 's say this be our scoring function for two category .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-12-07 18:59:56.827332	624	00:24:32.46	00:24:38.58	So this is a score of a document for these two categories.	so this be a score of a document for these two category .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-12-07 19:00:10.706677	627	00:24:39.96	00:24:46.64	And we're going to score based on this probability ratio. So if the ratio is larger	and we be go to score base on this probability ratio . so if the ratio be large
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335201	631	00:24:47.21	00:24:58.37	then it means it's more likely to be in category one, so the larger the score is, the more likely the document is in category One.	 then it mean it be more likely to be in category one , so the large the score be , the more likely the document be in category one .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335203	634	00:25:00.3	00:25:08.35	So by using bayes rule we can write down this ratio as follows and you have seen this before.	so by use baye rule we can write down this ratio as follow and you have see this before .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335205	638	00:25:09.04	00:25:20.74	Now, we generally take logarithm of this ratio and to avoid small probabilities, and this would then give us this formula in the second line.	now , we generally take logarithm of this ratio and to avoid small probability , and this would then give we this formula in the second line .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335206	642	00:25:21.3	00:25:28.36	And here we see something really interesting, because this is our scoring function for deciding between the two categories.	and here we see something really interesting , because this be our scoring function for decide between the two category .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335207	644	00:25:29.71	00:25:33.49	And if you look at this function, we'll see it has several parts.	and if you look at this function , we 'll see it have several part .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335208	647	00:25:34.7	00:25:40.67	The first part here is actually log of prior probability ratio and so this is the category bias.	the first part here be actually log of prior probability ratio and so this be the category bias .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.33521	652	00:25:41.27	00:25:50.51	So it doesn't really depend on the document, it just says which category is more likely and then would. We would then favor this category slightly.	so it do n't really depend on the document , it just say which category be more likely and then would . we would then favor this category slightly .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335211	654	00:25:53.11	00:25:57.62	So the second part has a sum of all the words.	so the second part have a sum of all the word .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335214	661	00:25:58.65	00:26:10.94	Right, so these are the words that are observed in the document, but in general we can consider all the words in the vocabulary. So here we're going to collect evidence about which category is more likely.	right , so these be the word that be observe in the document , but in general we can consider all the word in the vocabulary . so here we be go to collect evidence about which category be more likely .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-30 02:28:06.573072	664	00:26:11.56	00:26:18.66	So inside the sum you can see there is product of two things. The first is count of the word.	so inside the sum you can see there be product of two thing . the first be count of the word .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335216	666	00:26:20.2	00:26:25.74	And this count of the word serves as a feature and to represent the document.	and this count of the word serve as a feature and to represent the document .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335216	668	00:26:26.95	00:26:29.7	And this is what we can collect from document.	and this be what we can collect from document .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335218	672	00:26:30.26	00:26:38.35	The second part is the weight of this feature. Here it's the weight on each word and this weight.	the second part be the weight of this feature . here it be the weight on each word and this weight .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335218	673	00:26:39.67	00:26:40.56	Tells us.	tell we .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-30 02:29:13.529605	697	00:26:42.28	00:27:34.33	To what extent observing this word helps contributing to our decision to put this document in Category One. I remember the higher the scoring function is more likely it's in category one. Now if you look at this ratio basically or sorry this weight It's basically based on the ratio of the probability of the word from of the two distributions. Essentially we are comparing the probability of the word from the two distributions and if it's higher according to theta one, then according to theta 2 then this weight would be positive and therefore it means when we observe such a word. We'll say that it's more likely to be from category One, and the more we observe such a word, the more likely the document will be classified as theta one.	to what extent observe this word help contribute to our decision to put this document in Category one . I remember the high the scoring function be more likely it be in category one . now if you look at this ratio basically or sorry this weight it be basically base on the ratio of the probability of the word from of the two distribution . essentially we be compare the probability of the word from the two distribution and if it be high accord to theta one , then accord to theta 2 then this weight would be positive and therefore it mean when we observe such a word . we 'll say that it be more likely to be from category one , and the more we observe such a word , the more likely the document will be classify as theta one .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-30 02:30:02.836211	707	00:27:35.06	00:27:56.97	If, on the other hand, the probability of the word from theta one is smaller than the probability of the word from theta 2, then you can see this weight is negative. Therefore this is the negative evidence for supporting category one. That means the more we observe such a word, the more likely the document is actually from theta 2.	if , on the other hand , the probability of the word from theta one be small than the probability of the word from theta 2 , then you can see this weight be negative . therefore this be the negative evidence for support category one . that mean the more we observe such a word , the more likely the document be actually from theta 2 .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335234	712	00:27:58.13	00:28:08.93	So this formula now makes a lot of sense, so we're going to aggregate all the evidence from the document. We take a sum over all the words we can call this the features.	so this formula now make a lot of sense , so we be go to aggregate all the evidence from the document . we take a sum over all the word we can call this the feature .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335238	721	00:28:09.68	00:28:30.49	That we collect from the document that would help us make the decision and that each feature has a weight that tells us how does this feature support category one or support that support the category two, and this is estimated as the log of probability ratio. Here in naive Bayes.	that we collect from the document that would help we make the decision and that each feature have a weight that tell we how do this feature support category one or support that support the category two , and this be estimate as the log of probability ratio . here in naive baye .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-12-07 19:04:49.7331	731	00:28:31.76	00:28:57.64	And then finally we have this constant of bias here, so that formula actually is a formula that can be generalized to accommodate more features. And that's why I've introduced some other symbols here. So introduce the beta zero to denote the bias and Fi to denote each feature, and then beta sub i, denoted. the weight on which feature.	and then finally we have this constant of bias here , so that formula actually be a formula that can be generalize to accommodate more feature . and that be why I 've introduce some other symbol here . so introduce the beta zero to denote the bias and fi to denote each feature , and then beta sub I , denote . the weight on which feature .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335245	742	00:28:58.33	00:29:25.8	Now if we do this generalization, what we see is that in general we can represent the document by feature vector F, FI here. Of course in this case FI is the count of a word, but in general we can put any features that we think are relevant for categorization. For example document length or the font size or counts of other patterns in the document.	now if we do this generalization , what we see be that in general we can represent the document by feature vector F , FI here . of course in this case FI be the count of a word , but in general we can put any feature that we think be relevant for categorization . for example document length or the font size or count of other pattern in the document .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335247	746	00:29:26.55	00:29:39.94	And then ouurscoring function can be defined as a sum of constant beta zero and sum of the feature weights over all the features.	and then ouurscore function can be define as a sum of constant beta zero and sum of the feature weight over all the feature .
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-30 02:32:38.891959	753	00:29:41.99	00:29:55.49	So if HF sub I is a feature value then we multiply value by the corresponding weight beta sub i and we just take sum and this is the aggregate. All evidence that we can collect from all these features. And of course there are parameter	so if HF sub I be a feature value then we multiply value by the corresponding weight beta sub I and we just take sum and this be the aggregate . all evidence that we can collect from all these feature . and of course there be parameter
6962b043-7dd8-4050-bad0-bbdb13e2c302	2020-11-02 23:17:20.335254	767	00:30:02.56	00:30:31.27	These betas are the weights, and with appropriate settings of weights then we can expect the such a scoring function to work well to classify documents. Just like in the case of Naive Bayes we can clearly see naive Bayes classifier is a special case of this general classifier. Actually, this general form is very close to a classifier called logistical regression, and this is actually one of those conditional approaches or discriminative approaches to classification.	these beta be the weight , and with appropriate setting of weight then we can expect the such a scoring function to work well to classify document . just like in the case of Naive Bayes we can clearly see naive Bayes classifier be a special case of this general classifier . actually , this general form be very close to a classifier call logistical regression , and this be actually one of those conditional approach or discriminative approach to classification .
6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb	2020-11-02 23:15:15.772359	3	00:00:00.3	00:00:05.63	This lecture is about the probabilistic topic models for topic mining and analysis.	this lecture be about the probabilistic topic model for topic mining and analysis .
6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb	2020-11-24 05:41:05.332155	5	00:00:12.71	00:00:16.85	In this lecture we're going to continue talking about the top mining and analysis.	in this lecture we be go to continue talk about the top mining and analysis .
6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb	2020-11-02 23:15:15.77236	7	00:00:18.07	00:00:20.58	We're going to introduce probabilistic topic models.	we be go to introduce probabilistic topic model .
6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb	2020-11-02 23:15:15.772365	27	00:00:22.31	00:01:14.81	So this is a slide that you have seen earlier where we discussed the problems with using a term as a topic. So to solve these problems intuitively we need to use more words to describe the topic and this would address the problem of lack of expressive power. When we have more words that we can use to describe the topic, we can describe complicated topics, to address the second problem, we need to introduce weights of words. This would allow you to distinguish subtle differences in topics and to introduce semantically related words in the fuzzy manner. Finally, to solve the problem of word ambiguity, we need to split an ambiguous word so that we can disambiguate its topic.	so this be a slide that you have see early where we discuss the problem with use a term as a topic . so to solve these problem intuitively we need to use more word to describe the topic and this would address the problem of lack of expressive power . when we have more word that we can use to describe the topic , we can describe complicated topic , to address the second problem , we need to introduce weight of word . this would allow you to distinguish subtle difference in topic and to introduce semantically relate word in the fuzzy manner . finally , to solve the problem of word ambiguity , we need to split an ambiguous word so that we can disambiguate its topic .
6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb	2020-11-02 23:15:15.772368	39	00:01:15.6	00:01:39.97	It turns out that all these can be done by using a probabilistic topic model, and that's why we're going to spend a lot of lectures to talk about this topic. So the basic idea here is improved representation of topic as a word distribution. So what you see now is the old representation, where we represent each topic with just one word or one term or one phrase.	it turn out that all these can be do by use a probabilistic topic model , and that be why we be go to spend a lot of lecture to talk about this topic . so the basic idea here be improve representation of topic as a word distribution . so what you see now be the old representation , where we represent each topic with just one word or one term or one phrase .
6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb	2020-11-24 06:58:18.007698	45	00:01:40.64	00:01:53.35	But now we're going to use a word distribution to describe the topic. So here you see that for sports, we're going to use a word distribution over theoretical speaking all the words in our vocabulary.	but now we be go to use a word distribution to describe the topic . so here you see that for sport , we be go to use a word distribution over theoretical speak all the word in our vocabulary .
6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb	2020-11-02 23:15:15.772371	55	00:01:54.34	00:02:17.41	So for example, the high probability words here are sports, game, basketball, football, play, star, etc. These are sports-related terms and of course it would also give a non zero probability to some other words like """travel"" which might be related to" sports. But in general not so much related to the topic.	so for example , the high probability word here be sport , game , basketball , football , play , star , etc . these be sport - relate term and of course it would also give a non zero probability to some other word like " " " travel " " which might be relate to " sport . but in general not so much related to the topic .
6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb	2020-11-02 23:15:15.772373	62	00:02:18.76	00:02:33.88	In general, we can imagine a non zero probability for all the words and some words that are not relevant would have very very small probabilities and these probabilities will sum to one. So that it forms a distribution of all the words.	in general , we can imagine a non zero probability for all the word and some word that be not relevant would have very very small probability and these probability will sum to one . so that it form a distribution of all the word .
6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb	2020-11-02 23:15:15.772374	66	00:02:36.53	00:02:46.85	Now intuitively, this distribution represents a topic in that if we sample words from the distribution, we tend to see words that already do sports.	now intuitively , this distribution represent a topic in that if we sample word from the distribution , we tend to see word that already do sport .
6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb	2020-11-02 23:15:15.772375	73	00:02:48.35	00:03:03.51	You can also see it as a very special case if the probability mass is concentrated entire of just one word. Let's sports, and this basically degenerates to the simple representation of topic with just one word.	you can also see it as a very special case if the probability mass be concentrated entire of just one word . let 's sport , and this basically degenerate to the simple representation of topic with just one word .
6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb	2020-12-10 19:24:09.748728	84	00:03:04.57	00:03:30.98	But as a distribution, this topic representation can in general involve many words to describe the topic and can model subtle differences in semantics of the topic. Similarly, we can model travel and science with their respective distributions. So in the distribution for travel we "see top words like ""attraction, trip," "flight, hotel etc."""	but as a distribution , this topic representation can in general involve many word to describe the topic and can model subtle difference in semantic of the topic . similarly , we can model travel and science with their respective distribution . so in the distribution for travel we " see top word like " " attraction , trip , " " flight , hotel etc . " " "
6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb	2020-11-02 23:15:15.772381	95	00:03:31.59	00:03:56.66	"Whereas in science, we see ""scientist," "spaceship, telescope or genomics"" and new" science-related terms, now, that doesn't mean sports-related terms necessary have zero probabilities for science in general, we can imagine all these words. We have non zero probabilities, it's just that for a particular topic of some words we have very very small probabilities.	" whereas in science , we see " " scientist , " " spaceship , telescope or genomic " " and new " science - relate term , now , that do n't mean sport - relate term  necessary have zero probability for science in general , we can imagine all these word . we have non zero probability , it be just that for a particular topic of some word we have very very small probability .
6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb	2020-11-02 23:15:15.772384	106	00:03:58.09	00:04:18.57	Now you can also see there are some words that are shared by these topics. Well, when I say shared, that just means even with some probability threshold you can still see one word to occur in multiple topics. In this case I marked them in black so "you can see ""travel""," for example, occured in all the three topics here, but with different probabilities.	now you can also see there be some word that be share by these topic . well , when I say share , that just mean even with some probability threshold you can still see one word to occur in multiple topic . in this case I mark they in black so " you can see " " travel " " , " for example , occur in all the three topic here , but with different probability .
6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb	2020-11-02 23:15:15.772385	110	00:04:19.29	00:04:28.74	It has the highest probability for the travel topic 0.05. But with much smaller probabilities for sports and science, which makes sense.	it have the high probability for the travel topic 0.05 . but with much small probability for sport and science , which make sense .
6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb	2020-11-02 23:15:15.772392	141	00:04:29.29	00:05:37.88	"And similarly you can see ""star"" also" occurred in sports and science with reasonably high probabilities, because they might be actually related to the two topics. So with this representation is addresses the three problems that mentioned earlier. First, it now uses multiple words that describe topic, so it allows us to describe fairly complicated topics. Second, it assigns weights to terms, so now we can model several differences of semantics and you can bring in related words together to model topic. Third, because we have probabilities for the same word in different topics. We can disambiguate the sense of word in the text to decode its underlying topic, so we address all these three problems with this new way of representing a topic. So now, of course, our problem definition has been refined just slightly. The slide is very similar to what you have seen before, except that we have added refinement for what the topic is. So now each topic is word distribution.	" and similarly you can see " " star " " also " occur in sport and science with reasonably high probability , because they might be actually relate to the two topic . so with this representation be address the three problem that mention early . first , it now use multiple word that describe topic , so it allow we to describe fairly complicated topic . second , it assign weight to term , so now we can model several difference of semantic and you can bring in related word together to model topic . third , because we have probability for the same word in different topic . we can disambiguate the sense of word in the text to decode its underlie topic , so we address all these three problem with this new way of represent a topic . so now , of course , our problem definition have be refine just slightly . the slide be very similar to what you have see before , except that we have add refinement for what the topic be . so now each topic be word distribution .
6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb	2020-11-02 23:15:15.772395	150	00:05:38.85	00:05:58.35	And for each word distribution, we know that all the probabilities should sum to one over all the words in the vocabulary. So you see a constraint here and we still have another constraint on the topic coverage, namely pis. So all the pis of IGS must sum to one for the same document.	and for each word distribution , we know that all the probability should sum to one over all the word in the vocabulary . so you see a constraint here and we still have another constraint on the topic coverage , namely pis . so all the pis of IGS must sum to one for the same document .
6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb	2020-11-02 23:15:15.772399	169	00:05:59.5	00:06:41.72	So how do we solve this problem? Well, let's look at this problem as a computation problem now. So we clearly specify the input and output as illustrated here on this side. The input, of course is our text data C is the collection, but we also generally assume we know the number of topics K or we hypothesize a number and then try to mine K topics, even though we don't know the exact topics that exist in the collection and these vocabulary set. As a set of words that determines what units would be treated as the basic units for analysis. In most cases, we use words as the basis.	so how do we solve this problem ? well , let 's look at this problem as a computation problem now . so we clearly specify the input and output as illustrate here on this side . the input , of course be our text datum c be the collection , but we also generally assume we know the number of topic k or we hypothesize a number and then try to mine K topic , even though we do n't know the exact topic that exist in the collection and these vocabulary set . as a set of word that determine what unit would be treat as the basic unit for analysis . in most case , we use word as the basis .
6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb	2020-11-24 05:50:21.138815	175	00:06:43.34	00:06:55.46	For analysis, and that means each word is a unit. Now the output would consist of as first a set of topics represented by Theta i's Each theta_i is a word distribution.	for analysis , and that mean each word be a unit . now the output would consist of as first a set of topic represent by Theta I be each theta_i be a word distribution .
6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb	2020-11-24 05:50:27.266782	176	00:06:56.31	00:06:57.06	And	and
6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb	2020-11-02 23:15:15.772402	183	00:06:58.59	00:07:17.08	We also want to know the coverage of topics in each document so that that's the same pi_ij's that we have seen before. So given a set of text data, we would like to compute all these distributions and all these coverages, as you have seen on this slide.	we also want to know the coverage of topic in each document so that that be the same pi_ij 's that we have see before . so give a set of text datum , we would like to compute all these distribution and all these coverage , as you have see on this slide .
6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb	2020-11-02 23:15:15.772405	196	00:07:18	00:07:48.84	Now of course, there may be many different ways of solving this problem. Indeed, you can write a heuristic program to solve this problem, but here we're going to introduce a general way of solving this problem called generative model, and this is in fact very general idea, and it's a principle way of using statistical modeling to solve text mining problems, and here I dim the picture that you have seen before in order to show the generation process.	now of course , there may be many different way of solve this problem . indeed , you can write a heuristic program to solve this problem , but here we be go to introduce a general way of solve this problem call generative model , and this be in fact very general idea , and it be a principle way of use statistical modeling to solve text mining problem , and here I dim the picture that you have see before in order to show the generation process .
6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb	2020-12-10 19:35:10.833101	227	00:07:49.39	00:08:56.449999	So the idea of this approach is actually to 1st design a model for our data. So we design a probabilistic model to model how the data are generated. Of course this is based on our assumption. The actual data aren't necessary generating this way, so that would give us a probability distribution of the data that you are seeing on this slide given a particular model and parameters that are denoted by Lambda. So this capital lambda actually consists of all the parameters that we're interested in. And these parameters in general, we control the behavior of the probabilistic model, meaning that if you set these parameters for different values, it will give some data points higher probabilities than others. Now in this case, of course, for our tax mining problem, or more precisely topic mining problem, we have the following parameters. First, we have theta_i's Each is a word distribution and then we have a set of pi's for each document. And since we have N documents so we have N sets of pis.	so the idea of this approach be actually to 1st design a model for our datum . so we design a probabilistic model to model how the datum be generate . of course this be base on our assumption . the actual datum be n't necessary generate this way , so that would give we a probability distribution of the datum that you be see on this slide give a particular model and parameter that be denote by Lambda . so this capital lambda actually consist of all the parameter that we be interested in . and these parameter in general , we control the behavior of the probabilistic model , mean that if you set these parameter for different value , it will give some datum point high probability than other . now in this case , of course , for our tax mining problem , or more precisely topic mining problem , we have the follow parameter . first , we have theta_i 's each be a word distribution and then we have a set of pi 's for each document . and since we have n document so we have n set of pis .
6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb	2020-12-10 19:36:08.961068	248	00:08:57.41	00:09:50.65	And each set of the pi values will sum to one. So this is to say that we first pretend we already have these word distributions and coverage numbers, and then we're going to see how we can generate data by using such distributions. So how do we model the data in this way? And we assume that data are actually samples drawn from such a model that depends on these parameters. Now one interesting question here is to think about how many parameters are there in total. Now obviously we can already see N * K parameters for pi's. We also see K theta_i's, but each theta_i is actually a set of probability values. Right? it's a distribution over words.	and each set of the pi value will sum to one . so this be to say that we first pretend we already have these word distribution and coverage number , and then we be go to see how we can generate datum by use such distribution . so how do we model the datum in this way ? and we assume that datum be actually sample draw from such a model that depend on these parameter . now one interesting question here be to think about how many parameter be there in total . now obviously we can already see N * k parameter for pi 's . we also see K theta_i 's , but each theta_i be actually a set of probability value . right ? it be a distribution over word .
6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb	2020-11-02 23:15:15.772417	251	00:09:51.48	00:09:58.98	So I leave this as exercise for you to figure out exactly how many parameters there are here.	so I leave this as exercise for you to figure out exactly how many parameter there be here .
6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb	2020-11-02 23:15:15.772419	259	00:09:59.83	00:10:19.58	Now, once we set up with a model, then we can fit the model to our data, meaning that we can estimate the parameters or infer the parameters based on the data. In other words, we would like to adjust these parameter values until we give our data set the maximum probability.	now , once we set up with a model , then we can fit the model to our datum , mean that we can estimate the parameter or infer the parameter base on the datum . in other word , we would like to adjust these parameter value until we give our datum set the maximum probability .
6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb	2020-11-02 23:15:15.772424	282	00:10:20.18	00:11:09.5	I just say that depending on the parameter values, some data points will have higher probabilities than others. What we're interested in here is what parameter values will give our data set the highest probability. So I also illustrate the problem with the picture that you see here. On the X axis, I just illustrate the Lambda, the parameters as one dimensional variable. It's oversimplification obviously, but it suffices is to show the idea and the Y axis shows the probability of the data observed this probability obviously depends on the setting of Lambda, so that's why it varies as you change the value of Lambda. What we're interested in here is to find the Lambda star that would maximize the probability of the observed data.	I just say that depend on the parameter value , some data point will have high probability than other . what we be interested in here be what parameter value will give our datum set the high probability . so I also illustrate the problem with the picture that you see here . on the x axis , I just illustrate the Lambda , the parameter as one dimensional variable . it be oversimplification obviously , but it suffice be to show the idea and the Y axis show the probability of the datum observe this probability obviously depend on the setting of Lambda , so that be why it vary as you change the value of Lambda . what we be interested in here be to find the Lambda star that would maximize the probability of the observed datum .
6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb	2020-11-02 23:15:15.77243	310	00:11:10.34	00:12:12.72	So this would be then our estimate of the parameters and these parameters note that are precisely what we hope to discover from text data, so would treat these parameters as actually the outcome or the output of the data mining algorithm. So this is a general idea of using a generative model for text mining. First we design a model with some parameters that we are interested in, and then we model the data. We adjust the parameters to fit the data as well as we can. After we have fitted data then we will recover some parameter values will get this specific parameter values and those would be the output of the algorithm and we treat those as actually the discovered knowledge from text data. By varying the model, of course we can discover different knowledge. So to summarize, we introduced a new way of representing a topic, namely represented as word distribution, and this has advantage of using multiple words to describe a complicated topic.	so this would be then our estimate of the parameter and these parameter note that be precisely what we hope to discover from text datum , so would treat these parameter as actually the outcome or the output of the datum mining algorithm . so this be a general idea of use a generative model for text mining . first we design a model with some parameter that we be interested in , and then we model the datum . we adjust the parameter to fit the datum as well as we can . after we have fit datum then we will recover some parameter value will get this specific parameter value and those would be the output of the algorithm and we treat those as actually the discover knowledge from text datum . by vary the model , of course we can discover different knowledge . so to summarize , we introduce a new way of represent a topic , namely represent as word distribution , and this have advantage of use multiple word to describe a complicate topic .
6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb	2020-11-24 05:58:07.311956	320	00:12:13.38	00:12:34.59	It also allows us to assign weights on words so we can model subtle variations of semantics. We talked about the task of topic mining and analysis when we define a topic as a distribution, so the input is a collection of text articles. The number of topics and vocabulary set and the output is a set of topics. Each is word distribution.	it also allow we to assign weight on word so we can model subtle variation of semantic . we talk about the task of topic mining and analysis when we define a topic as a distribution , so the input be a collection of text article . the number of topic and vocabulary set and the output be a set of topic . each be word distribution .
6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb	2020-11-02 23:15:15.772437	345	00:12:35.2	00:13:35.9	And also the coverage of all the topics in each document and these are formally represented by theta_i's and pi_i's an we have two constraints here for these parameters. The first is the constraints on the word distributions. In each world distribution, the probabilities on all the words must sum to one over all the words in the vocabulary. The second constraint is on the topic coverage in each document. A document is not allowed to cover a topic outside the set of topics that we are discovering. So the coverage of each of these K topics would sum to one for a document. We also introduce the general idea of using a generative model for text mining and the idea here is to first design a model to model the generation of data. We simply assume that they are generated this way and inside the model we embed some parameters that were interested in denoted by Lambda.	and also the coverage of all the topic in each document and these be formally represent by theta_i 's and pi_i 's an we have two constraint here for these parameter . the first be the constraint on the word distribution . in each world distribution , the probability on all the word must sum to one over all the word in the vocabulary . the second constraint be on the topic coverage in each document . a document be not allow to cover a topic outside the set of topic that we be discover . so the coverage of each of these K topic would sum to one for a document . we also introduce the general idea of use a generative model for text mining and the idea here be to first design a model to model the generation of datum . we simply assume that they be generate this way and inside the model we embed some parameter that be interested in denote by Lambda .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501041	2	00:00:00.3	00:00:03.71	This lecture is the first one about the text clustering.	this lecture be the first one about the text clustering .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501042	4	00:00:18.32	00:00:23.76	This is very important that technique for doing topic mining an analysis.	this be very important that technique for do topic mining an analysis .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-12-07 20:36:22.276295	7	00:00:24.48	00:00:30.52	In particular, in this lecture organ to start with some basic questions about the clustering: What is text clustering and why we are interested in text clustering?	in particular , in this lecture organ to start with some basic question about the clustering : what be text clustering and why we be interested in text clustering ?
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501046	11	00:00:37.94	00:00:44.92	In the following lectures, we're going to talk about how to do text clustering and how to evaluate the clustering results.	in the follow lecture , we be go to talk about how to do text clustering and how to evaluate the clustering result .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-25 18:23:15.027787	12	00:00:46.47	00:00:48.07	"So what is text	" so what be text
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-25 18:23:20.429861	16	00:00:49.42	00:00:55.78	Clustering actually is a very general technique for data mining. As you might have learned in some other courses.	clustering actually be a very general technique for datum mining . as you might have learn in some other course .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.50105	18	00:00:56.69	00:01:00.44	The idea is to discover natural structures in the data.	the idea be to discover natural structure in the datum .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501053	24	00:01:01.04	00:01:16.26	In other words, we want to group similar objects together. In our case, these objects are of course texture objects. For example, they can be documents, turns, passages, sentences or websites.	in other word , we want to group similar object together . in our case , these object be of course texture object . for example , they can be document , turn , passage , sentence or website .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501057	30	00:01:17.44	00:01:31.77	And then our goal is to group similar texture objects together. So let's see a example here. You don't really see text objects, but I just use some shapes to denote objects that can be grouped together.	and then our goal be to group similar texture object together . so let 's see a example here . you do n't really see text object , but I just use some shape to denote object that can be group together .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501061	36	00:01:33.34	00:01:51.84	Now, if I ask you what are some natural structures or natural groups well you, if you look at it, you might agree that we can group these objects based on shapes or their locations on this 2 dimensional space.	now , if I ask you what be some natural structure or natural group well you , if you look at it , you might agree that we can group these object base on shape or their location on this 2 dimensional space .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501062	38	00:01:52.83	00:01:54.95	So we got the three clusters in this case.	so we get the three cluster in this case .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501064	42	00:01:56.85	00:02:06.26	And then may not be so much disagreement about these three clusters, but it really depends on the perspective to look at the objects.	and then may not be so much disagreement about these three cluster , but it really depend on the perspective to look at the object .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501068	49	00:02:07.54	00:02:27.59	Maybe some of you have also seen it in a different way, so we might get different clusters. And you will see another example about this ambiguity more clearly, but the main point here is the problem is actually not so well defined.	maybe some of you have also see it in a different way , so we might get different cluster . and you will see another example about this ambiguity more clearly , but the main point here be the problem be actually not so well define .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.50107	52	00:02:29.09	00:02:36.3	And the problem lies in how to define similarity. What do you mean by similar objects?	and the problem lie in how to define similarity . what do you mean by similar object ?
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.50107	53	00:02:38.02	00:02:39.45	Now this problem.	now this problem .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501071	55	00:02:40	00:02:44.75	Has to be clearly defined in order to have well defined clustering problem.	have to be clearly define in order to have well define clustering problem .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501074	59	00:02:46.18	00:02:54.71	And the problem is in general that any two objects can be similar that depending on how you look at them. So for example.	and the problem be in general that any two object can be similar that depend on how you look at they . so for example .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501075	61	00:02:55.26	00:02:58.26	Let's look at the two words like car and horse.	let 's look at the two word like car and horse .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-12-07 20:38:27.379426	62	00:03:00.1	00:03:03.24	So are the two words similar	so be the two word similar
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501077	64	00:03:04.52	00:03:09.16	It depends on how you look at it. If you look at the physical.	it depend on how you look at it . if you look at the physical .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501078	66	00:03:10	00:03:14.84	Physical properties of car and horse. They are very different.	physical property of car and horse . they be very different .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501084	76	00:03:15.82	00:03:43.29	But if you look at the them functionally, a car in the horse can both be transportation tool, so in that sense they may be similar. So as you can see, it really depends on our perspective to look at the objects and so in order to make the clustering problem well defined, a user must define the perspective. For assessing similarity.	but if you look at the they functionally , a car in the horse can both be transportation tool , so in that sense they may be similar . so as you can see , it really depend on our perspective to look at the object and so in order to make the clustering problem well define , a user must define the perspective . for assess similarity .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501086	78	00:03:44.2	00:03:47.66	And we call this perspective the clustering bias.	and we call this perspective the clustering bias .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501089	84	00:03:49.15	00:04:06.93	And when you define a clustering problem, it's important to specify your perspective for similarity or for defining the similarity that would be used to group similar objects 'cause otherwise.	and when you define a clustering problem , it be important to specify your perspective for similarity or for define the similarity that would be use to group similar object 'cause otherwise .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.50109	87	00:04:08.54	00:04:15.65	Similarity is not well defined. An one can have different ways to group objects.	Similarity be not well define . an one can have different way to group object .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501092	91	00:04:16.76	00:04:27.94	So let's look at a concrete example. Here you are seeing some objects or some shapes that are very similar to what you have seen on the 1st slide.	so let 's look at a concrete example . here you be see some object or some shape that be very similar to what you have see on the 1st slide .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501095	93	00:04:28.6	00:04:34	But if I ask you to group these objects again, you might.	but if I ask you to group these object again , you might .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501097	97	00:04:36.57	00:04:43.99	Might. Feel there's more uncertainty here than on the previous slide. For example.	might . feel there be more uncertainty here than on the previous slide . for example .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501098	100	00:04:45.02	00:04:52.41	You might think, well, we can still group by shapes, so that would give us cluster that looks like this.	you might think , well , we can still group by shape , so that would give we cluster that look like this .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501098	101	00:04:53.4	00:04:56.62	However, you might also feel that.	however , you might also feel that .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501105	115	00:04:57.4	00:05:32.07	Maybe the objects can be grouped based on the sizes, so that would give us a different way to cluster the data. If we look at the size and look at the similarity in size. So as you can see clearly here, depending on the perspective will get different clustering results, so that also clearly tells us that in order to evaluate the clustering result we must use perspective. Without perspective, it's very hard to define what is the best clustering result.	maybe the object can be group base on the size , so that would give we a different way to cluster the datum . if we look at the size and look at the similarity in size . so as you can see clearly here , depend on the perspective will get different clustering result , so that also clearly tell we that in order to evaluate the clustering result we must use perspective . without perspective , it be very hard to define what be the good clustering result .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501106	117	00:05:36.19	00:05:39.04	So there are many examples of text clustering.	so there be many example of text clustering .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501106	118	00:05:40.46	00:05:41.17	Set up.	set up .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501109	123	00:05:42.14	00:05:51.42	And so, for example, we can cluster documents in the whole text collection. So in this case documents are the units to be clustered.	and so , for example , we can cluster document in the whole text collection . so in this case document be the unit to be cluster .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.50111	126	00:05:52.2	00:05:57.63	We may be able to cluster terms in this case. Terms are objects.	we may be able to cluster term in this case . term be object .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501114	134	00:05:58.2	00:06:17.72	And Cluster of terms can be used to define the concept or theme or topic. In fact, the topic models that you have seen some previous lectures. Can give you cluster of terms in some sense. If you take the terms with high probabilities from world distribution.	and Cluster of term can be use to define the concept or theme or topic . in fact , the topic model that you have see some previous lecture . can give you cluster of term in some sense . if you take the term with high probability from world distribution .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-25 18:31:42.545084	139	00:06:19.5	00:06:31.03	Another example is to just a cluster any texts segments, for example passages, sentences or any segments that you can extract the from a large text objects.	another example be to just a cluster any text segment , for example passage , sentence or any segment that you can extract the from a large text object .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501125	147	00:06:32.01	00:06:55.82	For example, we might extract all the text segments about the topic, let's say by using a topic model. Now, once we've got those text objects, then we can cluster. The segments that we've got to discover interesting clusters that might also represent the subtopics.	for example , we might extract all the text segment about the topic , let 's say by use a topic model . now , once we 've get those text object , then we can cluster . the segment that we 've get to discover interesting cluster that might also represent the subtopic .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501131	161	00:06:56.82	00:07:30.29	So this is a case of combining text clustering with some other techniques, and in general you will see a lot of text mining algorithms can be actually combined in a flexible way to achieve. The goal of doing more sophisticated mining and analysis of text data. We can also cluster fairly large text law gets, and by that I just mean text objects may contain a lot of documents. So for example we might cluster websites. Each website is actually composed of multiple documents.	so this be a case of combine text cluster with some other technique , and in general you will see a lot of text mining algorithm can be actually combine in a flexible way to achieve . the goal of do more sophisticated mining and analysis of text datum . we can also cluster fairly large text law get , and by that I just mean text object may contain a lot of document . so for example we might cluster website . each website be actually compose of multiple document .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501133	164	00:07:31.27	00:07:36.17	Similarly, we can also cluster articles written by the same author, for example.	similarly , we can also cluster article write by the same author , for example .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501135	170	00:07:37.94	00:07:51.93	So we can treat all the articles published by author as one unit for Clustering. In this way, we might group authors together based on whether they are published papers or similar.	so we can treat all the article publish by author as one unit for Clustering . in this way , we might group author together base on whether they be publish paper or similar .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501138	175	00:07:55.04	00:08:06.46	Furthermore, text clusters can also be further clustered. Regenerate the hierarchy that that's 'cause we can in general, cluster any text object at different levels.	furthermore , text cluster can also be far cluster . regenerate the hierarchy that that be 'cause we can in general , cluster any text object at different level .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.50114	180	00:08:08.13	00:08:18.87	So more generally, why is text clustering interesting? Well, it's brcause it's a very useful technique for text mining, particularly exploratory text analysis.	so more generally , why be text clustering interesting ? well , it be brcause it be a very useful technique for text mining , particularly exploratory text analysis .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501145	193	00:08:20.16	00:08:47.86	And so a typical scenario is that you are getting a lot of text data. Let's say all the email messages from customers in some time period, or all the literature, articles, etc. And then you hope to get the sense about what are the overall content of the collection. So, for example, you might be interested in getting. A sense about the major topics or what are some typical or representative document in the collection?	and so a typical scenario be that you be get a lot of text datum . let 's say all the email message from customer in some time period , or all the literature , article , etc . and then you hope to get the sense about what be the overall content of the collection . so , for example , you might be interested in get . a sense about the major topic or what be some typical or representative document in the collection ?
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501147	197	00:08:48.56	00:08:57.64	And clustering help us achieve this goal. We sometimes also want to link similar text objects together and these.	and clustering help we achieve this goal . we sometimes also want to link similar text object together and these .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501151	206	00:08:59.9	00:09:17.389999	These objects might be duplicated content for example, and in that case such a technique can help us remove redundancy, removing duplicated documents. Sometimes they are about the same topic and by linking them together we can have more complete coverage of the topic.	these object might be duplicate content for example , and in that case such a technique can help we remove redundancy , remove duplicate document . sometimes they be about the same topic and by link they together we can have more complete coverage of the topic .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501153	211	00:09:19.59	00:09:29.45	We may also use text the clustering to create a structure on the text data, and sometimes we can create a hierarchy of structures and this is very useful for browsing.	we may also use text the clustering to create a structure on the text datum , and sometimes we can create a hierarchy of structure and this be very useful for browse .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-02 23:18:04.501159	224	00:09:31.19	00:09:58.06	We may also use text clustering to induce additional features to represent text data when we cluster documents together, we can treat each cluster as a feature and then we can say when a document is in this cluster and then the feature value would be one and if a document is not in this cluster, then the future value is zero and this helps provide additional discrimination that might be used for texture classification as we will discuss later.	we may also use text clustering to induce additional feature to represent text datum when we cluster document together , we can treat each cluster as a feature and then we can say when a document be in this cluster and then the feature value would be one and if a document be not in this cluster , then the future value be zero and this help provide additional discrimination that might be use for texture classification as we will discuss later .
6a9b1334-5f53-407b-8864-2cff7edbc603	2020-11-25 18:34:35.051548	238	00:09:59.77	00:10:26.8	So there are in general many applications of text clustering any. I just saw it with two very specific ones. One is to cluster search results for example and You can imagine a search engine can cluster the search results so that user can see overall structure of those. Results returned for a query. And when the query is ambiguous, this is particularly useful. Becausw clusters likely represent different senses of ambiguous word.	so there be in general many application of text cluster any . I just see it with two very specific one . one be to cluster search result for example and you can imagine a search engine can cluster the search result so that user can see overall structure of those . result return for a query . and when the query be ambiguous , this be particularly useful . Becausw cluster likely represent different sense of ambiguous word .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164909	2	00:00:00.3	00:00:03.12	This lecture is a continued discussion of.	this lecture be a continue discussion of .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.16491	4	00:00:10.87	00:00:13.51	Discriminative classifiers for text categorization.	discriminative classifier for text categorization .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164911	11	00:00:15.66	00:00:29	So in this lecture will introduce yet another discriminative classifier called a support vector machine or VM, which is a very popular classification method, and there has been also shown to be effective for text categorization.	so in this lecture will introduce yet another discriminative classifier call a support vector machine or VM , which be a very popular classification method , and there have be also show to be effective for text categorization .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164912	16	00:00:31.18	00:00:42.54	So to introduce this classifier, let's also think about the simple case of two categories and we have two public categories, season one and Season 2 here.	so to introduce this classifier , let 's also think about the simple case of two category and we have two public category , season one and Season 2 here .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164912	20	00:00:43.16	00:00:52.05	An we want to classify documents into these two categories and we're going to represent again a document by a feature vector X here.	an we want to classify document into these two category and we be go to represent again a document by a feature vector x here .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164915	34	00:00:53.05	00:01:27.71	Now the idea of this classifier is do design. Also a linear separator. Here that you see and it's very similar to what you have seen or just for logistic regression. And we're going to also say that if the sign of this function value is positive, then we're going to say the object is in Category 1. Otherwise, we're going to say it's in Category 2, so that makes 0 value. The decision boundary between two categories.	now the idea of this classifier be do design . also a linear separator . here that you see and it be very similar to what you have see or just for logistic regression . and we be go to also say that if the sign of this function value be positive , then we be go to say the object be in Category 1 . otherwise , we be go to say it be in category 2 , so that make 0 value . the decision boundary between two category .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164915	37	00:01:28.68	00:01:37.15	So in general in high dimensional space such a zero point corresponds to a hyperplane.	so in general in high dimensional space such a zero point correspond to a hyperplane .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164916	42	00:01:38.04	00:01:52.06	I show you a simple case of two dimensional space with just X1 and X2. In this case this corresponds to a line that you can see here. So this is.	I show you a simple case of two dimensional space with just x1 and x2 . in this case this correspond to a line that you can see here . so this be .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-12-20 21:19:12.99532	44	00:01:53.41	00:02:01.16	A line defined by just three parameters "here beta0	a line define by just three parameter " here beta0
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164916	45	00:02:02.24	00:02:03.69	Now this line.	now this line .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164917	51	00:02:04.97	00:02:18.82	Is the heading in this direction, so it shows that as we increase X1, X2 will also increase. So know that beta1 and beta2 have different signs or one is negative and there is positive.	be the heading in this direction , so it show that as we increase X1 , X2 will also increase . so know that beta1 and beta2 have different sign or one be negative and there be positive .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-12-20 21:20:13.701357	53	00:02:19.62	00:02:26.88	I so let's just assume that beta one is negative and beta two is positive.	I so let 's just assume that beta one be negative and beta two be positive .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164918	59	00:02:28.66	00:02:41.84	Now it's interesting to examine then the data instances on the two sides of this line, so here that there are incidences are visualized as circles for one class and diamonds for the other class.	now it be interesting to examine then the datum instance on the two side of this line , so here that there be incidence be visualize as circle for one class and diamond for the other class .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164919	63	00:02:42.99	00:02:54.18	Now one question is to take a point like this one and to ask the question what's the value of this expression or this classifier for this data point.	now one question be to take a point like this one and to ask the question what be the value of this expression or this classifier for this data point .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-12-20 21:20:46.113746	66	00:02:55.2	00:03:00.8	So what do you think? Basically working to evaluate its value by using this function.	so what do you think ? basically work to evaluate its value by use this function .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164921	78	00:03:01.58	00:03:25.52	And as we said, if this value is positive we're gonna say this is in category one, and if it's negative it's going to be in category Two. Intuitively, this line separates these two categories, so we expect the points on one side would be positive and points on the other side would be negative. Or the question is under the assumption that I just mentioned, let's examine a particular point like this one.	and as we say , if this value be positive we be gon na say this be in category one , and if it be negative it be go to be in category two . intuitively , this line separate these two category , so we expect the point on one side would be positive and point on the other side would be negative . or the question be under the assumption that I just mention , let 's examine a particular point like this one .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164922	80	00:03:26.55	00:03:29.72	So what do you think is the sign of this expression?	so what do you think be the sign of this expression ?
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164922	85	00:03:31.49	00:03:47.19	To examine the sign, we can simply look at this expression. Here we can compare this with, let's say, value on the line. Let's say compare this with this point.	to examine the sign , we can simply look at this expression . here we can compare this with , let 's say , value on the line . let 's say compare this with this point .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164923	87	00:03:48.32	00:03:53.82	They have identical X one, but then one has a higher value for its too.	they have identical x one , but then one have a high value for its too .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164923	90	00:03:54.59	00:04:01.75	Now let's look at the sign of the coefficient for X2, where we know this is a positive.	now let 's look at the sign of the coefficient for X2 , where we know this be a positive .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164924	95	00:04:02.7	00:04:14.93	So what that means is that the F value for this point should be higher than the F value for this point on the line. That means this will be positive, right?	so what that mean be that the F value for this point should be high than the F value for this point on the line . that mean this will be positive , right ?
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164924	97	00:04:16.07	00:04:21.48	So we know in general for all the points on this side, the.	so we know in general for all the point on this side , the .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164925	104	00:04:22.51	00:04:35.95	Functions about it would be positive. And you can also verify all the points on this side would be negative, and so this is how this kind of linear classifier or linear separator can then separate the points in the two categories.	function about it would be positive . and you can also verify all the point on this side would be negative , and so this be how this kind of linear classifier or linear separator can then separate the point in the two category .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164927	117	00:04:37.63	00:05:04.89	So now the natural question is, which linear separate is the best? Now I've again she want lying here that can separate the two classes. And this line, of course, is determined by the vector beta, the coefficients, different coefficient will give us a different line. So we could imagine there are other lines that can do the same job. So gamma, for example, could give us another line that can also separate these instances.	so now the natural question be , which linear separate be the good ? now I 've again she want lie here that can separate the two class . and this line , of course , be determine by the vector beta , the coefficient , different coefficient will give we a different line . so we could imagine there be other line that can do the same job . so gamma , for example , could give we another line that can also separate these instance .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164928	125	00:05:05.76	00:05:19.58	And of course there are also lines that won't separate them, and those are bad lines. But the question is when we have multiple lines that can separate the both clauses, which line is the best? In fact, you can imagine there are many different ways of choosing the line.	and of course there be also line that wo n't separate they , and those be bad line . but the question be when we have multiple line that can separate the both clause , which line be the good ? in fact , you can imagine there be many different way of choose the line .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164931	138	00:05:20.8	00:05:48.44	So the logistical regression classifier that you have seen earlier actually uses some criteria to determine where this line should be, and it's a linear separate as well and uses a conditional likelihood on the training data to determine which line is the best. But in this VM, we're going to look at another criteria for determining which lines best and this time the criteria is more tide to the classification error. As you will see.	so the logistical regression classifier that you have see early actually use some criterion to determine where this line should be , and it be a linear separate as well and use a conditional likelihood on the training datum to determine which line be the good . but in this vm , we be go to look at another criterion for determine which line well and this time the criterion be more tide to the classification error . as you will see .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164932	140	00:05:49.33	00:05:52.69	So the basic idea is to choose the separator.	so the basic idea be to choose the separator .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164932	141	00:05:53.49	00:05:55.23	To maximize the margin.	to maximize the margin .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164932	143	00:05:56.01	00:05:58.06	So what is the margin? Well, I choose.	so what be the margin ? well , I choose .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164933	146	00:05:59.29	00:06:06.28	So I've shown some daughter lines here to indicate the boundaries of those data points in.	so I 've show some daughter line here to indicate the boundary of those datum point in .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164933	150	00:06:07.56	00:06:17.47	In each class and the margin is simply the distance between the line, the separator and the closest points from each class.	in each class and the margin be simply the distance between the line , the separator and the close point from each class .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164934	152	00:06:18.37	00:06:22.66	So you can see the margin of this side is as I've shown here.	so you can see the margin of this side be as I 've show here .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164934	154	00:06:23.33	00:06:25.91	And you can also define the margin on the other side.	and you can also define the margin on the other side .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164935	159	00:06:26.87	00:06:38.98	And in order for the separate to maximizing the margin, it has to be kind of in the middle of the two boundaries, and you don't want this separator to be very close to one side.	and in order for the separate to maximize the margin , it have to be kind of in the middle of the two boundary , and you do n't want this separator to be very close to one side .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164935	161	00:06:39.57	00:06:42.83	And then that inducing intuitively makes a lot of sense.	and then that induce intuitively make a lot of sense .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164935	164	00:06:44.04	00:06:50.03	So this is the basic idea of ecfmg. We're going to choose a linear separator to maximize the margin.	so this be the basic idea of ecfmg . we be go to choose a linear separator to maximize the margin .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164937	172	00:06:51.98	00:07:09.98	Now on this slide I've also changed the notation so that I'm not going to use beta. Didn't know the parameters and, but instead I'm going to use W, although W was used to denote the words before. So don't be confused here. W here is actually wait set of weights.	now on this slide I 've also change the notation so that I be not go to use beta . do n't know the parameter and , but instead I be go to use w , although W be use to denote the word before . so do n't be confuse here . w here be actually wait set of weight .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164937	173	00:07:10.99	00:07:11.44	And.	and .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164937	177	00:07:12.6	00:07:22.84	So I'm also using locates be to denote beta zero, the bias constant. And there are instances do represented as X.	so I be also use locate be to denote beta zero , the bias constant . and there be instance do represent as X.
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164938	181	00:07:23.86	00:07:34.24	And I also use the vector form of multiplication here. So we see transpose of W vector multiplied by the feature vector.	and I also use the vector form of multiplication here . so we see transpose of W vector multiply by the feature vector .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164939	186	00:07:35.16	00:07:46.6	So P is a biased constant and W is a set of weights and with one wait for each feature we have M features and so have aim weights and are represented as a vector.	so p be a biased constant and W be a set of weight and with one wait for each feature we have m feature and so have aim weight and be represent as a vector .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.16494	192	00:07:47.47	00:08:01.1	An similarly the data instance. Here the text object is represented by also a feature vector of the same number of elements. XI is future value. For example word count.	an similarly the data instance . here the text object be represent by also a feature vector of the same number of element . xi be future value . for example word count .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164941	203	00:08:02.06	00:08:25.82	I can you can verify when we multiply these two vectors together, take the dot product that we get the same form of the NIA separate as you have seen before. It's just a different way of representing this. Now I use this way so that it's more consistent with what notations people usually use when they talk about SVM. This way you can.	I can you can verify when we multiply these two vector together , take the dot product that we get the same form of the nia separate as you have see before . it be just a different way of represent this . now I use this way so that it be more consistent with what notation people usually use when they talk about SVM . this way you can .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164942	205	00:08:26.58	00:08:29.75	Better connected the slides with some other readings you might do.	well connect the slide with some other reading you might do .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164942	206	00:08:31.03	00:08:31.66	OK.	ok .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164942	207	00:08:32.28	00:08:33	So.	so .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164943	216	00:08:34.84	00:08:55.3	When we maximize the margins of separate, it just means with the boundary of. The separate is only determined by a few data points, and these are the data points that we call support vectors. So here are illustrated to support vectors for one class and two for the other class.	when we maximize the margin of separate , it just mean with the boundary of . the separate be only determine by a few data point , and these be the data point that we call support vector . so here be illustrate to support vector for one class and two for the other class .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164944	218	00:08:56.06	00:08:59.47	At this, porters define the margin basically.	at this , porter define the margin basically .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164944	222	00:09:00.59	00:09:10.29	And you can imagine once we know which are support vectors, then this center separate line will be determined by them so.	and you can imagine once we know which be support vector , then this center separate line will be determine by they so .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164944	224	00:09:11.84	00:09:15.54	The other data points actually don't really matter that much.	the other data point actually do n't really matter that much .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164946	232	00:09:16.16	00:09:31.71	And you can see if they you change other data points, it won't really affect the margin, so the separate with the stay the same mainly affected by the support vector machines. Sorry it's mainly affected by the support vectors and that's why it is called a support vector machine.	and you can see if they you change other datum point , it wo n't really affect the margin , so the separate with the stay the same mainly affect by the support vector machine . sorry it be mainly affect by the support vector and that be why it be call a support vector machine .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164946	233	00:09:32.79	00:09:34.52	OK, so.	ok , so .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164946	236	00:09:35.52	00:09:45.46	The next question is of course, how can we set it up to optimize the line? How can we actually find the line?	the next question be of course , how can we set it up to optimize the line ? how can we actually find the line ?
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164947	241	00:09:46.01	00:09:55.93	Or the separator. Now this is equivalent to finding values for W&B because they would determine where exactly the separator is.	or the separator . now this be equivalent to find value for W&B because they would determine where exactly the separator be .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164949	258	00:09:57.86	00:10:37.15	So in the simplest case, the linear osfm is just a simple optimization problem. So again we let's recall that our classifier is such a linear separator where we have weights for all the features and the main goal is to learn these weights W&B. And the classifier will say X is in category one if it's positive. Otherwise it's going to say it's in the other category. So this is our assumption or setup. So in the linear is UVM, we're going to then seek these parameter values to optimize the margins and then the training error.	so in the simple case , the linear osfm be just a simple optimization problem . so again we let 's recall that our classifier be such a linear separator where we have weight for all the feature and the main goal be to learn these weight W&B. and the classifier will say X be in category one if it be positive . otherwise it be go to say it be in the other category . so this be our assumption or setup . so in the linear be UVM , we be go to then seek these parameter value to optimize the margin and then the training error .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.16495	263	00:10:38.51	00:10:48.91	The training laid out would be basically like a in other classifiers we have a set of training points where we know the X vector and then we also the corresponding label, why I?	the training lay out would be basically like a in other classifier we have a set of training point where we know the x vector and then we also the corresponding label , why I ?
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164952	274	00:10:50.12	00:11:15.78	An here we define why I as two values, but these two values are not 01 as you have seen before, but rather negative one and positive one and their corresponding to these two categories as I've shown here. Now you might wonder why we don't define them as zero and one, but instead of having negative 11 and this is purely for mathematical convenience, as you will see in a moment.	an here we define why I as two value , but these two value be not 01 as you have see before , but rather negative one and positive one and their correspond to these two category as I 've show here . now you might wonder why we do n't define they as zero and one , but instead of have negative 11 and this be purely for mathematical convenience , as you will see in a moment .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164953	287	00:11:16.58	00:11:47.5	So the goal of optimization first is to make sure the labeling on training data is all correct. So that just means if Yi, the known label, for instance XI is one we would like this classify value to be large. And here we just choose threshold one here. But if you use another threshold, you can see you can easily affect that constant into the parameter values B&W to make the right hand side. Just one.	so the goal of optimization first be to make sure the labeling on training datum be all correct . so that just mean if Yi , the know label , for instance XI be one we would like this classify value to be large . and here we just choose threshold one here . but if you use another threshold , you can see you can easily affect that constant into the parameter value B&W to make the right hand side . just one .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164955	295	00:11:48.77	00:12:04.25	Now, if, on the other hand, why I is negative one that means it's in a different class then we want this classifier to give us a very small value. In fact a negative value. And we want this value to be less than or equal to negative one.	now , if , on the other hand , why I be negative one that mean it be in a different class then we want this classifier to give we a very small value . in fact a negative value . and we want this value to be less than or equal to negative one .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164956	303	00:12:05.3	00:12:25.87	These are the two different instances, different kinds of cases and how can we combine them together now. This is where it's convenient when we have chosen why I as negative one for the other category cause it turns out that we can easily combine the two into one constraint.	these be the two different instance , different kind of case and how can we combine they together now . this be where it be convenient when we have choose why I as negative one for the other category cause it turn out that we can easily combine the two into one constraint .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164956	306	00:12:26.51	00:12:32.29	Why I multiplied by the classifier value must be larger than or equal to 1?	why I multiply by the classifier value must be large than or equal to 1 ?
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164957	310	00:12:33.07	00:12:39.1	An obviously when? Why is just one you see. This is the same as the constraint on the left hand side.	an obviously when ? why be just one you see . this be the same as the constraint on the left hand side .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.16496	329	00:12:40.04	00:13:18.07	But when Yi is negative one you also see a new. This is equivalent to the other inequality, so this one actually captures both constraints in a unified way, and that's a convenient way of capturing these constraints. What's our second goal? That's true. Maximizing margin, right? So we want to ensure the separate can do well on the training data, but then, among all the cases where we can separate the data, we also would like to choose the separate that has the largest margin. Now the margin can be shown to be related to the magnitude of the weights.	but when Yi be negative one you also see a new . this be equivalent to the other inequality , so this one actually capture both constraint in a unified way , and that be a convenient way of capture these constraint . what be our second goal ? that be true . maximize margin , right ? so we want to ensure the separate can do well on the training datum , but then , among all the case where we can separate the datum , we also would like to choose the separate that have the large margin . now the margin can be show to be relate to the magnitude of the weight .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164961	334	00:13:25.97	00:13:40.24	The sum of squares of all those weights. So this to have a small value for this expression. It means all the eyes must be small.	the sum of square of all those weight . so this to have a small value for this expression . it mean all the eye must be small .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164962	343	00:13:42.31	00:14:05.34	So we've just assume that we have a constraint for the getting the data on the training set to be classified correctly. Now we also have the objective that's Tide to maximization of margin and this is simply to maximize sorry to minimize W transpose multiplied by W and we often denote this by file W.	so we 've just assume that we have a constraint for the get the datum on the training set to be classify correctly . now we also have the objective that be Tide to maximization of margin and this be simply to maximize sorry to minimize W transpose multiply by W and we often denote this by file W.
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164967	378	00:14:06.44	00:15:23.19	So now you can see this is basically optimization problem, right? We have some variables to optimize and these are the weights and B and we have some constraints. These are linear constraints and the objective function is a quadratic function of the weights. So this is a quadratic program with linear constraints and there are standard algorithms that are available for solving this problem. And once we solve, the problem, will obtain the weights W&B and then this would give us a well defined the classifier, so we can then use this classifier to classify any new texture objects. Now the previous formulation did not allow any error in the classification, but sometimes the data may not be linearly separable. That means they may not look as nice as you have seen on the previous slide where align can separate all of them. And what would happen if we. Allow some errors. The principle can stay right, so we want to minimize the training error, but try to also maximize the margin. But in this case we have a soft margin because the data points may not be a completely separate bowl. So it turns out that we can easily modify it as VM to accommodate this.	so now you can see this be basically optimization problem , right ? we have some variable to optimize and these be the weight and b and we have some constraint . these be linear constraint and the objective function be a quadratic function of the weight . so this be a quadratic program with linear constraint and there be standard algorithm that be available for solve this problem . and once we solve , the problem , will obtain the weight W&B and then this would give we a well define the classifier , so we can then use this classifier to classify any new texture object . now the previous formulation do not allow any error in the classification , but sometimes the datum may not be linearly separable . that mean they may not look as nice as you have see on the previous slide where align can separate all of they . and what would happen if we . allow some error . the principle can stay right , so we want to minimize the training error , but try to also maximize the margin . but in this case we have a soft margin because the data point may not be a completely separate bowl . so it turn out that we can easily modify it as VM to accommodate this .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164969	387	00:15:24.48	00:15:43.35	So what you see here is very similar to what you have seen before, but we have introduced the extra variables. Cassie I an we in fact will have one for each data instance and this is going to model the error that will allow for each instance. But the optimization problem will be very similar.	so what you see here be very similar to what you have see before , but we have introduce the extra variable . cassie I an we in fact will have one for each data instance and this be go to model the error that will allow for each instance . but the optimization problem will be very similar .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164969	391	00:15:44.76	00:15:52.76	So specifically, you will see we have added something to the optimization problem. First we have added some.	so specifically , you will see we have add something to the optimization problem . first we have add some .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.16497	393	00:15:54.15	00:16:00.77	Some error to the constraint so that now we allow.	some error to the constraint so that now we allow .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.16497	399	00:16:02.08	00:16:22.33	Allow the classifier to make some mistakes here, so this KCI is allowed error if we set KCI to 0, then we go back to the original constraint. We want every instance we classified accurately, but if we allow this to be.	allow the classifier to make some mistake here , so this KCI be allow error if we set KCI to 0 , then we go back to the original constraint . we want every instance we classify accurately , but if we allow this to be .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164973	420	00:16:23.62	00:17:14.92	Zero, then we allow some errors here. In fact, the one CI is very large. The error can be very, very large, so naturally we don't want this to happen. So we want to then also minimize this CI. So Cassie, I needs to be minimized in order to control the error. And so as a result in the objective function we also add more to the original 1, which is only an by basically ensuring that we're going to not only minimize the weights, but also minimize the errors as you see here, we simply take a sum over all the instances. Each one has a CI to model the error allowed for that instance an when we combine them together, we basically want to minimize the errors on. All of them.	Zero , then we allow some error here . in fact , the one CI be very large . the error can be very , very large , so naturally we do n't want this to happen . so we want to then also minimize this CI . so Cassie , I need to be minimize in order to control the error . and so as a result in the objective function we also add more to the original 1 , which be only an by basically ensure that we be go to not only minimize the weight , but also minimize the error as you see here , we simply take a sum over all the instance . each one have a ci to model the error allow for that instance an when we combine they together , we basically want to minimize the error on . all of they .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164974	427	00:17:16.19	00:17:33.18	Now you see there's a parameter. See here and that's a constant to control the tradeoff between minimizing the errors and maximizing the region of the margin if C is set to zero, you can see we go back to the original object function where we only maximize margin.	now you see there be a parameter . see here and that be a constant to control the tradeoff between minimize the error and maximize the region of the margin if C be set to zero , you can see we go back to the original object function where we only maximize margin .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164978	451	00:17:34.13	00:18:25.58	And we don't really optimize the training errors and then see I can be set to a very large value to make the constraints easy to satisfy. That's not very good of course, so see should be set to a non 0 value and a positive value. But when she is settled very, very large value would see the objective function will be dominated mostly by the training errors and so the optimization of margin will then play a secondary role. So if that happens, what would happen? What would happen is then we will try to do our best to minimize the training errors. But then we're not going to take care of the margin and that affects the generalization capacity of the classifier for future data. So it's also not good. So apparently this parameter C has to be actually set.	and we do n't really optimize the training error and then see I can be set to a very large value to make the constraint easy to satisfy . that be not very good of course , so see should be set to a non 0 value and a positive value . but when she be settle very , very large value would see the objective function will be dominate mostly by the training error and so the optimization of margin will then play a secondary role . so if that happen , what would happen ? what would happen be then we will try to do our good to minimize the training error . but then we be not go to take care of the margin and that affect the generalization capacity of the classifier for future datum . so it be also not good . so apparently this parameter C have to be actually set .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164979	461	00:18:27.21	00:18:47.68	Carefully, and this is just like in the case of nearest neighbor way you need to optimize the number of neighbors. Here you need to optimize the C and this is the general also achievable by doing cross validation. Basically you look at the empirical data to see what values should be set to in order to optimize the performance.	carefully , and this be just like in the case of near neighbor way you need to optimize the number of neighbor . here you need to optimize the c and this be the general also achievable by do cross validation . basically you look at the empirical datum to see what value should be set to in order to optimize the performance .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.16498	467	00:18:48.87	00:19:00.32	Now with this modification in the problem, is there a quadratic program with linear constraints, so the optimization algorithm can be actually applied to solve this different version of the program?	now with this modification in the problem , be there a quadratic program with linear constraint , so the optimization algorithm can be actually apply to solve this different version of the program ?
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164982	473	00:19:01.94	00:19:13.62	Again, once we have obtained the weights and the bias, then we can have classified. That's ready for classifying new objects. So that's the basic idea of Sven.	again , once we have obtain the weight and the bias , then we can have classify . that be ready for classify new object . so that be the basic idea of Sven .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164984	483	00:19:17.04	00:19:43.37	So to summarize, the text categorisation methods we have introduced many methods and some are generative models, some more discriminative methods, and these tend to perform similarly when optimized, so there's still no clear winner, although each one has its pros and cons, and the performance might also very different data sets for different problems.	so to summarize , the text categorisation method we have introduce many method and some be generative model , some more discriminative method , and these tend to perform similarly when optimize , so there be still no clear winner , although each one have its pro and con , and the performance might also very different datum set for different problem .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164986	497	00:19:44.17	00:20:18.84	Ann One reason is also becausw. The feature representation is very critical an so that these methods all require effective feature representation and to design effective feature set that we need domain knowledge and humans definitely play important role here. Although there are new machine learning methods like representation learning that can help with learning features. An another common scene is that they might be.	Ann one reason be also becausw . the feature representation be very critical an so that these method all require effective feature representation and to design effective feature set that we need domain knowledge and human definitely play important role here . although there be new machine learn method like representation learn that can help with learn feature . an another common scene be that they might be .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164987	507	00:20:21.05	00:20:50.95	Be performing similarly on the data set but with different mistakes and so their performance might be similar, but then the mistakes that make might be different, so that means it's useful to compare different methods for particular problem and then maybe combine multiple methods 'cause this can improve the robustness and they want to make the same mistakes so.	be perform similarly on the datum set but with different mistake and so their performance might be similar , but then the mistake that make might be different , so that mean it be useful to compare different method for particular problem and then maybe combine multiple method 'cause this can improve the robustness and they want to make the same mistake so .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164988	511	00:20:51.44	00:20:58.43	And symbol approaches that would combine different methods and tend to be more robust and can be useful in practice.	and symbol approach that would combine different method and tend to be more robust and can be useful in practice .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.16499	522	00:20:59.74	00:21:24.9	Most techniques that we introduce the use supervised machine learning and which is a very general method. So that means these methods can be actually applied to any text categorization problem as long as we have humans to help annotate some training data set and design features, then supervised machine learning an all these classifiers can be easily applied to those.	Most technique that we introduce the use supervise machine learning and which be a very general method . so that mean these method can be actually apply to any text categorization problem as long as we have human to help annotate some training datum set and design feature , then supervise machine learn an all these classifier can be easily apply to those .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164991	529	00:21:25.73	00:21:40.46	Problems to solve the categorization problem. To allow us to characterize content of text concisely with categories or the predictor, some properties of real world variables that are associated with text data.	problem to solve the categorization problem . to allow we to characterize content of text concisely with category or the predictor , some property of real world variable that be associate with text datum .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164992	536	00:21:42.35	00:21:55.89	The computers of course here are trying to optimize the combinations of the features provided by human an. As I say that there are many different ways of combining them and they also optimize different objects and functions.	the computer of course here be try to optimize the combination of the feature provide by human an . as I say that there be many different way of combine they and they also optimize different object and function .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164992	540	00:21:57.27	00:22:03.32	But in order to achieve good performance, they all require effective features and also plenty of training data.	but in order to achieve good performance , they all require effective feature and also plenty of training datum .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164994	551	00:22:04.64	00:22:27.47	So as a general rule, and if you can improve the feature representation an and then provide more training data, then you can generate do better. So performance is often much more affected by the effectiveness of features and then by the choice of specific classifiers. So feature design tends to be more important than the choice of specific classifier.	so as a general rule , and if you can improve the feature representation an and then provide more training datum , then you can generate do well . so performance be often much more affect by the effectiveness of feature and then by the choice of specific classifier . so feature design tend to be more important than the choice of specific classifier .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164994	555	00:22:30.83	00:22:41.61	So how do we design effective features? Well, unfortunately this is very application specific, so there's no really much general thing to say here.	so how do we design effective feature ? well , unfortunately this be very application specific , so there be no really much general thing to say here .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164995	556	00:22:43.47	00:22:43.95	But	but
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164995	557	00:22:44.76	00:22:45.98	We can.	we can .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164996	564	00:22:46.59	00:22:59.66	And do some analysis of the categorization problem and try to understand the what kind of features might help us distinguish categories, and in general we can use a lot of domain knowledge to help us design features.	and do some analysis of the categorization problem and try to understand the what kind of feature might help we distinguish category , and in general we can use a lot of domain knowledge to help we design feature .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164998	577	00:23:00.88	00:23:30.95	An another way to figure out effective features is to do error analysis on the categorisation results. You could, for example, look at the which category tends to be confused with each other categories and you can use a confusion matrix to examine the errors systematically across categories, and then you can look into specific instances to see why the mistake has been made and what features can prevent the. This can allow you to obtain.	an another way to figure out effective feature be to do error analysis on the categorisation result . you could , for example , look at the which category tend to be confuse with each other category and you can use a confusion matrix to examine the error systematically across category , and then you can look into specific instance to see why the mistake have be make and what feature can prevent the . this can allow you to obtain .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.164998	578	00:23:31.61	00:23:34.46	Insights for design new features.	insight for design new feature .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.165	593	00:23:35.15	00:24:00.85	So error analysis very important in general, and that's where you can get the insights about your specific problem. And then finally we can leverage some machine learning techniques. So for example, feature selection is a technique that we haven't really talked about, but it's very important and it has to do with trying to select the most useful features before you actually trainer for classifier, and sometimes training a classifier would also help you identify which features have high values.	so error analysis very important in general , and that be where you can get the insight about your specific problem . and then finally we can leverage some machine learn technique . so for example , feature selection be a technique that we have n't really talk about , but it be very important and it have to do with try to select the most useful feature before you actually trainer for classifier , and sometimes train a classifier would also help you identify which feature have high value .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.165001	601	00:24:01.54	00:24:19.169999	And there are also other ways to ensure the sparsity of the model. Meaning to recognize the weights. So for example, the SVM actually tries to minimize the weights on features, but you can further for some features to falsely use only a small number of features.	and there be also other way to ensure the sparsity of the model . mean to recognize the weight . so for example , the SVM actually try to minimize the weight on feature , but you can far for some feature to falsely use only a small number of feature .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.165002	611	00:24:20.18	00:24:45.77	There are also techniques for dimension reduction, and that's to reduce the high dimensional feature space into a lower dimensional space. Typical biclustering of features in various ways, so metrics factorization has been used to do such a job, and this and some of the techniques are after very similar to the topic models that we discussed, so topic models.	there be also technique for dimension reduction , and that be to reduce the high dimensional feature space into a lower dimensional space . typical biclustering of feature in various way , so metric factorization have be use to do such a job , and this and some of the technique be after very similar to the topic model that we discuss , so topic model .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.165005	631	00:24:47.96	00:25:28.42	LDA can actually help us reduce the dimension of features. Imagine the words are original feature representation, but the representation can be mapped to the topic space representation. Let's say we have K topics, so a document cannot be represented as a vector of justice K values corresponding to the topics. So we can let each topic define one dimension. So we have K dimensional space instead of the original high dimensional space corresponding to words. And this is. Often another way to learn factor features, especially, we could also use the categories to supervise learning of such low dimensional structures.	LDA can actually help we reduce the dimension of feature . imagine the word be original feature representation , but the representation can be map to the topic space representation . let 's say we have K topic , so a document can not be represent as a vector of justice K value correspond to the topic . so we can let each topic define one dimension . so we have K dimensional space instead of the original high dimensional space correspond to word . and this be . often another way to learn factor feature , especially , we could also use the category to supervise learning of such low dimensional structure .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.165006	637	00:25:29.69	00:25:45.05	An so the original word features can be also combined with such such latent dimension features or low dimensional space features to provide a multiresolution representation, which is often very useful.	an so the original word feature can be also combine with such such latent dimension feature or low dimensional space feature to provide a multiresolution representation , which be often very useful .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.165008	649	00:25:46.41	00:26:16.75	Deep learning is a new technique that has been developed in machine learning. It's particularly useful for learning representations, so different learning refers to deep neural network. It's another kind of classifier where you can have intermediate features embedded in the model so that it's highly non linear classifier. An some reason advance has allowed us to train such a complex network effectively.	deep learning be a new technique that have be develop in machine learning . it be particularly useful for learn representation , so different learning refer to deep neural network . it be another kind of classifier where you can have intermediate feature embed in the model so that it be highly non linear classifier . an some reason advance have allow we to train such a complex network effectively .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.165009	654	00:26:17.13	00:26:26.71	Ann is the technique has been shown to be quite effective for speech recognition, computer vision and recently it has been applied through text as well.	Ann be the technique have be show to be quite effective for speech recognition , computer vision and recently it have be apply through text as well .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.165012	678	00:26:27.31	00:27:29.97	It has shown some promise and one important advantage of this approach in relationship with the feature design is that they can learn intermediate representations or compound features automatically, and this is very valuable for learning effective representation for text localization. Although in Texas domain cause words are excellent representation of text content because these are. Humans invention for communication and they are generous sufficient for representing content for many tasks. If there's a need for some new representation, people would have invented a new words and new World. So because of this reason, the value of deep learning for text processing tends to be lower than for computer vision and speech recognition, where there aren't corresponding wedding design. The words. As features.	it have show some promise and one important advantage of this approach in relationship with the feature design be that they can learn intermediate representation or compound feature automatically , and this be very valuable for learn effective representation for text localization . although in Texas domain cause word be excellent representation of text content because these be . human invention for communication and they be generous sufficient for represent content for many task . if there be a need for some new representation , people would have invent a new word and new World . so because of this reason , the value of deep learning for text processing tend to be low than for computer vision and speech recognition , where there be n't correspond wedding design . the word . as feature .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.165013	685	00:27:30.66	00:27:44.77	But deep learning is still very promising for learning effective features, especially for complicated tasks like a sentiment analysis, and has been shown to be effective because it can provide replenishing that goes beyond bag of words.	but deep learning be still very promising for learn effective feature , especially for complicated task like a sentiment analysis , and have be show to be effective because it can provide replenish that go beyond bag of word .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.165014	689	00:27:46.88	00:27:54.78	Regarding the training examples, it's generally hard to get a lot of training examples because it involves human labor.	regard the training example , it be generally hard to get a lot of training example because it involve human labor .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.165016	701	00:27:56.16	00:28:25.47	But there are also some ways to help with this, so one is to assume some low quality training examples can also be used so those can be called a pseudo training examples. For example, if you take a reviews from the Internet, they might have overall ratings. So to train a sentiment categorizer meaning we want to distinguish positive from negative opinions and categorize reviews into these two categories then.	but there be also some way to help with this , so one be to assume some low quality training example can also be use so those can be call a pseudo training example . for example , if you take a review from the internet , they might have overall rating . so to train a sentiment categorizer mean we want to distinguish positive from negative opinion and categorize review into these two category then .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.165017	709	00:28:26.53	00:28:43.33	We could assume five star reviews are all positive training examples. OnStar negative but of course sometimes in five star reviews. We also mention negative opinions so that rain example is not all of that high quality, but they can still be useful.	we could assume five star review be all positive training example . onstar negative but of course sometimes in five star review . we also mention negative opinion so that rain example be not all of that high quality , but they can still be useful .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.165027	761	00:28:45.05	00:30:44.88	Another idea is really exploit unable data and there are techniques called a semi supervised machine learning techniques that can allow you to combine label data with unlabeled data. So in our case actually it's easy to see the mixture model can be used for both text clustering and categorisation, so even imagine if you have a lot of unable text data for categorization then you can actually do clustering on these text data to learn categories. And then try to somehow align these categories with the categories defined by the training data where we already know which documents are in which category. So you can in fact use the EM algorithm to actually combine both. That would allow you essentially to also pick up a useful words in the unlabeled data. You can think of this in another way. Basically, we can use, let's say a naive Bayes classifier to classify all the unlabeled text documents. And then we're going to assume the high confidence classification results, or actually reliable. Then you certainly have more training data. The cause from the unlabeled data we some are labeled as category ones and more labeled as category two. Although the label is not completely reliable. But then they can still be useful. So let's assume they are actually training label examples and then we combine them with the true training examples. To improve categorization method and so this idea is very powerful and when the enable data and training data are very different and we might need to use other advanced machine learning techniques called domain adaptation or transfer learning, this is when we can borrow some training examples from a related problem that may be different or from a categorisation task that.	another idea be really exploit unable datum and there be technique call a semi supervised machine learn technique that can allow you to combine label datum with unlabeled datum . so in our case actually it be easy to see the mixture model can be use for both text clustering and categorisation , so even imagine if you have a lot of unable text datum for categorization then you can actually do clustering on these text datum to learn category . and then try to somehow align these category with the category define by the training datum where we already know which document be in which category . so you can in fact use the EM algorithm to actually combine both . that would allow you essentially to also pick up a useful word in the unlabeled datum . you can think of this in another way . basically , we can use , let 's say a naive Bayes classifier to classify all the unlabeled text document . and then we be go to assume the high confidence classification result , or actually reliable . then you certainly have more training datum . the cause from the unlabeled datum we some be label as category one and more label as category two . although the label be not completely reliable . but then they can still be useful . so let 's assume they be actually train label example and then we combine they with the true training example . to improve categorization method and so this idea be very powerful and when the enable datum and training datum be very different and we might need to use other advanced machine learn technique call domain adaptation or transfer learning , this be when we can borrow some training example from a relate problem that may be different or from a categorisation task that .
7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	2020-11-02 23:07:01.165029	770	00:30:46.03	00:31:01.56	That involves data that follow very different distributions from what we are working on. But basically when the two domains are very different than we need to be careful not to overfit the training domain, but yet we can still want to use some signals from the related training data.	that involve datum that follow very different distribution from what we be work on . but basically when the two domain be very different than we need to be careful not to overfit the training domain , but yet we can still want to use some signal from the related training datum .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-24 09:59:43.792404	3	00:00:00.3	00:00:05	This lecture is about the generative probabilistic models for text clustering.	this lecture be about the generative probabilistic model for text clustering .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-24 10:00:11.636987	8	00:00:13.74	00:00:23.58	In this lecture we can do continue discussing text clustering, and we're going to introduce generative probabilistic models as a way to do text clustering	in this lecture we can do continue discuss text clustering , and we be go to introduce generative probabilistic model as a way to do text clustering
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-30 14:11:31.208949	13	00:00:25.65	00:00:36.44	So this is the overall plan for covering text clustering in the previous lecture we have talked about what is text clustering and why text clustering is interesting.	so this be the overall plan for cover text clustering in the previous lecture we have talk about what be text clustering and why text clustering be interesting .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060701	21	00:00:37.12	00:00:52.77	In this lecture we're going to talk about how to do text clustering, in general, as you see on this slide, there are two kinds of approaches. One is generating probabilistic models, which is the topic of this lecture, and later will also discuss similarity based approaches.	in this lecture we be go to talk about how to do text clustering , in general , as you see on this slide , there be two kind of approach . one be generate probabilistic model , which be the topic of this lecture , and later will also discuss similarity base approach .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060702	25	00:00:53.75	00:01:05.33	So to talk about generative models for text clustering, it would be useful to revisit the topic mining problem using topic models.	so to talk about generative model for text clustering , it would be useful to revisit the topic mining problem use topic model .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-24 10:02:19.871608	35	00:01:06.22	00:01:31.54	Because the two problems are very similar, so this is a slide that you have seen earlier in the lecture on topic model. Here we show that we have input of text collection C and number of topics K and vocabulary V, and we hope to generate as output two things. One is a set of topics denoted by Theta i's.	because the two problem be very similar , so this be a slide that you have see early in the lecture on topic model . here we show that we have input of text collection c and number of topic k and vocabulary v , and we hope to generate as output two thing . one be a set of topic denote by Theta I be .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.06071	43	00:01:32.09	00:01:50.8	Each is a word distribution and the other is a pi ij's and these are the probabilities that each document covers each topic. So this is a topic coverage and it's also visualized here on this slide you can see that this is what we can get by using a topic model.	each be a word distribution and the other be a pi ij 's and these be the probability that each document cover each topic . so this be a topic coverage and it be also visualize here on this slide you can see that this be what we can get by use a topic model .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060713	49	00:01:51.42	00:02:11.61	Now a main difference between this and text clustering problem is that here a document is assumed to possibly cover multiple topics, and indeed in general document will be covering more than one topic with non zero probabilities.	now a main difference between this and text clustering problem be that here a document be assume to possibly cover multiple topic , and indeed in general document will be cover more than one topic with non zero probability .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060715	52	00:02:12.69	00:02:22.58	In text clustering, however, we only allow a document to cover one topic. If we assume one topic is a cluster.	in text clustering , however , we only allow a document to cover one topic . if we assume one topic be a cluster .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060715	53	00:02:24.19	00:02:24.93	So.	so .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-24 10:04:14.39388	57	00:02:26.41	00:02:35.99	That means if we change the topic definition just slightly by assuming that each document can only be generated by using precisely one topic.	that mean if we change the topic definition just slightly by assume that each document can only be generate by use precisely one topic .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060718	59	00:02:37.07	00:02:39.98	Then we'll have a definition of the clustering problem.	then we 'll have a definition of the clustering problem .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-24 10:04:25.019251	60	00:02:40.58	00:02:41.42	As shown here.	as show here .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060722	69	00:02:42.09	00:03:08.27	So here the output is changed so that we no longer have the detailed coverage distributions pi ij's, but instead will have cluster assignment decisions. An CI and CI is decision for the document i. And c sub i is going to take a value from one through K to indicate one of the K clusters.	so here the output be change so that we no long have the detailed coverage distribution pi ij 's , but instead will have cluster assignment decision . an CI and CI be decision for the document i. and c sub I be go to take a value from one through K to indicate one of the K cluster .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060723	71	00:03:09.12	00:03:14.83	And basically tells us document Di is in which cluster.	and basically tell we document Di be in which cluster .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060726	76	00:03:15.69	00:03:26.52	As illustrated here, we no longer have multiple topics covered in each document is precisely one topic, although which topic is still uncertain.	as illustrate here , we no long have multiple topic cover in each document be precisely one topic , although which topic be still uncertain .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060726	77	00:03:27.18	00:03:30.52	There is also a connection with the.	there be also a connection with the .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060728	81	00:03:31.1	00:03:37.94	Problem of mining. One topic that we discussed earlier. So here again it's a slide that you have seen before.	problem of mining . one topic that we discuss early . so here again it be a slide that you have see before .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.06073	86	00:03:38.54	00:03:51.309999	And here we hope to estimate a topic model or word distribution based on precisely one document, and that's when we assume that this document covers precisely one topic.	and here we hope to estimate a topic model or word distribution base on precisely one document , and that be when we assume that this document cover precisely one topic .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060736	102	00:03:52.89	00:04:23.23	But we can also consider some variations of the problem. For example, we can consider there are N documents, each covers different topic. So that's N documents and topics. Of course, in this case these documents are independent and these topics also independent. But we can further allow these documents share topics and then. We can also assume that we are going to assume there are fewer topics. The number of documents. So this document must share some topics.	but we can also consider some variation of the problem . for example , we can consider there be n document , each cover different topic . so that be n document and topic . of course , in this case these document be independent and these topic also independent . but we can far allow these document share topic and then . we can also assume that we be go to assume there be few topic . the number of document . so this document must share some topic .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060738	106	00:04:24	00:04:32.5	And if we have N documents for share k topics, then will again have precisely the document clustering problem.	and if we have n document for share k topic , then will again have precisely the document clustering problem .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.06074	110	00:04:34.24	00:04:41.43	So because of these connections, naturally we can think about how to use a probabilistic generating model to solve the problem of text clustering.	so because of these connection , naturally we can think about how to use a probabilistic generating model to solve the problem of text clustering .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060741	112	00:04:43.33	00:04:47.82	So the question now is what generating model can be used to do clustering.	so the question now be what generate model can be use to do clustering .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060745	123	00:04:48.7	00:05:13.75	As in all cases of designing a generative model, we hope the generative model would adopt the output that we hope to generate, or the structure that we hope to model. So in this case it's a clustering structure. The topics and each document that covers one topic, and we hope to embed such such preferences in a generative model.	as in all case of design a generative model , we hope the generative model would adopt the output that we hope to generate , or the structure that we hope to model . so in this case it be a clustering structure . the topic and each document that cover one topic , and we hope to embed such such preference in a generative model .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060748	130	00:05:15.62	00:05:32.52	But if you think about the main difference between this problem and the topic model that we talked about earlier and then you will see a main requirement is how can we force every document to be generated from precisely one topic instead of K topics?	but if you think about the main difference between this problem and the topic model that we talk about early and then you will see a main requirement be how can we force every document to be generate from precisely one topic instead of K topic ?
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060748	131	00:05:33.55	00:05:34.89	As in the topic model.	as in the topic model .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-24 10:07:28.378288	139	00:05:35.84	00:05:56.41	So let's revisit the topic model again in more detail. So this is a detailed view of two component mixture model and when we have K components it looks similar. So here we see that when we generate a document. We generated each word independently.	so let 's revisit the topic model again in more detail . so this be a detailed view of two component mixture model and when we have k component it look similar . so here we see that when we generate a document . we generate each word independently .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060753	143	00:05:57.38	00:06:08.7	And we generated each word First make a choice between these distributions with decided to use one of them with probability.	and we generate each word first make a choice between these distribution with decide to use one of they with probability .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060754	145	00:06:09.58	00:06:15.98	So P of theta one is the probability of choosing the distribution on the top.	so p of theta one be the probability of choose the distribution on the top .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-24 10:08:09.828959	152	00:06:17.88	00:06:30.78	Now we first make this decision regarding which distribution should be used to generate the world, and then we're going to use this distribution to sample word. Now. Notice that in such a generative model.	now we first make this decision regard which distribution should be use to generate the world , and then we be go to use this distribution to sample word . now . notice that in such a generative model .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.06076	160	00:06:31.49	00:06:48.8	The decision on which distribution to use for each word is independent, so "that means, for example, ""the"" here could" have been generated from the second distribution. Theta two, whereas text is more likely generated from the first one on the top.	the decision on which distribution to use for each word be independent , so " that mean , for example , " " the " " here could " have be generate from the second distribution . theta two , whereas text be more likely generate from the first one on the top .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060761	163	00:06:49.54	00:06:56.78	That means the words in the document could have been generated in general from multiple distributions.	that mean the word in the document could have be generate in general from multiple distribution .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060763	168	00:06:57.54	00:07:07.53	Now this is not what we want to see for text clustering. For document clustering where we hope this document will be generated from precisely one topic.	now this be not what we want to see for text clustering . for document clustering where we hope this document will be generate from precisely one topic .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060768	181	00:07:09.44	00:07:40.49	So now that means we need to modify the model, but how well, let's first think about why this model cannot be used for clustering, and I just say the reason is because. It has allowed multiple topics to contribute the words to the document. And that causes confusion because we're not going to know which cluster this document is from an it's more importantly, it's violating our assumption about the partitioning of documents in the clusters.	so now that mean we need to modify the model , but how well , let 's first think about why this model can not be use for clustering , and I just say the reason be because . it have allow multiple topic to contribute the word to the document . and that cause confusion because we be not go to know which cluster this document be from an it be more importantly , it be violate our assumption about the partitioning of document in the cluster .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060774	194	00:07:41.14	00:08:13.25	If we really have one topic to correspond to one cluster of documents, then we would have a document to be generated from precisely one topic. That means all the words in the document must have been generated from precisely one distribution, and this is not true for such a topic model that we're seeing here, and that's why this cannot be used for clustering because it did not ensure that only one distribution has been used to generate. All the words in one document.	if we really have one topic to correspond to one cluster of document , then we would have a document to be generate from precisely one topic . that mean all the word in the document must have be generate from precisely one distribution , and this be not true for such a topic model that we be see here , and that be why this can not be use for clustering because it do not ensure that only one distribution have be use to generate . all the word in one document .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060777	204	00:08:15.02	00:08:36.539999	So if you realize this problem, then we can naturally design alternative mixture model for doing clustering. So this is what you're seeing here and we again would have to make a decision regarding which is distributing to use to generate document, because the document that could potentially be generated from any of the K word distributions that we have.	so if you realize this problem , then we can naturally design alternative mixture model for do clustering . so this be what you be see here and we again would have to make a decision regard which be distribute to use to generate document , because the document that could potentially be generate from any of the K word distribution that we have .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060779	209	00:08:37.75	00:08:48.38	But this time, once we have made the decision to choose one of the topics, we're going to stay with this distribution to generate the all the words in the document.	but this time , once we have make the decision to choose one of the topic , we be go to stay with this distribution to generate the all the word in the document .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060781	215	00:08:49.94	00:09:03.99	And that means once we have made the choice of the distribution for in generating the first word. We're going to stay with this decision in generating all the other words in the document.	and that mean once we have make the choice of the distribution for in generate the first word . we be go to stay with this decision in generate all the other word in the document .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-24 10:10:34.689595	218	00:09:04.8	00:09:09.4	So in other words, we only make the choice once. for all.	so in other word , we only make the choice once . for all .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-24 10:10:48.497333	221	00:09:10.42	00:09:16.35	Basically we make the decision once for this document and stay with this to generate all the words.	basically we make the decision once for this document and stay with this to generate all the word .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060785	225	00:09:18.41	00:09:27.87	Similarly, if I had chosen the second distribution, theta sub two here, you can see we will stay with this one and then generate the entire document D.	similarly , if I have choose the second distribution , theta sub two here , you can see we will stay with this one and then generate the entire document D.
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060786	228	00:09:28.79	00:09:34.54	Now, if you compare this picture with the previous one, you will see the desicion of.	now , if you compare this picture with the previous one , you will see the desicion of .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060789	237	00:09:36.35	00:09:57.15	Of using a particular distribution is made of just once for this document. In the case of document clustering. But in the case of topic model we have to make as many decisions as the number of words in the document because for each word we can make a potential different decision and that's the key difference between the two models.	of use a particular distribution be make of just once for this document . in the case of document clustering . but in the case of topic model we have to make as many decision as the number of word in the document because for each word we can make a potential different decision and that be the key difference between the two model .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.06079	241	00:09:58.16	00:10:05.549999	But this is obviously also a mixture model, so we can just group them together as one box to show that this is.	but this be obviously also a mixture model , so we can just group they together as one box to show that this be .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060791	243	00:10:06.21	00:10:09.34	Model that will give us a probability of a document.	Model that will give we a probability of a document .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060792	247	00:10:10.38	00:10:18.1	Now inside this model there's also this, which of choosing a different distribution and we don't observe that, so that's a mixture model.	now inside this model there be also this , which of choose a different distribution and we do n't observe that , so that be a mixture model .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060793	249	00:10:19.11	00:10:23.58	And of course, the main problem in document clustering is to infer.	and of course , the main problem in document clustering be to infer .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-24 10:12:22.554062	253	00:10:24.76	00:10:32.41	Which distribution has been used to generator a document and that would allow us to recover the cluster identity over document	which distribution have be use to generator a document and that would allow we to recover the cluster identity over document
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060796	256	00:10:37.69	00:10:44.77	So it would be useful to think about the difference from the topic model, as I have also mentioned multiple times.	so it would be useful to think about the difference from the topic model , as I have also mention multiple time .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060796	257	00:10:45.85	00:10:48.6	There are many.	there be many .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060797	259	00:10:50.05	00:10:54.35	Two differences. One is the choice of.	two difference . one be the choice of .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060798	263	00:10:56.07	00:11:05.96	Using a particular distribution is made just once for document clustering model, whereas in the topic model it's made multiple times.	use a particular distribution be make just once for document clustering model , whereas in the topic model it be make multiple time .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.0608	267	00:11:06.77	00:11:17.93	Four different words. The second is that word distribution here is going to be used to generate all the words for a document.	four different word . the second be that word distribution here be go to be use to generate all the word for a document .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060802	273	00:11:19.17	00:11:31.52	But in the case of topic modeling, one distribution doesn't have to generate with all the words in a document. Multiple distribution could have been used to generate the words in the document.	but in the case of topic modeling , one distribution do n't have to generate with all the word in a document . multiple distribution could have be use to generate the word in the document .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060803	277	00:11:34.44	00:11:42.49	It's also think about the special case when one of the one of the probability of choosing a particular distribution is equal to 1.	it be also think about the special case when one of the one of the probability of choose a particular distribution be equal to 1 .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060808	289	00:11:43.42	00:12:11.549999	Now that just means we have no uncertainty. We just stick with one particular distribution. Now in that case, clearly we will see this is no longer mixture model 'cause there's no certainty here and we're going to just use precise one of the distributions for generating a document, and we're going back to the case of estimating one word distribution based on one document.	now that just mean we have no uncertainty . we just stick with one particular distribution . now in that case , clearly we will see this be no long mixture model 'cause there be no certainty here and we be go to just use precise one of the distribution for generate a document , and we be go back to the case of estimate one word distribution base on one document .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060809	291	00:12:12.79	00:12:14.89	So that's the connection that we discussed earlier.	so that be the connection that we discuss early .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-24 10:14:14.663392	292	00:12:15.62	00:12:17.31	But now you can see more clearly.	but now you can see more clearly .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060811	299	00:12:18.55	00:12:30.84	So as more cases of using a generative model to solve a problem, we first look at theta and then think about how to design the model. But once we design model, the next step is to write down the likelihood function.	so as more case of use a generative model to solve a problem , we first look at theta and then think about how to design the model . but once we design model , the next step be to write down the likelihood function .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060812	301	00:12:31.54	00:12:35.14	And after that we can do is to look at the how to estimate the parameters.	and after that we can do be to look at the how to estimate the parameter .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060813	306	00:12:36.11	00:12:44.13	so in this case what's the likelihood function or it's going to be very similar to what we have seen before in topic models, but it will be also different.	so in this case what be the likelihood function or it be go to be very similar to what we have see before in topic model , but it will be also different .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060815	312	00:12:45.11	00:12:59.23	If you still recall what the likelihood function looks like in PLSA, then you realize that in general the probability of observing a theta point from mixture model is going to be a sum over all the possibilities of generating the data.	if you still recall what the likelihood function look like in PLSA , then you realize that in general the probability of observe a theta point from mixture model be go to be a sum over all the possibility of generate the datum .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060817	318	00:13:00.27	00:13:13.56	I in this case, so it's going to be some over these K topics because everyone can be used to generate the document and then inside the sum you can still recall what the formula looks like an it's going to be.	I in this case , so it be go to be some over these k topic because everyone can be use to generate the document and then inside the sum you can still recall what the formula look like an it be go to be .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060818	324	00:13:15.2	00:13:26.61	A product of two probabilities and one is the probability of choosing a distribution. The other is the probability of observing a particular data point from that distribution.	a product of two probability and one be the probability of choose a distribution . the other be the probability of observe a particular data point from that distribution .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060821	332	00:13:27.53	00:13:46.84	So if you are map, this formula is kind of formula to our problem. Here you will see the probability of observing a document D is basically a sum, in this case over two different distributions. Because we have a very simplified situation of just two clusters.	so if you be map , this formula be kind of formula to our problem . here you will see the probability of observe a document d be basically a sum , in this case over two different distribution . because we have a very simplified situation of just two cluster .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060822	336	00:13:47.49	00:13:55.91	And so in this case you can see it's a sum of two cases. In each case it's indeed the probability of choosing the.	and so in this case you can see it be a sum of two case . in each case it be indeed the probability of choose the .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060823	342	00:13:56.81	00:14:13.65	Choosing the world distribution. Is theta one or theta two right? And then it's this probability is multiplied by the probability of observing this document from this particular distribution.	choose the world distribution . be theta one or theta two right ? and then it be this probability be multiply by the probability of observe this document from this particular distribution .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060825	351	00:14:16.3	00:14:38.91	And if you further expand this probability of observing the whole document, we see that it's product of observing each word X sub i. Here we made the assumption that each word is generated independently, so the probability of the whole document is just a product of the probability of each word in the document.	and if you far expand this probability of observe the whole document , we see that it be product of observe each word x sub i. here we make the assumption that each word be generate independently , so the probability of the whole document be just a product of the probability of each word in the document .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060827	356	00:14:39.95	00:14:52.74	So this form should be very similar to the topic model, but it's also useful to think about the difference and for that purpose I am also copying the probability of.	so this form should be very similar to the topic model , but it be also useful to think about the difference and for that purpose I be also copy the probability of .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060828	360	00:14:53.39	00:15:01.46	topic model is two components here. So here you can see at the formula looks very similar or in many ways they are similar.	topic model be two component here . so here you can see at the formula look very similar or in many way they be similar .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060828	361	00:15:02.41	00:15:05.16	But there's also some difference.	but there be also some difference .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060829	365	00:15:06	00:15:15.72	And in particular, the differences on the top you see for the mixture model, document clustering, we first take a product and then take a sum.	and in particular , the difference on the top you see for the mixture model , document clustering , we first take a product and then take a sum .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-24 10:18:21.439731	371	00:15:16.51	00:15:29.41	And that's corresponding to our assumption of 1st make a choice of choosing one distribution and then stay with this distribution with all the words. And that's why we had the product inside the sum.	and that be correspond to our assumption of 1st make a choice of choose one distribution and then stay with this distribution with all the word . and that be why we have the product inside the sum .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060831	372	00:15:30.81	00:15:32.71	The sum corresponds to the choice.	the sum correspond to the choice .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-24 10:18:25.574262	373	00:15:33.39	00:15:33.73	right.	right .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-02 23:09:48.060833	377	00:15:34.93	00:15:43.23	Now in the topic model, we see that the sum is actually inside the product and that's be cause we generated each word independently.	now in the topic model , we see that the sum be actually inside the product and that be be cause we generate each word independently .
8717e27a-33fb-4d06-ae68-2e0d915b1568	2020-11-24 10:18:43.987787	383	00:15:44.64	00:15:55.14	And that's why we have the product outside. But when we generate each each word, we have to make a decision regarding which distribution we use. So we have sum there for each word.	and that be why we have the product outside . but when we generate each each word , we have to make a decision regard which distribution we use . so we have sum there for each word .
8eaf2971-31ff-40b7-9fc6-b91c7637f916	2020-11-02 23:14:02.027779	2	00:00:00.3	00:00:03.99	This lecture is about a mixture of unigram language models.	this lecture be about a mixture of unigram language model .
8eaf2971-31ff-40b7-9fc6-b91c7637f916	2020-11-02 23:14:02.02778	11	00:00:11.79	00:00:34.42	In this lecture we will continue discussing probabilistic topic models. In particular, we're going to introduce a mixture of unigram language models. This is a slide that you have seen earlier where we talked about how to get rid of the background words that we have on top of estimated language model for one document.	in this lecture we will continue discuss probabilistic topic model . in particular , we be go to introduce a mixture of unigram language model . this be a slide that you have see early where we talk about how to get rid of the background word that we have on top of estimate language model for one document .
8eaf2971-31ff-40b7-9fc6-b91c7637f916	2020-11-02 23:14:02.02778	12	00:00:36.44	00:00:38.74	So if you want to solve the problem.	so if you want to solve the problem .
8eaf2971-31ff-40b7-9fc6-b91c7637f916	2020-11-02 23:14:02.027782	21	00:00:39.29	00:00:58.75	It will be useful to think about why we end up having this problem. Well, this is obviously because these words are very frequent in our data and we are using a maximum likelihood estimate and then the estimator obviously would have to assign high probabilities for these words in order to maximize the likelihood.	it will be useful to think about why we end up have this problem . well , this be obviously because these word be very frequent in our datum and we be use a maximum likelihood estimate and then the estimator obviously would have to assign high probability for these word in order to maximize the likelihood .
8eaf2971-31ff-40b7-9fc6-b91c7637f916	2020-11-02 23:14:02.027783	30	00:00:59.55	00:01:18.42	So in order to get rid of them, that would mean we have to do something different here. In particular, we have to say that this distribution doesn't have to explain all the words in the text data, or we're going to say these common words should not be explained by this distribution.	so in order to get rid of they , that would mean we have to do something different here . in particular , we have to say that this distribution do n't have to explain all the word in the text datum , or we be go to say these common word should not be explain by this distribution .
8eaf2971-31ff-40b7-9fc6-b91c7637f916	2020-11-02 23:14:02.027783	34	00:01:19.69	00:01:28.64	So one natural way to solve the problem is to think about using another distribution to account for just these common words.	so one natural way to solve the problem be to think about use another distribution to account for just these common word .
8eaf2971-31ff-40b7-9fc6-b91c7637f916	2020-11-02 23:14:02.027785	43	00:01:29.27	00:01:51.56	This way the two distributions can be mixed together to generate the text data and will let the other model which we called background topic model to generate the common words. This way our target is the topic theta here would be only generating the content words that characterize the content of the document.	this way the two distribution can be mix together to generate the text datum and will let the other model which we call background topic model to generate the common word . this way our target be the topic theta here would be only generate the content word that characterize the content of the document .
8eaf2971-31ff-40b7-9fc6-b91c7637f916	2020-12-10 17:50:30.19598	65	00:01:52.81	00:02:39.83	So how does this work? It's just a small modification of the previous set up where we have just one distribution. Since we now have two distributions, we have to decide which distribution to use when we generate the word, but each word will still be sampled from one of the two distributions, right? So text data is still generating the same way. Namely, we're going to generate a one word at each time. An eventually we generated a lot of words. When we generate the word, however, we're going to 1st decide which of the two distributions to use, and this is controlled by another probability. Probability of theta sub D and probability of theta sub B here.	so how do this work ? it be just a small modification of the previous set up where we have just one distribution . since we now have two distribution , we have to decide which distribution to use when we generate the word , but each word will still be sample from one of the two distribution , right ? so text datum be still generate the same way . namely , we be go to generate a one word at each time . an eventually we generate a lot of word . when we generate the word , however , we be go to 1st decide which of the two distribution to use , and this be control by another probability . probability of theta sub d and probability of theta sub b here .
8eaf2971-31ff-40b7-9fc6-b91c7637f916	2020-11-02 23:14:02.027789	70	00:02:41.77	00:02:54.69	So this is the probability of selecting the topic word distribution. This is the probability of selecting the background word distribution denoted by Theta sub B.	so this be the probability of select the topic word distribution . this be the probability of select the background word distribution denote by Theta sub B.
8eaf2971-31ff-40b7-9fc6-b91c7637f916	2020-11-02 23:14:02.027791	89	00:02:55.37	00:03:40.8	Now in this case I just give example where we can set both to .5. So if you can do basically flip a coin a fair coin to decide which one to use. But in general these probabilities don't have to be equal, so you might bias towards using one topic more than the other. So now the process of generating a word would be the first to flip a coin based on these probabilities of choosing each model and if. Let's say the coin shows up as head, which means we're going to use the topic world distribution. Then we're going to use this word distribution to generator a word. Otherwise we might be going through this path.	now in this case I just give example where we can set both to .5 . so if you can do basically flip a coin a fair coin to decide which one to use . but in general these probability do n't have to be equal , so you might bias towards use one topic more than the other . so now the process of generate a word would be the first to flip a coin base on these probability of choose each model and if . let 's say the coin show up as head , which mean we be go to use the topic world distribution . then we be go to use this word distribution to generator a word . otherwise we might be go through this path .
8eaf2971-31ff-40b7-9fc6-b91c7637f916	2020-12-12 06:51:43.140597	91	00:03:41.55	00:03:45.69	And we're going to use the background word distribution to generate the word.	and we be go to use the background word distribution to generate the word .
8eaf2971-31ff-40b7-9fc6-b91c7637f916	2020-11-02 23:14:02.027792	97	00:03:46.78	00:04:01.38	So in such a case we have a model that has some uncertainty associated with the use of a word distribution. But we can still think of this as a model for generating text data and such a model is called a mixture model.	so in such a case we have a model that have some uncertainty associate with the use of a word distribution . but we can still think of this as a model for generate text datum and such a model be call a mixture model .
8eaf2971-31ff-40b7-9fc6-b91c7637f916	2020-11-02 23:14:02.027794	113	00:04:02.64	00:04:32.77	So now let's see. In this case, what's the probability of observing the word w? Now here I showed some words like the "and ""text"", so as in all cases, once we" set up the model, we're interested in computing the likelihood function. The basic question is, so what's the probability of observing a specific word here? Now we know that the word can be observed from each of the two distributions, so we have to consider 2 cases. Therefore it's a sum over these two cases.	so now let 's see . in this case , what be the probability of observe the word w ? now here I show some word like the " and " " text " " , so as in all case , once we " set up the model , we be interested in compute the likelihood function . the basic question be , so what be the probability of observe a specific word here ? now we know that the word can be observe from each of the two distribution , so we have to consider 2 case . therefore it be a sum over these two case .
8eaf2971-31ff-40b7-9fc6-b91c7637f916	2020-11-02 23:14:02.027797	130	00:04:34.35	00:05:14.08	The first case is to use the topic word distribution to generate the word, and in such a case, then the probability would be the probability of Theta sub D, which is the probability of choosing the model multiplied by the probability of actually observing the word from that model. Both events must happen in order to "observe ""the""." We first must have chosen the topic of and then we also have to actually "have sampled the word ""the"" from the" distribution and similarly the second part accounts for a different way of generating the word from the background.	the first case be to use the topic word distribution to generate the word , and in such a case , then the probability would be the probability of Theta sub D , which be the probability of choose the model multiply by the probability of actually observe the word from that model . both event must happen in order to " observe " " the " " . " we first must have choose the topic of  and then we also have to actually " have sample the word " " the " " from the " distribution and similarly the second part account for a different way of generate the word from the background .
8eaf2971-31ff-40b7-9fc6-b91c7637f916	2020-11-02 23:14:02.027798	139	00:05:15.09	00:05:34.79	Now obviously the probability of text the same is all similar, right? So we also consider two ways of generating text, and each case is a product of the probability of choosing a particular word distribution multiplied by the probability of observing the word from that distribution.	now obviously the probability of text the same be all similar , right ? so we also consider two way of generate text , and each case be a product of the probability of choose a particular word distribution multiply by the probability of observe the word from that distribution .
8eaf2971-31ff-40b7-9fc6-b91c7637f916	2020-12-10 17:58:59.948334	151	00:05:35.44	00:05:59.59	Now later you will see this is actually general form, so you might want to make sure that you have really understood this expression here. And you should convince yourself that this is indeed the probability of observing text. So to summarize, what we observe here, the probability of a word from a mixture model is in general a sum over all different ways of generating the word.	now later you will see this be actually general form , so you might want to make sure that you have really understand this expression here . and you should convince yourself that this be indeed the probability of observe text . so to summarize , what we observe here , the probability of a word from a mixture model be in general a sum over all different way of generate the word .
8eaf2971-31ff-40b7-9fc6-b91c7637f916	2020-11-02 23:14:02.027801	159	00:06:00.48	00:06:18.87	And in each case it's a product of the probability of selecting that component model. multiplied by the probability of actually observing the data point from that component model, and this is something quite general and you will see this occurring often later.	and in each case it be a product of the probability of select that component model . multiply by the probability of actually observe the data point from that component model , and this be something quite general and you will see this occurring often later .
8eaf2971-31ff-40b7-9fc6-b91c7637f916	2020-11-02 23:14:02.027803	169	00:06:20.87	00:06:41.44	So the basic idea of a mixture model is just to treated these two distributions together as one model. So I use the box to bring all these components together. So if you view this whole box as one model, it's just like any other generative model. It would just give us the probability of a word.	so the basic idea of a mixture model be just to treat these two distribution together as one model . so I use the box to bring all these component together . so if you view this whole box as one model , it be just like any other generative model . it would just give we the probability of a word .
8eaf2971-31ff-40b7-9fc6-b91c7637f916	2020-11-02 23:14:02.027803	172	00:06:42.76	00:06:48.97	But the way that determines this probability is quite different from when we have just one distribution.	but the way that determine this probability be quite different from when we have just one distribution .
8eaf2971-31ff-40b7-9fc6-b91c7637f916	2020-11-02 23:14:02.027804	177	00:06:49.94	00:06:58.93	And this is basically a more complicated mixture model. Sorry, more complicated model than just one distribution, and it's called a mixture model.	and this be basically a more complicated mixture model . sorry , more complicated model than just one distribution , and it be call a mixture model .
8eaf2971-31ff-40b7-9fc6-b91c7637f916	2020-11-02 23:14:02.027807	197	00:07:00.37	00:07:46.14	So as I just said, we can treat this as just a generative model and it's often useful to think of just the likelihood function. The illustration that you have seen before, which is dimmer now is just the illustration of this generation model. So mathematically, this model. This is nothing but to just define the following generative model where the probability of word is assumed to be a sum over 2 cases of generating the word. The form you're seeing now is more general form than. what you have seen in the calculation earlier. I just used a simple w to denote any word, but you can still see. This is basically the first sum. Like	so as I just say , we can treat this as just a generative model and it be often useful to think of just the likelihood function . the illustration that you have see before , which be dimmer now be just the illustration of this generation model . so mathematically , this model . this be nothing but to just define the follow generative model where the probability of word be assume to be a sum over 2 case of generate the word . the form you be see now be more general form than . what you have see in the calculation early . I just use a simple w to denote any word , but you can still see . this be basically the first sum . like
8eaf2971-31ff-40b7-9fc6-b91c7637f916	2020-11-02 23:14:02.027808	203	00:07:47.45	00:07:59.46	And this sum is due to the fact that the word can be generating multiple ways. Two ways in this case. At inside sum, each term is a product again of two terms.	and this sum be due to the fact that the word can be generate multiple way . two way in this case . at inside sum , each term be a product again of two term .
8eaf2971-31ff-40b7-9fc6-b91c7637f916	2020-12-10 18:40:44.70111	210	00:08:00.22	00:08:19.42	And the two terms are ,first, the probability of selecting a component like Theta sub D of the second, the probability of actually observing the word from this component model. And so this is a very general description of, in fact, all the mixture models.	and the two term be , first , the probability of select a component like Theta sub d of the second , the probability of actually observe the word from this component model . and so this be a very general description of , in fact , all the mixture model .
8eaf2971-31ff-40b7-9fc6-b91c7637f916	2020-11-02 23:14:02.027809	214	00:08:20.08	00:08:27.22	And I just want to make sure that you understand this, because this is really the basis for understanding all kinds of topic models.	and I just want to make sure that you understand this , because this be really the basis for understand all kind of topic model .
8eaf2971-31ff-40b7-9fc6-b91c7637f916	2020-11-02 23:14:02.027812	233	00:08:28.4	00:09:10.6	So now once we set up the model and we can write down the likelihood function as we see here, the next question is how can we estimate the parameter or what to do with the parameters given the data? Well, in general we can use some observed text data to estimate the model parameters and this mission would allow us to discover the interesting knowledge about the text, so in this case, what do we discover? Well, these are represented by our parameters, and we have two kinds of parameters. One is the two word distributions. Those are two topics and the other is the coverage of each topic in each.	so now once we set up the model and we can write down the likelihood function as we see here , the next question be how can we estimate the parameter or what to do with the parameter give the datum ? well , in general we can use some observe text datum to estimate the model parameter and this mission would allow we to discover the interesting knowledge about the text , so in this case , what do we discover ? well , these be represent by our parameter , and we have two kind of parameter . one be the two word distribution . those be two topic and the other be the coverage of each topic in each .
8eaf2971-31ff-40b7-9fc6-b91c7637f916	2020-11-02 23:14:02.027814	245	00:09:12.48	00:09:35.41	The coverage of each topic and this is determined by probability of Theta sub D sub D. and probability of Theta sub B. Note that they sum to one. Now what's interesting is also to think about the special cases, like when we set one of them to one. What would happen? Well, the other would be 0, right? And if you look at the likelihood function.	the coverage of each topic and this be determine by probability of Theta sub D sub D. and probability of Theta sub B. Note that they sum to one . now what be interesting be also to think about the special case , like when we set one of they to one . what would happen ? well , the other would be 0 , right ? and if you look at the likelihood function .
8eaf2971-31ff-40b7-9fc6-b91c7637f916	2020-11-02 23:14:02.027815	250	00:09:36.21	00:09:48.11	It will then degenerate to the special case of just one distribution right so you can easily verify that by assuming one of these two is 1.0 and the other is 0.	it will then degenerate to the special case of just one distribution right so you can easily verify that by assume one of these two be 1.0 and the other be 0 .
8eaf2971-31ff-40b7-9fc6-b91c7637f916	2020-11-02 23:14:02.027815	254	00:09:49.04	00:09:58.81	So in this sense, the mixture model is more general than the previous model where we have just one distribution and it can cover that as a special case.	so in this sense , the mixture model be more general than the previous model where we have just one distribution and it can cover that as a special case .
8eaf2971-31ff-40b7-9fc6-b91c7637f916	2020-11-02 23:14:02.027816	257	00:09:59.88	00:10:04.49	So to summarize, and we talked about the mixture of two unigram language models.	so to summarize , and we talk about the mixture of two unigram language model .
8eaf2971-31ff-40b7-9fc6-b91c7637f916	2020-11-02 23:14:02.027816	259	00:10:05.04	00:10:08.4	And the data we consider here is just still 1 document.	and the datum we consider here be just still 1 document .
8eaf2971-31ff-40b7-9fc6-b91c7637f916	2020-11-02 23:14:02.027817	268	00:10:09.02	00:10:28.95	And the model is a mixture model with two components: two unigram language models. Specifically, Theta sub D which is intended to denote the topic of document D and Theta sub B which is representing a background topic that we can set to attract the common words.	and the model be a mixture model with two component : two unigram language model . specifically , Theta sub d which be intend to denote the topic of document D and Theta sub B which be represent a background topic that we can set to attract the common word .
8eaf2971-31ff-40b7-9fc6-b91c7637f916	2020-12-10 18:10:55.308553	270	00:10:29.54	00:10:32.98	Because common words would be assigned high probabilities in this model.	because common word would be assign high probability in this model .
8eaf2971-31ff-40b7-9fc6-b91c7637f916	2020-11-02 23:14:02.027821	282	00:10:33.89	00:10:58.86	So the parameters can be collectively called a Lambda, which I show here again, and you can again think about the question about how many parameters are we talking about exactly. This is usually good exercise to do because it allows you to see the model index and to have a complete understanding of what's going on in this model and we have mixing weights of course also.	so the parameter can be collectively call a Lambda , which I show here again , and you can again think about the question about how many parameter be we talk about exactly . this be usually good exercise to do because it allow you to see the model index and to have a complete understanding of what be go on in this model and we have mix weight of course also .
8eaf2971-31ff-40b7-9fc6-b91c7637f916	2020-11-02 23:14:02.027822	292	00:10:59.7	00:11:23.71	So what is the likelihood function look like? It looks very similar to what we had before, so for the document, first, it's a product of all the words in the document exactly the same as before. The only difference is that inside here now it's a sum instead of just one, so you might recall before we just had this one.	so what be the likelihood function look like ? it look very similar to what we have before , so for the document , first , it be a product of all the word in the document exactly the same as before . the only difference be that inside here now it be a sum instead of just one , so you might recall before we just have this one .
8eaf2971-31ff-40b7-9fc6-b91c7637f916	2020-11-02 23:14:02.027823	298	00:11:25.35	00:11:37.81	But now we had this sum because of the mixture model and because of the mixed model We also have to introduce the probability of choosing that particular component distribution.	but now we have this sum because of the mixture model and because of the mixed model we also have to introduce the probability of choose that particular component distribution .
8eaf2971-31ff-40b7-9fc6-b91c7637f916	2020-12-10 18:14:46.186312	309	00:11:39.449	00:12:11.539	And so this is just another way of writing it again by using a product over all the unique words in our vocabulary, instead of having a product of all the positions in the document and this form where we look at different unique words is a convenient form for computing the maximum likelihood estimator later. An the maximum likelihood estimator is, as usual, just to find the parameters.	and so this be just another way of write it again by use a product over all the unique word in our vocabulary , instead of have a product of all the position in the document and this form where we look at different unique word be a convenient form for compute the maximum likelihood estimator later . an the maximum likelihood estimator be , as usual , just to find the parameter .
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-02 23:15:08.58075	3	00:00:00.3	00:00:06.06	This lecture is about the expectation maximization algorithm, also called the EM algorithm.	this lecture be about the expectation maximization algorithm , also call the EM algorithm .
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-29 03:42:32.203427	15	00:00:13.88	00:00:39.48	In this lecture, we're going to continue the discussion of probabilistic topic models. In particular, we're going to introduce the EM algorithm, which is a family of useful algorithms for computing the maximum likelihood estimate of mixture models. So this is now familiar scenario of using a two component mixture model to try to factor out the background words from one topic words distribution here.	in this lecture , we be go to continue the discussion of probabilistic topic model . in particular , we be go to introduce the EM algorithm , which be a family of useful algorithm for compute the maximum likelihood estimate of mixture model . so this be now familiar scenario of use a two component mixture model to try to factor out the background word from one topic word distribution here .
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-12-05 18:18:46.840758	28	00:00:40.99	00:01:11.15	So we are interested in computing this estimate. And we're going to try to adjust these probability values to maximize the probability of the observed document, and know that we assume that all the other parameters are known. So the only thing unknown is this word probabilities given by theta sub d update. And in this lecture were going to look into how to compute this maximum likelihood estimator.	so we be interested in compute this estimate . and we be go to try to adjust these probability value to maximize the probability of the observed document , and know that we assume that all the other parameter be know . so the only thing unknown be this word probability give by theta sub d update . and in this lecture be go to look into how to compute this maximum likelihood estimator .
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-02 23:15:08.580761	37	00:01:12.29	00:01:31.55	Now let's start with the idea of separating the words in the text data into two groups. One group would be explained by the background model, the other group would be explained by the Unknown topic word distribution After all, this is the basic idea of mixture model.	now let 's start with the idea of separate the word in the text datum into two group . one group would be explain by the background model , the other group would be explain by the unknown topic word distribution after all , this be the basic idea of mixture model .
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-29 03:45:46.84955	42	00:01:32.2	00:01:44.79	But suppose we actually know which word is from which distribution, so that would mean, for example these words: the is and we are known to be from this background word distribution.	but suppose we actually know which word be from which distribution , so that would mean , for example these word : the be and we be know to be from this background word distribution .
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-29 03:46:17.261118	49	00:01:45.77	00:02:02.14	On the other hand, the other words, text, mining, clustering, etc are known to be from the topic word distribution. If you can see the color, then these are shown in blue. These blue words are then assumed to be from the topic word distribution.	on the other hand , the other word , text , mining , clustering , etc be know to be from the topic word distribution . if you can see the color , then these be show in blue . these blue word be then assume to be from the topic word distribution .
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-02 23:15:08.580768	62	00:02:03.1	00:02:32.9	If we already know how to separate these words, then the problem of estimating the world distribution would be extremely simple, right? If you think about this for a moment, you realize that well we can simply take all these words that are known to be from this word distribution theta sub d and normalize them. So indeed this problem will be very easy to solve. If we had known which words are from which distribution precisely.	if we already know how to separate these word , then the problem of estimate the world distribution would be extremely simple , right ? if you think about this for a moment , you realize that well we can simply take all these word that be know to be from this word distribution theta sub d and normalize they . so indeed this problem will be very easy to solve . if we have know which word be from which distribution precisely .
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-29 03:47:14.464699	63	00:02:33.44	00:02:34.99	And this is in fact	and this be in fact
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-12-05 18:21:08.401457	75	00:02:36.17	00:03:08.45	Making this model no longer mixture model because we can already observe which distribution has been used to generate which part of the data, so we actually go back to the Single word distribution problem, and in this case let's call these words that are known to be from theta d pseudo document d prime and then all we need to do is just normalize these word counts for each word w sub I.	make this model no long mixture model because we can already observe which distribution have be use to generate which part of the datum , so we actually go back to the single word distribution problem , and in this case let 's call these word that be know to be from theta d pseudo document d prime and then all we need to do be just normalize these word count for each word w sub I.
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-29 03:48:39.866628	78	00:03:09.86	00:03:17.95	And that's fairly straightforward, and it's just dictated by the maximum likelihood estimate now.	and that be fairly straightforward , and it be just dictate by the maximum likelihood estimate now .
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-29 03:49:05.466275	85	00:03:18.78	00:03:33.27	This idea, however, doesn't work, because we in practice don't really know which word is from which distribution. But this gives us the idea of perhaps we can guess which word is from which. distribution.	this idea , however , do n't work , because we in practice do n't really know which word be from which distribution . but this give we the idea of perhaps we can guess which word be from which . distribution .
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-29 03:49:10.993791	86	00:03:34.33	00:03:36.72	Specifically, given all the parameters	specifically , give all the parameter
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-29 03:50:10.910632	89	00:03:41.79	00:03:49.59	So let's assume that we actually know tentative probabilities for these words in theta sub D.	so let 's assume that we actually know tentative probability for these word in theta sub D.
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-02 23:15:08.580775	91	00:03:50.38	00:03:53.41	So now all the parameters are known for this mixture model.	so now all the parameter be know for this mixture model .
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-02 23:15:08.580776	93	00:03:55.18	00:03:58.34	And now let's consider word like a text.	and now let 's consider word like a text .
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-12-05 18:22:08.753701	97	00:03:59.04	00:04:07.91	So the question is, do you think text is more likely have been having been generated from theta sub d or from theta sub B?	so the question be , do you think text be more likely have be having be generate from theta sub d or from theta sub b ?
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-02 23:15:08.580778	100	00:04:08.64	00:04:13.51	So in other words, we want to infer which distribution has been used to generate this text.	so in other word , we want to infer which distribution have be use to generate this text .
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-29 03:51:30.031316	103	00:04:14.94	00:04:22.69	Now, this inference process is a typical Bayesian inference situation where we have some prior about	now , this inference process be a typical bayesian inference situation where we have some prior about
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-29 03:51:56.576486	110	00:04:23.27	00:04:40.14	These two distributions so can you see what is our prior here? Well the prior here is the probability of each distribution, right? So the prior is given by these two probabilities. In this case the prior is	these two distribution so can you see what be our prior here ? well the prior here be the probability of each distribution , right ? so the prior be give by these two probability . in this case the prior be
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-29 03:52:12.79165	113	00:04:41.7	00:04:47.6	Saying that each model is equally likely, but we can imagine perhaps a different prior possible.	say that each model be equally likely , but we can imagine perhaps a different prior possible .
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-29 03:52:44.456994	118	00:04:48.26	00:05:00.44	So this is called prior because this is our guess of which distribution has been used to generate the world before we even observe the word. So that's why we call it prior.	so this be call prior because this be our guess of which distribution have be use to generate the world before we even observe the word . so that be why we call it prior .
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-02 23:15:08.580784	122	00:05:01.32	00:05:07.62	if we don't observe the world or we don't know what word has been observed, our best guess is just say well.	if we do n't observe the world or we do n't know what word have be observe , our good guess be just say well .
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-02 23:15:08.580784	123	00:05:08.55	00:05:09.65	They are equally likely.	they be equally likely .
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-02 23:15:08.580785	125	00:05:10.23	00:05:11.94	Alright, so it's just a flipping a coin.	alright , so it be just a flip a coin .
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-02 23:15:08.580786	131	00:05:13.31	00:05:23.65	Now in Bayesian inference, we typically then would update our belief after we have observed evidence. So what is evidence here? While the evidence here is the word text.	now in Bayesian inference , we typically then would update our belief after we have observe evidence . so what be evidence here ? while the evidence here be the word text .
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-02 23:15:08.580787	134	00:05:24.44	00:05:31.5	Now that we are interested in the word text, so text can be regarded as evidence.	now that we be interested in the word text , so text can be regard as evidence .
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-02 23:15:08.58079	144	00:05:33.18	00:06:05.96	And in the if we use Bayes rule to combine the prior and the data likelihood, what we will end up with is to combine the prior with the likelihood that you see here, which is basically the probability of the word text from each distribution and we see that in both cases text is possible that even in the background it is still possible. It just has a very small probability.	and in the if we use Bayes rule to combine the prior and the data likelihood , what we will end up with be to combine the prior with the likelihood that you see here , which be basically the probability of the word text from each distribution and we see that in both case text be possible that even in the background it be still possible . it just have a very small probability .
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-29 03:54:27.52896	147	00:06:07.05	00:06:11.46	So intuitively, what would be your guess? So in this case	so intuitively , what would be your guess ? so in this case
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-29 03:55:28.291211	152	00:06:13.41	00:06:25.75	Now, if you're like many others, you will guess text is probably from theta sub d is more likely from philosophy, why? And you will probably see that it's because.	now , if you be like many other , you will guess text be probably from theta sub d be more likely from philosophy , why ? and you will probably see that it be because .
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-02 23:15:08.580792	154	00:06:26.85	00:06:29.78	Text has a much higher probability here.	text have a much high probability here .
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-02 23:15:08.580793	157	00:06:30.41	00:06:38.38	"By the background model, which has a very small probability.	" by the background model , which have a very small probability .
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-12-05 18:25:28.686132	164	00:06:39.08	00:06:58.32	And by this we are going to say text is "more likely from So you see our guess of which distribution has been used to generate the text would depend on how high the probability of the data the text is in each word distribution.	and by this we be go to say text be " more likely from so you see our guess of which distribution have be use to generate the text would depend on how high the probability of the datum the text be in each word distribution .
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-29 03:57:01.444957	168	00:06:59.08	00:07:09.25	We are going do tend to guess the distribution that gives the word higher probability and this is likely to maximize the likelihood right so.	we be go do tend to guess the distribution that give the word high probability and this be likely to maximize the likelihood right so .
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-02 23:15:08.580796	172	00:07:10.23	00:07:19.79	We're going to choose word that has a higher likelihood. So in other words, we're going to compare these two probabilities.	we be go to choose word that have a high likelihood . so in other word , we be go to compare these two probability .
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-02 23:15:08.580797	174	00:07:21.32	00:07:23.66	Of the word given by each distributions.	of the word give by each distribution .
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-02 23:15:08.580798	181	00:07:25.05	00:07:43.88	But our guess must also be affected by the prior, so we also need to compare these two priors why? because imagine if we adjust these probabilities, we're going to say the probability of choosing a background model is almost 100%.	but our guess must also be affect by the prior , so we also need to compare these two prior why ? because imagine if we adjust these probability , we be go to say the probability of choose a background model be almost 100 % .
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-02 23:15:08.5808	188	00:07:44.66	00:07:57.58	Now if we have that kind of strong prior, then that would affect your guess. You might think well, wait a moment, maybe text could have been from the background as well, although the probability is very small here.	now if we have that kind of strong prior , then that would affect your guess . you might think well , wait a moment , maybe text could have be from the background as well , although the probability be very small here .
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-02 23:15:08.580801	194	00:07:58.32	00:08:12.53	The prior is very high. So in the end we have to combine the two and the bayse formula provides provides us a solid and principled way of making these kind of guess to quantify that.	the prior be very high . so in the end we have to combine the two and the bayse formula provide provide we a solid and principle way of make these kind of guess to quantify that .
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-29 03:59:31.492899	200	00:08:13.33	00:08:27.85	So more specifically, let's think about the probability that this word text has been generated. In fact from theta sub D, The in order "for texture to be generated from two things must happen first.	so more specifically , let 's think about the probability that this word text have be generate . in fact from theta sub d , the in order " for texture to be generate from two thing must happen first .
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-02 23:15:08.580804	211	00:08:28.51	00:08:59.81	"The selected, so we have the selection probability here, and Secondly, we also have to actually have observed text from the distribution. So when we multiply the two together, we get the probability that text has in "fact been generated from Similarly, for the background model an. The probability of generating text is another product of similar form.	" the select , so we have the selection probability here , and secondly , we also have to actually have observe text from the distribution . so when we multiply the two together , we get the probability that text have in " fact be generate from similarly , for the background model an . the probability of generate text be another product of similar form .
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-29 04:01:07.566838	217	00:09:00.41	00:09:15.81	We also introduced a latent variable Z here to denote whether the word is from the background or the topic. When Z is zero, it means it's from the topic	we also introduce a latent variable z here to denote whether the word be from the background or the topic . when Z be zero , it mean it be from the topic
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-29 04:02:06.683867	225	00:09:16.41	00:09:41.78	"theta sub d "it's from the background So now we have the probability that text is generated from each. Then we simply we can simply normalize them to have estimate of the probability that the word text is from "theta sub d	" theta sub d " it be from the background so now we have the probability that text be generate from each . then we simply we can simply normalize they to have estimate of the probability that the word text be from " theta sub d
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-02 23:15:08.580808	228	00:09:42.37	00:09:50.22	And equivalently, the probability that Z is equal to 0 given that the observed evidence is text.	and equivalently , the probability that Z be equal to 0 give that the observe evidence be text .
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-02 23:15:08.580809	232	00:09:51.54	00:10:00.49	So this is Application of Bayes rule. But this step is very crucial for understanding the EM algorithm.	so this be Application of Bayes rule . but this step be very crucial for understand the EM algorithm .
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-29 04:02:51.637365	235	00:10:01.33	00:10:09.51	Because if we can do this, then we would be able to 1st initialize the parameter values	because if we can do this , then we would be able to 1st initialize the parameter value
93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	2020-11-29 04:03:16.171921	244	00:10:10.42	00:10:34.679999	Somewhat randomly, and then we're going to take a guess of these Z values and or which distribution has been used to generate which word and the initialized parameter values would allow us to have a complete specification of the mixture model, which further allows us to apply Bayes rule to infer which distribution is more likely to generate	somewhat randomly , and then we be go to take a guess of these z value and or which distribution have be use to generate which word and the initialize parameter value would allow we to have a complete specification of the mixture model , which far allow we to apply Bayes rule to infer which distribution be more likely to generate
95f92696-1963-4307-83c6-a8370ff03b30	2020-11-02 22:54:19.681178	2	00:00:00.3	00:00:03.18	This lecture is about the contextual text mining.	this lecture be about the contextual text mining .
95f92696-1963-4307-83c6-a8370ff03b30	2020-11-02 22:54:19.681183	11	00:00:10.94	00:00:32.7	Contextual text mining is related to multiple kinds of knowledge that we mine from text data. As I'm showing here, is related to topic mining because can make topics associated with context, like a time or location, and similarly it can make opinion mining more contextualized, making opinions connected to context.	contextual text mining be relate to multiple kind of knowledge that we mine from text datum . as I be show here , be relate to topic mining because can make topic associate with context , like a time or location , and similarly it can make opinion mining more contextualize , make opinion connect to context .
95f92696-1963-4307-83c6-a8370ff03b30	2020-11-24 17:47:58.983514	22	00:00:33.52	00:00:57.61	It's related to text based prediction because it allows us to combine non text data with text data to derive sophisticated predictors for the prediction problem. So more specifically, why are we interested in contextual text mining? Well that's, first, Because text often has rich context information and this can include direct context such as meta data.	it be relate to text base prediction because it allow we to combine non text datum with text datum to derive sophisticated predictor for the prediction problem . so more specifically , why be we interested in contextual text mining ? well that be , first , because text often have rich context information and this can include direct context such as meta datum .
95f92696-1963-4307-83c6-a8370ff03b30	2020-11-02 22:54:19.681193	26	00:00:58.38	00:01:09.81	And also interacted context, so the direct context can include the matter such as time, location, authors, and source of the text data.	and also interact context , so the direct context can include the matter such as time , location , author , and source of the text datum .
95f92696-1963-4307-83c6-a8370ff03b30	2020-11-02 22:54:19.681193	27	00:01:10.36	00:01:12.91	And they almost always available to us.	and they almost always available to we .
95f92696-1963-4307-83c6-a8370ff03b30	2020-11-24 17:53:02.602648	45	00:01:14.18	00:01:53.93	An indirect text context refers to additional data related to the meta data. So, for example, from authors, we can further obtain additional context, such as social network of the author or the author's age and such information is not ,in general, directly related to the text yet through the authors we can connect them. There could be also other text data from the same source as this one, so the other context data can be connected with this text, as well. So in general, any related data can be regarded as context, so there could be remotely related to context. context.	an indirect text context refer to additional datum relate to the meta datum . so , for example , from author , we can far obtain additional context , such as social network of the author or the author 's age and such information be not , in general , directly relate to the text yet through the author we can connect they . there could be also other text datum from the same source as this one , so the other context datum can be connect with this text , as well . so in general , any related datum can be regard as context , so there could be remotely relate to context . context .
95f92696-1963-4307-83c6-a8370ff03b30	2020-11-02 22:54:19.681208	55	00:01:55.35	00:02:16.42	And so what's the use of, why is text Contacts useful? Well, context can be used to partition text are in many interesting ways. It can almost allows partition text data in arbitrary ways as we need. And this is very important because this allows us to do interesting comparative analysis.	and so what be the use of , why be text contact useful ? well , context can be use to partition text be in many interesting way . it can almost allow partition text datum in arbitrary way as we need . and this be very important because this allow we to do interesting comparative analysis .
95f92696-1963-4307-83c6-a8370ff03b30	2020-11-02 22:54:19.681209	58	00:02:17.95	00:02:23.83	It also in general provides meaning to the discovery topics if we gonna associate the text with context.	it also in general provide meaning to the discovery topic if we gon na associate the text with context .
95f92696-1963-4307-83c6-a8370ff03b30	2020-11-24 17:55:37.489519	59	00:02:25.17	00:02:29.87	So here's illustration of how context.	so here be illustration of how context .
95f92696-1963-4307-83c6-a8370ff03b30	2020-11-02 22:54:19.681212	63	00:02:30.44	00:02:39.65	can be regarded as interesting ways of partitioning of text data. So here I just show some research papers published in different years.	can be regard as interesting way of partitioning of text datum . so here I just show some research paper publish in different year .
95f92696-1963-4307-83c6-a8370ff03b30	2020-11-24 17:56:52.079434	66	00:02:40.33	00:02:47.11	on different venues, different conference names here listed on the bottom, like SIGIR, ACL, etc.	on different venue , different conference name here list on the bottom , like SIGIR , ACL , etc .
95f92696-1963-4307-83c6-a8370ff03b30	2020-11-02 22:54:19.681215	69	00:02:49.52	00:02:55.56	Now, such text data can be partitioning, many interesting ways because we have context.	now , such text datum can be partition , many interesting way because we have context .
95f92696-1963-4307-83c6-a8370ff03b30	2020-11-02 22:54:19.681216	71	00:02:56.45	00:03:00.35	So the context here just includes time and the conference venues.	so the context here just include time and the conference venue .
95f92696-1963-4307-83c6-a8370ff03b30	2020-11-02 22:54:19.681217	73	00:03:01.31	00:03:05.12	And but perhaps we can include some other variables as well.	and but perhaps we can include some other variable as well .
95f92696-1963-4307-83c6-a8370ff03b30	2020-11-02 22:54:19.68122	80	00:03:06.4	00:03:19.46	But let's see how we can partition data in interesting ways. First, we can treat each paper as a separate unit. So in this case a paper ID and each paper has its own context, it's independent.	but let 's see how we can partition datum in interesting way . first , we can treat each paper as a separate unit . so in this case a paper ID and each paper have its own context , it be independent .
95f92696-1963-4307-83c6-a8370ff03b30	2020-11-24 18:00:25.202469	81	00:03:20.4	00:03:21	And.	and .
95f92696-1963-4307-83c6-a8370ff03b30	2020-11-02 22:54:19.681223	88	00:03:22.57	00:03:38.38	But we can also treat all the papers written in 1998 as one group, and this is only possible because of the availability of time and we can partition data in this way. This would allow us to compare topics, for example in different years.	but we can also treat all the paper write in 1998 as one group , and this be only possible because of the availability of time and we can partition datum in this way . this would allow we to compare topic , for example in different year .
95f92696-1963-4307-83c6-a8370ff03b30	2020-11-02 22:54:19.681225	94	00:03:39.71	00:03:51.57	Similarly, we can partition the data based on the venues. We can get all the SIGIR papers and compare those papers with the rest or compare SIGIR papers with KDD papers with ACL papers.	similarly , we can partition the datum base on the venue . we can get all the SIGIR paper and compare those paper with the rest or compare sigir paper with KDD paper with ACL paper .
95f92696-1963-4307-83c6-a8370ff03b30	2020-11-02 22:54:19.681229	102	00:03:52.62	00:04:12.51	We can also partition the data to obtain the papers written by authors in the US, and that of course uses additional context. of the authors and this would allow us to then compare such a subset with another set of papers written by authors in other countries.	we can also partition the datum to obtain the paper write by author in the US , and that of course use additional context . of the author and this would allow we to then compare such a subset with another set of paper write by author in other country .
95f92696-1963-4307-83c6-a8370ff03b30	2020-11-02 22:54:19.681233	113	00:04:13.79	00:04:36.06	Or we can obtain a set of papers about the text mining, and this can be compared with papers about another topic. topic. And note that these partitioning can be also intersect with each other to generate even more complicated partitions. And so in general, this enables discovery of knowledge associated with different context as needed.	or we can obtain a set of paper about the text mining , and this can be compare with paper about another topic . topic . and note that these partitioning can be also intersect with each other to generate even more complicated partition . and so in general , this enable discovery of knowledge associate with different context as need .
95f92696-1963-4307-83c6-a8370ff03b30	2020-11-02 22:54:19.68124	131	00:04:37.05	00:05:16.59	And in particular, we can compare different contexts, and this often gives us a lot of useful knowledge. For example, comparing topics overtime, we can see trends of topics and comparing topics in different context can also reveal differences about the two contexts. So there are many interesting questions that require contextual text mining here, I list some very specific ones. For example, what topics have been gaining increasing attention recently in data mining research? Now to answer this question, obviously we need to analyze text in the context of time. So time is a context in this case.	and in particular , we can compare different context , and this often give we a lot of useful knowledge . for example , compare topic overtime , we can see trend of topic and compare topic in different context can also reveal difference about the two context . so there be many interesting question that require contextual text mining here , I list some very specific one . for example , what topic have be gain increase attention recently in data mining research ? now to answer this question , obviously we need to analyze text in the context of time . so time be a context in this case .
95f92696-1963-4307-83c6-a8370ff03b30	2020-11-02 22:54:19.681242	137	00:05:17.35	00:05:27.88	Is there any difference in the responses of people in different regions to the event, to any event? So this is a very broad analysis question, in this case, of course, location is the context.	be there any difference in the response of people in different region to the event , to any event ? so this be a very broad analysis question , in this case , of course , location be the context .
95f92696-1963-4307-83c6-a8370ff03b30	2020-11-02 22:54:19.681246	147	00:05:28.55	00:05:46.18	What are the common research interests of two researchers? In this case, authors can be the context. Is there any difference in the research topics published by authors in the USA and those outside? Now, in this case, the context would include the authors and their affiliation and location.	what be the common research interest of two researcher ? in this case , author can be the context . be there any difference in the research topic publish by author in the USA and those outside ? now , in this case , the context would include the author and their affiliation and location .
95f92696-1963-4307-83c6-a8370ff03b30	2020-11-24 18:09:31.868535	151	00:05:47.72	00:05:54.8	So this goes beyond just the author himself or herself. We need to look at the additional information connected to the author.	so this go beyond just the author himself or herself . we need to look at the additional information connect to the author .
95f92696-1963-4307-83c6-a8370ff03b30	2020-11-02 22:54:19.68125	157	00:05:55.4	00:06:04.98	Is there any difference in the opinions about the topic expressed on one social network and another? In this case, the social network of authors and the topic can be the context.	be there any difference in the opinion about the topic express on one social network and another ? in this case , the social network of author and the topic can be the context .
95f92696-1963-4307-83c6-a8370ff03b30	2020-11-24 18:10:22.252812	162	00:06:06	00:06:16.18	Are there topics in news data that are correlated with sudden changes in stock prices? In this case, we can use a time series such as stock prices as context.	be there topic in news datum that be correlate with sudden change in stock price ? in this case , we can use a time series such as stock price as context .
95f92696-1963-4307-83c6-a8370ff03b30	2020-11-02 22:54:19.681253	165	00:06:17.12	00:06:21.9	What issues mattered in the 2012 presidential campaign or presidential election?	what issue matter in the 2012 presidential campaign or presidential election ?
95f92696-1963-4307-83c6-a8370ff03b30	2020-11-24 18:26:47.208709	167	00:06:22.83	00:06:25.66	Now in this case, time series again as context. df	now in this case , time series again as context . df
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069141	9	00:00:00.29	00:00:23.57	In this lecture, we continue discussing paradigmatic relation discovery. Earlier, we introduced a method called expected overlap of words in context. In this method, we represent each context by a word vector that represents the probability of word in the context and we measure the similarity by using the DOT product.	in this lecture , we continue discuss paradigmatic relation discovery . early , we introduce a method call expect overlap of word in context . in this method , we represent each context by a word vector that represent the probability of word in the context and we measure the similarity by use the DOT product .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-24 11:14:13.087772	17	00:00:30.64	00:00:49.89	Which can be interpreted as the probability that to randomly pick the words from the two contexts are identical, we also discuss the two problems of this method. The first is that it favors matching one frequent term very well over matching more distinct terms.	which can be interpret as the probability that to randomly pick the word from the two context be identical , we also discuss the two problem of this method . the first be that it favor match one frequent term very well over match more distinct term .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-24 08:38:46.052048	19	00:00:50.89	00:00:54.79	It put too much emphasis on matching one term very well.	it put too much emphasis on match one term very well .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069148	21	00:00:55.35	00:00:59.48	The second is that it treats every word equally.	the second be that it treat every word equally .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-24 08:39:19.462331	24	00:01:00.93	00:01:08.48	Even a common word like 'the' would contribute equally as content word like 'eats'.	even a common word like ' the ' would contribute equally as content word like ' eat ' .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069155	33	00:01:09.21	00:01:30.37	So now we are going to talk about how to solve these problems. Most specifically, we're going to introduce some retrieval heuristics used in text retrieval, and these heuristics can effectively solve these problems, as these problems also occur in text retrieval when we match a query vector with document vector.	so now we be go to talk about how to solve these problem . most specifically , we be go to introduce some retrieval heuristic use in text retrieval , and these heuristic can effectively solve these problem , as these problem also occur in text retrieval when we match a query vector with document vector .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069165	49	00:01:31.08	00:02:04.32	So to address the first problem, we can use a sub linear transformation of term frequency. That is, we don't have to use the raw frequency count of term to represent the context. We can transform it into some form that wouldn't emphasize so much on the raw frequency. To address the second problem, we can put more weight on rare terms. That is, we can reward matching a rare word and this heuristic is called IDF term weighting in text retrieval. IDF stands for inverse document frequency.	so to address the first problem , we can use a sub linear transformation of term frequency . that is , we do n't have to use the raw frequency count of term to represent the context . we can transform it into some form that would n't emphasize so much on the raw frequency . to address the second problem , we can put more weight on rare term . that is , we can reward match a rare word and this heuristic be call IDF term weight in text retrieval . IDF stand for inverse document frequency .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.06917	57	00:02:05.85	00:02:26.56	So now we're going to talk about the two heuristics in more detail. First, let's talk about the TF transformation. That is, to convert the raw count of word in the document into some weight that reflects our belief about how important this word in the document.	so now we be go to talk about the two heuristic in more detail . first , let 's talk about the TF transformation . that is , to convert the raw count of word in the document into some weight that reflect our belief about how important this word in the document .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069174	64	00:02:27.11	00:02:51.53	And so that will be denoted by TF of W&D as shown in the Y axis. Now in general there are many ways to map that, and let's first look at the simple way of mapping. In this case, we're going to say, any non zero counts will be mapped to one.	and so that will be denote by TF of W&D as show in the Y axis . now in general there be many way to map that , and let 's first look at the simple way of mapping . in this case , we be go to say , any non zero count will be map to one .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069186	84	00:02:53.12	00:03:44.77	And then zero count will be mapped to 0. So with this mapping, all the frequencies will be mapped to only two values, zero or one, and the mapping function is shown here as a flat line here. Now this is naive because it ignored the frequency of words. However, this actually has the advantage of emphasizing matching all the words in the context, so it does not allow a frequent word to dominate the matching. Now the approach that we have taken earlier in the expected overlap account approach is a linear transformation. We basically take Y as the same as X. So we use the raw count as representation.	and then zero count will be map to 0 . so with this mapping , all the frequency will be map to only two value , zero or one , and the mapping function be show here as a flat line here . now this be naive because it ignore the frequency of word . however , this actually have the advantage of emphasize match all the word in the context , so it do not allow a frequent word to dominate the matching . now the approach that we have take early in the expect overlap account approach be a linear transformation . we basically take Y as the same as X. so we use the raw count as representation .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-24 08:43:51.590586	89	00:03:45.78	00:03:57.8	And that created the problem that we just talked about. Namely it answers too much on just matching one frequent term. Matching one frequent term can contribute a lot.	and that create the problem that we just talk about . namely it answer too much on just match one frequent term . match one frequent term can contribute a lot .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069192	92	00:03:59.33	00:04:06.47	So we can have a lot of other interesting transformations in between the two extremes.	so we can have a lot of other interesting transformation in between the two extreme .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069195	99	00:04:07.23	00:04:20.78	And they generally form a sub linear transformation. So for example, one possibility is to take logarithm of the raw count, and this will give us curve that looks like this, right? That you're seeing here.	and they generally form a sub linear transformation . so for example , one possibility be to take logarithm of the raw count , and this will give we curve that look like this , right ? that you be see here .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-24 08:45:37.56839	106	00:04:21.51	00:04:38.429999	In this case, you can see the high frequency counts, The high counts are penalized a little bit right? So the curve is a sub linear curve, an it brings down the weight of really those really high counts.	in this case , you can see the high frequency count , the high count be penalize a little bit right ? so the curve be a sub linear curve , an it bring down the weight of really those really high count .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069202	109	00:04:39.66	00:04:46.46	And this is what we want, because it prevents that kind of terms from dominating the scoring function.	and this be what we want , because it prevent that kind of term from dominate the scoring function .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069205	115	00:04:49.61	00:05:04.54	Now there is also another interesting transformation called a BM25 transformation which has been shown to be very effective for retrieval and in this transformation we have a form that.	now there be also another interesting transformation call a BM25 transformation which have be show to be very effective for retrieval and in this transformation we have a form that .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-24 08:47:39.358495	119	00:05:06	00:05:17.92	Looks like this. I saw it's (K + 1) * X /( X + K) where K is a parameter. X is the count, the raw count of word.	look like this . I see it be ( K + 1 ) * x / ( X + K ) where K be a parameter . x be the count , the raw count of word .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.06921	123	00:05:19.18	00:05:29.4	Now the transformation is very interesting in that it can actually kind of go from one extreme to the other extreme by varying K.	now the transformation be very interesting in that it can actually kind of go from one extreme to the other extreme by vary K.
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-24 11:20:05.562251	125	00:05:31.04	00:05:34.42	And it also is interesting that it has upper bound	and it also be interesting that it have upper bind
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-24 11:20:14.850745	126	00:05:35.14	00:05:36.76	K +1 in this case.	k +1 in this case .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069215	133	00:05:37.45	00:05:55.27	So this puts a very strict constraint on high frequency terms, because their weight would never exceed K+1. As we vary K, if we can simulate the two extremes. So one case is set to zero. We roughly have the 01 vector.	so this put a very strict constraint on high frequency term , because their weight would never exceed k+1 . as we vary K , if we can simulate the two extreme . so one case be set to zero . we roughly have the 01 vector .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069217	136	00:05:56.18	00:06:01.58	Whereas when we set the key to a very large value, it would behave more like the linear transformation.	whereas when we set the key to a very large value , it would behave more like the linear transformation .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-24 08:54:31.697733	158	00:06:02.69	00:06:51.34	So this transformation function is by far the most effective transformation function for text retrieval, and it also makes sense for our problem set up. So we just talk about how to solve the problem of over emphasizing a frequently frequent term. Now let's look at the second problem, and that is how we can penalize popular terms. Matching 'the' is not surprising because 'the' occurs everywhere, but matching 'eats' with account alot. So how can we address that problem. In this case we can use the IDF weighting...that's commonly used in retrieval. IDHfor stands for inverse document frequency. Document frequency means the count of the total number of documents that contain a particular word.	so this transformation function be by far the most effective transformation function for text retrieval , and it also make sense for our problem set up . so we just talk about how to solve the problem of over emphasize a frequently frequent term . now let 's look at the second problem , and that be how we can penalize popular term . match ' the ' be not surprising because ' the ' occur everywhere , but match ' eat ' with account alot . so how can we address that problem . in this case we can use the IDF weighting ... that be commonly use in retrieval .   IDHfor stand for inverse document frequency . document frequency mean the count of the total number of document that contain a particular word .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-24 09:37:51.774501	162	00:06:52.9	00:07:02.42	So here we show that the IDM of measure is defined as a logarithm function of the number of documents that match the term.	so here we show that the IDM of measure be define as a logarithm function of the number of document that match the term .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-24 11:29:54.588308	163	00:07:03.06	00:07:04.39	or document frequency.	or document frequency .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069234	168	00:07:05.79	00:07:14.2	So K is the number of documents containing word or document frequency And M here is the total number of documents in the collection.	so K be the number of document contain word or document frequency and M here be the total number of document in the collection .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-24 09:38:56.293939	171	00:07:15.16	00:07:24.33	The IDF function is giving a higher value for a lower K, meaning that it rewards a rare term.	the IDF function be give a high value for a low k , mean that it reward a rare term .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069238	174	00:07:25.19	00:07:33	And the maximum value is log of M + 1. That's when the word occurs just once in the context.	and the maximum value be log of M + 1 . that be when the word occur just once in the context .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-24 09:39:17.514274	175	00:07:34.13	00:07:36.8	So that's a very rare term.	so that be a very rare term .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069239	177	00:07:37.69	00:07:40.16	The rarest term in the whole collection.	the rare term in the whole collection .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.06924	180	00:07:41.16	00:07:48.69	The lowest value you can see here is when K reaches its maximum, which would be M.	the low value you can see here be when K reach its maximum , which would be m.
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-24 09:39:58.850483	181	00:07:49.5	00:07:52.97	That would be a very low value.	that would be a very low value .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-24 11:31:28.132538	182	00:07:54.96	00:07:56.44	close to 0 in fact.	close to 0 in fact .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069242	183	00:07:58.29	00:07:59.52	Right so this.	right so this .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069244	186	00:08:01.3	00:08:06.39	This of course measure is used in search where we naturally have a collection.	this of course measure be use in search where we naturally have a collection .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069253	202	00:08:07.14	00:08:46.2	In our case, what will be our collection? We can also use the context that we can collect for all the words as our collection and that is to say, a word that's popular in the collection in general would also have a low IDF. Because depending on the data set, we can construct the context vectors in different ways, but in the end, if a term is very frequently in the original data set, then it would still be frequently in the collected context documents.	in our case , what will be our collection ? we can also use the context that we can collect for all the word as our collection and that be to say , a word that be popular in the collection in general would also have a low idf . because depend on the datum set , we can construct the context vector in different way , but in the end , if a term be very frequently in the original data set , then it would still be frequently in the collected context document .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-24 09:41:24.389649	204	00:08:48.64	00:08:53.89	So how can we add these heuristics to improve our.....	so how can we add these heuristic to improve our .....
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-24 09:42:02.15807	210	00:08:55.62	00:09:08.62	Our similarity function. Here's one way, and there are many other ways that are possible. But this is a reasonable way where we can adapt the BM25 retrieval model for paradigmatic relation mining.	our similarity function . here be one way , and there be many other way that be possible . but this be a reasonable way where we can adapt the BM25 retrieval model for paradigmatic relation mining .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069259	213	00:09:10.09	00:09:18.65	So here we define in this case we define the document vector.	so here we define in this case we define the document vector .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-24 11:34:05.506785	215	00:09:19.44	00:09:26.04	As containing elements representing normalized BM 25 values.	as contain element represent normalize BM 25 value .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-24 11:34:29.389091	221	00:09:27.45	00:09:47.51	So in this normalization function we see we take sum over some of all the words and then we normalize the weight of each word by the sum of the weights of all the words.	so in this normalization function we see we take sum over some of all the word and then we normalize the weight of each word by the sum of the weight of all the word .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069267	229	00:09:48.65	00:10:06.26	This is to again ensure all the x(i) will sum to one in this vector. So this would be very similar to what we had before in that this vector is actually something similar to word distribution or the exercise with sum to one.	this be to again ensure all the x(i ) will sum to one in this vector . so this would be very similar to what we have before in that this vector be actually something similar to word distribution or the exercise with sum to one .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069268	231	00:10:07.29	00:10:12.65	Now the weight of BM25 for each word is defined here.	now the weight of BM25 for each word be define here .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.06927	234	00:10:15.12	00:10:21.41	And if you compare this with our old definition where we just have a normalized count.	and if you compare this with our old definition where we just have a normalize count .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069272	238	00:10:22.07	00:10:30.79	On this one, right? So we only have this one and the document length or the total count of words in that context document.	on this one , right ? so we only have this one and the document length or the total count of word in that context document .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069273	241	00:10:31.42	00:10:37.66	And that's what we had before. But now with the BM 25 transformation, we introduced something else.	and that be what we have before . but now with the BM 25 transformation , we introduce something else .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069275	244	00:10:38.76	00:10:45.74	First, of course, this extra occurrence of this count is just to achieve the sub linear normalization.	first , of course , this extra occurrence of this count be just to achieve the sub linear normalization .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069275	246	00:10:46.51	00:10:49.65	But we also see we introduce the parameter K here.	but we also see we introduce the parameter K here .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069277	249	00:10:50.76	00:10:58.08	And this parameter is generally non negetive number, although zero is also possible.	and this parameter be generally non negetive number , although zero be also possible .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069279	253	00:10:59.6	00:11:10.34	This controls the upper bound and the kinds controls can choose To what extent is simulates the linear transformation.	this control the upper bind and the kind control can choose to what extent be simulate the linear transformation .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069281	259	00:11:11.42	00:11:24.7	And so this is 1 parameter. But we also see there is another parameter here b and this will be within zero and one. And this is a parameter to control Lance normalization.	and so this be 1 parameter . but we also see there be another parameter here b and this will be within zero and one . and this be a parameter to control Lance normalization .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069283	262	00:11:25.5	00:11:31.45	And, and in this case the normalizing formula has average document length here.	and , and in this case the normalizing formula have average document length here .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069285	268	00:11:32.16	00:11:44.62	And this is the computed by taking the average of the lengths of all the documents in the collection. In this case, all the lengths of all the context documents that we are considering.	and this be the compute by take the average of the length of all the document in the collection . in this case , all the length of all the context document that we be consider .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069287	272	00:11:45.45	00:11:56.97	So this average documents will be a constant for any given collection, so it actually is only affecting the effect of the parameter B here.	so this average document will be a constant for any give collection , so it actually be only affect the effect of the parameter B here .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069287	273	00:11:58.93	00:12:00.68	Because this is a constant.	because this be a constant .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-24 11:37:24.993558	279	00:12:01.75	00:12:19.43	But I kept it here because it's constant that's useful in retrieval, where it would give us a stabilized interpretation of parameter b. But for our purpose this will be a constant, so it would only be	but I keep it here because it be constant that be useful in retrieval , where it would give we a stabilize interpretation of parameter b. but for our purpose this will be a constant , so it would only be
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-24 09:52:32.7803	280	00:12:20.77	00:12:23.94	Affecting the lenngth formalization.	affect the lenngth formalization .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-24 11:37:44.446727	281	00:12:25.24	00:12:27.79	together with parameter B.	together with parameter B.
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069294	290	00:12:28.67	00:12:51.599999	Now with this definition, then we have a new way to define our document vectors and we can compute the vector D2 in the same way. The difference is that the high frequency terms will now have a somewhat lower weights and this would help control the influence of these high frequency terms.	now with this definition , then we have a new way to define our document vector and we can compute the vector D2 in the same way . the difference be that the high frequency term will now have a somewhat low weight and this would help control the influence of these high frequency term .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069297	297	00:12:53.93	00:13:10.78	Now the IDF can be added here in the scoring function. That means we'll introduce weight for matching each term. So you may recall this sum indicates all the possible words that can be a overlap between the two contexts.	now the IDF can be add here in the scoring function . that mean we 'll introduce weight for match each term . so you may recall this sum indicate all the possible word that can be a overlap between the two context .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-24 11:39:11.796546	298	00:13:11.42	00:13:16.51	And the Xi and Yi probabilities of	and the Xi and Yi probability of
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.06931	323	00:13:17.66	00:14:17.18	Of picking the word from both contexts, therefore it indicates how likely will see a match on this word. Now IDF would give us the importance of matching this word. A common word will be worth less than rare word, so we emphasize more on matching rare words now. So with this modification, then the new function will likely address those two problems. Now interestingly we can also use this approach to discover syntagmatic relations. In general, when we represent a term vector to represent the sorry to represent context with the term vector, we would likely see some terms have higher weights and other terms have lower weights depending on how we assign weights to these terms, we might be able to use these weights to discover the words that are strongly associated with the candidate word in the context.	of pick the word from both context , therefore it indicate how likely will see a match on this word . now IDF would give we the importance of match this word . a common word will be worth less than rare word , so we emphasize more on match rare word now . so with this modification , then the new function will likely address those two problem . now interestingly we can also use this approach to discover syntagmatic relation . in general , when we represent a term vector to represent the sorry to represent context with the term vector , we would likely see some term have high weight and other term have low weight depend on how we assign weight to these term , we might be able to use these weight to discover the word that be strongly associate with the candidate word in the context .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069312	328	00:14:18.91	00:14:32.92	So let's take a look at the term vector in more detail here. And we have each Xi, defined as a normalized weight of BM 25.	so let 's take a look at the term vector in more detail here . and we have each Xi , define as a normalize weight of BM 25 .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069313	331	00:14:33.86	00:14:40.23	Now this weight alone only reflects how frequently the word occurs in the context.	now this weight alone only reflect how frequently the word occur in the context .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-24 10:01:36.241422	334	00:14:41.57	00:14:47.82	But we can't just say any frequent term in the context that would be correlated with the candidate word.	but we ca n't just say any frequent term in the context that would be correlate with the candidate word .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069315	336	00:14:49.19	00:14:53.76	Because many common words like 'the' there will occur frequently in all the context.	because many common word like ' the ' there will occur frequently in all the context .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-24 11:41:05.637961	338	00:14:55.31	00:15:01.51	But if we apply IDF weighting as you see here, we can then	but if we apply IDF weighting as you see here , we can then
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069321	350	00:15:03.49	00:15:33.71	We wait these terms based on IDF That means the words that are common, like 'the' will get penalized. So now the highest weighted terms will not be those common terms because they have lower IDFs. Instead, those terms would be the terms that are frequent in the context, but not frequently in the collection. So those are clearly the words that tend to occur in the context of the candidate word, for example, cat.	we wait these term base on IDF that mean the word that be common , like ' the ' will get penalize . so now the high weight term will not be those common term because they have low idfs . instead , those term would be the term that be frequent in the context , but not frequently in the collection . so those be clearly the word that tend to occur in the context of the candidate word , for example , cat .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-24 10:03:26.612647	352	00:15:34.04	00:15:39.56	So for this reason, the highly weighted terms in this IDF weighted vector.	so for this reason , the highly weight term in this IDF weight vector .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-24 10:04:21.33962	360	00:15:40.24	00:16:00.78	can also be assumed to be candidate for Syntagmatic relations. Now of course, this is only a bi-product of our approach for discovering paradigmatic relations. And in the next lecture, we're going to talk more about how to discover Syntagmatic relations.	can also be assume to be candidate for syntagmatic relation . now of course , this be only a bi - product of our approach for discover paradigmatic relation . and in the next lecture , we be go to talk more about how to discover syntagmatic relation .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069327	365	00:16:03.28	00:16:17.73	But it clearly shows the relation between discovering the two relations. And indeed they can be discussed, discovered in a joint manner by leveraging such associations.	but it clearly show the relation between discover the two relation . and indeed they can be discuss , discover in a joint manner by leverage such association .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-24 11:42:45.605565	366	00:16:18.9	00:16:20.22	So to summarize,	so to summarize ,
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069331	374	00:16:21.07	00:16:39.57	The main idea for discovering paradigmatic relations is to collect the context of a candidate word to form a pseudo document, and this is typically represented as a bag of words. And then compute the similarity of the corresponding context documents of two candidate words.	the main idea for discover paradigmatic relation be to collect the context of a candidate word to form a pseudo document , and this be typically represent as a bag of word . and then compute the similarity of the corresponding context document of two candidate word .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-24 10:05:40.535664	377	00:16:40.87	00:16:49.92	An then we can take the highly similar word pairs and treat them as having paradigmatic relations.	an then we can take the highly similar word pair and treat they as have paradigmatic relation .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069333	379	00:16:50.69	00:16:52.85	These are the words that share similar context.	these be the word that share similar context .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069334	382	00:16:53.88	00:17:00.96	And there are many different ways to implement this general idea and we just talk about some of the approaches.	and there be many different way to implement this general idea and we just talk about some of the approach .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-02 23:06:23.069337	387	00:17:01.61	00:17:14.21	And more specifically, we talked about using text retrieval models to help us design effective similarity function to compute the paradigmatic relations.	and more specifically , we talk about use text retrieval model to help we design effective similarity function to compute the paradigmatic relation .
9a443634-7f2e-4d3a-9ccd-0f1b6604c939	2020-11-24 10:06:53.611501	389	00:17:16.91	00:17:22.76	More specifically, we have used the BM25 An idea of waiting to.	more specifically , we have use the BM25 an idea of wait to .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.794003	3	00:00:00.29	00:00:04.69	This lecture is about the syntagmatic relation discovery and mutual information.	this lecture be about the syntagmatic relation discovery and mutual information .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-27 23:15:01.405275	9	00:00:13.27	00:00:23.59	In this lecture, we're going to continue discussing syntagmatic relation discovery. In particular, we're going to talk about another concept, the information theory, called Mutual information.	in this lecture , we be go to continue discuss syntagmatic relation discovery . in particular , we be go to talk about another concept , the information theory , call mutual information .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.794012	25	00:00:24.55	00:01:03.47	And how it can be used to discover syntagmatic relations? Before we talked about a problem of conditional entropy, and that is the conditional entropy computed on different pairs of words is not really comparable, so that makes it hard to discover strong syntagmatic relations globally from corpus. So now we're going to introduce mutual information, which is another concept in information theory that allows us to, in some sense, normalize the conditional entropy to make. a more comparable across different pairs.	and how it can be use to discover syntagmatic relation ? before we talk about a problem of conditional entropy , and that be the conditional entropy compute on different pair of word be not really comparable , so that make it hard to discover strong syntagmatic relation globally from corpus . so now we be go to introduce mutual information , which be another concept in information theory that allow we to , in some sense , normalize the conditional entropy to make . a more comparable across different pair .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-27 23:17:38.18785	33	00:01:04.78	00:01:25.48	In particular, mutual information, denoted by I of X&Y, measures the entropy reduction of X obtained from knowing Y. More specifically the question we're interested in here, is how much reduction in the entropy of X can we obtain by knowing Y.	in particular , mutual information , denote by I of X&Y , measure the entropy reduction of X obtain from know Y. more specifically the question we be interested in here , be how much reduction in the entropy of x can we obtain by know Y.
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.794016	37	00:01:26.91	00:01:36.82	So mathematically, it can be defined as the difference between the original entropy of X and the conditional entropy of X given Y.	so mathematically , it can be define as the difference between the original entropy of x and the conditional entropy of x give Y.
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.794018	42	00:01:37.84	00:01:47.81	And you might see here you can see here. It can also be defined as a reduction of entropy of Y, because of knowing X.	and you might see here you can see here . it can also be define as a reduction of entropy of Y , because of know X.
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-27 23:20:48.132665	63	00:01:48.76	00:02:40.35	Normally the two conditional entropies H(X|Y) and H(Y|X) are not equal, but interestingly But interestingly, the reduction of entropy. by knowing one of them is actually equal, so this quantity is called mutual information denoted by I here and this function has some interesting properties. First, it's also non negative. This is easy to understand becausw the original entropy is always not going to be lower than the possibly reduce the conditional entropy. In other words, the conditional entropy would never exceed the original entropy. Knowing some information can always help us potentially, but won't hurt us in predicting X.	normally the two conditional entropy H(X|Y ) and H(Y|X ) be not equal , but interestingly  but interestingly , the reduction of entropy . by know one of they be actually equal , so this quantity be call mutual information denote by I here and this function have some interesting property . first , it be also non negative . this be easy to understand becausw the original entropy be always not go to be low than the possibly reduce the conditional entropy . in other word , the conditional entropy would never exceed the original entropy . know some information can always help we potentially , but wo n't hurt we in predict X.
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.794027	67	00:02:41.43	00:02:48.25	The second property is that it's symmetric while conditional entropy is not symmetrical. Mutual information is.	the second property be that it be symmetric while conditional entropy be not symmetrical . mutual information be .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-27 23:21:07.784405	68	00:02:49.8	00:02:52.44	The third property is that?	the third property be that ?
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.79403	73	00:02:53.52	00:03:05.39	it reaches its minimum zero if and only if the two random variables are completely independent. That means knowing one of them doesn't tell us anything about the other.	it reach its minimum zero if and only if the two random variable be completely independent . that mean know one of they do n't tell we anything about the other .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.79403	76	00:03:07.19	00:03:14.47	And this last property can be verified by simply looking at the equation above.	and this last property can be verify by simply look at the equation above .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.794033	83	00:03:15.03	00:03:30.64	And it reaches 0 if and only if the conditional entropy of X given Y is exactly the same as original entropy of X. So that means knowing why did not help at all, and that's when X&Y are completely independent.	and it reach 0 if and only if the conditional entropy of x give Y be exactly the same as original entropy of X. So that mean know why do not help at all , and that be when X&Y be completely independent .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.794036	92	00:03:31.99	00:03:56.32	Now when we fix X to rank different Ys using conditional entropy would give the same order as ranking based on mutual information, because in the function here H of X is fixed because X is fixed. So ranking based on mutual information is exactly the same as ranking based on the conditional entropy of X given Y.	now when we fix x to rank different Ys use conditional entropy would give the same order as rank base on mutual information , because in the function here H of X be fix because X be fix . so rank base on mutual information be exactly the same as rank base on the conditional entropy of x give Y.
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.794038	96	00:03:57.28	00:04:08.12	But the mutual information allows us to compare different pairs of X&Y, so that's why mutual information is more general and in general more useful.	but the mutual information allow we to compare different pair of X&Y , so that be why mutual information be more general and in general more useful .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.794039	99	00:04:10.55	00:04:16.03	So let's examine them intuition of using mutual information for syntagmatic relation mining.	so let 's examine they intuition of use mutual information for syntagmatic relation mining .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.79404	103	00:04:17.01	00:04:24.43	Now the question we ask for syntactic relation mining is whenever eats occurs, what other words also tend to occur?	now the question we ask for syntactic relation mining be whenever eat occur , what other word also tend to occur ?
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.794043	110	00:04:25.47	00:04:37.71	So this question can be framed as a mutual information question, that is, which was have higher mutual information with eats. So we're going to compute the mutual information between eats and other words.	so this question can be frame as a mutual information question , that is , which be have high mutual information with eat . so we be go to compute the mutual information between eat and other word .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-27 23:24:12.366178	124	00:04:38.46	00:05:08.58	And if we do that, and it's basically a based on the same intuition as in conditional entropy, we will see that words that are strongly associated with each will tend to have high mutual information, whereas words that are not related. We have lower mutual information, so this I give some example here. The mutual information between eats and meat, which is the same as between meats and eats cause major information is symmetric is expected to be higher than	and if we do that , and it be basically a base on the same intuition as in conditional entropy , we will see that word that be strongly associate with each will tend to have high mutual information , whereas word that be not relate . we have low mutual information , so this I give some example here . the mutual information between eat and meat , which be the same as between meat and eat cause major information be symmetric be expect to be high than
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.79405	130	00:05:09.13	00:05:20.6	The mutual information between East and the. Because knowing the doesn't really help us predict eats. Similarly knowing eats doesn't help us predicting the as well.	the mutual information between East and the . because know the do n't really help we predict eat . similarly know eat do n't help we predict the as well .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-27 23:25:37.466987	135	00:05:21.32	00:05:35.86	And you also can easily see that the mutual information between a word and itself is the largest which is equal to the mutual info. The entropy of this word.	and you also can easily see that the mutual information between   a word and itself be the large which be equal to the mutual info . the entropy of this word .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-27 23:27:19.3113	145	00:05:37.62	00:06:01.71	So because in this case the reduction is maximum because knowing one would allow the predict the other completely so the conditional entropy is zero. Therefore the mutual information reaches its maximum. It's going to be larger than or equal to the machine between it's an another word.	so because in this case the reduction be maximum because know one would allow the predict the other completely so the conditional entropy be zero . therefore the mutual information reach its maximum . it be go to be large than or equal to the machine between it be an another word .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-27 23:28:25.157105	151	00:06:02.39	00:06:13.67	In other words, picking any other word, and computing measure information between eats and that word. you won't get any visual information larger than the mutual information between eats and itself.	in other word , pick any other word , and compute measure information between eat and that word . you wo n't get any visual information large than the mutual information between eat and itself .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.794061	154	00:06:16.44	00:06:23.59	So now let's think about how to compute the mutual information. Now, in order to do that, we often.	so now let 's think about how to compute the mutual information . now , in order to do that , we often .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.794065	164	00:06:24.83	00:06:48.91	use a different form of mutual information, and we can mathematically write the mutual information into the form shown on this slide, where we essentially see a formula that computes what's called KL-divergences or callback labeler divergance. This is another term in information theory that measures the divergance between two distributions.	use a different form of mutual information , and we can mathematically write the mutual information into the form show on this slide , where we essentially see a formula that compute what be call KL - divergence or callback labeler divergance . this be another term in information theory that measure the divergance between two distribution .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.79407	179	00:06:50.49	00:07:22.85	Now if you look at the formula, it's also sum over many combinations of different values of the two random variables, but inside the sum mainly we're doing a comparison between 2 joint distributions. The numerator has the joint actual observed. Join the distribution of the two random variables. The bottom part of the denominator can be interpreted as the expected joint distribution of the two random variables. If there were independent.	now if you look at the formula , it be also sum over many combination of different value of the two random variable , but inside the sum mainly we be do a comparison between 2 joint distribution . the numerator have the joint actual observe . join the distribution of the two random variable . the bottom part of the denominator can be interpret as the expect joint distribution of the two random variable . if there be independent .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.794072	183	00:07:24.28	00:07:32.9	Because when two random variables are independent, they joined distribution is equal to the product of the two probabilities.	because when two random variable be independent , they join distribution be equal to the product of the two probability .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.794074	188	00:07:35.17	00:07:43.4	So this comparison would tell us whether the two variables are indeed independent if there indeed independent, then we would expect that the two are the same.	so this comparison would tell we whether the two variable be indeed independent if there indeed independent , then we would expect that the two be the same .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-27 23:30:36.055046	192	00:07:44.31	00:07:54.7	But if the numerator is different from the denominator, that would mean the two variables are not independent, and that helps measure the association.	but if the numerator be different from the denominator , that would mean the two variable be not independent , and that help measure the association .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.794079	199	00:07:55.6	00:08:11	The sum is simply to take into consideration of all the combinations of the values of these two random variables. In our case, each random variable can choose one of the two values 0 or 1, so we have four combinations here.	the sum be simply to take into consideration of all the combination of the value of these two random variable . in our case , each random variable can choose one of the two value 0 or 1 , so we have four combination here .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.794083	207	00:08:12.31	00:08:29.55	So if we look at this form of mutual information it shows that the mutual information measures the diversions of the actual joint distribution from the expected distribution under the independence assumption. The larger this divergence is, the higher the mutual information would be.	so if we look at this form of mutual information it show that the mutual information measure the diversion of the actual joint distribution from the expect distribution under the independence assumption . the large this divergence be , the high the mutual information would be .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.794084	210	00:08:33.47	00:08:40	So now let's further look at the what are exactly the probabilities involved in this formula of mutual information.	so now let 's far look at the what be exactly the probability involve in this formula of mutual information .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.794085	215	00:08:41.13	00:08:55.64	And here I listed all the probabilities involved and it's easy for you to verify that basically we have first 2 probabilities corresponding to the presence or absence of each word.	and here I list all the probability involve and it be easy for you to verify that basically we have first 2 probability correspond to the presence or absence of each word .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.794086	217	00:08:56.25	00:08:59.8	So for W1, we have two probabilities shown here.	so for W1 , we have two probability show here .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.794087	220	00:09:00.66	00:09:08.26	They should sum to 1 because a word can either be present or absent in the segment.	they should sum to 1 because a word can either be present or absent in the segment .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.79409	229	00:09:11.02	00:09:29.26	And similarly for the second word, we also have two probabilities representing presence or absence of this word, and there's something one as well. And then finally we have a lot of joint probabilities that represented the scenarios of Co-occurrences of the two words.	and similarly for the second word , we also have two probability represent presence or absence of this word , and there be something one as well . and then finally we have a lot of joint probability that represent the scenario of Co - occurrence of the two word .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.794091	230	00:09:30.06	00:09:31.36	And they are shown here.	and they be show here .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.794092	234	00:09:33.59	00:09:42.82	Right, so this sums to 1 cause. the two words can only have these four possible scenarios. Either they both occur.	right , so this sum to 1 cause . the two word can only have these four possible scenario . either they both occur .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.794094	238	00:09:43.46	00:09:50.69	So in that case both variables will have a value of one or one of them occurs. There are two scenarios.	so in that case both variable will have a value of one or one of they occur . there be two scenario .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.794097	245	00:09:51.51	00:10:06.61	In these two cases, one of the random variables will be equal to 1 and the other would be 0. And finally we have the scenario when none of them occurs. So this is when the two variables taking a value of 0.	in these two case , one of the random variable will be equal to 1 and the other would be 0 . and finally we have the scenario when none of they occur . so this be when the two variable take a value of 0 .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.794098	248	00:10:07.34	00:10:12.49	And they're summing up to 1, so these are the probabilities involved in the calculation of mutual information.	and they be sum up to 1 , so these be the probability involve in the calculation of mutual information .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-27 23:33:40.070493	249	00:10:13.09	00:10:13.79	here.	here .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.794099	252	00:10:15.74	00:10:20.67	Once we know how to calculate these probabilities, we can easily calculate the mutual information.	once we know how to calculate these probability , we can easily calculate the mutual information .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.794104	266	00:10:24.09	00:10:57.54	It's also interesting to note that there are after some relations or constraints among these probabilities, and we already saw two of them, so the in the previous slide that you have seen that the marginal probabilities of these words sum to one, and we also have seen this constraint that says the two words can only have these four different scenarios of Co occurrences, but we also have some additional constraints listed in the bottom.	it be also interesting to note that there be after some relation or constraint among these probability , and we already see two of they , so the in the previous slide that you have see that the marginal probability of these word sum to one , and we also have see this constraint that say the two word can only have these four different scenario of co occurrence , but we also have some additional constraint list in the bottom .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.79411	282	00:10:58.19	00:11:32.629999	And so, for example, this one means if we add up the probabilities that we observe the two words occur together and the probabilities when the word the first word occurs and the second word doesn't occur, we get exactly the probability that the first word is observed. In other words, and when the word is observed when the first word is observed and there are only two scenarios depending on weather second word is also observed. So this probability captures the first scenario when the signal word actually is also observed.	and so , for example , this one mean if we add up the probability that we observe the two word occur together and the probability when the word the first word occur and the second word do n't occur , we get exactly the probability that the first word be observe . in other word , and when the word be observe when the first word be observe and there be only two scenario depend on weather second word be also observe . so this probability capture the first scenario when the signal word actually be also observe .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.794111	285	00:11:33.59	00:11:39.77	And this captures the second scenario. when the seond word is not observed, so we only see the first word.	and this capture the second scenario . when the seond word be not observe , so we only see the first word .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.794112	288	00:11:40.44	00:11:45.57	And it's easy to see the other equations also follow the same reasoning.	and it be easy to see the other equation also follow the same reasoning .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.794113	292	00:11:46.84	00:11:54.73	Now these equations allow us to compute some probabilities based on other probabilities. And this can simplify the computation.	now these equation allow we to compute some probability base on other probability . and this can simplify the computation .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-27 23:37:11.155481	302	00:11:55.62	00:12:20.27	So more specifically, and if we know the probability that a word is present, and in this case right? So if we know this. And if we know the presence of the probability of presence of the second word, then we can easily compute their absence probability, right? It's very easy to use this equation to do that.	so more specifically , and if we know the probability that a word be present , and in this case right ? so if we know this . and if we know the presence of the probability of presence of the second word , then we can easily compute their absence probability , right ? it be very easy to use this equation to do that .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.794119	310	00:12:21.06	00:12:38.77	An so we this will take care of the computation of these probabilities of presence or absence of each word. Now let's look at their joint distribution, right? Let's assume that we also have available probability that they occur together.	an so we this will take care of the computation of these probability of presence or absence of each word . now let 's look at their joint distribution , right ? let 's assume that we also have available probability that they occur together .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.79412	313	00:12:39.38	00:12:45.88	Now it's easy to see that we can actually compute the all the rest of these probabilities based on these.	now it be easy to see that we can actually compute the all the rest of these probability base on these .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.794122	319	00:12:46.73	00:12:59.34	Specifically, for example, using this equation, we can compute the probability that the first word occurred and the second word did not, because we know these probabilities in the boxes.	specifically , for example , use this equation , we can compute the probability that the first word occur and the second word do not , because we know these probability in the box .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.794123	322	00:13:00.13	00:13:05.75	And similarly, using this equation we can compute the probability that we observe only the second word.	and similarly , use this equation we can compute the probability that we observe only the second word .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-27 23:38:12.676317	327	00:13:06.51	00:13:18.18	And then finally we. This probability can be calculated by using this equation, because now this is known and this is also known and this is already known right?	and then finally we . this probability can be calculate by use this equation , because now this be know and this be also know and this be already know right ?
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.794125	328	00:13:19.46	00:13:21.68	So this can be easier to calculate.	so this can be easy to calculate .
9e5a0c5f-ff4a-42a6-b37b-f43628632860	2020-11-02 23:05:13.794125	329	00:13:22.81	00:13:24.54	Right, so now this can be calculated.	right , so now this can be calculate .
a97a9d5e-48b7-4f4e-9754-4c5b30a31424	2020-11-24 16:54:33.091649	2	00:00:00.3	00:00:03.24	This lecture is about text based "	this lecture be about text base "
a97a9d5e-48b7-4f4e-9754-4c5b30a31424	2020-11-02 22:57:03.986601	9	00:00:11.29	00:00:29.58	In this lecture we're going to start talking about mining a different kind of knowledge as you, you can see here on this slide. Namely, we're going to use text data to infer values of some other variables in the real world.	in this lecture we be go to start talk about mine a different kind of knowledge as you , you can see here on this slide . namely , we be go to use text datum to infer value of some other variable in the real world .
a97a9d5e-48b7-4f4e-9754-4c5b30a31424	2020-11-24 16:19:50.960062	22	00:00:30.45	00:01:03.21	That may not be directly related to the text, or only remotely related to text So this is very different from content analysis or topic mining where we directly characterize the content of text. " It's also different from opinion mining or sentiment analysis, which still have to do with characterizing mostly the content only that we focus more on the subjective which reflects what we know about the opinion holder.	that may not be directly relate to the text , or only remotely relate to text so this be very different from content analysis or topic mining where we directly characterize the content of text . " it be also different from opinion mining or sentiment analysis , which still have to do with characterize mostly the content only that we focus more on the subjective which reflect what we know about the opinion holder .
a97a9d5e-48b7-4f4e-9754-4c5b30a31424	2020-11-24 17:17:14.92448	30	00:01:04.59	00:01:27.17	But this only provides limited view of what we can predict. In this lecture and the following lectures, we're going to talk more about how we can predict more information about the world. How can we get sophisticated patterns of text together with other kinds ofdata? "	but this only provide limited view of what we can predict . in this lecture and the follow lecture , we be go to talk more about how we can predict more information about the world . how can we get sophisticated pattern of text together with other kind of  datum ? "
a97a9d5e-48b7-4f4e-9754-4c5b30a31424	2020-11-24 17:17:07.883617	34	00:01:28.25	00:01:35.61	It would be useful to first take a look at the big picture of prediction in data mining in general and I call this "	it would be useful to first take a look at the big picture of prediction in data mining in general and I call this "
a97a9d5e-48b7-4f4e-9754-4c5b30a31424	2020-11-02 22:57:03.986608	44	00:01:36.33	00:01:58.79	So the picture that you're seeing right now is that there are multiple sensors, including human sensors to report what we have seen in the real world in theform of data. " And of course the data are in the form of non text data and text data. And our goal is to see if we can predict some values of important the real world variables that matter to us.	so the picture that you be see right now be that there be multiple sensor , include human sensor to report what we have see in the real world in the  form of datum . " and of course the datum be in the form of non text datum and text datum . and our goal be to see if we can predict some value of important the real world variable that matter to we .
a97a9d5e-48b7-4f4e-9754-4c5b30a31424	2020-11-02 22:57:03.986609	50	00:01:59.39	00:02:13.29	For example, someone's health condition or the weather, or etc. So these variables would be important because we might want to act on that. We might want to make decisions based on that.	for example , someone 's health condition or the weather , or etc . so these variable would be important because we might want to act on that . we might want to make decision base on that .
a97a9d5e-48b7-4f4e-9754-4c5b30a31424	2020-11-24 16:22:33.776164	55	00:02:14.57	00:02:22.809999	So how can we get from the data to these predicted values? Well, in general we first have to do data mining and analysis of the data.	so how can we get from the datum to these predict value ? well , in general we first have to do data mining and analysis of the datum .
a97a9d5e-48b7-4f4e-9754-4c5b30a31424	2020-11-02 22:57:03.986611	57	00:02:23.64	00:02:28.8	Because we in general should treat all the data that we collected.	because we in general should treat all the datum that we collect .
a97a9d5e-48b7-4f4e-9754-4c5b30a31424	2020-11-02 22:57:03.986611	61	00:02:29.74	00:02:39.64	in such a prediction problem set up, we we are very much interested in joint mining of non text and text data. We should mine all the data together.	in such a prediction problem set up , we we be very much interested in joint mining of non text and text datum . we should mine all the datum together .
a97a9d5e-48b7-4f4e-9754-4c5b30a31424	2020-11-02 22:57:03.986613	69	00:02:41.73	00:03:01.48	And then through the analysis, we generally can generate the multiple predictors of this interesting variables to us, and we call these features. And these features can then be put into a predictive model to actually predict the value of any interesting variable.	and then through the analysis , we generally can generate the multiple predictor of this interesting variable to we , and we call these feature . and these feature can then be put into a predictive model to actually predict the value of any interesting variable .
a97a9d5e-48b7-4f4e-9754-4c5b30a31424	2020-11-24 14:42:37.731726	73	00:03:02.51	00:03:15.95	So this then allows us to change the world and so this basically is the general process for making a prediction based on data including text data.	so this then allow we to change the world and so this basically be the general process for make a prediction base on datum include text datum .
a97a9d5e-48b7-4f4e-9754-4c5b30a31424	2020-11-02 22:57:03.986614	76	00:03:16.9	00:03:23.16	Now it's important to emphasize that human actually plays very important role in this process.	now it be important to emphasize that human actually play very important role in this process .
a97a9d5e-48b7-4f4e-9754-4c5b30a31424	2020-11-02 22:57:03.986616	86	00:03:24.15	00:03:47.11	Especially because of the involvement of text data. And so human first would be involved in the mining of the data. It will control the generation of these features. And also help us understand the text data because text data are created to be consumed by humans. Humans are the best in consuming or interpreting text data.	especially because of the involvement of text datum . and so human first would be involve in the mining of the datum . it will control the generation of these feature . and also help we understand the text datum because text datum be create to be consume by human . human be the good in consume or interpret text datum .
a97a9d5e-48b7-4f4e-9754-4c5b30a31424	2020-11-24 17:23:04.481342	90	00:03:48.18	00:03:54.65	But when there are, of course a lot of text than machines have to help, and that's why we need to do text data mining. "	but when there be , of course a lot of text than machine have to help , and that be why we need to do text datum mining . "
a97a9d5e-48b7-4f4e-9754-4c5b30a31424	2020-11-02 22:57:03.98662	107	00:03:55.57	00:04:35.19	Sometimes machines can see patterns in a lot of data that humans may not see, but in general human would play an important role in analyzing text data in all applications. Next human also must be involved in predictive model building and adjusting or testing. So in particular we will have a lot of domain knowledge about the problem of prediction that we can build into this predictive model, and then next, of course, when we have predicted values for the variables, then humans would be involved in taking actions to change the world or make decisions based on these predictive values.	sometimes machine can see pattern in a lot of datum that human may not see , but in general human would play an important role in analyze text datum in all application . next human also must be involve in predictive model building and adjust or testing . so in particular we will have a lot of domain knowledge about the problem of prediction that we can build into this predictive model , and then next , of course , when we have predict value for the variable , then human would be involve in take action to change the world or make decision base on these predictive value .
a97a9d5e-48b7-4f4e-9754-4c5b30a31424	2020-11-02 22:57:03.98662	110	00:04:36.62	00:04:42.8	And finally, it's interesting that human could be also involved in controlling the sensors.	and finally , it be interesting that human could be also involve in control the sensor .
a97a9d5e-48b7-4f4e-9754-4c5b30a31424	2020-11-02 22:57:03.98662	113	00:04:43.82	00:04:51.01	And, this is so that we can adjust the sensors to collect the most useful data for prediction.	and , this be so that we can adjust the sensor to collect the most useful datum for prediction .
a97a9d5e-48b7-4f4e-9754-4c5b30a31424	2020-11-02 22:57:03.986627	150	00:04:52.38	00:06:14.83	So that's why I called this data mining loop because as we perturb the sensors to collect the new data and more useful data then we will obtain more data for prediction. This data generally will help us improve the prediction accuracy and in this loop are humans will recognize what additional data needs to be collected and machines would of course help humans identify what data should be collected next. In general, we want to collect data that are most useful for learning. And this there is actually a subarea in machine learning called active learning that has to do with this. How do you identify data points? That would be most helpful for machine learning programs if you can label them, right. So in general, you can see there's a loop here from data acquisition to data analysis or data mining to prediction of values, and to take actions to change the world and then observe what happens. And then you can then decide what additional data. Have to be collected by adjusting the snsor or from the prediction errors. you can also know what additional data we need to acquire in order to improve the accuracy of prediction. And this big picture is actually very general and it's reflecting a lot of important applications of big data.	so that be why I call this datum mining loop because as we perturb the sensor to collect the new datum and more useful datum then we will obtain more datum for prediction . this datum generally will help we improve the prediction accuracy and in this loop be human will recognize what additional datum need to be collect and machine would of course help human identify what datum should be collect next . in general , we want to collect datum that be most useful for learn . and this there be actually a subarea in machine learning call active learning that have to do with this . how do you identify data point ? that would be most helpful for machine learning program if you can label they , right . so in general , you can see there be a loop here from datum acquisition to datum analysis or datum mining to prediction of value , and to take action to change the world and then observe what happen . and then you can then decide what additional datum . have to be collect by adjust the snsor or from the prediction error . you can also know what additional datum we need to acquire in order to improve the accuracy of prediction . and this big picture be actually very general and it be reflect a lot of important application of big datum .
a97a9d5e-48b7-4f4e-9754-4c5b30a31424	2020-11-24 17:33:06.506176	153	00:06:16.05	00:06:20.76	So it's useful to keep that in mind while we're looking at some text mining techniques. "	so it be useful to keep that in mind while we be look at some text mining technique . "
a97a9d5e-48b7-4f4e-9754-4c5b30a31424	2020-11-02 22:57:03.986629	167	00:06:21.86	00:06:52.22	So from text mining perspective and we're interested in text based prediction, of course sometimes text alone can make predictions. And this is most useful for prediction about human behavior or human preferences or opinions. But in general text data will be put together with non text data. So the interesting questions here would be first how can we design effective predictors? And how do we generate such effective predictors from text?	so from text mining perspective and we be interested in text base prediction , of course sometimes text alone can make prediction . and this be most useful for prediction about human behavior or human preference or opinion . but in general text datum will be put together with non text datum . so the interesting question here would be first how can we design effective predictor ? and how do we generate such effective predictor from text ?
a97a9d5e-48b7-4f4e-9754-4c5b30a31424	2020-11-02 22:57:03.986633	191	00:06:53.61	00:07:45.58	... This question has been addressed to some extent in some previous lectures where we talked about what kind of features we can design for text data. It has also been addressed to some extent by talking about the other knowledge that we can mine from text. So for example, topic mining can be very useful to generate the patterns or topic based indicators or predictors that can be further fed into a predictive model. So topics can be intermediate representation of text. That would allow us to design high level features or predictors that are useful for prediction of some other variable. It maybe, although it's generated from original text data, it provides a much better representation of the problem and it serves as more effective predictors.	... this question have be address to some extent in some previous lecture where we talk about what kind of feature we can design for text datum . it have also be address to some extent by talk about the other knowledge that we can mine from text . so for example , topic mining can be very useful to generate the pattern or topic base indicator or predictor that can be far feed into a predictive model . so topic can be intermediate representation of text . that would allow we to design high level feature or predictor that be useful for prediction of some other variable . it maybe , although it be generate from original text datum , it provide a much well representation of the problem and it serve as more effective predictor .
a97a9d5e-48b7-4f4e-9754-4c5b30a31424	2020-11-02 22:57:03.986634	196	00:07:46.5	00:07:56.4	And similarly, sentiment analysis can lead to such predictors as well. So those are the than a mining or text mining algorithms can be used to generate the predictors.	and similarly , sentiment analysis can lead to such predictor as well . so those be the than a mining or text mining algorithm can be use to generate the predictor .
a97a9d5e-48b7-4f4e-9754-4c5b30a31424	2020-11-02 22:57:03.986637	213	00:07:58.36	00:08:37.98	The other question is how can we join mine text and non text data together? Now this is a question that we have not addressed yet. So in this lecture and the following lectures were going to address this problem because this is where we can generate the much more enriched features for prediction and allows us to review a lot of interesting knowledge about the world. These patterns that are generated from text and non text data themselves can sometimes already be useful for prediction, but when they are put together with many other predictors they can really help improving the accuracy of prediction.	the other question be how can we join mine text and non text datum together ? now this be a question that we have not address yet . so in this lecture and the follow lecture be go to address this problem because this be where we can generate the much more enriched feature for prediction and allow we to review a lot of interesting knowledge about the world . these pattern that be generate from text and non text datum themselves can sometimes already be useful for prediction , but when they be put together with many other predictor they can really help improve the accuracy of prediction .
a97a9d5e-48b7-4f4e-9754-4c5b30a31424	2020-11-02 22:57:03.986638	220	00:08:38.98	00:08:54.08	Basically you can see text based prediction character serve as a unified framework to combine many text mining and analysis techniques, including topic mining and content, any content mining techniques or sentiment analysis.	basically you can see text base prediction character serve as a unified framework to combine many text mining and analysis technique , include topic mining and content , any content mining technique or sentiment analysis .
a97a9d5e-48b7-4f4e-9754-4c5b30a31424	2020-11-02 22:57:03.986638	222	00:08:55.42	00:09:00.43	The goal here is mainly to infer values of real world variables.	the goal here be mainly to infer value of real world variable .
a97a9d5e-48b7-4f4e-9754-4c5b30a31424	2020-11-02 22:57:03.986639	227	00:09:01.05	00:09:14.28	But in order to achieve the goal, we can do some other preparations and these are sub tasks. So one sub task could be mine, mine the content of text data like topic mining.	but in order to achieve the goal , we can do some other preparation and these be sub task . so one sub task could be mine , mine the content of text datum like topic mining .
a97a9d5e-48b7-4f4e-9754-4c5b30a31424	2020-11-02 22:57:03.98664	232	00:09:15.39	00:09:26.32	And the other could be to mine knowledge about the observer so sentiment analysis or opinion analysis And both can help provide predictors for the prediction problem.	and the other could be to mine knowledge about the observer so sentiment analysis or opinion analysis and both can help provide predictor for the prediction problem .
a97a9d5e-48b7-4f4e-9754-4c5b30a31424	2020-11-02 22:57:03.986642	247	00:09:27.72	00:10:04.57	And of course we can also add non text data directly to the predictive model, but then not text data also helps provide context for text analysis that further improves the topic mining and the opinion analysis. And And such improvement often leads to more effective predictors for our problems it would enlarge the space of patterns of opinions or topics that we can mine from text. As we'll discuss more later, so the join analysis of text and non text can be actually understood from 2 perspectives.	and of course we can also add non text datum directly to the predictive model , but then not text datum also help provide context for text analysis that far improve the topic mining and the opinion analysis . and and such improvement often lead to more effective predictor for our problem it would enlarge the space of pattern of opinion or topic that we can mine from text . as we 'll discuss more later , so the join analysis of text and non text can be actually understand from 2 perspective .
a97a9d5e-48b7-4f4e-9754-4c5b30a31424	2020-11-24 16:07:26.934084	249	00:10:05.61	00:10:10.4	In one perspective, we can see non text data can help text mining.	in one perspective , we can see non text datum can help text mining .
a97a9d5e-48b7-4f4e-9754-4c5b30a31424	2020-11-02 22:57:03.986644	261	00:10:11.59	00:10:37.93	Be cause non text data can provide a context for mining text data. Provide a way to partition text data in different ways, and this leads to a number of techniques for contextual text mining. And that's to mine text in the context defined by non text data. And you can see this reference here for a large body of work in this direction, and we're going to highlight some of them in the next lectures.	be cause non text datum can provide a context for mining text datum . provide a way to partition text datum in different way , and this lead to a number of technique for contextual text mining . and that be to mine text in the context define by non text datum . and you can see this reference here for a large body of work in this direction , and we be go to highlight some of they in the next lecture .
a97a9d5e-48b7-4f4e-9754-4c5b30a31424	2020-11-02 22:57:03.986644	264	00:10:39.24	00:10:45.61	Now the other perspective is text data can help, but not text their mining as well.	now the other perspective be text datum can help , but not text their mining as well .
a97a9d5e-48b7-4f4e-9754-4c5b30a31424	2020-11-02 22:57:03.986648	284	00:10:46.19	00:11:28.64	And this is because text data can help interpret patterns discovered from non text data. This help you discover some frequent patterns from non text data. Now we can use the text data that are associated with instances where the pattern occurs as well as text data that are associated with instances where the pattern doesn't occur. And this gives us two sets of text data and then we can see what's the difference and this difference in text data is interpretable because text content content is easy to digest and that difference might suggest some meaning for this pattern that we've found from non text data, so that helps interpret such patterns.	and this be because text datum can help interpret pattern discover from non text datum . this help you discover some frequent pattern from non text datum . now we can use the text datum that be associate with instance where the pattern occur as well as text datum that be associate with instance where the pattern do n't occur . and this give we two set of text datum and then we can see what be the difference and this difference in text datum be interpretable because text content content be easy to digest and that difference might suggest some meaning for this pattern that we 've find from non text datum , so that help interpret such pattern .
a97a9d5e-48b7-4f4e-9754-4c5b30a31424	2020-11-02 22:57:03.986649	288	00:11:29.3	00:11:37.4	And this technique is called pattern annotation. And, you can see this reference listed here for more detail.	and this technique be call pattern annotation . and , you can see this reference list here for more detail .
a97a9d5e-48b7-4f4e-9754-4c5b30a31424	2020-11-24 17:43:26.990762	292	00:11:38.29	00:11:43.45	So here are the reference that I just mentioned. The first is reference for patternannotation. "	so here be the reference that I just mention . the first be reference for pattern  annotation . "
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-11-02 23:16:02.738053	2	00:00:00.3	00:00:04.22	This lecture is about the methods for text categorization.	this lecture be about the method for text categorization .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-12-02 06:05:45.686224	5	00:00:12.57	00:00:16.25	So in this lecture were going to discuss how to do text categorization.	so in this lecture be go to discuss how to do text categorization .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-11-02 23:16:02.738055	6	00:00:19.33	00:00:20.22	1st.	1st .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-12-02 06:06:11.949224	8	00:00:21.45	00:00:24.48	There are many methods for text categorization	there be many method for text categorization
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-11-02 23:16:02.73806	19	00:00:25.34	00:00:51.78	In such a method, the idea is to determine the category based on some rules that we design carefully to reflect the domain knowledge about the categorization problem. So, for example, if you want to do topical categorisation for news articles, you can say if the news article mentions word like game and Sports three times that we're going to say it's about sports.	in such a method , the idea be to determine the category base on some rule that we design carefully to reflect the domain knowledge about the categorization problem . so , for example , if you want to do topical categorisation for news article , you can say if the news article mention word like game and sport three time that we be go to say it be about sport .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-11-02 23:16:02.738061	22	00:00:52.72	00:00:59.92	Things like that and this would allow us to deterministically decide which category A document should be put into.	thing like that and this would allow we to deterministically decide which category a document should be put into .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-11-02 23:16:02.738063	28	00:01:01.96	00:01:19.55	Now such a strategy would work well if the following conditions hold. First, the categories must be very well defined, and this allows the person to clearly decide the category based on some clear rules.	now such a strategy would work well if the follow condition hold . first , the category must be very well define , and this allow the person to clearly decide the category base on some clear rule .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-12-02 06:08:00.852711	34	00:01:21.58	00:01:40.529	Secondly, the categories have to be easy to distinguish based on surface features in text, so that means superficial features like keywords or punctuations or whatever. You can easily identify text data.	secondly , the category have to be easy to distinguish base on surface feature in text , so that mean superficial feature like keyword or punctuation or whatever . you can easily identify text datum .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-11-02 23:16:02.738068	41	00:01:41.399	00:01:56.149	For example, if there is some special vocabulary that is known to only occur in a particular category, and that would be most effective because we can easily use such a vocabulary or pattern of such a vocabulary to recognize this category.	for example , if there be some special vocabulary that be know to only occur in a particular category , and that would be most effective because we can easily use such a vocabulary or pattern of such a vocabulary to recognize this category .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-11-02 23:16:02.738069	43	00:01:57.549	00:02:01.719	Now we also should have sufficient knowledge.	now we also should have sufficient knowledge .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-12-11 06:26:01.230985	55	00:02:02.659	00:02:31.749	For designing these rules and so if that's the case, then such a method can be effective, and so it does have a provisions in some domains and sometimes. However in general there are several problems with this approach. First, of course it's labor intensive. It requires a lot of manual work. Obviously we can't do this for all kinds of categorization problems. We have to do it	for design these rule and so if that be the case , then such a method can be effective , and so it do have a provision in some domain and sometimes . however in general there be several problem with this approach . first , of course it be labor intensive . it require a lot of manual work . obviously we ca n't do this for all kind of categorization problem . we have to do it
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-11-02 23:16:02.738074	59	00:02:33.209	00:02:39.929	From scratch for a different problem, becauses different rules would be needed so it doesn't scale up as well.	from scratch for a different problem , because different rule would be need so it do n't scale up as well .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-11-02 23:16:02.738076	65	00:02:40.979	00:02:56.649	Secondly, it cannot handle uncertainties in rules. Often the rules aren't 100% reliable take for example, and looking at the occurrences of words in text and try to decide the topic.	 secondly , it can not handle uncertainty in rule . often the rule be n't 100 % reliable take for example , and look at the occurrence of word in text and try to decide the topic .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-11-02 23:16:02.738079	72	00:02:57.229	00:03:14.269	It's actually very hard to have 1% correct the rule. So for example, you can say if it has games, sports, basketball, then for sure it's about sports. But one can also imagine some text articles that mention these keywords.	it be actually very hard to have 1 % correct the rule . so for example , you can say if it have game , sport , basketball , then for sure it be about sport . but one can also imagine some text article that mention these keyword .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-11-02 23:16:02.738081	77	00:03:15.169	00:03:26.079	But that may not be exactly about the sports, or only marginally touching sports. The main topic could be another topic, different topic then sports.	but that may not be exactly about the sport , or only marginally touch sport . the main topic could be another topic , different topic then sport .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-11-02 23:16:02.738086	90	00:03:27.379	00:03:57.859	So that's one disadvantage of this approach, and then finally the rules may be inconsistent and this would need to concern about robustness more specifically, and sometimes the results of categorization may be different depending on which rule to be applied. So in that case then you will face uncertainty and you will also have to decide the order of applying the rules or combination of results that are contradictory. So all these.	so that be one disadvantage of this approach , and then finally the rule may be inconsistent and this would need to concern about robustness more specifically , and sometimes the result of categorization may be different depend on which rule to be apply . so in that case then you will face uncertainty and you will also have to decide the order of apply the rule or combination of result that be contradictory . so all these .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-11-02 23:16:02.738087	94	00:03:58.009	00:04:06.309	Problems with this approach, and it turns out that the both problems can be solved or alleviated by using machine learning.	problem with this approach , and it turn out that the both problem can be solve or alleviate by use machine learning .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-11-02 23:16:02.738092	105	00:04:07.189	00:04:35.739	So these machine learning methods are more automatic, but I still put automatic in quotation marks cause they're not really completely automatic because it still require manual work. More specifically, we have to use human experts to help in two ways. First, the human experts must annotate datasets with category labels, will tell the computer which documents should not receive which categories.	so these machine learning method be more automatic , but I still put automatic in quotation mark cause they be not really completely automatic because it still require manual work . more specifically , we have to use human expert to help in two way . first , the human expert must annotate dataset with category label , will tell the computer which document should not receive which category .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-11-02 23:16:02.738105	142	00:04:36.409	00:05:56.479	And this is called a training data. And then Secondly the human experts also need to provide a set of features to represent each text object that can potentially provide a clue about the category. So we need to provide some basic features for the computers to look into. And in the case of text, natural choice would be the words. So using each word as a feature is a very common choice to start with. But of course there are other sophisticated features like a phrases or even policy feature tags or even syntactic structures. So once human experts can provide this, then we can use machine learning to learn soft rules for categorization from the training data. So soft rules just means we're going to still decide which category should be assigned to the document. But it's not going to be used using a rule that is deterministic, so we might use something similar to saying that if it matches game sports many times, it's likely to be a sports. But we're not going to say exactly for sure, but instead we're going to use probabilities or weights so that we can combine multiple evidences, so the learning process basically is going to figure out which features are most useful for separating different categories.	and this be call a training datum . and then secondly the human expert also need to provide a set of feature to represent each text object that can potentially provide a clue about the category . so we need to provide some basic feature for the computer to look into . and in the case of text , natural choice would be the word . so use each word as a feature be a very common choice to start with . but of course there be other sophisticated feature like a phrase or even policy feature tag or even syntactic structure . so once human expert can provide this , then we can use machine learn to learn soft rule for categorization from the training datum . so soft rule just mean we be go to still decide which category should be assign to the document . but it be not go to be use use a rule that be deterministic , so we might use something similar to say that if it match game sport many time , it be likely to be a sport . but we be not go to say exactly for sure , but instead we be go to use probability or weight so that we can combine multiple evidence , so the learning process basically be go to figure out which feature be most useful for separate different category .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-11-02 23:16:02.73811	160	00:05:57.029	00:06:40.929	And it's going to also figure out how to optimally combine features to minimize errors of categorisation on the training data, so the training data as you can see very important. It's the basis for learning. And then the train classifier can be applied to a new text object to predict the most likely category, and that's to simulate the prediction of what a human would assign to this text object. If the human would to make a judgement. So when we use machine learning for text categorization, we can also talk about the problem in the general setting of supervised learning. So the setup is.	and it be go to also figure out how to optimally combine feature to minimize error of categorisation on the training datum , so the training datum as you can see very important . it be the basis for learn . and then the train classifier can be apply to a new text object to predict the most likely category , and that be to simulate the prediction of what a human would assign to this text object . if the human would to make a judgement . so when we use machine learn for text categorization , we can also talk about the problem in the general setting of supervised learning . so the setup be .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-11-02 23:16:02.738115	180	00:06:41.259	00:07:36.469	To learn a classifier to map a value of X into a map of Y. So here X is all the text objects. And Y is all the categories a set of categories, so the classifier would take any value in X as input and we generate the value in Y as output, and we hope the output Y would be the right category for X, and here correct of course is judged based on the training data, so that's the general goal, like in all the machine learning problems or supervised learning problems where you are given some examples of Input and output for function and then the computer is going to figure out how the function behaves like based on these examples and then try to be able to compute the values for future access that we have not seen.	to learn a classifier to map a value of x into a map of Y. So here X be all the text object . and Y be all the category a set of category , so the classifier would take any value in x as input and we generate the value in Y as output , and we hope the output Y would be the right category for x , and here correct of course be judge base on the training datum , so that be the general goal , like in all the machine learn problem or supervise learning problem where you be give some example of input and output for function and then the computer be go to figure out how the function behave like base on these example and then try to be able to compute the value for future access that we have not see .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-11-02 23:16:02.738116	186	00:07:38.699	00:07:51.799	So in general, all methods would rely on discriminating features of text objects to distinguish different categories, so that's why these features are very important and they have to be provided by humans.	so in general , all method would rely on discriminate feature of text object to distinguish different category , so that be why these feature be very important and they have to be provide by human .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-11-02 23:16:02.738118	194	00:07:52.599	00:08:10.898999	And they will also combine multiple features in a weighted matter with weights to be optimized to minimize the errors on the training data. So ultimately, the learning processes optimization problem and the objective function is often tide to the errors on the training data.	and they will also combine multiple feature in a weighted matter with weight to be optimize to minimize the error on the training datum . so ultimately , the learning process optimization problem and the objective function be often tide to the error on the training datum .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-11-02 23:16:02.73812	200	00:08:12.519	00:08:25.459	Different methods tend to vary in their ways of measuring the errors on the training data. They might optimize a different object function, which is often also called a loss function or cost function.	different method tend to vary in their way of measure the error on the training datum . they might optimize a different object function , which be often also call a loss function or cost function .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-11-02 23:16:02.738123	211	00:08:26.409	00:08:48.719	They also tend to vary in their ways of combining the features, so linear combination for example is simple is often used. But they're not as powerful as non linear combination, but nonlinear models might be more complex for training. So there are tradeoffs as well, but that would lead to different variations of.	they also tend to vary in their way of combine the feature , so linear combination for example be simple be often use . but they be not as powerful as non linear combination , but nonlinear model might be more complex for training . so there be tradeoff as well , but that would lead to different variation of .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-11-02 23:16:02.738123	213	00:08:50.039	00:08:52.649	Many variations of these learning methods.	many variation of these learning method .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-11-02 23:16:02.738125	221	00:08:53.429	00:09:09.759	So in general, we can distinguish the two kinds of classifiers at a high level one is going to generative classifiers. The other is called discriminative classifiers. The generative classifiers try to learn what the data looks like in each category.	so in general , we can distinguish the two kind of classifier at a high level one be go to generative classifier . the other be call discriminative classifier . the generative classifier try to learn what the datum look like in each category .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-11-02 23:16:02.738126	224	00:09:10.699	00:09:16.399	So it attempts to model the join the distribution of the data and the label X&Y.	so it attempt to model the join the distribution of the datum and the label x&y.
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-11-02 23:16:02.738129	233	00:09:17.349	00:09:46.469	And, this can then be factored out to a product of Y. The distribution of labels and join the probability of sorry the conditional probability of X given Y so it's Y. So we first model distribution of labels and then we model how the data is generated given a particular label here.	and , this can then be factor out to a product of Y. the distribution of label and join the probability of sorry the conditional probability of x give Y so it be Y. so we first model distribution of label and then we model how the datum be generate give a particular label here .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-12-02 06:23:25.540267	238	00:09:48.379	00:10:01.409	And once we can estimate these models, then we can compute this conditional probability of label given data based on. The probability of data given label.	and once we can estimate these model , then we can compute this conditional probability of label give datum base on . the probability of datum give label .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-12-02 06:23:35.577651	240	00:10:02.559	00:10:05.989	And the label distribution here by using the base rule.	and the label distribution here by use the base rule .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-11-02 23:16:02.738132	244	00:10:07.069	00:10:16.929	Now this is the most important thing 'cause this conditional probability of the label can then be used directly to decide which label is most likely.	now this be the most important thing 'cause this conditional probability of the label can then be use directly to decide which label be most likely .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-11-02 23:16:02.738134	249	00:10:18.679	00:10:30.509	So in such approaches, the objective function is actually likelihood, so we model how the data are generated, so only thus it only indirectly captures the training errors.	so in such approach , the objective function be actually likelihood , so we model how the datum be generate , so only thus it only indirectly capture the training error .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-11-02 23:16:02.738135	254	00:10:31.279	00:10:42.599	But if we can model the data in each category accurately, then we can also classify accurately. One example is naive Bayes classifier. In this case.	but if we can model the datum in each category accurately , then we can also classify accurately . one example be naive Bayes classifier . in this case .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-11-02 23:16:02.738137	261	00:10:43.329	00:10:57.919	The other kind of approaches are called discriminative classifiers. These classifiers try to learn what features separate categories, so they directly tackle the problem of categorisation or separation of classes.	the other kind of approach be call discriminative classifier . these classifier try to learn what feature separate category , so they directly tackle the problem of categorisation or separation of class .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-11-02 23:16:02.738137	262	00:10:58.799	00:11:03.199	So sorry for the problem.	so sorry for the problem .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-11-02 23:16:02.738137	264	00:11:04.209	00:11:09.459	So these discriminative classifiers attempted to model the.	so these discriminative classifier attempt to model the .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-11-02 23:16:02.738138	265	00:11:10.339	00:11:11.319	Conditional.	conditional .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-11-02 23:16:02.738138	267	00:11:12.039	00:11:15.849	Probability of the label given the data point directly.	probability of the label give the data point directly .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-11-02 23:16:02.738142	270	00:11:16.519	00:11:22.539	So the objective function tends to directly measure the errors of categorisation on the training data.	so the objective function tend to directly measure the error of categorisation on the training datum .
aac4e33c-97bb-46c8-a108-3e3e3322a85c	2020-11-02 23:16:02.738143	273	00:11:24.169	00:11:30.339	Some examples include the logistical regression support vector machines and the K nearest neighbors.	some example include the logistical regression support vector machine and the k near neighbor .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-02 22:59:05.963482	5	00:00:00.29	00:00:11.06	So looking at the text mining problem more closely, we see that the problem is similar to general data mining, except that we'll be focusing more on text data.	so look at the text mining problem more closely , we see that the problem be similar to general datum mining , except that we 'll be focus more on text datum .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-25 15:34:52.189608	9	00:00:21.59	00:00:31.54	And we're going to have text mining algorithms to help us to turn text data into actionable knowledge that we can use in (the) real world.	and we be go to have text mining algorithm to help we to turn text datum into actionable knowledge that we can use in ( the ) real world .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-02 22:59:05.963492	28	00:00:32.12	00:01:19.67	Especially for decision making or for completing whatever tasks that require text data to support, now because in general in many real world problems of data mining, we also tend to have other kinds of data that are non textual. So a more general picture would be to include non text data as well. And for this reason, we might be concerned with joint mining of text and non text data and so in this course we're going to focus more on text mining. But we can also touch how to join the analysis of both text data and non-text data. With this problem definition we can now look at the landscape of the topics in text mining analytics.	especially for decision making or for complete whatever task that require text datum to support , now because in general in many real world problem of datum mining , we also tend to have other kind of datum that be non textual . so a more general picture would be to include non text datum as well . and for this reason , we might be concern with joint mining of text and non text datum and so in this course we be go to focus more on text mining . but we can also touch how to join the analysis of both text datum and non - text datum . with this problem definition we can now look at the landscape of the topic in text mining analytic .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-24 20:47:12.443241	30	00:01:20.81	00:01:25.97	Now this slide shows the process of generating text data in more detail.	now this slide show the process of generate text datum in more detail .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-02 22:59:05.963494	33	00:01:26.88	00:01:33.56	Most specifically, human sensor or human observer would look at the world from some perspective.	most specifically , human sensor or human observer would look at the world from some perspective .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-02 22:59:05.963496	37	00:01:34.46	00:01:40.44	Different people would be looking at the world from different angles and they will pay attention to different things.	different people would be look at the world from different angle and they will pay attention to different thing .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-02 22:59:05.963497	40	00:01:41.12	00:01:49.42	The same person at a different time might also pay attention to different aspects of the observed world.	the same person at a different time might also pay attention to different aspect of the observed world .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-02 22:59:05.963498	42	00:01:50.13	00:01:54.06	And so the human sensor would perceive the world from some perspective.	and so the human sensor would perceive the world from some perspective .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-02 22:59:05.963502	51	00:01:55.31	00:02:15	And that human... The sensor would then form a view of the world and that can be called the observed world. Of course this would be different from the real world because of the perspective that the person has taken. This can often be biased also.	and that human ... the sensor would then form a view of the world and that can be call the observed world . of course this would be different from the real world because of the perspective that the person have take . this can often be bias also .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-02 22:59:05.963507	64	00:02:16.7	00:02:54.92	Now the observable world can be represented ss for example entity relation graphs or more in a more general way, using knowledge representation language. But in general, this is basically what a person has in mind about the world, and we don't really know what exactly it looks like, of course. But then the human would express what the person has observed using a natural language such as English, and the result is text data.	now the observable world can be represent ss for example entity relation graph or more in a more general way , use knowledge representation language . but in general , this be basically what a person have in mind about the world , and we do n't really know what exactly it look like , of course . but then the human would express what the person have observe use a natural language such as English , and the result be text datum .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-02 22:59:05.963509	70	00:02:55.75	00:03:08.31	Of course, the person could have used a different language to express what he or she has observed. In that case, we might have text data of mixed languages for different languages.	of course , the person could have use a different language to express what he or she have observe . in that case , we might have text datum of mixed language for different language .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-02 22:59:05.96351	73	00:03:09.85	00:03:18.02	So the main goal of text mining is actually to revert this process of generating test data.	so the main goal of text mining be actually to revert this process of generate test datum .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-02 22:59:05.963511	75	00:03:19.04	00:03:24.51	And we hope to be able to uncover some aspect in this process.	and we hope to be able to uncover some aspect in this process .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-02 22:59:05.963513	78	00:03:26.49	00:03:33.68	And so specifically we can think about the mining, for example, knowledge about the language.	and so specifically we can think about the mining , for example , knowledge about the language .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-25 15:40:13.504772	82	00:03:35.31	00:03:43.68	And that means by looking at text data in English, we may be able to discover something about English... Some usage of English...	and that mean by look at text datum in English , we may be able to discover something about English ... some usage of English ...
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-24 20:50:33.448232	83	00:03:44.46	00:03:46.4	some patterns of English.	some pattern of English .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-24 20:51:05.366636	87	00:03:47.66	00:03:57.12	So this is 1 type of mining problems where the result is some knowledge about language which may be useful in various ways.	so this be 1 type of mining problem where the result be some knowledge about language which may be useful in various way .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-25 15:40:44.745908	90	00:03:58.79	00:04:05.65	If you look at the picture, we can also "then mine knowledge about the ""Observed" "World""."	if you look at the picture , we can also " then mine knowledge about the " " observe " " world " " . "
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-02 22:59:05.963518	92	00:04:06.25	00:04:10.2	As so, this has much to do with mining the content of text data.	as so , this have much to do with mine the content of text datum .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-02 22:59:05.963521	98	00:04:11.39	00:04:25.76	we're going to look at the what the text data are about and then try to get the essence of it. Or extracting high quality information about a particular aspect of the world that we're interested in.	we be go to look at the what the text datum be about and then try to get the essence of it . or extract high quality information about a particular aspect of the world that we be interested in .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-02 22:59:05.963523	104	00:04:26.78	00:04:43.55	For example, everything that has been said about a particular person or particular entity, and this can be regarded as mining content to describe the observed world in the user's mind in the person's mind.	for example , everything that have be say about a particular person or particular entity , and this can be regard as mining content to describe the observed world in the user 's mind in the person 's mind .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-02 22:59:05.963526	110	00:04:44.89	00:05:02.42	If you look further than you can also imagine we can mine knowledge about this observer himself or herself. So this has also to do with using text data to infer some properties of this person.	if you look far than you can also imagine we can mine knowledge about this observer himself or herself . so this have also to do with use text datum to infer some property of this person .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-02 22:59:05.963527	113	00:05:03.24	00:05:09.06	And these properties could include the mood of the person or sentiment of the person.	and these property could include the mood of the person or sentiment of the person .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-02 22:59:05.963554	136	00:05:10.08	00:06:03.73	And note that we distinguish the observed the world from the person because text that I can't describe what the person has observed in an objective way, but the description can be also subject with sentiment, and so in general you can imagine the text data would contain some factual descriptions of the world plus some subjective comments, so that's why it's also possible to do text mining to mine knowledge about the observer. Finally, if you look at the picture to the left side of this picture, then you can see we can certainly also say something about the real world, right? So indeed we can do text mining to infer other real world variables, and this is often called predictive analytics. And we want to predict the value of certain interesting variables.	and note that we distinguish the observe the world from the person because text that I ca n't describe what the person have observe in an objective way , but the description can be also subject with sentiment , and so in general you can imagine the text datum would contain some factual description of the world plus some subjective comment , so that be why it be also possible to do text mining to mine knowledge about the observer . finally , if you look at the picture to the left side of this picture , then you can see we can certainly also say something about the real world , right ? so indeed we can do text mining to infer other real world variable , and this be often call predictive analytic . and we want to predict the value of certain interesting variable .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-25 16:08:14.965427	139	00:06:06.25	00:06:13.6	So this picture basically covered multiple types of knowledge that we can mine from text in general.	so this picture basically cover multiple type of knowledge that we can mine from text in general .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-25 16:09:44.688729	156	00:06:14.36	00:07:03.96	When we infer other real world variables, we could also use some of the results from mining text data as intermediate results to help the prediction. For example, after we mine the content of text data, we might generate some summary of content, and that summary could be then used to help us predict the variables of the real world. Now of course, this is still generated from the original text data, but I want to emphasize here that often the processing of text data to generate some features that can help with the prediction, is very important.	when we infer other real world variable , we could also use some of the result from mining text datum as intermediate result to help the prediction . for example , after we mine the content of text datum , we might generate some summary of content , and that summary could be then use to help we predict the variable of the real world . now of course , this be still generate from the original text datum , but I want to emphasize here that often the processing of text datum to generate some feature that can help with the prediction , be very important .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-02 22:59:05.963587	162	00:07:04.81	00:07:19.65	And that's why here we show that the results of some other mining tasks, including mining the content of text data and mining knowledge above the observer can all be very helpful for prediction.	and that be why here we show that the result of some other mining task , include mine the content of text datum and mining knowledge above the observer can all be very helpful for prediction .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-02 22:59:05.963588	165	00:07:21.24	00:07:28.71	In fact, when we have a non-text data, we could also use the non-text data to help prediction.	in fact , when we have a non - text datum , we could also use the non - text datum to help prediction .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-02 22:59:05.963589	167	00:07:29.46	00:07:31.79	And of course, it depends on the problem.	and of course , it depend on the problem .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-02 22:59:05.963591	172	00:07:33.18	00:07:41.78	In general, non-text data can be very important for such prediction tasks. For example, if you want to predict the stocks.	in general , non - text datum can be very important for such prediction task . for example , if you want to predict the stock .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-02 22:59:05.963597	186	00:07:42.54	00:08:17.34	Stock prices or changes of stock prices based on discussion in the news articles or in social media, then this is an example of using text data to predict some other real world variables. Now in this case, obviously the historical stock price data would be very important for this prediction, and so that's example of non-text data that would be very useful for the prediction and we can combine both kinds of data to make the prediction.	stock price or change of stock price base on discussion in the news article or in social medium , then this be an example of use text datum to predict some other real world variable . now in this case , obviously the historical stock price datum would be very important for this prediction , and so that be example of non - text datum that would be very useful for the prediction and we can combine both kind of datum to make the prediction .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-02 22:59:05.963598	189	00:08:18.31	00:08:24.58	Now non-text data can be also useful for analyzing text by supplying context.	now non - text datum can be also useful for analyze text by supply context .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-02 22:59:05.963599	192	00:08:25.44	00:08:31.81	When we look at the text data alone will be mostly looking at the content and opinions expressed in text.	when we look at the text datum alone will be mostly look at the content and opinion express in text .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-02 22:59:05.9636	194	00:08:32.67	00:08:36.56	But text data generally have also context associated.	but text datum generally have also context associate .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-02 22:59:05.963601	198	00:08:37.28	00:08:47.74	For example, the time, the location, of that associated with the text data and these are useful context information.	for example , the time , the location , of that associate with the text datum and these be useful context information .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-02 22:59:05.963604	204	00:08:48.61	00:08:59.96	And the context can provide interesting angles for analyzing text data. For example, we might partition text data into different time periods because of the availability of time.	and the context can provide interesting angle for analyze text datum . for example , we might partition text datum into different time period because of the availability of time .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-02 22:59:05.963604	206	00:09:00.53	00:09:05.7	Now we can analyze text data in each time period and then make a comparison.	now we can analyze text datum in each time period and then make a comparison .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-02 22:59:05.963612	223	00:09:06.34	00:09:53.82	Similarly, we can partition text data based on locations or any metadata that's associated to form interesting comparison scenarios. So in this sense, non-text data can actually provide interesting angles or perspectives for text analysis, and can help us make context sensitive analysis of content or the language usage or the opinions about the observer or the authors of text data. We could analyze the sentiment in different context, so this is fairly general landscape of the topics in text mining and analytics.	similarly , we can partition text datum base on location or any metadata that be associate to form interesting comparison scenario . so in this sense , non - text datum can actually provide interesting angle or perspective for text analysis , and can help we make context sensitive analysis of content or the language usage or the opinion about the observer or the author of text datum . we could analyze the sentiment in different context , so this be fairly general landscape of the topic in text mining and analytic .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-02 22:59:05.963613	225	00:09:54.37	00:09:58.45	In this course we're going to selectively cover some of those topics.	in this course we be go to selectively cover some of those topic .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-02 22:59:05.963614	227	00:09:59.72	00:10:04.58	We actually hope to cover most of these general topics.	we actually hope to cover most of these general topic .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-02 22:59:05.963621	244	00:10:06.78	00:10:51.52	First, we are going to cover natural language processing very briefly because this has to do with understanding text data, and this determines how we can represent text for text mining. Second, we're going to talk about how to mine word associations from text data and word associations is a form of useful lexical knowledge about a language. Third, we're going to talk about the topic mining and analysis, and this is only one way to analyze content of text, but it's a very useful way of analyzing content. It's also one of the most useful techniques in text mining.	first , we be go to cover natural language processing very briefly because this have to do with understand text datum , and this determine how we can represent text for text mining . second , we be go to talk about how to mine word association from text datum and word association be a form of useful lexical knowledge about a language . third , we be go to talk about the topic mining and analysis , and this be only one way to analyze content of text , but it be a very useful way of analyze content . it be also one of the most useful technique in text mining .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-02 22:59:05.963622	248	00:10:53.61	00:11:05.46	And then we're going to talk about opinion mining and sentiment analysis. So this can be regarded as one example of mining knowledge about the observer.	and then we be go to talk about opinion mining and sentiment analysis . so this can be regard as one example of mining knowledge about the observer .
aca8d826-412b-4134-8d2c-87537fdc4a76	2020-11-24 21:05:17.296558	252	00:11:07	00:11:16.19	And finally, we are going to cover a text based prediction problems where we try to predict some real world variable based on text data.	and finally , we be go to cover a text base prediction problem where we try to predict some real world variable base on text datum .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342028	3	00:00:00.3	00:00:06.59	This lecture is about the latent aspect rating analysis or opinion mining and sentiment analysis.	this lecture be about the latent aspect rating analysis or opinion mining and sentiment analysis .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.34203	8	00:00:14.7	00:00:22.5	In this lecture, we're going to continue discussing opinion mining and sentiment analysis. In particular, we're going to introduce.	in this lecture , we be go to continue discuss opinion mining and sentiment analysis . in particular , we be go to introduce .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342031	12	00:00:23.14	00:00:31.34	Late in the aspect of rating analysis, which allows us to perform detailed analysis of reviews with overall ratings.	late in the aspect of rating analysis , which allow we to perform detailed analysis of review with overall rating .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342033	18	00:00:34.61	00:00:48.65	First, motivation. Here are two reviews that you often see on the Internet about the Hotel and You see some overall ratings. In this case, both reviewers have given five stars.	first , motivation . here be two review that you often see on the internet about the Hotel and you see some overall rating . in this case , both reviewer have give five star .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342034	20	00:00:49.24	00:00:52.56	And of course there are also reviews that are in text.	and of course there be also review that be in text .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342035	25	00:00:53.65	00:01:04.94	Now, if you just look at these reviews, it's not very clear whether a hotel is good for its location or for its service, and it's also unclear why are. If you are like this hotel.	now , if you just look at these review , it be not very clear whether a hotel be good for its location or for its service , and it be also unclear why be . if you be like this hotel .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342036	27	00:01:06.26	00:01:10.42	So what we want to do is to decompose this overall rating.	so what we want to do be to decompose this overall rating .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342037	29	00:01:11.03	00:01:17.48	Into ratings on different aspects such as value, rooms, location and service.	into rating on different aspect such as value , room , location and service .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342037	31	00:01:18.33	00:01:22.78	So if we can decompose overrating two ratings on these different aspects.	so if we can decompose overrate two rating on these different aspect .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342038	34	00:01:23.4	00:01:29.64	Then we can obtain more detailed understanding of the reviewers opinions about the hotel.	then we can obtain more detailed understanding of the reviewer opinion about the hotel .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342041	44	00:01:30.53	00:01:57.76	And this would also allow us to rank hotels along different dimensions, such as valuable rooms, but in general such detailed understanding would reveal more information about the users, preferences, reviews, preferences and also we can understand better how reviewers view this hotel from different perspectives. Now, not only do we want to.	and this would also allow we to rank hotel along different dimension , such as valuable room , but in general such detailed understanding would reveal more information about the user , preference , review , preference and also we can understand well how reviewer view this hotel from different perspective . now , not only do we want to .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342044	52	00:01:58.67	00:02:17.18	Infer this aspect ratings. We also want to infer the aspect of weights, so some reviewers may care more about values as opposed to service, and that would be a case like what's shown on the left for the weight distribution where you can see a lot of weight is placed on value.	infer this aspect rating . we also want to infer the aspect of weight , so some reviewer may care more about value as oppose to service , and that would be a case like what be show on the left for the weight distribution where you can see a lot of weight be place on value .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342045	55	00:02:18.48	00:02:24.78	But others might care more about service and therefore they might place more weight on service then value.	but other might care more about service and therefore they might place more weight on service then value .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342052	80	00:02:25.56	00:03:22.87	Now, the reason why this is also important that is be cause do you think about a five star on value? It might still be very expensive if the reviewer cares a lot about service, right? For this kind of service, this price is good, so the reviewer might give it a five star. But if reviewer really cares about the value of the hotel, then the five star most likely would mean really cheaper prices. So in order to interpret the ratings on different aspects accurately, we also need to know these aspect weights. When they are combined together, we can have a more detailed understanding of the opinion. So the task here is to get these reviews and their overall ratings as input and then generate the both the aspect ratings, decomposed aspect ratings and the aspect of weights as output.	now , the reason why this be also important that be be cause do you think about a five star on value ? it might still be very expensive if the reviewer care a lot about service , right ? for this kind of service , this price be good , so the reviewer might give it a five star . but if reviewer really care about the value of the hotel , then the five star most likely would mean really cheap price . so in order to interpret the rating on different aspect accurately , we also need to know these aspect weight . when they be combine together , we can have a more detailed understanding of the opinion . so the task here be to get these review and their overall rating as input and then generate the both the aspect rating , decompose aspect rating and the aspect of weight as output .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342053	82	00:03:23.74	00:03:28.66	And this is a problem called latent aspect rating analysis.	and this be a problem call latent aspect rating analysis .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342053	85	00:03:30.96	00:03:37.94	So the task in general is given a set of review articles about the topic with overall ratings.	so the task in general be give a set of review article about the topic with overall rating .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342055	89	00:03:38.54	00:03:45.62	An we hope to generate the three things. One is the major aspects comment on in the reviews.	an we hope to generate the three thing . one be the major aspect comment on in the review .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342056	92	00:03:46.26	00:03:52.27	The second is the ratings on each aspect, such as value and room or service.	the second be the rating on each aspect , such as value and room or service .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342061	107	00:03:53.56	00:04:24.78	And 3rd is the relative weights placed on different aspects by the reviewers, and this task has a lot of applications. If we can do this and we would enable a lot of applications, I just listed some here and later. I will show you some results. And for example, we can do opinion based and the ranking. We can generate a aspect level opinion summary. We can also analyze reviewers preferences, compare them or compare their preferences on different hotels.	and 3rd be the relative weight place on different aspect by the reviewer , and this task have a lot of application . if we can do this and we would enable a lot of application , I just list some here and later . I will show you some result . and for example , we can do opinion base and the ranking . we can generate a aspect level opinion summary . we can also analyze reviewer preference , compare they or compare their preference on different hotel .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342062	109	00:04:25.38	00:04:27.84	And we can do personalized recommendation of products.	and we can do personalize recommendation of product .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342065	111	00:04:29.41	00:04:32.53	So of course the question is how can we solve this problem?	so of course the question be how can we solve this problem ?
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.34207	129	00:04:34.66	00:05:12.89	Now, as in other cases of these advanced topics, we won't have time to really cover the technique in detail, but I'm going to give a press basic introduction to the technique developed for this problem. So first we're going to talk about how to solve the problem in two stages. Later, we're going to also mention that we can do this in the unified model. Now take this review with the overall reading as input. What we want to do is first we're going to segment the aspects. So we're going to figure out what words are talking about location in what words are talking about, the room conditioning, etc.	now , as in other case of these advanced topic , we wo n't have time to really cover the technique in detail , but I be go to give a press basic introduction to the technique develop for this problem . so first we be go to talk about how to solve the problem in two stage . later , we be go to also mention that we can do this in the unified model . now take this review with the overall reading as input . what we want to do be first we be go to segment the aspect . so we be go to figure out what word be talk about location in what word be talk about , the room conditioning , etc .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342073	137	00:05:13.53	00:05:32.53	So with this we would be able to obtain aspect segments. In particular, we're going to obtain the counts of all the words in each segment, and this is denoted by C supply of WND. This can be done by using seed words like location and room.	so with this we would be able to obtain aspect segment . in particular , we be go to obtain the count of all the word in each segment , and this be denote by c supply of WND . this can be do by use seed word like location and room .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342077	149	00:05:33.27	00:06:04.76	Or price to retrieve the relevant the segments and then from those segments we can further mine correlated words. With these seed words and that would allow us to segment the text into segments. Discussing different aspects, but of course later as we would see, we can also use topic models to do the segmentation, But anyway, that's the first stage where we would obtain the counts of words in each segment.	or price to retrieve the relevant the segment and then from those segment we can far mine correlate word . with these seed word and that would allow we to segment the text into segment . discuss different aspect , but of course later as we would see , we can also use topic model to do the segmentation , but anyway , that be the first stage where we would obtain the count of word in each segment .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342081	163	00:06:05.35	00:06:35.82	In the segmentation stage, which is called latent rating regression, we're going to use these words and their frequencies in different aspects to predict the overall rating, and this prediction happens in two stages. In the first stage, we're going to use the sentiment weights of these words in each aspect to predict the aspect rating. So, for example, if in the discussion of location using a word like amazing mentioned many times and it has a high weight.	in the segmentation stage , which be call latent rating regression , we be go to use these word and their frequency in different aspect to predict the overall rating , and this prediction happen in two stage . in the first stage , we be go to use the sentiment weight of these word in each aspect to predict the aspect rating . so , for example , if in the discussion of location use a word like amazing mention many time and it have a high weight .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342085	177	00:06:36.02	00:07:04.9	For example, here is 3.9. Then it would increase the aspect rating for location. But another word, like a far, which is a negative weight if it's mentioned many times and it will decrease the rating. So the aspect rating is assumed to be a weighted combination of these word frequencies where the weights are the sentiment weights on the words. Now of course these sentiment weights might be different for different aspects.	for example , here be 3.9 . then it would increase the aspect rating for location . but another word , like a far , which be a negative weight if it be mention many time and it will decrease the rating . so the aspect rating be assume to be a weight combination of these word frequency where the weight be the sentiment weight on the word . now of course these sentiment weight might be different for different aspect .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342086	179	00:07:05.55	00:07:10.39	So we have for each aspect a set of sentiment weights.	so we have for each aspect a set of sentiment weight .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-27 22:43:40.067851	181	00:07:11.1	00:07:16.22	As shown here, and that's denoted by beta sub I and W.	as show here , and that be denote by beta sub I and W.
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342089	190	00:07:18.29	00:07:41.919999	In the second stage, or in a second step, we're going to assume that the overall rating is simply weighted combination of these aspect ratings. So we're going to assume we have aspect weights in order by of R sub of D. And this would be used to take a weighted average of the aspect ratings, which are denoted by our supply of the.	in the second stage , or in a second step , we be go to assume that the overall rating be simply weight combination of these aspect rating . so we be go to assume we have aspect weight in order by of R sub of D. and this would be use to take a weighted average of the aspect rating , which be denote by our supply of the .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.34209	193	00:07:42.72	00:07:47.85	And we can assume the overall rating is simply a weighted average of this aspect ratings.	and we can assume the overall rating be simply a weighted average of this aspect rating .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342094	202	00:07:48.47	00:08:08.25	So this setup allows us to predict the overall rating based on the observed word frequencies. So on the left side you will see all these observed information, the arts, the and the count. But on the right side you see all the information that we're interested in is actually latent.	so this setup allow we to predict the overall rating base on the observe word frequency . so on the left side you will see all these observed information , the art , the and the count . but on the right side you see all the information that we be interested in be actually latent .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342095	203	00:08:09.05	00:08:11.01	So we hope to discover them.	so we hope to discover they .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342097	210	00:08:12.27	00:08:30.63	Now this is a typical case of generating model where we would embed the interesting variables in the generating model. And then we're going to set up a generation probability for the overall rating given the observed words.	now this be a typical case of generate model where we would embed the interesting variable in the generating model . and then we be go to set up a generation probability for the overall rating give the observe word .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.3421	217	00:08:31.33	00:08:49.71	And then of course, then we can adjust these parameter values including beta, r, alpha i. In order to maximize the probability of the data in this case, the conditional probability of the observed rating given the document.	and then of course , then we can adjust these parameter value include beta , r , alpha i. in order to maximize the probability of the datum in this case , the conditional probability of the observed rating give the document .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342107	231	00:08:51.32	00:09:22.82	And so we have seen such cases before in, for example, PLSA, where we predict the text data. But here we predicting the rating and the parameters of course are also very different. But if you can see if we can uncover these parameters, that would be nice because also R of D is precisely the aspect ratings that we want to get, and these are decomposer ratings on different aspects of our sub ID is precisely the aspect weights that we hope to get.	and so we have see such case before in , for example , PLSA , where we predict the text datum . but here we predict the rating and the parameter of course be also very different . but if you can see if we can uncover these parameter , that would be nice because also R of d be precisely the aspect rating that we want to get , and these be decomposer rating on different aspect of our sub ID be precisely the aspect weight that we hope to get .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342108	235	00:09:23.31	00:09:32.51	As a bi product that will also get the beta vector and these are the aspects of specifica sentiment, weights of words, so more formally.	as a bi product that will also get the beta vector and these be the aspect of specifica sentiment , weight of word , so more formally .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342113	246	00:09:33.34	00:09:59.64	They thought we are modeling. Here is a set of review documents with overall ratings. And each review documents denoted by AT and overall rating is denoted by R sub D and these pre segmented into K as their segments and we're going to use C sub I of R and to denote the count of world W in aspect segment I. Of course it's zero if the world doesn't occur in the segment.	they think we be model . here be a set of review document with overall rating . and each review document denote by AT and overall rating be denote by r sub d and these pre segment into k as their segment and we be go to use c sub I of R and to denote the count of world W in aspect segment I. of course it be zero if the world do n't occur in the segment .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342119	258	00:10:01.5	00:10:31.07	Now the model is going to predict the rating based on the. So we are interested in the conditional probability of R sub T given D. And this model is set up as follows. So all of this is assumed to follow a normal distribution with a mean that denotes actually await the average of the aspect ratings. R sub of D as shown here is normal distribution has a variance of or square.	now the model be go to predict the rating base on the . so we be interested in the conditional probability of r sub T give D. and this model be set up as follow . so all of this be assume to follow a normal distribution with a mean that denote actually await the average of the aspect rating . r sub of D as show here be normal distribution have a variance of or square .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342123	267	00:10:32.69	00:10:50.51	Now of course, this is just what our assumption in the actual reading is not necessary generating this way. But as always when we make this assumption, we have a formal way to model the problem, and that allows us to compute interesting quantities. In this case, the aspect ratings and aspect of weights.	now of course , this be just what our assumption in the actual reading be not necessary generate this way . but as always when we make this assumption , we have a formal way to model the problem , and that allow we to compute interesting quantity . in this case , the aspect rating and aspect of weight .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342124	271	00:10:51.88	00:11:03.66	Now the aspect rating as you see on the second line is assumed to be weighted sum of these weights where the weight is just sentiment wait.	now the aspect rating as you see on the second line be assume to be weight sum of these weight where the weight be just sentiment wait .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342125	272	00:11:04.83	00:11:05.7	So.	so .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342126	275	00:11:06.76	00:11:11.88	As I said, the overall rating is assumed to be a weighted average of aspect ratings.	as I say , the overall rating be assume to be a weighted average of aspect rating .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-27 22:47:00.768156	276	00:11:15.13	00:11:16.94	Now this alpha	now this alpha
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342131	287	00:11:18.05	00:11:48.47	Values of a alpha sub of D together by our vector that depends on D is the document specific weights and we can assume this factor itself is drawn from another multivariate Gaussian distribution with mean denoted by a mule vector and covariance matrix Sigma, yeah. Now, so this means when we generate our overall rating, we're going to first draw.	value of a alpha sub of d together by our vector that depend on d be the document specific weight and we can assume this factor itself be draw from another multivariate gaussian distribution with mean denote by a mule vector and covariance matrix Sigma , yeah . now , so this mean when we generate our overall rating , we be go to first draw .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342134	294	00:11:49.57	00:12:07.48	A set of other values from this multivariate Gaussian prior distribution and once we get these alpha values were going to use, then the weighted average of aspect ratings as the mean here to use the normal distribution.	a set of other value from this multivariate Gaussian prior distribution and once we get these alpha value be go to use , then the weighted average of aspect rating as the mean here to use the normal distribution .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342134	295	00:12:08.28	00:12:11.56	And to generate the overall rating.	and to generate the overall rating .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342137	302	00:12:13.82	00:12:27.79	Now the aspect rating as I just said is the sum of the sentiment weights of words in their spectrum. Note that here the sentiment weights are specifically to aspects, so beta is indexed by I. And As for aspect.	now the aspect rating as I just say be the sum of the sentiment weight of word in their spectrum . note that here the sentiment weight be specifically to aspect , so beta be index by I. and as for aspect .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342138	304	00:12:28.49	00:12:34.02	And that gives us way to model different segment of award.	and that give we way to model different segment of award .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.34214	308	00:12:36.17	00:12:44.47	This is neither because of the same word might have positive sentiment for once back, but negative sentiment for another aspect.	this be neither because of the same word might have positive sentiment for once back , but negative sentiment for another aspect .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.34214	309	00:12:45.43	00:12:48.36	It's also useful to then see.	it be also useful to then see .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342143	317	00:12:49.94	00:13:11.43	What premise we have here, but I just said that the beta sub I W gives us a aspect specific sentiment of W. So obviously that's one of the important parameters, but in general we can see we have these parameters. The beta values that Delta and then the mu and Sigma.	what premise we have here , but I just say that the beta sub I w give we a aspect specific sentiment of W. so obviously that be one of the important parameter , but in general we can see we have these parameter . the beta value that Delta and then the mu and Sigma .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342145	321	00:13:12.44	00:13:19.21	So next question is, how can we estimate these parameters and so we collectively denote all the parameters by Lambda here.	so next question be , how can we estimate these parameter and so we collectively denote all the parameter by Lambda here .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342147	326	00:13:20.1	00:13:29.32	Now we can, as usual, use The maximum likelihood is made and this will give us the settings of this premise that with the maximizer observed.	now we can , as usual , use the maximum likelihood be make and this will give we the setting of this premise that with the maximizer observe .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342148	328	00:13:30.95	00:13:35.46	Observer ratings condition on their respective reviews.	observer rating condition on their respective review .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342149	331	00:13:36.64	00:13:43.8	And of course, this would then give us all the useful variables that will interest in computing.	and of course , this would then give we all the useful variable that will interest in computing .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342155	345	00:13:45.32	00:14:15.84	So now more specifically, we can now once we estimate the parameters, we can easily compute the abstract rating for aspect I or sub I of D and that's simply to take all the words that occurred in the segment I and then take their accounts and then multiply that by the sentiment weight of each word and take a sum. So of course this counter would be 04 words that are not occurring in the aspect I, and that's why we can take some over all the words in the vocabulary.	so now more specifically , we can now once we estimate the parameter , we can easily compute the abstract rating for aspect I or sub I of D and that be simply to take all the word that occur in the segment I and then take their account and then multiply that by the sentiment weight of each word and take a sum . so of course this counter would be 04 word that be not occur in the aspect I , and that be why we can take some over all the word in the vocabulary .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342157	350	00:14:16.83	00:14:27.29	Now, what about the aspect weights? Alpha sub I of D? It's not part of our parameter, right? So we have to use Bayesian inference to compute it.	now , what about the aspect weight ? alpha sub I of D ? it be not part of our parameter , right ? so we have to use bayesian inference to compute it .
b1854d1c-3199-4c42-ab7d-f219f70259a3	2020-11-02 23:19:12.342158	352	00:14:28.13	00:14:32.94	And in this case we can use the maximum a posteriori.	and in this case we can use the maximum a posteriori .
b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	2020-11-24 19:24:40.088668	2	00:00:00.3	00:00:03.77	This lecture is about the topic mining and analysis.	this lecture be about the topic mining and analysis .
b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	2020-11-27 23:46:50.641365	7	00:00:12.64	00:00:24.49	We are going to talk about using a term as topic. This is a slide that you have seen in the earlier lecture where we defined the task of top mining and analysis.	we be go to talk about use a term as topic . this be a slide that you have see in the early lecture where we define the task of top mining and analysis .
b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	2020-11-24 19:25:09.786591	9	00:00:25.04	00:00:30.86	We also raised the question how do we exactly define the topic theta?	we also raise the question how do we exactly define the topic theta ?
b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	2020-11-02 23:11:16.765826	15	00:00:31.66	00:00:44.26	So in this lecture we are going to offer one way to define it, and that's our initial idea. Our idea here is to define a topic simply as a term. A term can be a word or a phrase.	so in this lecture we be go to offer one way to define it , and that be our initial idea . our idea here be to define a topic simply as a term . a term can be a word or a phrase .
b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	2020-11-27 23:47:29.423687	22	00:00:45.13	00:00:59.65	And in general, we can use these terms to describe topics, so our first thought is just to define a topic as one term. For example, we might have terms like sports, travel or science as you see here.	and in general , we can use these term to describe topic , so our first thought be just to define a topic as one term . for example , we might have term like sport , travel or science as you see here .
b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	2020-11-24 19:26:40.752896	29	00:01:00.78	00:01:19.8	Now if we define a topic in this way, we can analyze the coverage of such topics in each document. Here, for example, we might want to discover to what extent document 1 covers sports and we found that 30% of the content of document 1 is about sports.	now if we define a topic in this way , we can analyze the coverage of such topic in each document . here , for example , we might want to discover to what extent document 1 cover sport and we find that 30 % of the content of document 1 be about sport .
b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	2020-11-02 23:11:16.765834	33	00:01:20.47	00:01:31.45	And 12% is about the travel etc. We might also discover Document 2 does not cover sports at all, so the coverage is zero, etc.	and 12 % be about the travel etc . we might also discover document 2 do not cover sport at all , so the coverage be zero , etc .
b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	2020-11-02 23:11:16.765834	34	00:01:32.53	00:01:35.24	So now of course, as we discussed.	so now of course , as we discuss .
b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	2020-11-02 23:11:16.765837	43	00:01:36.76	00:01:59.53	In the task definition for topic mining and analysis, we have two tasks, one is to discover the topics and the 2nd is to analyze the coverage. So let's first think about how we can discover topics if we represent each topic by a term. So that means we need to mine K topical terms from a collection.	in the task definition for topic mining and analysis , we have two task , one be to discover the topic and the 2nd be to analyze the coverage . so let 's first think about how we can discover topic if we represent each topic by a term . so that mean we need to mine K topical term from a collection .
b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	2020-11-24 19:27:42.199532	45	00:02:00.93	00:02:04.78	Now there are of course many different ways of doing that and.	now there be of course many different way of do that and .
b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	2020-11-02 23:11:16.765841	60	00:02:05.59	00:02:33.87	We're going to talk about a natural way of doing that, which is also likely effective. So first we're going to parse the text data in the collection to obtain candidate terms. Here, candidate terms can be words or phrases. Let's say the simplest solution is to just take each word as a term. These words then become candidate topics. Then we're going to design a scoring function to measure how good each term is as a topic.	we be go to talk about a natural way of do that , which be also likely effective . so first we be go to parse the text datum in the collection to obtain candidate term . here , candidate term can be word or phrase . let 's say the simple solution be to just take each word as a term . these word then become candidate topic . then we be go to design a scoring function to measure how good each term be as a topic .
b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	2020-11-02 23:11:16.765842	63	00:02:35.36	00:02:39.35	So how can we design such a function? Well, there are many things that we can consider.	so how can we design such a function ? well , there be many thing that we can consider .
b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	2020-11-02 23:11:16.765843	65	00:02:40.03	00:02:44.35	For example, we can use pure statistics to design such as scoring function.	for example , we can use pure statistic to design such as scoring function .
b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	2020-11-02 23:11:16.765844	71	00:02:45.43	00:02:57.54	Intuitively, we would like to favor representative terms, meaning terms that can represent a lot of content in the collection. So that would mean we want to favor a frequent term.	intuitively , we would like to favor representative term , mean term that can represent a lot of content in the collection . so that would mean we want to favor a frequent term .
b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	2020-11-24 19:29:43.219363	78	00:02:58.47	00:03:13.69	However, if we simply use the frequency to design the scoring function, then the highest scored terms would be "general terms or functional terms, like ""the"", ""a""" etc. Those terms are very frequent in English.	however , if we simply use the frequency to design the scoring function , then the high score term would be " general term or functional term , like " " the " " , " " a " " " etc . those term be very frequent in English .
b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	2020-11-02 23:11:16.765849	86	00:03:14.54	00:03:34.16	So we also want to avoid having such words on the top, so we want to penalize such words, but in general would like the favor terms that are fairly frequently but not so frequent. So a particular approach could be based on, TF-IDF weighting from retrieval.	so we also want to avoid have such word on the top , so we want to penalize such word , but in general would like the favor term that be fairly frequently but not so frequent . so a particular approach could be base on , TF - IDF weighting from retrieval .
b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	2020-11-24 19:31:23.859934	98	00:03:35.03	00:04:01.97	And TF stands for term frequency idea IDF stands for inverse document frequency and we talked about some of these ideas in the lectures about the discovery of word associations. So these are statistical methods, meaning that the function is defined mostly based on statistics. So the scoring function would be very general. It can be applied to any language and any text.	and TF stand for term frequency idea IDF stand for inverse document frequency and we talk about some of these idea in the lecture about the discovery of word association . so these be statistical method , mean that the function be define mostly base on statistic . so the scoring function would be very general . it can be apply to any language and any text .
b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	2020-11-02 23:11:16.765861	129	00:04:02.78	00:05:23.62	But when we apply such an approach to a particular problem, we might also be able to leverage some domain specific heuristics. For example, in news we might favor title words. Actually, in general, we might want to favor title words becauses the authors tend to use the title to describe the topic of an article. If we're dealing with tweets, we could also favor hashtags which are invented to denote topics so naturally hashtags can be good candidates for representing topics. Anyway, after we have designed the scoring function, then we can discover the K topical terms by simply picking K terms with the highest scores. Now of course we might encounter a situation where the highest scored terms are all very similar. They are semantically similar or closely related or even synonyms. So that's not desirable, so we also want to have coverage over all the content in the collection. So we would like to remove redundancy and one way to do that is to do a greedy algorithm, which is sometimes called maximal marginal relevance ranking.	but when we apply such an approach to a particular problem , we might also be able to leverage some domain specific heuristic . for example , in news we might favor title word . actually , in general , we might want to favor title word because the author tend to use the title to describe the topic of an article . if we be deal with tweet , we could also favor hashtag which be invent to denote topic so naturally hashtag can be good candidate for represent topic . anyway , after we have design the scoring function , then we can discover the k topical term by simply pick K term with the high score . now of course we might encounter a situation where the high score term be all very similar . they be semantically similar or closely related or even synonym . so that be not desirable , so we also want to have coverage over all the content in the collection . so we would like to remove redundancy and one way to do that be to do a greedy algorithm , which be sometimes call maximal marginal relevance ranking .
b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	2020-11-02 23:11:16.765864	139	00:05:24.36	00:05:44.46	Basically, the idea is to go down the list based on our scoring function an gradually take terms to collect the K topical terms. The first term of course will be picked when we pick the next term. We're going to look at the what terms have already been picked and try to avoid picking a term that's too similar. similar.	basically , the idea be to go down the list base on our scoring function an gradually take term to collect the K topical term . the first term of course will be pick when we pick the next term . we be go to look at the what term have already be pick and try to avoid pick a term that be too similar . similar .
b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	2020-11-24 21:55:31.022525	154	00:05:45.01	00:06:22.27	So while we are considering the ranking of term in the list, we're also consider in the redundancy of the candidate term with respect to the terms that we already picked. With some thresholding then we can get balance of redundancy removal and also high score over term. OK so after this then we will get K topical terms and those can be regarded as the topics that we discovered from the collection. Next let's think about how we can "compute the topic coverage i j	so while we be consider the ranking of term in the list , we be also consider in the redundancy of the candidate term with respect to the term that we already pick . with some thresholding then we can get balance of redundancy removal and also high score over term . ok so after this then we will get K topical term and those can be regard as the topic that we discover from the collection . next let 's think about how we can " compute the topic coverage I j
b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	2020-11-02 23:11:16.76587	160	00:06:23.29	00:06:35.36	So looking at this picture, we have sports, travel and science and these topics and now suppose you are given a document How should we figure out the coverage of each topic in the document?	so look at this picture , we have sport , travel and science and these topic and now suppose you be give a document how should we figure out the coverage of each topic in the document ?
b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	2020-11-02 23:11:16.765874	174	00:06:36.57	00:07:13.67	One approach can be to simply count occurrences of these terms. So for example, sports might have occurred four times in this document, and travel occurred twice, etc, and then we can just normalize. these counts as our estimate of the coverage probability for each topic. So in general the formula would be to collect the counts of all the terms that represented the topics and then simply normalize them so that. The coverage of each topic in the document would add to one.	one approach can be to simply count occurrence of these term . so for example , sport might have occur four time in this document , and travel occur twice , etc , and then we can just normalize . these count as our estimate of the coverage probability for each topic . so in general the formula would be to collect the count of all the term that represent the topic and then simply normalize they so that . the coverage of each topic in the document would add to one .
b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	2020-11-02 23:11:16.765875	178	00:07:15.05	00:07:23.78	This forms a distribution over the topics for the document to characterize coverage of different topics in the document.	this form a distribution over the topic for the document to characterize coverage of different topic in the document .
b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	2020-11-02 23:11:16.765877	183	00:07:25.85	00:07:37.37	Now, as always when we think about the idea for solving problem, we have to ask the question, how good is this one? Or is this the best way of solving the problem?	now , as always when we think about the idea for solve problem , we have to ask the question , how good be this one ? or be this the good way of solve the problem ?
b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	2020-11-24 21:58:00.025673	187	00:07:38.37	00:07:50.32	So now let's examine this approach. In general, we have to do some empirical evaluation by using actual datasets and to see how well it works.	so now let 's examine this approach . in general , we have to do some empirical evaluation by use actual dataset and to see how well it work .
b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	2020-11-24 21:58:27.957163	191	00:07:52.28	00:08:03.35	In this case, let's take a look at a simple example. Here we have the text document that is about the NBA basketball game.	in this case , let 's take a look at a simple example . here we have the text document that be about the NBA basketball game .
b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	2020-11-24 21:58:36.564284	193	00:08:04.7	00:08:07.81	So in terms of the content, it's about the sports.	so in term of the content , it be about the sport .
b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	2020-11-02 23:11:16.765881	198	00:08:08.88	00:08:21.52	But if we simply count these words that represent our topics, and we will find that the word sports actually did not occur in the article, even though the content is about the sports.	but if we simply count these word that represent our topic , and we will find that the word sport actually do not occur in the article , even though the content be about the sport .
b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	2020-11-24 21:28:18.158105	202	00:08:22.44	00:08:31.52	So the count of sports is zero. That means the coverage of sports will be "estimated	so the count of sport be zero . that mean the coverage of sport will be " estimate
b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	2020-11-02 23:11:16.765882	203	00:08:33.06	00:08:34.88	Now of course.	now of course .
b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	2020-11-02 23:11:16.765885	211	00:08:35.46	00:08:49.32	The term science also did not occur in the document, and it's estimated also zero. That's OK, but sports certainly is not OK. 'cause we know the content is about sports. So this estimate has problem.	the term science also do not occur in the document , and it be estimate also zero . that be ok , but sport certainly be not ok . 'cause we know the content be about sport . so this estimate have problem .
b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	2020-11-02 23:11:16.765887	218	00:08:50.79	00:09:07.97	What's worse, term travel actually occurred in the document, so when we estimate the coverage of the topic travel, we have gotten a non-zero count, so it's estimated coverage would be non zero. So this obviously is also not desirable.	what be bad , term travel actually occur in the document , so when we estimate the coverage of the topic travel , we have get a non - zero count , so it be estimate coverage would be non zero . so this obviously be also not desirable .
b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	2020-11-24 22:00:16.938746	225	00:09:08.69	00:09:23.47	So this simple example illustrates some problems of this approach. First, when we count what words belong to the topic, we also need to consider related words. We can't simply just count the topic. word sports.	so this simple example illustrate some problem of this approach . first , when we count what word belong to the topic , we also need to consider related word . we ca n't simply just count the topic . word sport .
b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	2020-11-27 23:57:09.620055	238	00:09:24.13	00:09:53.16	In this case, it did not occur at all, but there are many related words like basketball, game, etc. So we need to count related words. Also. The second problem is that a word like sport can be actually ambiguous. So here it probably means a basketball star But we can imagine it might also mean a star on the Sky. So in that case the star might actually suggest perhaps a topic of science. So we need to deal with that as well.	in this case , it do not occur at all , but there be many related word like basketball , game , etc . so we need to count related word . also . the second problem be that a word like sport can be actually ambiguous . so here it probably mean a basketball star but we can imagine it might also mean a star on the Sky . so in that case the star might actually suggest perhaps a topic of science . so we need to deal with that as well .
b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	2020-11-02 23:11:16.765895	248	00:10:08.41	00:10:30.54	Finally, the main restriction of this approach is that we only have one term to describe this topic. So it cannot really describe complicated topics. For example a very specialized topic would be hard to describe by using just a word or one phrase, we need to use more words, so this example illustrates some general problems with this approach of treating a term as topic. First, it lacks expressive power, meaning that it can only represent the symbol general topics.	finally , the main restriction of this approach be that we only have one term to describe this topic . so it can not really describe complicated topic . for example a very specialized topic would be hard to describe by use just a word or one phrase , we need to use more word , so this example illustrate some general problem with this approach of treat a term as topic . first , it lack expressive power , mean that it can only represent the symbol general topic .
b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	2020-11-02 23:11:16.765901	267	00:10:31.16	00:11:08.84	But it cannot represent the complicated topics that might require more words to describe. Second, it's incomplete in vocabulary coverage, meaning that the topic itself is only represented as one term. It does not suggest what other terms are related to the topic, even if we're talking about the sports, there are many terms that are related, so it does not allow us to easily count related terms toward contributing to coverage of this topic. Finally, there's this problem of word sense ambiguation, a topical term or related term can be ambiguous. For example, basketball star versus star in the Sky.	but it can not represent the complicated topic that might require more word to describe . second , it be incomplete in vocabulary coverage , mean that the topic itself be only represent as one term . it do not suggest what other term be relate to the topic , even if we be talk about the sport , there be many term that be relate , so it do not allow we to easily count relate term toward contribute to coverage of this topic . finally , there be this problem of word sense ambiguation , a topical term or related term can be ambiguous . for example , basketball star versus star in the Sky .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-24 05:18:15.071271	2	00:00:00.29	00:00:03.94	This lecture is about the paradigmatic relation discovery.	this lecture be about the paradigmatic relation discovery .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-24 05:23:58.241981	6	00:00:14.29	00:00:22.58	In this lecture we're going to talk about how to discover a particular kind of word Association called paradigmatic relations.	in this lecture we be go to talk about how to discover a particular kind of word Association call paradigmatic relation .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-24 05:27:07.982595	9	00:00:25.07	00:00:33.54	By definition, 2 words are paradigmatically related if they share similar contexts.	by definition , 2 word be paradigmatically relate if they share similar context .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-02 23:03:08.640505	11	00:00:34.37	00:00:38.52	Namely, they occur in similar positions in text.	namely , they occur in similar position in text .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-02 23:03:08.640505	15	00:00:39.11	00:00:49.07	So naturally, our idea for discovering such relation is to look at the context of each word and then try to compute the similarity of those contexts.	so naturally , our idea for discover such relation be to look at the context of each word and then try to compute the similarity of those context .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-24 05:27:36.088467	17	00:00:50.07	00:00:54.62	So here's an example of context of word Cat.	so here be an example of context of word Cat .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-02 23:03:08.640507	19	00:00:55.67	00:01:00.34	Here I have taken the word cat out of the context.	here I have take the word cat out of the context .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-02 23:03:08.640507	22	00:01:01.49	00:01:08.13	And you can see we are seeing some remaining words in the sentences that contain cat.	and you can see we be see some remain word in the sentence that contain cat .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-02 23:03:08.640508	24	00:01:09.48	00:01:12.67	Now we can do the same thing for another word like a dog.	now we can do the same thing for another word like a dog .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-24 05:28:21.425176	28	00:01:13.54	00:01:23.45	So in general we would like to capture such a context and then try to assess the similarity of the context of cat and the context of a word like dog.	so in general we would like to capture such a context and then try to assess the similarity of the context of cat and the context of a word like dog .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-02 23:03:08.640509	31	00:01:24.69	00:01:31.9	So now the question is, how can we formally represent the context and then define the similarity function?	so now the question be , how can we formally represent the context and then define the similarity function ?
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-02 23:03:08.64051	33	00:01:33.21	00:01:37.85	So first we note that the context actually contains a lot of words.	so first we note that the context actually contain a lot of word .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-24 05:29:02.817877	36	00:01:38.43	00:01:44.2	So they can be regarded as a pseudo document. An imaginary document.	so they can be regard as a pseudo document . an imaginary document .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-24 07:39:45.124649	49	00:01:45.02	00:02:17.6	But there are also different ways of looking at the context. For example, we can look at the word that occurs before the word cat. We can call. We can call this context left1 context. So in this case you will see words like my, his or big, a, the, etc. These are the words that can occur to the left of the world cat. So we say my cat, his cat big cat. a cat etc.	but there be also different way of look at the context . for example , we can look at the word that occur before the word cat . we can call . we can call this context left1 context . so in this case you will see word like my , his or big , a , the , etc . these be the word that can occur to the left of the world cat . so we say my cat , his cat big cat . a cat etc .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-24 05:33:52.522068	53	00:02:18.2	00:02:25.81	Similarly, we can also collect the words that occur right after the word cat. We can call this context right1.	similarly , we can also collect the word that occur right after the word cat . we can call this context right1 .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-02 23:03:08.640515	55	00:02:27.91	00:02:32.92	And here we see words eats, ate, is, has, etc.	and here we see word eat , eat , be , have , etc .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-24 05:35:04.2796	62	00:02:33.67	00:02:48.81	Or more generally, we can look at the all the words in the window of text around the word cat here. let's say we can take a window of eight words around the world cat. We call this context Window8	or more generally , we can look at the all the word in the window of text around the word cat here . let 's say we can take a window of eight word around the world cat . we call this context Window8
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-02 23:03:08.640518	66	00:02:49.71	00:02:59.14	Now of course, you can see all the words from left or from right, and so we have a bag of words in general to represent the context.	now of course , you can see all the word from left or from right , and so we have a bag of word in general to represent the context .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-02 23:03:08.640521	78	00:03:01.14	00:03:30.97	Now, such a word based representation would actually give us interesting way to define the perspective of measuring the similarity. " similarity of left1, then we'll see words that share just the words in the left context and we kind of ignore the other words that are also in the general context. So that gives us one perspective to measure the similarity.	now , such a word base representation would actually give we interesting way to define the perspective of measure the similarity . " similarity of left1 , then we 'll see word that share just the word in the left context and we kind of ignore the other word that be also in the general context . so that give we one perspective to measure the similarity .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-02 23:03:08.640522	81	00:03:31.71	00:03:37.56	And similarly, if we only use the right1 context will capture the similarity from another perspective.	and similarly , if we only use the right1 context will capture the similarity from another perspective .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-02 23:03:08.640523	85	00:03:38.25	00:03:47.84	Using both left1 and right1, ofcourse would allow us to capture the similarity with even more strict criteria.	use both left1 and right1 , ofcourse would allow we to capture the similarity with even more strict criterion .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-02 23:03:08.640525	90	00:03:49.81	00:04:02.85	So in general, context may contain adjacent words like eats and my that you see here or non-adjacent words like Saturday, Tuesday or some other words in the context.	so in general , context may contain adjacent word like eat and my that you see here or non - adjacent word like Saturday , Tuesday or some other word in the context .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-02 23:03:08.640531	104	00:04:05.48	00:04:40.01	And this flexibility also allows us to measure the similarity similarity in some other different ways. Sometimes this is useful as we might want to capture similarity based on general content that would give us loosely related paradigmatic relations, whereas if you use only the words immediately to the left and to the right of the world, then you likely will capture words that are very much related by their syntactical categories, or an semantics.	and this flexibility also allow we to measure the similarity similarity in some other different way . sometimes this be useful as we might want to capture similarity base on general content that would give we loosely relate paradigmatic relation , whereas if you use only the word immediately to the left and to the right of the world , then you likely will capture word that be very much relate by their syntactical category , or an semantic .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-24 05:43:50.454512	107	00:04:41.05	00:04:49.9	So the general idea of discovering paradigmatic relations is to compute the similarity of context of two words.	so the general idea of discover paradigmatic relation be to compute the similarity of context of two word .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-02 23:03:08.640539	130	00:04:50.71	00:05:46.72	So here for example, we can measure the similarity of cat and dog based on the similarity of their contexts. In general, we can combine all kinds of views of the context and so the similarity function is in general combination of similarities on different contexts. And of course we can also assign weights to these different similarities to allow us to focus more on particular kind of context, and this would be naturally application specific, but again here that main idea for discovering paradigmatically related words is to compute the similarity of their context. So next, let's see how we exactly compute these similarity functions. Now to answer this question it's useful to think of bag of words representation as vectors in the vector space model.	so here for example , we can measure the similarity of cat and dog base on the similarity of their context . in general , we can combine all kind of view of the context and so the similarity function be in general combination of similarity on different context . and of course we can also assign weight to these different similarity to allow we to focus more on particular kind of context , and this would be naturally application specific , but again here that main idea for discover paradigmatically relate word be to compute the similarity of their context . so next , let 's see how we exactly compute these similarity function . now to answer this question it be useful to think of bag of word representation as vector in the vector space model .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-02 23:03:08.640543	146	00:05:48.22	00:06:26.25	Now those of you who have been familiar with information retrieval or text retrieval techniques would realize that vector space model has been used frequently for modeling documents and queries for search. But here we also find it convenient to model the context of a word for paradigmatically relation discovery. So the idea of this approach is to view each word in our vocabulary as defining one dimension in high dimensional space so we have N words in total in the vocabulary. Then we have N dimensions as illustrated here.	now those of you who have be familiar with information retrieval or text retrieval technique would realize that vector space model have be use frequently for model document and query for search . but here we also find it convenient to model the context of a word for paradigmatically relation discovery . so the idea of this approach be to view each word in our vocabulary as define one dimension in high dimensional space so we have n word in total in the vocabulary . then we have n dimension as illustrate here .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-02 23:03:08.640544	149	00:06:27.51	00:06:33.29	And on the bottom you can see frequency vector representing a context.	and on the bottom you can see frequency vector represent a context .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-02 23:03:08.640545	154	00:06:34.48	00:06:47.43	And here we see when eats occured five times in this context, are occurred three times etc. So this vector can then be placed in this vector space model.	and here we see when eat occur five time in this context , be occur three time etc . so this vector can then be place in this vector space model .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-02 23:03:08.640545	157	00:06:48.02	00:06:55.56	So in general, we can represent a pseudo document or context of cat as one vector.	so in general , we can represent a pseudo document or context of cat as one vector .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-24 05:47:35.848271	158	00:06:56.15	00:06:56.8	d1.	d1 .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-24 05:47:44.516872	160	00:06:57.44	00:07:02.45	An another word dog might give us a different context, so d2.	an another word dog might give we a different context , so d2 .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-02 23:03:08.640548	167	00:07:04.12	00:07:19.04	And then we can measure the similarity of these two vectors. So by viewing context in the vector space model, we convert the problem of paradigmatic relations discovery into the problem of computing the vectors and their similarity.	and then we can measure the similarity of these two vector . so by view context in the vector space model , we convert the problem of paradigmatic relation discovery into the problem of compute the vector and their similarity .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-24 05:48:49.073279	171	00:07:20.17	00:07:29.21	So the two questions that we have to address is first how to compute each vector, that is, how to compute the xi or yi.	so the two question that we have to address be first how to compute each vector , that is , how to compute the xi or yi .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-02 23:03:08.64055	173	00:07:30.92	00:07:33.73	And the other question is, how do you compute the similarity?	and the other question be , how do you compute the similarity ?
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-02 23:03:08.640551	177	00:07:35.44	00:07:42.66	Now in general there are many approaches that can be used to solve the problem, and most of them are developed for information retrieval.	now in general there be many approach that can be use to solve the problem , and most of they be develop for information retrieval .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-24 05:49:52.563151	188	00:07:43.84	00:08:15.51	And they have been shown to work well for matching a query vector and a document vector, but we can adapt the many of the ideas to compute the similarity of context documents for our purpose here. So let's first look at the one possible approach, where we try to measure the similarity of context based on the expected overlap of words and we call this EOWC.	and they have be show to work well for match a query vector and a document vector , but we can adapt the many of the idea to compute the similarity of context document for our purpose here . so let 's first look at the one possible approach , where we try to measure the similarity of context base on the expect overlap of word and we call this EOWC .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-24 05:51:02.3142	197	00:08:16.91	00:08:45.5	So the idea here is represent a context by award vector where each word has a weight that is equal to the probability that a randomly picked word from this document vector is this word. So in other words. xi is defined as the normalized count of word wi in the context.	so the idea here be represent a context by award vector where each word have a weight that be equal to the probability that a randomly pick word from this document vector be this word . so in other word . xi be define as the normalize count of word wi in the context .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-24 05:51:32.181144	201	00:08:46.19	00:08:54.74	And this can be interpreted as a probability that you would actually pick this word from d1 if you randomly pick the word.	and this can be interpret as a probability that you would actually pick this word from d1 if you randomly pick the word .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-02 23:03:08.640558	204	00:08:56.63	00:09:01.69	Now of course these xi's will sum to 1 because they are normalized frequencies.	now of course these xi 's will sum to 1 because they be normalize frequency .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-02 23:03:08.640558	207	00:09:02.81	00:09:08.25	And this means the vector is actually probability distribution over words.	and this mean the vector be actually probability distribution over word .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-02 23:03:08.640559	210	00:09:10.37	00:09:17.61	So, the vector d2 can be also computed in the same way.	so , the vector d2 can be also compute in the same way .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-24 05:52:47.240093	213	00:09:18.3	00:09:23.56	And this would give us then two probability distributions representing two contexts.	and this would give we then two probability distribution represent two context .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-02 23:03:08.640562	223	00:09:24.71	00:09:44.04	So that addresses the problem how to compute the vectors? Next, let's see how we can define similarity in this approach. Well, here we simply define the similarity as a dot product of two vectors and this is defined as the sum of the products of all the corresponding elements of the two vectors.	so that address the problem how to compute the vector ? next , let 's see how we can define similarity in this approach . well , here we simply define the similarity as a dot product of two vector and this be define as the sum of the product of all the corresponding element of the two vector .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-02 23:03:08.640562	226	00:09:46.49	00:09:52.98	Now it's interesting to see that this similarity function actually has a nice interpretation.	now it be interesting to see that this similarity function actually have a nice interpretation .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-24 05:55:11.733884	245	00:09:55.21	00:10:39.92	And there is this dot product infact gives us the probability that to randomly picked words from the two contexts are identical that means if we try to pick a word from one context and try to pick another word from another context, we can then ask the question, are they identical? If the two contexts are very similar, then we should expect that we frequently will see the two words picked from the two Contacts are identical. If they are very different then the chance of seeing identical words being picked from the two contexts would be small. So this intuitively makes sense for measuring similarity of contexts.	and there be this dot product infact give we the probability that to randomly pick word from the two context be identical that mean if we try to pick a word from one context and try to pick another word from another context , we can then ask the question , be they identical ? if the two context be very similar , then we should expect that we frequently will see the two word pick from the two contact be identical . if they be very different then the chance of see identical word be pick from the two context would be small . so this intuitively make sense for measure similarity of context .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-02 23:03:08.640569	250	00:10:41.36	00:10:55.59	Now you might want to also take a look at the exact formulas and see why this can be interpreted as the probability that two randomly picked words are identical.	now you might want to also take a look at the exact formula and see why this can be interpret as the probability that two randomly pick word be identical .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-24 05:55:37.578746	251	00:10:56.19	00:11:01.25	So if you just stay at the formula.	so if you just stay at the formula .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-24 05:58:33.805462	264	00:11:02.11	00:11:33.53	to check what's inside this some then you will see, basically in each case it gives us the probability that we'll see overlap on a particular word, wi and where xi gives us the probability that will pick this particular word from d1 and Y yi gives us the probability of picking this word from d2 and when we pick the same word from the two contexts and we have identical pick.	to check what be inside this some then you will see , basically in each case it give we the probability that we 'll see overlap on a particular word , wi and where xi give we the probability that will pick this particular word from d1 and Y yi give we the probability of pick this word from d2 and when we pick the same word from the two context and we have identical pick .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-02 23:03:08.640581	295	00:11:34.13	00:12:50.19	Alright, so that's one possible approach. EOWC expected overlap of words in context. Now, as always, we would like to assess whether this approach it would work well. Now, of course, ultimately we have to test the approach it with real data and see if it gives us really semantically related words really give us a paradigmatic relations. But analytically, we can also analyze this formula little bit. So first, as I said, it does make sense right? because this formula will give a higher score if there is more overlap between the two contexts. So that's exactly what we want. But if you analyze the formula more carefully, then you also see there might be some potential problems. And specifically there are two potential problems. First might favor matching one frequently term very well over matching more distinct terms, and that is because in the dot product, if one element has a high value and this element is shared by both context and it contributes a lot to the overall sum.	alright , so that be one possible approach . EOWC expect overlap of word in context . now , as always , we would like to assess whether this approach it would work well . now , of course , ultimately we have to test the approach it with real datum and see if it give we really semantically relate word really give we a paradigmatic relation . but analytically , we can also analyze this formula little bit . so first , as I say , it do make sense right ? because this formula will give a high score if there be more overlap between the two context . so that be exactly what we want . but if you analyze the formula more carefully , then you also see there might be some potential problem . and specifically there be two potential problem . first might favor match one frequently term very well over match more distinct term , and that be because in the dot product , if one element have a high value and this element be share by both context and it contribute a lot to the overall sum .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-02 23:03:08.640583	300	00:12:51.07	00:13:03.86	And it might indeed make the score higher than in another case where the two vectors actually have a lot of overlap in different terms, but each term has a relatively low frequency.	and it might indeed make the score high than in another case where the two vector actually have a lot of overlap in different term , but each term have a relatively low frequency .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-02 23:03:08.640585	308	00:13:04.75	00:13:23.34	So this may not be desirable. Of course, this might be desirable in some other cases, but in our case we should intuitively prefer a case where we match more different terms in the context so that we have more confidence in saying that the two words indeed occur in similar context.	so this may not be desirable . of course , this might be desirable in some other case , but in our case we should intuitively prefer a case where we match more different term in the context so that we have more confidence in say that the two word indeed occur in similar context .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-24 06:01:27.2788	311	00:13:24.17	00:13:30.16	If you only rely on one term and that's a little bit questionable.	if you only rely on one term and that be a little bit questionable .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-02 23:03:08.640586	312	00:13:31.37	00:13:32.53	It may not be robust.	it may not be robust .
b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	2020-11-02 23:03:08.64059	326	00:13:34.59	00:14:11.45	The second problem is that it treats every word equally, so if you match on the water like the and match was, it would be the same as matching on the word like eats. But intuitively we know matching the isn't really surprising because the occurs everywhere, so matching the is not as such a strong evidence as matching award like eats which doesn't occur frequently. So this is another problem of this approach.	the second problem be that it treat every word equally , so if you match on the water like the and match be , it would be the same as match on the word like eat . but intuitively we know match the be n't really surprising because the occur everywhere , so match the be not as such a strong evidence as match award like eat which do n't occur frequently . so this be another problem of this approach .
d857a66b-1018-4ffb-821a-9d8acc6f5012	2020-11-02 23:15:08.9293	13	00:00:00.3	00:00:23.31	So now let's talk about the extension of PLSA to derive LDA and to motivate that we need to talk about some deficiency of PLSA. First, it's not really generating model because we can't compute the probability of a new document. You can see why, and that's because the pies are needed to generate the document, but the pis are tied to the document that we have in the training data.	so now let 's talk about the extension of PLSA to derive LDA and to motivate that we need to talk about some deficiency of PLSA . first , it be not really generate model because we ca n't compute the probability of a new document . you can see why , and that be because the pie be need to generate the document , but the pis be tie to the document that we have in the training datum .
d857a66b-1018-4ffb-821a-9d8acc6f5012	2020-11-02 23:15:08.929302	17	00:00:31.08	00:00:36.98	So we cannot compute the pis for future document. And there was some heuristic. A work around though.	so we can not compute the pis for future document . and there be some heuristic . a work around though .
d857a66b-1018-4ffb-821a-9d8acc6f5012	2020-11-02 23:15:08.929307	26	00:00:37.68	00:01:00.81	And Secondly, it has many parameters and I've asked you to compute how many parameters exactly there are in PLSA and "you will see there are many That means the model is very complex and that also means there are many "local overfitting and that means it's very hard to also find a good local maximum.	and secondly , it have many parameter and I 've ask you to compute how many parameter exactly there be in PLSA and " you will see there be many that mean the model be very complex and that also mean there be many " local overfitting and that mean it be very hard to also find a good local maximum .
d857a66b-1018-4ffb-821a-9d8acc6f5012	2020-12-07 20:24:20.581715	45	00:01:02.37	00:01:40.6	And that really represents global maximum. And in terms of explaining future data, we might find that it would overfit the training data because of the complexity of the model. The model is so flexible to fit the precisely what the training data looks like, and then it doesn't allow us to generalize the model for using other data. This, however, is not necessary problem for text mining because here we are often only interested in fitting the training documents that we have. We are not always interested in modeling future data, but in other cases or if we care about generality, we would worry about this over fitting.	and that really represent global maximum . and in term of explain future datum , we might find that it would overfit the training datum because of the complexity of the model . the model be so flexible to fit the precisely what the training datum look like , and then it do n't allow we to generalize the model for use other datum . this , however , be not necessary problem for text mining because here we be often only interested in fit the training document that we have . we be not always interested in model future datum , but in other case or if we care about generality , we would worry about this over fitting .
d857a66b-1018-4ffb-821a-9d8acc6f5012	2020-11-02 23:15:08.92932	57	00:01:42.22	00:02:04.35	So LDA is proposed to improve that and it basically to make PLSA a generative model by imposing a Dirichlet prior on the model parameters. Dirichlet is just a special distribution that we can use to specify prior. So in this sense, LDA is just a Bayesian version of PLSA and the parameters are now much more regularized. You will see there are many fewer parameters.	so LDA be propose to improve that and it basically to make PLSA a generative model by impose a Dirichlet prior on the model parameter . dirichlet be just a special distribution that we can use to specify prior . so in this sense , LDA be just a bayesian version of PLSA and the parameter be now much more regularized . you will see there be many few parameter .
d857a66b-1018-4ffb-821a-9d8acc6f5012	2020-11-02 23:15:08.929347	85	00:02:05.27	00:03:08.87	And you can achieve the same goal as PLSA for text mining. It means it can compute the topic coverage and topic word distributions as in PLSA. However, there is no free launch while the parameters for PLSA is much fewer, there were fewer parameters and in order to compute the topoc coverage and word distributions, we again face the problem of influence of these variables because they're not the parameters of the model. So the inference part. Again, face the local Maxima problem. So essentially they are doing something very similar, but theoretically LDA is more elegant way of looking at the topic modeling problem. So let's see how we can generalize PLSA to LDA or extend the PLSA to have LDA now a full treatment of LDA is beyond the scope of this course and we just don't have time to go in depth in talking about that. But here I just want to give you a brief idea about what's the extension and what it enables.	and you can achieve the same goal as PLSA for text mining . it mean it can compute the topic coverage and topic word distribution as in PLSA . however , there be no free launch while the parameter for PLSA   be much few , there be few parameter and in order to compute the topoc coverage and word distribution , we again face the problem of influence of these variable because they be not the parameter of the model . so the inference part . again , face the local Maxima problem . so essentially they be do something very similar , but theoretically LDA be more elegant way of look at the topic modeling problem . so let 's see how we can generalize PLSA to LDA or extend the PLSA to have LDA now a full treatment of LDA be beyond the scope of this course and we just do n't have time to go in depth in talk about that . but here I just want to give you a brief idea about what be the extension and what it enable .
d857a66b-1018-4ffb-821a-9d8acc6f5012	2020-11-02 23:15:08.929348	88	00:03:09.54	00:03:14.72	So this is a picture of LDA. Now I remove the background model just for simplicity.	so this be a picture of LDA . now I remove the background model just for simplicity .
d857a66b-1018-4ffb-821a-9d8acc6f5012	2020-11-02 23:15:08.929355	107	00:03:15.85	00:04:02.31	Now in this model, all these parameters are free to change and we do not impose any prior, so these word distributions are now represented as theta i vectors. So these word distributions. So here and the other set of parameters are pis and we present as a vector also. And this is for convenience to introduce LDA and we have one vector for each document. And in this case in theta we have one vector for each topic. Now that the difference between LDA and PLSA is that in LDA we're going to not allow them to free the change. Instead, we're going to force them to be drawn from another distribution.	now in this model , all these parameter be free to change and we do not impose any prior , so these word distribution be now represent as theta I vector . so these word distribution . so here and the other set of parameter be pis and we present as a vector also . and this be for convenience to introduce LDA and we have one vector for each document . and in this case in theta we have one vector for each topic . now that the difference between LDA and PLSA be that in LDA we be go to not allow they to free the change . instead , we be go to force they to be draw from another distribution .
d857a66b-1018-4ffb-821a-9d8acc6f5012	2020-11-25 17:54:02.009464	119	00:04:03.29	00:04:30.15	So more specifically they will be drawn from 2 Dirichlet distributions respectively. "The vectors, so it gives us a probability for a particular choice of a vector. Take for example pis, right? So this Dirichlet distribution tell us which vector of pis is more likely, and this distribution itself is controlled by another vector of parameters of alpha's.	so more specifically they will be draw from 2 dirichlet distribution respectively . " the vector , so it give we a probability for a particular choice of a vector . take for example pis , right ? so this dirichlet distribution tell we which vector of pis be more likely , and this distribution itself be control by another vector of parameter of alpha 's .
d857a66b-1018-4ffb-821a-9d8acc6f5012	2020-11-02 23:15:08.929363	129	00:04:31.59	00:04:52.44	"Depending on characterize the distribution in different ways and with force certain choices of pi's. To be more likely than others. For example, you might favor a choice of relatively uniform distribution of all the topics, or you might favor generating skewed coverage of topics, and this is controlled by Alpha.	" depend on characterize the distribution in different way and with force certain choice of pi 's . to be more likely than other . for example , you might favor a choice of relatively uniform distribution of all the topic , or you might favor generate skewed coverage of topic , and this be control by Alpha .
d857a66b-1018-4ffb-821a-9d8acc6f5012	2020-11-02 23:15:08.929367	139	00:04:53.06	00:05:16.82	And similar here. The topic word distributions are drawn from another Dirichlet distribution with beta parameters and note that here Alpha has K parameters corresponding to our inference on the k values of pis for a document, whereas here beta has N values corresponding to controlling the N words in our vocabulary.	and similar here . the topic word distribution be draw from another dirichlet distribution with beta parameter and note that here alpha have k parameter correspond to our inference on the k value of pis for a document , whereas here beta have n value correspond to control the N word in our vocabulary .
d857a66b-1018-4ffb-821a-9d8acc6f5012	2020-11-02 23:15:08.929369	145	00:05:17.58	00:05:32.77	Now, once we impose these price than the generation process will be different an we all start with drawing pi's from this Dirichlet distribution and this pi will tell us these probabilities.	now , once we impose these price than the generation process will be different an we all start with draw pi 's from this dirichlet distribution and this pi will tell we these probability .
d857a66b-1018-4ffb-821a-9d8acc6f5012	2020-11-08 19:52:06.371119	149	00:05:35.06	00:05:45.95	And then we're going to use the pi to further choose which topic to use, and this is of course very similar to the PLSA model.	and then we be go to use the pi to far choose which topic to use , and this be of course very similar to the PLSA model .
d857a66b-1018-4ffb-821a-9d8acc6f5012	2020-11-02 23:15:08.929376	166	00:05:46.79	00:06:25.91	A similar here we're not going to have these distributions free. Instead we can do draw one from the Dirichlet distribution, and then from this, then we're going to further sample a word and the rest is very similar to the PLSA. The likelihood function now is more complicated for LDA, but there's a close connection between the likelihood function of LDA and PLSA, so I'm going to illustrate the difference here. So in the top you see PLSA. Likelihood function that you have already seen before it's copied from previous slide only that I dropped the background for simplicity.	a similar here we be not go to have these distribution free . instead we can do draw one from the dirichlet distribution , and then from this , then we be go to far sample a word and the rest be very similar to the PLSA . the likelihood function now be more complicated for LDA , but there be a close connection between the likelihood function of LDA and PLSA , so I be go to illustrate the difference here . so in the top you see PLSA . Likelihood function that you have already see before it be copy from previous slide only that I drop the background for simplicity .
d857a66b-1018-4ffb-821a-9d8acc6f5012	2020-11-02 23:15:08.929378	172	00:06:27.06	00:06:39.2	So in the LDA formulas you see very similar things. First you see the first equation is essentially the same and this is the probability of generating a word from multiple word distributions.	so in the LDA formula you see very similar thing . first you see the first equation be essentially the same and this be the probability of generate a word from multiple word distribution .
d857a66b-1018-4ffb-821a-9d8acc6f5012	2020-11-02 23:15:08.92938	178	00:06:40.55	00:06:54.27	And this formula is a sum of all the possibilities of generating the word inside the sum is a product of the probability of choosing a topic multiplied by the probability of observing the world from that topic.	and this formula be a sum of all the possibility of generate the word inside the sum be a product of the probability of choose a topic multiply by the probability of observe the world from that topic .
d857a66b-1018-4ffb-821a-9d8acc6f5012	2020-11-02 23:15:08.929386	196	00:06:55.06	00:07:36.39	So this is a very important formula as I have stressed but multiple times and this is actually the core assumption in all the topic models and you might see other topic models that are extensions of LDA or PLSA and they all rely on this. So it's very important to understand this. And this gives us the probability of getting a word from a mixture model. Now next in the probability of a document we see there is a PLSA component in the LDA formula. But the LDA formula would add some integral here, and that's to explain to account for the fact that the pis are not fixed, so they are drawn from Dirichlet distribution.	so this be a very important formula as I have stress but multiple time and this be actually the core assumption in all the topic model and you might see other topic model that be extension of LDA or PLSA and they all rely on this . so it be very important to understand this . and this give we the probability of get a word from a mixture model . now next in the probability of a document we see there be a PLSA component in the LDA formula . but the LDA formula would add some integral here , and that be to explain to account for the fact that the pis be not fix , so they be draw from dirichlet distribution .
d857a66b-1018-4ffb-821a-9d8acc6f5012	2020-11-02 23:15:08.929387	197	00:07:37.2	00:07:38.33	And that's shown here.	and that be show here .
d857a66b-1018-4ffb-821a-9d8acc6f5012	2020-11-25 17:58:01.869164	201	00:07:39.1	00:07:46.79	That's why we have to take the integral to consider all the possible pi's that we could possibly draw from this "Dirichlet	that be why we have to take the integral to consider all the possible pi 's that we could possibly draw from this " dirichlet
d857a66b-1018-4ffb-821a-9d8acc6f5012	2020-11-02 23:15:08.929389	205	00:07:47.74	00:07:56.73	And similarly, in the likelihood for the whole collection, we also see further components added. Another integral here.	and similarly , in the likelihood for the whole collection , we also see further component add . another integral here .
d857a66b-1018-4ffb-821a-9d8acc6f5012	2020-11-25 17:58:35.127377	211	00:07:58.07	00:08:11.75	Right, so basically in the LDA we just added these integrals to account for the uncertainties and we added of "course the govern the choice of these parameters, pi's and theta's.	right , so basically in the LDA we just add these integral to account for the uncertainty and we add of " course the govern the choice of these parameter , pi 's and theta 's .
d857a66b-1018-4ffb-821a-9d8acc6f5012	2020-11-02 23:15:08.9294	240	00:08:12.81	00:09:16.5	So this is a likelihood function for LDA. Now let's next let's talk about parameter is making an inference is now the parameters can be now estimated using exactly the same approach maximum likelihood estimator for LDA. Now you might think about how many parameters are there in LDA versus PLSA. You will see there are fewer parameters in LDA because in this case the only parameters are alphas and betas. So we can use the maximum likelihood estimated to compute that. Of course it's more complicated because the form of likelihood functions more complicated. But what's also important is not set. Now. These parameters that we are interested in, namely the topics and the coverage, are no longer parameters in LDA. In this case we have to use Bayesian inference or posterior inference to compute them based on the parameters Alpha and beta. Unfortunately, this computation is intractable, so we generally have to resort to approximate.	so this be a likelihood function for LDA . now let 's next let 's talk about parameter be make an inference be now the parameter can be now estimate use exactly the same approach maximum likelihood estimator for LDA . now you might think about how many parameter be there in LDA versus PLSA . you will see there be few parameter in LDA because in this case the only parameter be alpha and beta . so we can use the maximum likelihood estimate to compute that . of course it be more complicated because the form of likelihood function more complicated . but what be also important be not set . now . these parameter that we be interested in , namely the topic and the coverage , be no long parameter in LDA . in this case we have to use bayesian inference or posterior inference to compute they base on the parameter Alpha and beta . unfortunately , this computation be intractable , so we generally have to resort to approximate .
d857a66b-1018-4ffb-821a-9d8acc6f5012	2020-11-02 23:15:08.9294	241	00:09:17	00:09:17.67	Influence.	influence .
d857a66b-1018-4ffb-821a-9d8acc6f5012	2020-11-02 23:15:08.929401	243	00:09:18.62	00:09:22.31	And there are many methods are available for and then.	and there be many method be available for and then .
d857a66b-1018-4ffb-821a-9d8acc6f5012	2020-11-02 23:15:08.929402	247	00:09:23.94	00:09:32.89	So you will see them when you use different toolkits for LDA, or you read the papers about that these different extensions of LDA.	so you will see they when you use different toolkit for LDA , or you read the paper about that these different extension of LDA .
d857a66b-1018-4ffb-821a-9d8acc6f5012	2020-11-02 23:15:08.929403	251	00:09:34.35	00:09:45.8	Now here we of course can't give in depth introduction to, but just know that they are computed based on Bayesian inference with.	now here we of course ca n't give in depth introduction to , but just know that they be compute base on Bayesian inference with .
d857a66b-1018-4ffb-821a-9d8acc6f5012	2020-11-25 18:00:33.158623	253	00:09:47.39	00:09:50.73	By using the parameters of alphas and beta.	by use the parameter of alpha and beta .
d857a66b-1018-4ffb-821a-9d8acc6f5012	2020-11-02 23:15:08.929406	262	00:09:51.31	00:10:08.97	But algorithmically, actually in the end, in some algorithm at least, it's very similar to PLSA an, especially when we use algorithm called collapsed Gibbs sampling. Then the algorithm looks very similar to the EM algorithm. So in the end they're doing something very similar.	but algorithmically , actually in the end , in some algorithm at least , it be very similar to PLSA an , especially when we use algorithm call collapse Gibbs sample . then the algorithm look very similar to the EM algorithm . so in the end they be do something very similar .
d857a66b-1018-4ffb-821a-9d8acc6f5012	2020-11-02 23:15:08.929407	267	00:10:09.92	00:10:20.97	So to summarize, our discussion of probabilistic topic models and these models provide a general principal way of mining and analyzing topics in texts with many applications.	so to summarize , our discussion of probabilistic topic model and these model provide a general principal way of mining and analyze topic in text with many application .
d857a66b-1018-4ffb-821a-9d8acc6f5012	2020-11-02 23:15:08.929409	274	00:10:22.21	00:10:37.15	The best basis test setup is to take tax data as input, and we're going to output the key topics. Each topic is characterized by a word distribution, and we're going to also output proportions of these topics covered in each document.	the good basis test setup be to take tax datum as input , and we be go to output the key topic . each topic be characterize by a word distribution , and we be go to also output proportion of these topic cover in each document .
d857a66b-1018-4ffb-821a-9d8acc6f5012	2020-11-25 18:01:52.127742	280	00:10:38.85	00:10:51.96	And PLSA is the basic topic model, and in fact the most basic topic Model. And this is also often adequate for most applications. That's why we spend a lot of time to explain PLSA in detail.	and PLSA be the basic topic model , and in fact the most basic topic Model . and this be also often adequate for most application . that be why we spend a lot of time to explain PLSA in detail .
d857a66b-1018-4ffb-821a-9d8acc6f5012	2020-11-25 18:02:04.902112	284	00:10:53.02	00:10:59.93	Now LDA improves over PLSA by imposing priors. This has led to theoretically more appealing models.	now LDA improve over PLSA by impose prior . this have lead to theoretically more appealing model .
d857a66b-1018-4ffb-821a-9d8acc6f5012	2020-11-02 23:15:08.929412	288	00:11:00.52	00:11:10.94	However, in practice, LDA and PLSA intended to give similar performance, so in practice, PLSA, an LDA, would work equally well for most tasks.	however , in practice , LDA and PLSA intend to give similar performance , so in practice , PLSA , an LDA , would work equally well for most task .
d857a66b-1018-4ffb-821a-9d8acc6f5012	2020-11-02 23:15:08.929413	292	00:11:12.17	00:11:19.42	Here are some suggested readings if you want to know more about the topic. First is a nice review of probabilistic topic models.	here be some suggest reading if you want to know more about the topic . first be a nice review of probabilistic topic model .
da74c929-efc1-4b65-9635-684c7ebcab3f	2020-11-03 02:04:37.721325	2	00:00:00.3	00:00:04.24	This lecture is about evaluation of text cluster.	this lecture be about evaluation of text cluster .
da74c929-efc1-4b65-9635-684c7ebcab3f	2020-11-03 02:04:37.721327	6	00:00:12.7	00:00:21.56	So far we have talked about multiple ways of doing text clustering but how do we know which method works the best?	so far we have talk about multiple way of do text clustering but how do we know which method work the good ?
da74c929-efc1-4b65-9635-684c7ebcab3f	2020-11-03 02:04:37.721327	7	00:00:22.87	00:00:24.78	So this has to do with evaluation.	so this have to do with evaluation .
da74c929-efc1-4b65-9635-684c7ebcab3f	2020-11-03 02:04:37.721329	11	00:00:25.42	00:00:31.61	Now to talk about evaluation, one must go to go back to the classroom bias that we introduced at the beginning.	now to talk about evaluation , one must go to go back to the classroom bias that we introduce at the beginning .
da74c929-efc1-4b65-9635-684c7ebcab3f	2020-11-03 02:04:37.721332	17	00:00:32.49	00:00:45.41	Because two objects can be similar depending on how you look at them, we must clearly specify the perspective of similarity. Without that, the problem of clustering is not well defined.	because two object can be similar depend on how you look at they , we must clearly specify the perspective of similarity . without that , the problem of clustering be not well define .
da74c929-efc1-4b65-9635-684c7ebcab3f	2020-12-05 02:05:47.345217	31	00:00:46.08	00:01:18.97	So this perspective is also very important for evaluation. If you look at this slide and you can see we have two different ways to cluster these shapes. An if you ask a question, which one is the best or which one is better you actually see there's no way to answer this question without knowing whether we'd like to cluster based on shapes or cluster based on sizes. And that's precisely why the perspective or clustering bias is crucial for evaluation.	so this perspective be also very important for evaluation . if you look at this slide and you can see we have two different way to cluster these shape . an if you ask a question , which one be the good or which one be well you actually see there be no way to answer this question without know whether we 'd like to cluster base on shape or cluster base on size . and that be precisely why the perspective or clustering bias be crucial for evaluation .
da74c929-efc1-4b65-9635-684c7ebcab3f	2020-11-03 02:04:37.721341	41	00:01:19.58	00:01:37.49	In general, we can evaluate text clusters in two ways. One is direct evaluation and the other is indirect valuation, so in directl valuation, we want to answer the following question; How close are the system generated clusters to the ideal clusters that are generated by humans?	in general , we can evaluate text cluster in two way . one be direct evaluation and the other be indirect valuation , so in directl valuation , we want to answer the follow question ; how close be the system generate cluster to the ideal cluster that be generate by human ?
da74c929-efc1-4b65-9635-684c7ebcab3f	2020-11-03 02:04:37.721343	47	00:01:38.49	00:01:55.54	So the closeness here can be assessed assessed from multiple perspectives and that would help us characterize the quality of clustering results in multiple angles. And this is sometimes desirable.	so the closeness here can be assess assess from multiple perspective and that would help we characterize the quality of clustering result in multiple angle . and this be sometimes desirable .
da74c929-efc1-4b65-9635-684c7ebcab3f	2020-11-03 02:04:37.721345	52	00:01:56.64	00:02:07.51	Now. We also want to quantify the closeness because this would allow us to easily compare different methods based on their performance figures.	now . we also want to quantify the closeness because this would allow we to easily compare different method base on their performance figure .
da74c929-efc1-4b65-9635-684c7ebcab3f	2020-11-03 02:04:37.721348	60	00:02:09.75	00:02:27.79	And finally, you can see in this case we essentially inject the clustering bias by using humans. Basically, humans would bring the need or desire clustering bias. Now how do we do that exactly? The general procedure would look like this.	and finally , you can see in this case we essentially inject the cluster bias by use human . basically , human would bring the need or desire clustering bias . now how do we do that exactly ? the general procedure would look like this .
da74c929-efc1-4b65-9635-684c7ebcab3f	2020-11-03 02:04:37.72135	66	00:02:28.51	00:02:41.67	Given the test set which consists of a lot of text objects, we can have humans who create the ideal clustering result. That is, we're going to ask humans to partition the objects to create the gold standard.	give the test set which consist of a lot of text object , we can have human who create the ideal clustering result . that is , we be go to ask human to partition the object to create the gold standard .
da74c929-efc1-4b65-9635-684c7ebcab3f	2020-11-03 02:04:37.721353	73	00:02:42.28	00:03:00.33	And they will use their judgments based on the need of a particular application to generate what they think are the best clustering results. And this would be then used to compare with the system generated clusters from the same test set.	and they will use their judgment base on the need of a particular application to generate what they think be the good clustering result . and this would be then use to compare with the system generate cluster from the same test set .
da74c929-efc1-4b65-9635-684c7ebcab3f	2020-11-03 02:04:37.721362	97	00:03:01.26	00:04:00.59	And ideally we wanted the system results to be the same as human generated results, but in general they are not going to be the same, so we would like to then qualify the similarity between the system generated clusters and the gold standard clusters, and this similarity can be also measured from multiple perspectives and this will give us various measures to quantitatively evaluate a cluster clustering result and some of the commonly used measures include purity, which measures whether a cluster has similar objects from the same cluster in the gold standard and normalized mutual information is a commonly used measure which basically measures based on the identity or the cluster of object in the system-generated results. How well can you predict the cluster of the object in the gold standard or vice versa.	and ideally we want the system result to be the same as human generate result , but in general they be not go to be the same , so we would like to then qualify the similarity between the system generate cluster and the gold standard cluster , and this similarity can be also measure from multiple perspective and this will give we various measure to quantitatively evaluate a cluster clustering result and some of the commonly use measure include purity , which measure whether a cluster have similar object from the same cluster in the gold standard and normalize mutual information be a commonly use measure which basically measure base on the identity or the cluster of object in the system - generate result . how well can you predict the cluster of the object in the gold standard or vice versa .
da74c929-efc1-4b65-9635-684c7ebcab3f	2020-12-05 02:18:16.259964	104	00:04:01.9	00:04:19.34	Mutual information captures the correlation between these cluster labels and normalized mutual information is often used for quantifying the similarity for this evaluation purpose. F measure is another possible measure.	mutual information capture the correlation between these cluster label and normalize mutual information be often use for quantify the similarity for this evaluation purpose . F measure be another possible measure .
da74c929-efc1-4b65-9635-684c7ebcab3f	2020-11-03 02:04:37.721366	108	00:04:21.18	00:04:28.85	Now again a thorough discussion of the evaluation, of these evaluations, would be beyond the scope of this course.	now again a thorough discussion of the evaluation , of these evaluation , would be beyond the scope of this course .
da74c929-efc1-4b65-9635-684c7ebcab3f	2020-11-03 02:04:37.721367	111	00:04:29.48	00:04:35.96	I've suggested some reading in the end that you can take a look to know more about that.	I 've suggest some reading in the end that you can take a look to know more about that .
da74c929-efc1-4b65-9635-684c7ebcab3f	2020-11-03 02:04:37.721372	124	00:04:36.84	00:05:05.7	So here I just want to discuss some high level ideas that would allow you to think about how to do evaluation in your applications. The 2nd way to evaluate text clusters is to do indirect evaluation. So in this case the question to answer is how useful are the clustering results for the intended applications? Now this of course is application specific question, so usefulness is is going to depend on specific applications.	so here I just want to discuss some high level idea that would allow you to think about how to do evaluation in your application . the 2nd way to evaluate text cluster be to do indirect evaluation . so in this case the question to answer be how useful be the clustering result for the intend application ? now this of course be application specific question , so usefulness be be go to depend on specific application .
da74c929-efc1-4b65-9635-684c7ebcab3f	2020-12-05 02:20:43.608981	130	00:05:07.03	00:05:17.92	In this case, the clustering bias imposed by the intended application as well. So what counts as the best clustering result would be dependent on the application.	in this case , the cluster bias impose by the intend application as well . so what count as the good clustering result would be dependent on the application .
da74c929-efc1-4b65-9635-684c7ebcab3f	2020-11-03 02:04:37.721376	134	00:05:18.92	00:05:30.12	Procedure wise we also would create the test set with text objects for the intended application to quantify the performance of the system.	procedure wise we also would create the test set with text object for the intended application to quantify the performance of the system .
da74c929-efc1-4b65-9635-684c7ebcab3f	2020-12-05 02:21:19.808318	139	00:05:31.5	00:05:43.71	In this case what we care about is the contribution of clustering to some application. So we often have a baseline system to compare with.	in this case what we care about be the contribution of clustering to some application . so we often have a baseline system to compare with .
da74c929-efc1-4b65-9635-684c7ebcab3f	2020-11-03 02:04:37.721382	151	00:05:44.82	00:06:10.24	This could be the current system for doing something and then you hope to add clustering to improve it or the baseline system could be using a different clustering method and you then what you are trying to experiment with and you hope to have a better idea for clustering. So in case you have a baseline system to work with and then you can add a clustering algorithm to the baseline system to produce a clustering system.	this could be the current system for do something and then you hope to add clustering to improve it or the baseline system could be use a different clustering method and you then what you be try to experiment with and you hope to have a well idea for clustering . so in case you have a baseline system to work with and then you can add a clustering algorithm to the baseline system to produce a cluster system .
da74c929-efc1-4b65-9635-684c7ebcab3f	2020-11-03 02:04:37.721386	162	00:06:11.6	00:06:35.61	And then we're going to compare the performance of your clustering system and the baseline system in terms of the performance measure for that particular application. So in this case we call it indirect evaluation of clusters because there's no explicit assessment of the quality of clusters, but rather its to assess the contribution of clusters to a particular application.	and then we be go to compare the performance of your clustering system and the baseline system in term of the performance measure for that particular application . so in this case we call it indirect evaluation of cluster because there be no explicit assessment of the quality of cluster , but rather its to assess the contribution of cluster to a particular application .
da74c929-efc1-4b65-9635-684c7ebcab3f	2020-11-03 02:04:37.721389	171	00:06:37.24	00:07:00.08	So to summarize text clustering, it's a very useful unsupervised general text mining technique as particularly useful for obtaining an overall picture of the text content. This is often needed to explore text data. And this is often the first step when you deal with a lot of text data.	so to summarize text clustering , it be a very useful unsupervised general text mining technique as particularly useful for obtain an overall picture of the text content . this be often need to explore text datum . and this be often the first step when you deal with a lot of text datum .
da74c929-efc1-4b65-9635-684c7ebcab3f	2020-11-03 02:04:37.721391	176	00:07:01.65	00:07:11.82	The second application or second kind of application is to discover interesting clustering structures in text data, and these structures can be very meaningful.	the second application or second kind of application be to discover interesting clustering structure in text datum , and these structure can be very meaningful .
da74c929-efc1-4b65-9635-684c7ebcab3f	2020-12-05 02:24:35.855972	179	00:07:13.25	00:07:19.85	There are many approaches that can be used for text clustering and we discussed them:	there be many approach that can be use for text clustering and we discuss they :
da74c929-efc1-4b65-9635-684c7ebcab3f	2020-11-03 02:04:37.721393	181	00:07:20.71	00:07:24.739999	Model based approaches and similarity based approaches.	Model base approach and similarity base approach .
da74c929-efc1-4b65-9635-684c7ebcab3f	2020-12-05 02:26:00.627643	200	00:07:25.47	00:08:03.96	In general, strong clusters tend to show up no matter what method is used. Also the effectiveness of a method highly depends on whether the desired clustering bias is captured appropriately, and this can be done either through using the right generative model, the model design, appropriate for clustering, or the right similarity function to explicitly define bias. Deciding the optimal number of clusters is very difficult problem for all the classroom methods, and that's because it's unsupervised algorithm and there's no training data to guide us to select the best number of clusters	in general , strong cluster tend to show up no matter what method be use . also the effectiveness of a method highly depend on whether the desire clustering bias be capture appropriately , and this can be do either through use the right generative model , the model design , appropriate for clustering , or the right similarity function to explicitly define bias . decide the optimal number of cluster be very difficult problem for all the classroom method , and that be because it be unsupervised algorithm and there be no training datum to guide we to select the good number of cluster
da74c929-efc1-4b65-9635-684c7ebcab3f	2020-12-05 02:26:15.751431	203	00:08:04.98	00:08:09.57	Now sometimes you may see some methods that can automatically determine the number of clusters.	now sometimes you may see some method that can automatically determine the number of cluster .
da74c929-efc1-4b65-9635-684c7ebcab3f	2020-11-03 02:04:37.721403	209	00:08:10.2	00:08:26.5	But in general, that has some implied application of clustering bias there, and that's just not specified. Without clearly defining a clustering bias, it's just impossible to say the optimal number of cluster is what?	but in general , that have some imply application of cluster bias there , and that be just not specify . without clearly define a cluster bias , it be just impossible to say the optimal number of cluster be what ?
da74c929-efc1-4b65-9635-684c7ebcab3f	2020-11-03 02:04:37.721415	241	00:08:27.11	00:09:39.16	So this is important to keep in mind. And I should also say sometimes we can use application to determine the number of clusters. For example, if you are clustering search results, then obviously you don't want to generate 100 clusters, right? So the number can be dictated by the interface design. In other situations, we might be able to use the fitness of data to assess whether we've got a good number of clusters to explain our data well and to do that, you can vary the number of clusters and watch how well you can fit the data. If it's in general, when you add more components to mixture model, you should fit the data better, because you can always set the probability of using the new component at 0, so you can't in general fit the data worse than before, but as the question is, as you add more components would you be able to significantly improve the fitness of the data and that can be used to determine the right number of clusters. And finally, evaluation of clustering results and can be done both directly and indirectly.	so this be important to keep in mind . and I should also say sometimes we can use application to determine the number of cluster . for example , if you be cluster search result , then obviously you do n't want to generate 100 cluster , right ? so the number can be dictate by the interface design . in other situation , we might be able to use the fitness of datum to assess whether we 've get a good number of cluster to explain our datum well and to do that , you can vary the number of cluster and watch how well you can fit the datum . if it be in general , when you add more component to mixture model , you should fit the datum well , because you can always set the probability of use the new component at 0 , so you ca n't in general fit the datum bad than before , but as the question be , as you add more component would you be able to significantly improve the fitness of the datum and that can be use to determine the right number of cluster . and finally , evaluation of clustering result and can be do both directly and indirectly .
da74c929-efc1-4b65-9635-684c7ebcab3f	2020-11-03 02:04:37.721416	244	00:09:40.29	00:09:45.93	And we also would like to do both in order to get good sense about how our method works.	and we also would like to do both in order to get good sense about how our method work .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-02 23:12:07.39061	2	00:00:00.29	00:00:03.25	This lecture is about the text representation.	this lecture be about the text representation .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-24 16:02:49.86432	4	00:00:12.98	00:00:16.25	In this lecture we're going to discuss text representation.	in this lecture we be go to discuss text representation .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-02 23:12:07.390612	7	00:00:17.24	00:00:23.89	And discuss how natural language processing can allow us to represent text in many different ways.	and discuss how natural language processing can allow we to represent text in many different way .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-02 23:12:07.390612	9	00:00:25.05	00:00:27.73	Let's take a look at this example sentence again.	let 's take a look at this example sentence again .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-02 23:12:07.390613	11	00:00:29.55	00:00:33.1	We can represent this sentence in many different ways.	we can represent this sentence in many different way .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-02 23:12:07.390613	12	00:00:34.38	00:00:35.24	1st.	1st .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-02 23:12:07.390614	14	00:00:36.4	00:00:41.38	We can always represent such a sentence as a string of characters.	we can always represent such a sentence as a string of character .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-02 23:12:07.390615	16	00:00:42.58	00:00:48.28	This is true for all the languages when we store them in the computer.	this be true for all the language when we store they in the computer .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-02 23:12:07.390623	22	00:00:50.3	00:01:04.94	When we store a natural language sentence as a string of characters, we have perhaps the most general way of representing text, since we can always use this approach to represent any text data.	when we store a natural language sentence as a string of character , we have perhaps the most general way of represent text , since we can always use this approach to represent any text datum .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-02 23:12:07.390624	27	00:01:05.91	00:01:17.19	But unfortunately, using such a representation would not help us do semantic analysis, which is often needed for many applications of text mining.	but unfortunately , use such a representation would not help we do semantic analysis , which be often need for many application of text mining .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-02 23:12:07.390628	36	00:01:18.07	00:01:42.54	The reason is because we're not even recognizing words. So as a string we're going to keep all the spaces and these ASCII symbols. We can perhaps count how... what's the most frequent character in English text, or the correlation between those characters, but we can't really analyze semantics.	the reason be because we be not even recognize word . so as a string we be go to keep all the space and these ASCII symbol . we can perhaps count how ... what be the most frequent character in english text , or the correlation between those character , but we ca n't really analyze semantic .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-02 23:12:07.390629	40	00:01:43.53	00:01:52.45	Yet this is the most general way of representing text, because we can use this to represent any natural language text.	yet this be the most general way of represent text , because we can use this to represent any natural language text .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-02 23:12:07.39063	43	00:01:53.63	00:02:00.11	If we try to do a little bit more natural language processing by doing word segmentation.	if we try to do a little bit more natural language processing by do word segmentation .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-02 23:12:07.390631	46	00:02:01.02	00:02:07.77	Then we can obtain a representation of the same text, but in the form of a sequence of words.	then we can obtain a representation of the same text , but in the form of a sequence of word .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-24 16:05:56.075175	48	00:02:08.8	00:02:16.19	So here we see that we can identify words like: a dog is chasing etc.	so here we see that we can identify word like : a dog be chase etc .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-02 23:12:07.390635	58	00:02:18.21	00:02:44.72	Now with this level of representation, we certainly can do a lot of things, and this is mainly because words are the basic units of human communication in natural language, so they are very powerful. By identifying words we can, for example, easily count what are the most frequent words in this document or in the whole collection, etc.	now with this level of representation , we certainly can do a lot of thing , and this be mainly because word be the basic unit of human communication in natural language , so they be very powerful . by identify word we can , for example , easily count what be the most frequent word in this document or in the whole collection , etc .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-12-06 14:52:51.4406	60	00:02:45.63	00:02:48.97	And these words can be used to form topics.	and these word can be use to form topic .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-02 23:12:07.390637	64	00:02:49.61	00:02:57.36	When we combine related words together "and some words are positive, some words negative, so we can also do sentiment analysis.	when we combine related word together " and some word be positive , some word negative , so we can also do sentiment analysis .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-02 23:12:07.390639	67	00:02:59.65	00:03:06.01	So representing text data as a sequence of words opens up a lot of interesting analysis possibilities.	so represent text datum as a sequence of word open up a lot of interesting analysis possibility .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-02 23:12:07.390642	76	00:03:07.51	00:03:30.95	However, this level of representation is slightly less general than string of characters, because in some languages such as Chinese, it's actually not that easy to identify all the word boundaries, because in such a language you see text as a sequence of characters with no space in between.	however , this level of representation be slightly less general than string of character , because in some language such as Chinese , it be actually not that easy to identify all the word boundary , because in such a language you see text as a sequence of character with no space in between .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-02 23:12:07.390642	78	00:03:31.83	00:03:35.91	So you have to rely on some special techniques to identify words.	so you have to rely on some special technique to identify word .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-02 23:12:07.390644	84	00:03:37.85	00:03:49.89	In such a language, of course, then we might make mistakes in segmenting words. So the sequence of words representation is not as robust as string of characters.	in such a language , of course , then we might make mistake in segment word . so the sequence of word representation be not as robust as string of character .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-02 23:12:07.390645	87	00:03:50.57	00:03:58.37	But in English it's very easy to obtain this level of representation, so we can do that all the time.	but in English it be very easy to obtain this level of representation , so we can do that all the time .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-02 23:12:07.390646	90	00:04:01.85	00:04:07.41	Now if we go further to do natural language processing, we can add a part of speech tags.	now if we go far to do natural language processing , we can add a part of speech tag .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-02 23:12:07.390654	111	00:04:08.82	00:04:58.14	Now, once we do that, we can count for example, the most frequent nouns or what kind of nouns are associated with what kind of verbs, etc. So this opens up a little bit more interesting opportunities for further analysis. Note that I use the plus sign here, because by representing text as a sequence of part of speech tags. We don't necessarily replace the original word sequence recommendation. Instead, we add this as an additional way of representing text data, so that now the data is represented as both a sequence of words, and a sequence of part of speech tags. This enriches the representation of text data and thus, also, enables a more interesting analysis.	now , once we do that , we can count for example , the most frequent noun or what kind of noun be associate with what kind of verb , etc . so this open up a little bit more interesting opportunity for further analysis . note that I use the plus sign here , because by represent text as a sequence of part of speech tag . we do n't necessarily replace the original word sequence recommendation . instead , we add this as an additional way of represent text datum , so that now the datum be represent as both a sequence of word , and a sequence of part of speech tag . this enrich the representation of text datum and thus , also ,  enable a more interesting analysis .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-02 23:12:07.390655	114	00:05:01.32	00:05:07.5	If we go further then we'll be parsing the sentence to obtain a syntactic structure.	if we go far then we 'll be parse the sentence to obtain a syntactic structure .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-24 16:13:38.05885	118	00:05:08.64	00:05:21.86	Now this of course further open up more interesting analysis of, for example, the writing styles, or correcting grammar mistakes.	now this of course far open up more interesting analysis of , for example , the writing style , or correct grammar mistake .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-02 23:12:07.390658	123	00:05:23.2	00:05:37.57	If we could go further for semantic analysis, then we might be able to recognize dog as animal and we also can recognize boy as a person and playground as a location.	if we could go far for semantic analysis , then we might be able to recognize dog as animal and we also can recognize boy as a person and playground as a location .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-12-06 14:57:41.361929	130	00:05:38.62	00:05:52.53	And we can further analyze their relations, for example, dog is chasing the boy and the boy is on the playground. Now this is to add more entities and relations through entity-relation recognition.	and we can far analyze their relation , for example , dog be chase the boy and the boy be on the playground . now this be to add more entity and relation through entity - relation recognition .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-24 16:16:50.177399	144	00:05:53.46	00:06:31.15	At this level, then we can do even more interesting things. For example, now we can count easily the most frequent person that's mentioned in this whole collection of news articles, or whenever you mention this person, you also tend to see mention of another person, etc. So this is very useful representation an it's also related to the Knowledge Graph that some of you may have heard of. That Google is doing as a more semantic way of representing text data.	at this level , then we can do even more interesting thing . for example , now we can count easily the most frequent person that be mention in this whole collection of news article , or whenever you mention this person , you also tend to see mention of another person , etc . so this be very useful representation an it be also related to the Knowledge Graph that some of you may have hear of . that Google be do as a more semantic way of represent text datum .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-02 23:12:07.390672	154	00:06:32.26	00:06:57.2	However, it's also less robust than sequence of words or even syntactic analysis, because it's not always easy to identify all the entities with the right types, and we might make mistakes, and relations are even harder to find and we might make mistakes. So this makes this level of representation less robust, yet it's very useful.	however , it be also less robust than sequence of word or even syntactic analysis , because it be not always easy to identify all the entity with the right type , and we might make mistake , and relation be even hard to find and we might make mistake . so this make this level of representation less robust , yet it be very useful .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-02 23:12:07.390674	159	00:06:58.93	00:07:28.38	Now if we move further to logical representation then we can have predicates and even inference rules. And with inference rules we can infer interesting, derived facts from the text. So that's very useful, but unfortunately at this level of representation it's even less robust and we can make mistakes, and we can't do that all the time for all kinds of sentences.	now if we move far to logical representation then we can have predicate and even inference rule . and with inference rule we can infer interesting , derive fact from the text . so that be very useful , but unfortunately at this level of representation it be even less robust and we can make mistake , and we ca n't do that all the time for all kind of sentence .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-12-06 15:01:08.196579	166	00:07:29.2	00:07:49.52	And finally, speech acts with added yet another level of representation of the intent of saying this sentence. So in this case it might be a request. So knowing that would allow us to analyze more, even more interesting things about this observer order.	and finally , speech act with add yet another level of representation of the intent of say this sentence . so in this case it might be a request . so know that would allow we to analyze more , even more interesting thing about this observer order .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-24 16:23:06.690315	171	00:07:50.07	00:07:58.79	Author of this sentence, what's the intention of saying that? What scenarios, what kind of actions will be made? So this is...	author of this sentence , what be the intention of say that ? what scenario , what kind of action will be make ? so this be ...
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-02 23:12:07.39068	177	00:08:01.98	00:08:14.94	Another level of analysis that would be very interesting. So this picture shows that if we move down, we generally see more sophisticated natural language processing techniques to be used.	another level of analysis that would be very interesting . so this picture show that if we move down , we generally see more sophisticated natural language processing technique to be use .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-02 23:12:07.390681	179	00:08:15.82	00:08:19.9	And unfortunately, such techniques would require more human effort.	and unfortunately , such technique would require more human effort .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-02 23:12:07.390681	180	00:08:20.8	00:08:22.71	And they are less accurate.	and they be less accurate .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-02 23:12:07.390683	185	00:08:23.45	00:08:37.37	That means there are mistakes. So if we analyze text data at the levels that are represented, deeper analysis of language, then we have to tolerate the errors.	that mean there be mistake . so if we analyze text datum at the level that be represent , deep analysis of language , then we have to tolerate the error .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-24 16:25:21.36008	194	00:08:38.19	00:09:07.61	So that also means it's still necessary to combine such deep analysis with shallow analysis based on, for example sequence of words. On the right side you see the arrow points down, to indicate that as we go down without representation of text, it's closer to knowledge representation in our mind, and need for solving a lot of problems.	so that also mean it be still necessary to combine such deep analysis with shallow analysis base on , for example sequence of word . on the right side you see the arrow point down , to indicate that as we go down without representation of text , it be close to knowledge representation in our mind , and need for solve a lot of problem .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-02 23:12:07.390693	214	00:09:08.44	00:09:57.7	Now, this is desirable because as we can represent text at the level of knowledge, we can easily extract the knowledge. That's the purpose of text mining. So there is a trade off here between doing deeper analysis that might have errors, but would give us direct knowledge that can be extracted from text and doing shallow analysis, which is more robust. But wouldn't they actually give us the necessary deeper representation of knowledge. I should also say that text data are generated by humans An are meant to be consumed by humans so as a result in a text data analysis text mining, humans play a very important role. They are always in the loop.	now , this be desirable because as we can represent text at the level of knowledge , we can easily extract the knowledge . that be the purpose of text mining . so there be a trade off here between do deep analysis that might have error , but would give we direct knowledge that can be extract from text and do shallow analysis , which be more robust . but would n't they actually give we the necessary deep representation of knowledge . I should also say that text datum be generate by human an be mean to be consume by human so as a result in a text datum analysis text mining , human play a very important role . they be always in the loop .
db1d54dd-bb05-46c0-995b-5f7d5243e3c4	2020-11-02 23:12:07.390694	216	00:09:58.36	00:10:03.26	Meaning that we should optimize the collaboration of humans and computers.	mean that we should optimize the collaboration of human and computer .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-12-08 15:04:19.080406	3	00:00:00.3	00:00:05.11	This lecture is a continued discussion of generative probabilistic models for text clustering.	this lecture be a continue discussion of generative probabilistic model for text clustering .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.613623	7	00:00:14.09	00:00:19.79	In this lecture we're going to finish the discussion of generative probabilistic models for text clustering.	in this lecture we be go to finish the discussion of generative probabilistic model for text clustering .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.613628	18	00:00:21.46	00:00:45.48	So this is a slide that you have seen before and here we show how we define the mixture model for text clustering an what the likelihood function looks like and we can also compute the maximum liklihood estimate to estimate the parameters. In this lecture, we're going to talk more about how exactly we're going to compute the maximum likelihood estimator.	so this be a slide that you have see before and here we show how we define the mixture model for text cluster an what the likelihood function look like and we can also compute the maximum liklihood estimate to estimate the parameter . in this lecture , we be go to talk more about how exactly we be go to compute the maximum likelihood estimator .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-12-08 15:01:41.612513	33	00:00:46.34	00:01:27.37	Now, as in most cases, the EM algorithm can be used to solve this problem for mixture models. So here's the detail of this EM algorithm for document clustering. Now, if you have understood how EML works for topic models, PSA and I think here it will be very similar and you just need to adapt a little bit to this new mixture model. So as you may recall, EM algorithm starts with initialization of all the parameters. So this is the same as what happened before for topic models.	now , as in most case , the EM algorithm can be use to solve this problem for mixture model . so here be the detail of this EM algorithm for document clustering . now , if you have understand how EML work for topic model , PSA and I think here it will be very similar and you just need to adapt a little bit to  this new mixture model . so as you may recall , EM algorithm start with initialization of all the parameter . so this be the same as what happen before for topic model .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.613639	45	00:01:28.35	00:01:56.98	And then we're going to repeat until they're likely converges. An in each step will do E step and M step in M step. we're going to infer which distribution has been used to generate each document. And so we have to introduce a hidden variable ZD for each document and this value variable could take a value from the range of one through K representing K different distributions.	and then we be go to repeat until they be likely converge . an in each step will do e step and m step in M step . we be go to infer which distribution have be use to generate each document . and so we have to introduce a hidden variable zd for each document and this value variable could take a value from the range of one through K represent k different distribution .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.613641	51	00:01:59.34	00:02:14.53	And more specifically, basically we're going to apply Bayes rule to infer, or which distribution is more likely to have generated this document or computing the posterior probability of the distribution.	and more specifically , basically we be go to apply Bayes rule to infer , or which distribution be more likely to have generate this document or compute the posterior probability of the distribution .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.613641	52	00:02:15.32	00:02:16.51	Given the document.	give the document .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.613648	71	00:02:17.29	00:03:11.63	An we know it's proportional to the probability of selecting this distribution P of Theta I and the probability of generating this whole document from that distribution, which is a product of all the probabilities of words for this document, as you see here. Now, as in all cases, it's useful to kind of remember the normalizer or the OR the constraint on this probability. So in this case we know the constraint on this probability in the E step is that all the probabilities of Z equals I must sum to one 'cause the document must have been generated from precise or one of these K topics. So the probability of the generator from each of them should sum to one.	an we know it be proportional to the probability of select this distribution p of Theta I and the probability of generate this whole document from that distribution , which be a product of all the probability of word for this document , as you see here . now , as in all case , it be useful to kind of remember the normalizer or the OR the constraint on this probability . so in this case we know the constraint on this probability in the e step be that all the probability of z equal I must sum to one 'cause the document must have be generate from precise or one of these K topic . so the probability of the generator from each of they should sum to one .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.613648	74	00:03:12.13	00:03:20.4	And if this constraint then you can easy to compute this distribution as long as.	and if this constraint then you can easy to compute this distribution as long as .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.613652	83	00:03:21.88	00:03:44.18	what it is proportional to, right? So once you compute this product that you see here, then you simply normalize this these probabilities to make them some to 1 what over all the topics. So that's E step after E Step we would know which distribution is more likely to have generated this document D, which is unlikely.	what it be proportional to , right ? so once you compute this product that you see here , then you simply normalize this these probability to make they some to 1 what over all the topic . so that be e step after E Step we would know which distribution be more likely to have generate this document d , which be unlikely .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.613655	94	00:03:45.18	00:04:07.59	And then in the M step, we're going to relist, made all the parameters based on the, infer the Z values, or in further knowledge about which district has been used to generate which document. So there estimation involves two kinds of parameters. One is P of Theta and this is the probability of selecting a particular distribution.	and then in the M step , we be go to relist , make all the parameter base on the , infer the z value , or in further knowledge about which district have be use to generate which document . so there estimation involve two kind of parameter . one be p of Theta and this be the probability of select a particular distribution .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.613657	99	00:04:08.32	00:04:18.57	Before we observe anything, we don't have any knowledge about which cluster is more likely, but after we have observed these documents, then we can collect the evidence.	before we observe anything , we do n't have any knowledge about which cluster be more likely , but after we have observe these document , then we can collect the evidence .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.613658	103	00:04:20.48	00:04:33.08	To infer which cluster is more likely, and so this is proportional to the sum of the probability of Z sub DJ is equal to I.	to infer which cluster be more likely , and so this be proportional to the sum of the probability of z sub DJ be equal to I.
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.61366	108	00:04:34.57	00:04:48.67	And so this gives us all the evidence about using topic. I said I to generate a document and we put them together and again we normalize them into probabilities.	and so this give we all the evidence about use topic . I say I to generate a document and we put they together and again we normalize they into probability .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-12-08 15:14:37.920518	109	00:04:49.89	00:04:53.59	And then so this is for P of Theta sub I.	and then so this be for p of Theta sub I.
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-12-08 15:14:52.245328	113	00:04:54.44	00:05:04.99	Now the other kind of parameters are the probabilities of words in each distribution, each cluster, and this is very similar to the case of PLSA.	now the other kind of parameter be the probability of word in each distribution , each cluster , and this be very similar to the case of PLSA .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-12-08 15:16:14.40375	120	00:05:05.66	00:05:27.91	And here we just pulled the counts of words that are in documents that are inverted to have been generated from a particular topic Theta I here and this would allow us to then estimate how many words have actually been generated from Theta I.	and here we just pull the count of word that be in document that be invert to have be generate from a particular topic Theta I here  and this would allow we to then estimate how many word have actually be generate from Theta I.
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.613668	129	00:05:28.78	00:05:53.61	And then we normalize again. These counts into probabilities so that the probabilities on all the words some to one. Note that it's very important to understand these constraints as they are precisely the normalizers in all these formulas, and it's also important to know that distribution is over what?	and then we normalize again . these count into probability so that the probability on all the word some to one . note that it be very important to understand these constraint as they be precisely the normalizer in all these formula , and it be also important to know that distribution be over what ?
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.61367	137	00:05:54.37	00:06:11.65	For example, the probability of Theta is overall the key topics and that's why these K probabilities sum to 1. well, whereas the probability of a word given Theta is a probability distribution over all the words. So there are many probabilities and they have to send one.	for example , the probability of Theta be overall the key topic and that be why these K probability sum to 1 . well , whereas the probability of a word give Theta be a probability distribution over all the word . so there be many probability and they have to send one .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.613673	143	00:06:12.62	00:06:22.64	So now let's take a look like this. Take a look at the simple example of two clusters. I have two clusters. I've shown some initializer values for the two distributions.	so now let 's take a look like this . take a look at the simple example of two cluster . I have two cluster . I 've show some initializer value for the two distribution .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.613675	147	00:06:23.32	00:06:32.03	And let's assume we randomly initialized to probabilities of selecting each cluster as .5. So equally likely.	and let 's assume we randomly initialize to probability of select each cluster as .5 . so equally likely .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.613677	153	00:06:33	00:06:43.01	And then let's consider one document that you have seen here. There are two words, sorry, two occurrences of text and two occurrences of mining. So there are four words together.	and then let 's consider one document that you have see here . there be two word , sorry , two occurrence of text and two occurrence of mining . so there be four word together .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.613678	156	00:06:44.01	00:06:49.56	Medical and health did not occur in this document, so this first thing about the hidden variables.	medical and health do not occur in this document , so this first thing about the hide variable .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.61368	159	00:06:50.24	00:06:59.5	Now for each document we must use a hidden variable and before in PLSA we used 1 hidden variable for each word.	now for each document we must use a hidden variable and before in PLSA we use 1 hidden variable for each word .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.613684	172	00:07:00.35	00:07:24.27	Because that's the output from what mixture model. So in our case the output from a mixture model or the observation from mixture model is a document not a word. So now we have 1 hidden variable attached to the document. That hidden variable must tell us which distribution has been used to generate the document, so it's going to take two values, one and two to indicate the two topics.	because that be the output from what mixture model . so in our case the output from a mixture model or the observation from mixture model be a document not a word . so now we have 1 hide variable attach to the document . that hidden variable must tell we which distribution have be use to generate the document , so it be go to take two value , one and two to indicate the two topic .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-12-08 15:20:36.881328	191	00:07:25.23	00:08:09.87	So now how do we infer which distribution has been used to generate the D? It's to use Bayes rule so it looks like this in order for the first topic is setup, want to generate the document. Two things must happen. First theater subway must have been selected, so it's given by P of 01 second. It must have also been generating the four words in the document, namely two occurrences of text and two occurrences of mining. That's why you see the numerator has the product of the probability of selecting Theta one and the probability of generating the document from Theta 1.	so now how do we infer which distribution have be use to generate the d ? it be to use Bayes rule so it look like this in order for the first topic be setup , want to generate the document . two thing must happen . first theater subway must have be select , so it be give by p of 01 second . it must have also be generate the four word in the document , namely two occurrence of text and two occurrence of mining . that be why you see the numerator have the product of the probability of select Theta one and the probability of generate the document from Theta 1 .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.613694	204	00:08:10.28	00:08:42.72	So the denominator is just the sum of two possibilities of generating this document, and you can plug in the numerical values to verify. Indeed in this case the document is more likely to be generated from Theta 1, much more likely than from than Theta 2. So once we have this problem that we can easily compute the probability of Z = 2 given this document, how we're going to use the constraint? Right now it's going to be 1 - 100 / 1,000,000 one.	so the denominator be just the sum of two possibility of generate this document , and you can plug in the numerical value to verify . indeed in this case the document be more likely to be generate from Theta 1 ,   much more likely than from than Theta 2 . so once we have this problem that we can easily compute the probability of z = 2 give this document , how we be go to use the constraint ? right now it be go to be 1 - 100 / 1,000,000 one .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.613696	215	00:08:43.45	00:09:08.64	So now it's important to note that in such a computation there is a potential problem of underflow, and that is because if you look at the numerator, the original numerator and denominator it involves the computation of a product of many small probabilities. Imagine if a document has many words and it's going to be a very small value here, as it can cause the problem of underflow.	so now it be important to note that in such a computation there be a potential problem of underflow , and that be because if you look at the numerator , the original numerator and denominator it involve the computation of a product of many small probability . imagine if a document have many word and it be go to be a very small value here , as it can cause the problem of underflow .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.613697	216	00:09:09.28	00:09:10.91	So to solve the problem.	so to solve the problem .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.613697	217	00:09:11.75	00:09:13.62	We can use a normalized.	we can use a normalize .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.613698	221	00:09:14.43	00:09:23.62	So here you see that we take average of all these two solutions to compute another average district called Theater Bar.	so here you see that we take average of all these two solution to compute another average district call Theater Bar .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.613699	224	00:09:24.41	00:09:29.97	And this does the average distribution will be comperable to each of these distributions.	and this do the average distribution will be comperable to each of these distribution .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.613699	226	00:09:30.59	00:09:32.93	In terms of the quantities, the magnitude.	in term of the quantity , the magnitude .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.613701	233	00:09:33.55	00:09:51.72	So we can then divide the numerator and the denominator both by this normalizer. So basically this normalizes the probability of generating this document by using this average word distribution.	so we can then divide the numerator and the denominator both by this normalizer . so basically this normalize the probability of generate this document by use this average word distribution .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.613702	234	00:09:52.88	00:09:55.18	So you can see the normalizer here.	so you can see the normalizer here .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.613703	238	00:09:56.44	00:10:05.64	And since we have used exact the same normalizer for the numerator and denominator, the whole value of this expression is not changed.	and since we have use exact the same normalizer for the numerator and denominator , the whole value of this expression be not change .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.613704	244	00:10:06.21	00:10:22.87	But by doing this normalization you can see we can make the numerators and denominators more manageable in that the overall value is not going to be very small for each, and thus we can avoid underflow problem.	but by do this normalization you can see we can make the numerator and denominator more manageable in that the overall value be not go to be very small for each , and thus we can avoid underflow problem .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.613707	256	00:10:24.46	00:10:54.04	In some other times we sometimes also use logarithm of the product to convert this into a sum of log of probabilities. This can help preserve precision as well, but in this case we cannot use logarithms to solve the problem because there's sum in the denominator, But this kind of normalizes can be effective for solving this problem, so it's a technique that's sometimes useful in other situations as well.	in some other time we sometimes also use logarithm of the product to convert this into a sum of log of probability . this can help preserve precision as well , but in this case we can not use logarithm to solve the problem because there be sum in the denominator , but this kind of normalize can be effective for solve this problem , so it be a technique that be sometimes useful in other situation as well .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-12-08 15:26:40.401532	270	00:10:55.14	00:11:21.54	Now let's look at the M step. So from the E step we can see our estimate of which distribution is more likely to have generated a document, and you can see D1 is more likely from the first topic. Where is D2 is more like from the second topic, etc. Now let's think about what we need to compute in the M step. Basically we need to re estimate all the parameters. Let's first look at the P of Theta 1 and P of Theta 2.	now let 's look at the M step . so from the e step we can see our estimate of which distribution be more likely to have generate a document , and you can see D1 be more likely from the first topic . where be D2 be more like from the second topic , etc . now let 's think about what we need to compute in the M step . basically we need to re estimate all the parameter . let 's first look at the p of Theta 1 and P of Theta 2 .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-12-08 15:28:13.619632	283	00:11:22.35	00:11:53.22	How do we estimate that? Intuitively, you can just pull together the Z probability Z probabilities from E Steps, right? So if all these documents say they're more likely from silouan, then we intuitively would give a high probability to see that one right? So in this case, so we can just take the average of these probabilities that you see here, and we obtain the .6 for Theta 1 so Theta 1 is more likely Theta 2.	how do we estimate that ? intuitively , you can just pull together the z probability z probability from E Steps , right ? so if all these document say they be more likely from silouan , then we intuitively would give a high probability to see that one right ? so in this case , so we can just take the average of these probability that you see here , and we obtain the .6 for theta 1 so Theta 1 be more likely theta 2 .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.613713	285	00:11:54.03	00:11:58.8	So you can see the probability of Theta 2 would be naturally .4.	so you can see the probability of theta 2 would be naturally .4 .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.613714	294	00:11:59.64	00:12:16.71	What about these world probabilities? What we do the same? And intuition is the same, so we're going to see in order to estimate the probabilities of words in Theta one, we're going to look at which documents have been generated from Scylla and we're going to pull together the words in those documents and normalize them.	what about these world probability ? what we do the same ? and intuition be the same , so we be go to see in order to estimate the probability of word in Theta one , we be go to look at which document have be generate from Scylla and we be go to pull together the word in those document and normalize they .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.613715	297	00:12:17.43	00:12:23.16	So this is basically what I just said. Most specifically, we're going to for example.	so this be basically what I just say . most specifically , we be go to for example .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.613722	331	00:12:24.57	00:13:36.66	Use all the counts of text in these documents to estimate the probability of tax given still awhile, but we're not to use their raw counts or total account. Instead, we can do that. Discount them by the probabilities that each document is likely be generated from Theta 1. So this gives us some fractional counts, and then these Council would be then normalized in order to get the probability. Now how do we normalize them? These probabilities of these words must sum to one. So to summarize, our discussion of generating models for clustering. We showed that a slight variation of Top Model can be used for clustering documents and this also shows the power of generating models in general by changing the generation assumption and changing the model slightly we can achieve different goals and we can capture different patterns in text data. So in this case, each class is represented by unigram language model or word distribution, and that's similar to topic model. So here you can see the word distribution actually generates a term cluster as a byproduct.	use all the count of text in these document to estimate the probability of tax give still awhile , but we be not to use their raw count or total account . instead , we can do that . discount they by the probability that each document be likely be generate from Theta 1 . so this give we some fractional count , and then these Council would be then normalize in order to get the probability . now how do we normalize they ? these probability of these word must sum to one . so to summarize , our discussion of generating model for clustering . we show that a slight variation of Top Model can be use for cluster document and this also show the power of generate model in general by change the generation assumption and change the model slightly we can achieve different goal and we can capture different pattern in text datum . so in this case , each class be represent by unigram language model or word distribution , and that be similar to topic model . so here you can see the word distribution actually generate a term cluster as a byproduct .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.613723	340	00:13:37.66	00:13:54.99	A document that is generated by first choosing a unigram language model and then generating all the words in the document that using this single language model and this is very different from again topic model where we can generate the words in the document by using multiple unigram language models.	a document that be generate by first choose a unigram language model and then generate all the word in the document that use this single language model and this be very different from again topic model where we can generate the word in the document by use multiple unigram language model .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.613724	345	00:13:56.6	00:14:06.34	And then the estimated model pamateter will give both a topic capitalization of each cluster and the probabilistic assignment of each document into a cluster.	and then the estimate model pamateter will give both a topic capitalization of each cluster and the probabilistic assignment of each document into a cluster .
dccc8a84-66da-47ce-ab88-28e8acf192b9	2020-11-02 23:09:28.613726	356	00:14:07.17	00:14:27.51	And this probabilistic assignment that sometimes is useful for some applications. But if we want to achieve a harder clusters mainly to partition documents into disjoint clusters. Then we can just force the document into the cluster corresponding to the water distribution. That's most likely to have generated the document.	and this probabilistic assignment that sometimes be useful for some application . but if we want to achieve a hard cluster mainly to partition document into disjoint cluster . then we can just force the document into the cluster correspond to the water distribution . that be most likely to have generate the document .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-02 23:10:55.652745	2	00:00:00.3	00:00:04.68	This lecture is a continued discussion of probabilistic topic models.	this lecture be a continue discussion of probabilistic topic model .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-02 23:10:55.652747	8	00:00:15.22	00:00:28.51	In this lecture, we're going to continue discussing probabilistic models, we are going to talk about a very simple case where we are interested in just mining one topic from one document.	in this lecture , we be go to continue discuss probabilistic model , we be go to talk about a very simple case where we be interested in just mine one topic from one document .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-02 23:10:55.652751	19	00:00:30.81	00:00:56.32	So in this simple setup we are interested in analyzing one document and trying to discover just one topic. So this is the simplest case of topic modeling. The input now no longer has K, which is the number of topics because we know there is only one topic. And the collection has only one document also.	so in this simple setup we be interested in analyze one document and try to discover just one topic . so this be the simple case of topic modeling . the input now no long have K , which be the number of topic because we know there be only one topic . and the collection have only one document also .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-02 23:10:55.652755	25	00:00:58.22	00:01:13.1	In the output we also no longer have coverage because we assumed that the document covers this topic 100%. So the main goal is just to discover the word probabilities for this single topic, as shown here.	in the output we also no long have coverage because we assume that the document cover this topic 100 % . so the main goal be just to discover the word probability for this single topic , as show here .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-02 23:10:55.652759	34	00:01:14.67	00:01:35.67	As always, when we think about using a generative model to solve such a problem, we'll start with thinking about what kind of data we're going to model or from what perspective we're going to model the data or data representation. "And then we're going to design a specific model for the generation of the data from our perspective.	as always , when we think about use a generative model to solve such a problem , we 'll start with think about what kind of datum we be go to model or from what perspective we be go to model the datum or datum representation . " and then we be go to design a specific model for the generation of the datum from our perspective .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-02 23:10:55.652771	64	00:01:36.44	00:02:52.99	Where our perspective just means we want to take a particular angle of looking at the data so that the model would have the right parameters for discovering the knowledge that we want, and then we'll be thinking about the likelihood function or write down the library function to capture more formally how likely a data point will be obtained from this model. And the likelihood function will have some parameters in the function and then we are usually interested in estimating those parameters, for example by maximizing the likelihood which would lead to maximum likelihood estimator and these estimated parameters would then become the output of the mining algorithm. Which means we'll take the estimated parameters as a knowledge that we discover from the text. So let's look at these steps for this very simple case. "Later, we'll look at this procedure for some more complicated cases. So our data in this case is just the document which is a sequence of words. Each word here is denoted by X sub I.	where our perspective just mean we want to take a particular angle of look at the datum so that the model would have the right parameter for discover the knowledge that we want , and then we 'll be think about the likelihood function or write down the library function to capture more formally how likely a data point will be obtain from this model . and the likelihood function will have some parameter in the function and then we be usually interested in estimate those parameter , for example by maximize the likelihood which would lead to maximum likelihood estimator and these estimate parameter would then become the output of the mining algorithm . which mean we 'll take the estimate parameter as a knowledge that we discover from the text . so let 's look at these step for this very simple case . " later , we 'll look at this procedure for some more complicated case . so our datum in this case be just the document which be a sequence of word . each word here be denote by X sub i.
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-02 23:10:55.652771	67	00:02:54.23	00:03:02.65	Our model is a unigram language model, a word distribution that we hope to denote a topic and that's our goal.	our model be a unigram language model , a word distribution that we hope to denote a topic and that be our goal .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-26 20:22:19.312132	73	00:03:03.35	00:03:18.83	So we will have as many parameters as many words in our vocabulary, in this case M. And for convenience we're going to use theta sub I to denote the probability of word W sub I.	so we will have as many parameter as many word in our vocabulary , in this case M. and for convenience we be go to use theta sub I to denote the probability of word W sub I.
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-02 23:10:55.652773	75	00:03:20.34	00:03:23.61	And obviously these thetas of i's would sum to one.	and obviously these theta of I be would sum to one .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-02 23:10:55.652776	85	00:03:24.37	00:03:41.319999	Now, what does the likelihood function look like? This is just the probability of generating this whole document given such a model. Because we assume the independence in generating each word, so the probability of the word the document would be just a product of the probability of each word.	now , what do the likelihood function look like ? this be just the probability of generate this whole document give such a model . because we assume the independence in generate each word , so the probability of the word the document would be just a product of the probability of each word .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-02 23:10:55.652777	88	00:03:42.68	00:03:51.28	And since some word might have repeated occurrences, so we can also rewrite this product in a different form.	and since some word might have repeat occurrence , so we can also rewrite this product in a different form .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-26 20:24:00.623979	91	00:03:52.54	00:04:00.41	So in this line we have rewritten the formula into a product over all the unique words in the vocabulary,	so in this line we have rewrite the formula into a product over all the unique word in the vocabulary ,
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-12-05 01:39:12.468189	92	00:04:01.63	00:04:04.55	W sub one through the W sub M.	w sub one through the W sub M.
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-02 23:10:55.65278	99	00:04:05.24	00:04:23.42	Now this is different from the previous line where the product is over different positions of words in the document. Now when we do this transformation, we then would need to introduce account function here.	now this be different from the previous line where the product be over different position of word in the document . now when we do this transformation , we then would need to introduce account function here .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-02 23:10:55.652781	101	00:04:24.06	00:04:27.85	This denotes the count of word one in document.	this denote the count of word one in document .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-02 23:10:55.652786	120	00:04:29.19	00:05:14.119999	And similarly, this is the count of words of M in the document. becausw. these words might have repeated occurrences. You can also see if a word did not occur in the document, it would have a zero count and therefore that corresponding term will disappear. So this is a very useful form of writing down the likelihood function that we will often use later. So I want you to pay attention to this. Just get familiar with this notation. It's just to change the product over all the different words in the vocabulary. So in the end, of course we'll use theta sub I to express this likelihood function and it would look like this.	and similarly , this be the count of word of M in the document . becausw . these word might have repeat occurrence . you can also see if a word do not occur in the document , it would have a zero count and therefore that corresponding term will disappear . so this be a very useful form of write down the likelihood function that we will often use later . so I want you to pay attention to this . just get familiar with this notation . it be just to change the product over all the different word in the vocabulary . so in the end , of course we 'll use theta sub I to express this likelihood function and it would look like this .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-02 23:10:55.652788	128	00:05:15.44	00:05:30.79	Next, we're going to find the theta values, or probabilities of these words that would maximize this likelihood function. So now let's take a look at the maximum likelihood estimate problem more closely.	next , we be go to find the theta value , or probability of these word that would maximize this likelihood function . so now let 's take a look at the maximum likelihood estimate problem more closely .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-02 23:10:55.652789	131	00:05:32.42	00:05:37.46	This line is copied from the previous slide. It's just our likelihood function.	this line be copy from the previous slide . it be just our likelihood function .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-12-05 01:43:37.180535	141	00:05:38.53	00:06:03.12	"So our goal is to maximize this likelihood function. We will find it often easy to maximize the log likelihood instead of the original likelihood and this is purely for mathematical convenience, because after the logarithm transformation, our function will become a sum instead of a product.	" so our goal be to maximize this likelihood function . we will find it often easy to maximize the log likelihood instead of the original likelihood and this be purely for mathematical convenience , because after the logarithm transformation , our function will become a sum instead of a product .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-26 20:29:05.705477	143	00:06:04.64	00:06:11.27	And we also have constraints over these probabilities.	and we also have constraint over these probability .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-02 23:10:55.652794	147	00:06:12.09	00:06:21.06	The sum makes it easier to take derivative, which is often needed for finding the optimal solution of this function.	the sum make it easy to take derivative , which be often need for find the optimal solution of this function .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-26 20:30:01.302611	151	00:06:22.56	00:06:37.52	So please take a look at this sum again here and this is a form of "function that you often see later also in more general topic models.	so please take a look at this sum again here and this be a form of " function that you often see later also in more general topic model .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-12-05 01:44:23.299242	154	00:06:38.38	00:06:48.18	So it's a sum over all the words in the vocabulary and inside the sum there is a count of words in the document.	so it be a sum over all the word in the vocabulary and inside the sum there be a count of word in the document .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-02 23:10:55.652801	167	00:06:49.01	00:07:17.28	And this is multiplied by the logarithm of the probability. So let's see how we can solve this problem. Now at this point the problem is purely a mathematical problem, because we're going to just to find the optimal solution of a constrained maximization problem. The objective function is the likelihood function, and the constraint is that all these probabilities must sum to one.	and this be multiply by the logarithm of the probability . so let 's see how we can solve this problem . now at this point the problem be purely a mathematical problem , because we be go to just to find the optimal solution of a constrained maximization problem . the objective function be the likelihood function , and the constraint be that all these probability must sum to one .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-12-05 01:46:36.012864	169	00:07:18.23	00:07:23.58	So one way to solve the problem is to use Lagrange multiplier approach.	so one way to solve the problem be to use Lagrange multipli approach .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-26 20:31:28.085362	175	00:07:24.38	00:07:38	Now this content is beyond the scope of this course. But since Lagrange multiplier is very useful approach, I also would like to just give a brief introduction to this for those of you who are interested.	now this content be beyond the scope of this course . but since Lagrange multiplier be very useful approach , I also would like to just give a brief introduction to this for those of you who be interested .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-12-05 01:46:53.258325	177	00:07:39.64	00:07:44.32	So in this approach we will construct a Lagrange function here.	so in this approach we will construct a Lagrange function here .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-02 23:10:55.652806	180	00:07:45.33	00:07:55.38	And this function would combine our objective function with another term that encodes our constraints.	and this function would combine our objective function with another term that encode our constraint .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-02 23:10:55.652806	182	00:07:56.6	00:08:02.04	And we introduce Lagrange multiplier here, Lambda.	and we introduce Lagrange multiplier here , Lambda .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-02 23:10:55.652807	183	00:08:03.39	00:08:04.97	So it's additional parameter.	so it be additional parameter .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-12-05 01:48:42.959033	189	00:08:05.61	00:08:18.65	Now the idea of this approach is to just turn the constrained optimization into, in some sense, unconstrained optimizing problem. So now we're just interested in optimizing this Lagrange function.	now the idea of this approach be to just turn the constrained optimization into , in some sense , unconstrained optimize problem . so now we be just interested in optimize this Lagrange function .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-02 23:10:55.65281	194	00:08:19.36	00:08:34.56	As you may recall from calculus, an optimal point would be achieved when the derivative is set to 0. This is a necessary condition. It's not sufficient though, so.	as you may recall from calculus , an optimal point would be achieve when the derivative be set to 0 . this be a necessary condition . it be not sufficient though , so .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-02 23:10:55.652811	197	00:08:35.28	00:08:42.61	If we do that, you will see the partial derivative with respect to theta i here is equal to this.	if we do that , you will see the partial derivative with respect to theta I here be equal to this .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-26 20:33:14.614752	199	00:08:43.26	00:08:51.12	And this part comes from the derivative of the logarithm function.	and this part come from the derivative of the logarithm function .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-02 23:10:55.652813	201	00:08:52.24	00:08:55.77	And this Lambda is simply taken from here.	and this Lambda be simply take from here .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-02 23:10:55.652814	204	00:08:58.52	00:09:05.83	And when we set it to zero, we can easily see theta sub i is related to Lambda in this way.	and when we set it to zero , we can easily see theta sub I be related to Lambda in this way .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-02 23:10:55.652815	208	00:09:06.74	00:09:15.87	Since we know all the theta I's must sum to one, we can plug this into this constraint here, and this will allow us to solve for Lambda.	since we know all the theta I be must sum to one , we can plug this into this constraint here , and this will allow we to solve for Lambda .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-02 23:10:55.652816	212	00:09:16.53	00:09:25.93	And this is just negative sum of all the counts and this further allows us to then solve optimization problem.	and this be just negative sum of all the count and this far allow we to then solve optimization problem .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-26 20:34:44.174182	214	00:09:26.72	00:09:30.17	Eventually to find the optimal setting for Theta Sub I.	eventually to find the optimal setting for Theta Sub I.
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-02 23:10:55.652819	221	00:09:31.28	00:09:48.07	And if you look at this formula, it turns out that it's actually very intuitive because this is just the normalized count of these words by the document length, which is also a sum of all the counts of words in the document.	and if you look at this formula , it turn out that it be actually very intuitive because this be just the normalize count of these word by the document length , which be also a sum of all the count of word in the document .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-02 23:10:55.652822	229	00:09:50.31	00:10:16.85	So after all this math, after all, we have just obtained something that's "very intuitive, and this will be just our intuition where we want to maximize the theta by assigning as much probability mass as possible to all the observed words here.	so after all this math , after all , we have just obtain something that be " very intuitive , and this will be just our intuition where we want to maximize the theta by assign as much probability mass as possible to all the observed word here .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-02 23:10:55.652827	247	00:10:17.77	00:10:56.17	And you might also notice that this is the general result of maximum likelihood estimator. In general, the estimate would be to normalize count and it's just sometimes the counts have to be done in a particular way, as you will also see later. So this is basically an analytical solution to our optimization problem. In general, though, when the likelihood function is very complicated, we're not going to be able to solve the optimization problem by having a closed form formula. Instead, we have to use some numerical algorithms, and we're going to see such cases later also.	and you might also notice that this be the general result of maximum likelihood estimator . in general , the estimate would be to normalize count and it be just sometimes the count have to be do in a particular way , as you will also see later . so this be basically an analytical solution to our optimization problem . in general , though , when the likelihood function be very complicated , we be not go to be able to solve the optimization problem by have a close form formula . instead , we have to use some numerical algorithm , and we be go to see such case later also .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-12-05 01:55:39.308953	266	00:10:59.04	00:11:44.6	So if you imagine what would we get if we use such a maximum likelihood estimator to estimate one topic for a single document D here, let's imagine this document is a text mining paper. Now what you might see is something that looks like this. On the top you will see the high probability words tend to be those very common words, often functional words in English, and this will be followed by some content words that really "characterized the topic well like text, mining etc and then in the end you also see various more probabilities of words that are not really related to the topic, but they might be externally mentioned in the document.	so if you imagine what would we get if we use such a maximum likelihood estimator to estimate one topic for a single document d here , let 's imagine this document be a text mining paper . now what you might see be something that look like this . on the top you will see the high probability word tend to be those very common word , often functional word in English , and this will be follow by some content word that really " characterize the topic well like text , mining etc and then in the end you also see various more probability of word that be not really related to the topic , but they might be externally mention in the document .
e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	2020-11-02 23:10:55.652835	274	00:11:45.54	00:11:58.29	As a topic representation, you will see this is not ideal, right? The because of the high probability words are functional words they are not really characterizing the topic. So one question is how can we get rid of such common words?	as a topic representation , you will see this be not ideal , right ? the because of the high probability word be functional word they be not really characterize the topic . so one question be how can we get rid of such common word ?
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-02 22:58:22.580869	2	00:00:00.3	00:00:05.25	This lecture is a continued discussion of latent aspect rating analysis.	this lecture be a continue discussion of latent aspect rating analysis .
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-02 22:58:22.580874	8	00:00:13.13	00:00:27.59	Earlier we talked about how to solve the problem of Lara in two stages when we first do segmentation of different aspects and then we use a little regression model to learn the aspect ratings and letting the weights.	early we talk about how to solve the problem of Lara in two stage when we first do segmentation of different aspect and then we use a little regression model to learn the aspect rating and let the weight .
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-02 22:58:22.580883	19	00:00:28.37	00:00:53.86	Now, it's also possible to develop a unified generative model for solving this problem, and that is we not only modeling, we not only model the generation of overrating based on text, we also model the generation of text and so a natural solution would be to use topic model. So given an entity, we can assume there are aspects that are described by word distributions.	now , it be also possible to develop a unified generative model for solve this problem , and that be we not only model , we not only model the generation of overrate base on text , we also model the generation of text and so a natural solution would be to use topic model . so give an entity , we can assume there be aspect that be describe by word distribution .
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-02 22:58:22.580884	22	00:00:54.41	00:01:00.51	Topics and then we can use a topic model to model the generation of the review text.	topic and then we can use a topic model to model the generation of the review text .
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-02 22:58:22.580887	27	00:01:01.49	00:01:12.21	Our assumed the words in the review text are drawn from these distributions. In the same way as we assumed for a generative model like PSA.	our assume the word in the review text be draw from these distribution . in the same way as we assume for a generative model like PSA .
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-29 02:56:41.37038	38	00:01:13.49	00:01:39.97	And then we can then plug in the latent regression model to use the text to further predict the Overall rating and that means we first predict the aspect rating and then combine them with aspect weights to predict the overall rating. So this would give us a unified generative model where we model both the generation of text and the overall rating condition on text.	and then we can then plug in the latent regression model to use the text to far predict the overall rating and that mean we first predict the aspect rating and then combine they with aspect weight to predict the overall rating . so this would give we a unified generative model where we model both the generation of text and the overall rating condition on text .
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-02 22:58:22.580894	44	00:01:40.82	00:01:55.99	So we don't have time to discuss this model in detail, as in many other cases in this part of the course where we discuss the cutting edge topics. But there is a reference site here where you can find more details.	so we do n't have time to discuss this model in detail , as in many other case in this part of the course where we discuss the cutting edge topic . but there be a reference site here where you can find more detail .
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-02 22:58:22.580902	62	00:01:57.02	00:02:37.83	So now I'm going to show you some simple results that you can get by using this kind of generative models. First it's about rating decomposition. So here what you see are the decomposed ratings for three hotels that have the same overall rating. So if you just look at the overall rating you don't. You can't really tell much difference between these hotels, but by decomposing these ratings into aspect ratings we can see some hotels have higher ratings for some. Dimensions like value, but others might score better in other dimensions like location and so this can reveal detailed opinions at the aspect level.	so now I be go to show you some simple result that you can get by use this kind of generative model . first it be about rating decomposition . so here what you see be the decomposed rating for three hotel that have the same overall rating . so if you just look at the overall rating you do n't . you ca n't really tell much difference between these hotel , but by decompose these rating into aspect rating we can see some hotel have high rating for some . dimension like value , but other might score well in other dimension like location and so this can reveal detailed opinion at the aspect level .
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-02 22:58:22.580905	68	00:02:38.63	00:02:52.54	Now here, the ground truth is shown in the plans, so this also allows you to see whether the prediction is accurate. It's not always accurate, but it's mostly still reflecting some of the trends.	now here , the ground truth be show in the plan , so this also allow you to see whether the prediction be accurate . it be not always accurate , but it be mostly still reflect some of the trend .
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-02 22:58:22.580924	103	00:02:53.42	00:04:09.06	The 2nd result is to compare different reviewers on the same hotel so the table shows the decompose ratings for two reviewers about same hotel again their high level overall ratings are the same. So if you just look at the overall ratings, you don't really get that much information about the difference between the two reviews. But after you decompose the ratings you can see clearly they have high scores on different dimensions. So this shows that the model can reveal differences in. Opinions of different reviewers and such a detailed understanding can help us understand better about reviews and also better about their feedback on the hotel. This is something very interesting because this is in some sense some byproduct in our problem formulation. We did not really have to do this, but the design of the generative model has this component and these are sentiment waits for words in different aspects. And you can see the highly weighted words versus the negatively lower weighted words here for each of the four dimensions. Value, rooms, location and cleanliness. I added the top words, cleared it, makes sense, and the bottom words also makes sense.	the 2nd result be to compare different reviewer on the same hotel so the table show the decompose rating for two reviewer about same hotel again their high level overall rating be the same . so if you just look at the overall rating , you do n't really get that much information about the difference between the two review . but after you decompose the rating you can see clearly they have high score on different dimension . so this show that the model can reveal difference in . opinion of different reviewer and such a detailed understanding can help we understand well about review and also well about their feedback on the hotel . this be something very interesting because this be in some sense some byproduct in our problem formulation . we do not really have to do this , but the design of the generative model have this component and these be sentiment wait for word in different aspect . and you can see the highly weight word versus the negatively low weight word here for each of the four dimension . value , room , location and cleanliness . I add the top word , clear it , make sense , and the bottom word also make sense .
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-02 22:58:22.580941	136	00:04:10.13	00:05:27.2	So this shows that with this apology, we can also learn sentiment information directly from the data. Now this kind of laxing is very useful becausw in general a word like long, let's say, may have different the sentiment polarities for different context. So if I say the battery life of this laptop is long, then that's positive. But if I say the rebooting time for the laptop is long, that's bad, right? So even for reviews about the same product laptop, the word long Is ambiguous, it could mean positive or could be negative, but this kind of lexicon that we can learn by using this kind of generative models can show whether a word is positive for a particular aspect, so this is clearly very useful, and in fact such a lexicon can be directly used to tag other reviews about hotels or tag comments about the hotels in social media like tweets. And, what's also interesting that since this is an almost computer and supervised, assuming that the reviews with overall ratings are available, and then this can allow us to learn from potentially a large amount of data on the Internet to reach sentiment lexicon.	so this show that with this apology , we can also learn sentiment information directly from the datum . now this kind of laxing be very useful becausw in general a word like long , let 's say , may have different the sentiment polarity for different context . so if I say the battery life of this laptop be long , then that be positive . but if I say the reboot time for the laptop be long , that be bad , right ? so even for review about the same product laptop , the word long be ambiguous , it could mean positive or could be negative , but this kind of lexicon that we can learn by use this kind of generative model can show whether a word be positive for a particular aspect , so this be clearly very useful , and in fact such a lexicon can be directly use to tag other review about hotel or tag comment about the hotel in social medium like tweet . and , what be also interesting that since this be an almost computer and supervise , assume that the review with overall rating be available , and then this can allow we to learn from potentially a large amount of datum on the internet to reach sentiment lexicon .
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-02 22:58:22.580946	145	00:05:28.08	00:05:44.8	And here are some results to validate the preference weights. Remember, the model can infer whether a reviewer cares more about service or the price. Now, how do we know whether the inferred weights are correct and this poses a very difficult challenge for evaluation.	and here be some result to validate the preference weight . remember , the model can infer whether a reviewer care more about service or the price . now , how do we know whether the infer weight be correct and this pose a very difficult challenge for evaluation .
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-02 22:58:22.580952	154	00:05:45.36	00:06:08.68	Now here we show some interesting way of evaluating result. what you here are the prices of hotels in different cities, and these are the prices of hotels that are favored by different groups of reviews. The top ten other reviewers with the highest inferred value to other aspect ratio.	now here we show some interesting way of evaluate result . what you here be the price of hotel in different city , and these be the price of hotel that be favor by different group of review . the top ten other reviewer with the high infer value to other aspect ratio .
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-02 22:58:22.580958	164	00:06:09.47	00:06:28.91	So for example, value versus location value versus room etc. But the top ten are the reviewers that have the highest ratios by this measure. And that means these reviewers tend to put a lot of weight on value as compared with other dimensions. That means they really emphasize on value.	so for example , value versus location value versus room etc . but the top ten be the reviewer that have the high ratio by this measure . and that mean these reviewer tend to put a lot of weight on value as compare with other dimension . that mean they really emphasize on value .
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-02 22:58:22.580964	174	00:06:30.35	00:06:51.61	The bottom ten, on the other hand, are the reviews that have the lowest ratio. What does that mean? Well, that means these reviewers have put higher weights on other aspects than value, so those are people that care about the another dimension and they didn't care so much about the value in some sense by this, less compared with the top ten group.	the bottom ten , on the other hand , be the review that have the low ratio . what do that mean ? well , that mean these reviewer have put high weight on other aspect than value , so those be people that care about the another dimension and they do n't care so much about the value in some sense by this , less compare with the top ten group .
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-29 03:02:01.71059	176	00:06:52.35	00:06:56.82	Now these ratios are computer based on the inferred weights from the model.	now these ratio be computer base on the infer weight from the model .
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-02 22:58:22.580968	180	00:06:57.7	00:07:06.56	So now you can see the average prices of hotels are favored by toptenreviews are indeed and much cheaper than those that are favored by the bottom 10.	so now you can see the average price of hotel be favor by toptenreview be indeed and much cheap than those that be favor by the bottom 10 .
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-29 03:02:16.255965	181	00:07:07.27	00:07:09.33	And this provides some	and this provide some
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-02 22:58:22.580984	206	00:07:10.92	00:07:58.66	Indirect way of validating the infer wait. It just means the weights are not random and they are actually meaningful here and in comparison with the average price in these three cities, you can actually the top ten tends to have below average price, whereas the bottom time where they care a lot about other things like service or room condition tend to have hotels that have higher prices than average. So with these results we can build a lot of interesting applications. For example, direct application would be the generator rated aspect, the summary. An because of the decomposition, we can now generate the summaries for each aspect. The positive sentence is negative sentences about each aspect. It's more informative than original review that has just overall rating and review test.	indirect way of validate the infer wait . it just mean the weight be not random and they be actually meaningful here and in comparison with the average price in these three city , you can actually the top ten tend to have below average price , whereas the bottom time where they care a lot about other thing like service or room condition tend to have hotel that have high price than average . so with these result we can build a lot of interesting application . for example , direct application would be the generator rate aspect , the summary . an because of the decomposition , we can now generate the summary for each aspect . the positive sentence be negative sentence about each aspect . it be more informative than original review that have just overall rating and review test .
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-02 22:58:22.580989	215	00:08:00.37	00:08:21.52	Here are also mother results about the aspects discovered from reviews with low ratings. These are MP3 three reviews an these results show that the model can discover some interesting aspects commented on low overall ratings versus those high overall ratings, and they care more about the different aspects.	here be also mother result about the aspect discover from review with low rating . these be mp3 three review an these result show that the model can discover some interesting aspect comment on low overall rating versus those high overall rating , and they care more about the different aspect .
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-02 22:58:22.58099	217	00:08:22.5	00:08:24.88	Or they comment more on different aspects.	or they comment more on different aspect .
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-02 22:58:22.580994	228	00:08:25.7	00:08:53.74	So that can help us discover, for example, consumers trained in appreciating different features of product. For example, one might have discovered the trend that people tend to like large screens of cell phones or lightweight of laptop etc. And such knowledge can be useful for manufacturers to design their next generation of products.	so that can help we discover , for example , consumer train in appreciate different feature of product . for example , one might have discover the trend that people tend to like large screen of cell phone or lightweight of laptop etc . and such knowledge can be useful for manufacturer to design their next generation of product .
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-02 22:58:22.580997	233	00:08:55.35	00:09:08.71	Here are some interesting results on analyzing users rating behavior. So what you see is average weights on different dimensions by different groups of reviewers.	here be some interesting result on analyze user rating behavior . so what you see be average weight on different dimension by different group of reviewer .
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-02 22:58:22.581002	245	00:09:09.36	00:09:33.69	And on the left side you see the weights of reviews like the expensive hotels they give. The whole expensive hotels five stars and you can see their average weights tend to be more focused on service and that suggests that people might be expensive hotels because of good service. And that's not surprising as also another way to validate the inferred weights.	and on the left side you see the weight of review like the expensive hotel they give . the whole expensive hotel five star and you can see their average weight tend to be more focused on service and that suggest that people might be expensive hotel because of good service . and that be not surprising as also another way to validate the infer weight .
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-29 03:05:17.664268	252	00:09:34.29	00:09:51.17	But if you look at the right side where look at the column of five stars, these are the reviewers that like the cheaper hotels and they give cheaper hotels, five stars as we expected, and they put more weight on value and that's why they like the cheaper hotels.	but if you look at the right side where look at the column of five star , these be the reviewer that like the cheap hotel and they give cheap hotel , five star as we expect , and they put more weight on value and that be why they like the cheap hotel .
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-02 22:58:22.581016	277	00:09:52.49	00:10:48.01	But if you look at the when they didn't like expensive hotels or cheaper hotels and you seal it tended to have more weights on the condition of the room cleanliness. So this shows that by using this model we can infer some information that's very hard to obtain, even if you read all the reviews. Even if you read all the reviews, it's very hard to infer such preferences or such emphasis. So this is a case where text mining algorithms can go beyond what humans can do to review interesting patterns in the data, and this of course can be very useful. You can compare different hotels, compare the opinions from different consumer groups in different locations, and of course the model is general. It can be applied to any reviews with overall ratings, so this is very useful technique that can support a lot of text mining applications.	but if you look at the when they do n't like expensive hotel or cheap hotel and you seal it tend to have more weight on the condition of the room cleanliness . so this show that by use this model we can infer some information that be very hard to obtain , even if you read all the review . even if you read all the review , it be very hard to infer such preference or such emphasis . so this be a case where text mining algorithm can go beyond what human can do to review interesting pattern in the datum , and this of course can be very useful . you can compare different hotel , compare the opinion from different consumer group in different location , and of course the model be general . it can be apply to any review with overall rating , so this be very useful technique that can support a lot of text mining application .
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-02 22:58:22.581017	280	00:10:49.91	00:10:56.14	Finally, there is also some result on applying this model for personalized ranking or recommendation of entities.	finally , there be also some result on apply this model for personalized ranking or recommendation of entity .
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-02 22:58:22.581022	291	00:10:57.69	00:11:20.76	So because we can infer the reviewers weights on different dimensions, we can allow a user to actually say what do you care about. So, for example, if a query here that shows 90% of the way it should be on value and 10% on others. So that just means I don't care about other aspects, I just care about getting a cheap hotel. My emphasis is on the value dimension.	so because we can infer the reviewer weight on different dimension , we can allow a user to actually say what do you care about . so , for example , if a query here that show 90 % of the way it should be on value and 10 % on other . so that just mean I do n't care about other aspect , I just care about get a cheap hotel . my emphasis be on the value dimension .
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-02 22:58:22.581028	306	00:11:21.33	00:11:51.28	Now what we can do is such a query is that we can use reviewers that we believe have a similar preference to recommend the hotels for you. How can we know that we can infer the weights of those reviewers on different aspects? We can find the reviewers whose weights or more precise whose inferred weights or similar to yours and then use those reviews to recommend the hotels for you. And this is what we call a personalized or rather query specific recommendation.	now what we can do be such a query be that we can use reviewer that we believe have a similar preference to recommend the hotel for you . how can we know that we can infer the weight of those reviewer on different aspect ? we can find the reviewer whose weight or more precise whose infer weight or similar to yours and then use those review to recommend the hotel for you . and this be what we call a personalized or rather query specific recommendation .
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-02 22:58:22.581032	316	00:11:52.31	00:12:16.98	The non personalized recommendation results are shown on the top. An you can see the top results generally have much higher price than the low Group, and that's because when reviewers cared more about the value as dictated by this query and they tend to really have favor low price hotels. So this is yet another application of this technique.	the non personalized recommendation result be show on the top . an you can see the top result generally have much high price than the low Group , and that be because when reviewer care more about the value as dictate by this query and they tend to really have favor low price hotel . so this be yet another application of this technique .
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-02 22:58:22.581037	324	00:12:18.16	00:12:31.74	And shows that by doing text mining we can understand the users better. And once we can end users better, we can serve these users better. So to summarize our discussion of opinion mining in general, this is a very important topic and with a lot of applications.	and show that by do text mining we can understand the user well . and once we can end user well , we can serve these user well . so to summarize our discussion of opinion mining in general , this be a very important topic and with a lot of application .
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-02 22:58:22.581041	333	00:12:33.06	00:12:51.98	And as a task sentiment analysis can be usually done by using just text categorization, but standard techniques tend not to be enough and so we need to have enriched feature representation. And we also need to consider the order of those categories and we talk about the ordinal regression. For solving this problem.	and as a task sentiment analysis can be usually do by use just text categorization , but standard technique tend not to be enough and so we need to have enrich feature representation . and we also need to consider the order of those category and we talk about the ordinal regression . for solve this problem .
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-02 22:58:22.581045	343	00:12:52.55	00:13:12.76	We have also shown that generative models are powerful for mining latent user preferences, in particular in the generating model for letting the rating regression, we embed some interesting preference information and sentiment weights of words in the model. As a result, we can learn those useful information when fitting the model to the data.	we have also show that generative model be powerful for mining latent user preference , in particular in the generate model for let the rating regression , we embed some interesting preference information and sentiment weight of word in the model . as a result , we can learn those useful information when fit the model to the datum .
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-29 03:09:52.131415	355	00:13:13.68	00:13:43.66	Most approaches have been proposed and evaluated for product reviews, and that was the cause in such a context of the opinion holder an opinion target or clear and they are easy to analyze and there of course also have a lot of practical applications, but opinion mining from news and social media is also important, but that's more difficult than analyzing review data, mainly because the opinion holders and opinion target are all.	Most approach have be propose and evaluate for product review , and that be the cause in such a context of the opinion holder an opinion target or clear and they be easy to analyze and there of course also have a lot of practical application , but opinion mining from news and social medium be also important , but that be more difficult than analyze review datum , mainly because the opinion holder and opinion target be all .
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-02 22:58:22.581051	358	00:13:44.23	00:13:49.9	implicit and so that calls for natural language processing techniques to uncover them accurately.	implicit and so that call for natural language processing technique to uncover they accurately .
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-02 22:58:22.581052	360	00:13:50.83	00:13:54.94	So here are some suggested readings, the first 2.	so here be some suggest reading , the first 2 .
ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	2020-11-02 22:58:22.581054	366	00:13:55.56	00:14:07.15	are small books that are excellent reviews of this topic where you can find a lot of discussion about the other variations of the problem and techniques proposal for solving the problem.	be small book that be excellent review of this topic where you can find a lot of discussion about the other variation of the problem and technique proposal for solve the problem .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538509	3	00:00:00.3	00:00:06.03	This lecture is about opinion mining and sentiment analysis covering its motivation.	this lecture be about opinion mining and sentiment analysis cover its motivation .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538511	11	00:00:15.18	00:00:31.14	In this lecture we are going to start talking about mining a different kind of knowledge, namely knowledge about the observer or humans that have generated text data. In particular, we're going to talk about the opinion mining and sentiment analysis.	in this lecture we be go to start talk about mine a different kind of knowledge , namely knowledge about the observer or human that have generate text datum . in particular , we be go to talk about the opinion mining and sentiment analysis .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538512	14	00:00:32.55	00:00:41.73	As we discussed earlier, text data can be regarded as the data generated from humans as subjective sensors.	as we discuss early , text datum can be regard as the datum generate from human as subjective sensor .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538513	19	00:00:42.69	00:00:56.23	In contrast, we have other devices such as video recorder that can report what's happening in the real world objectively to generate the video data, for example.	in contrast , we have other device such as video recorder that can report what be happen in the real world objectively to generate the video datum , for example .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538515	24	00:00:58.16	00:01:15.8	Now the main difference between text data and other data like video data is that it has rich and rich opinions and the content tends to be subjective because it's generated from humans.	now the main difference between text datum and other datum like video datum be that it have rich and rich opinion and the content tend to be subjective because it be generate from human .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538516	30	00:01:16.61	00:01:27.35	Now this is actually unique advantage of text data. as compared with other data because it offers us a great opportunity to understand the observers.	now this be actually unique advantage of text datum . as compare with other datum because it offer we a great opportunity to understand the observer .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538517	34	00:01:28.02	00:01:35.22	We can mine the text data to understand the opinions understand the people's preferences, how people think about something.	we can mine the text datum to understand the opinion understand the people 's preference , how people think about something .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538518	38	00:01:36.98	00:01:48.01	So this lecture and the following lectures will be mainly about how we can mine and analyze opinions buried in a lot of text data.	so this lecture and the follow lecture will be mainly about how we can mine and analyze opinion bury in a lot of text datum .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538521	44	00:01:49.48	00:02:07.11	So let's start with the concept of opinion that it's not that easy to formally define opinion, but mostly we would define opinion as a subjective statement describing what a person believes or thinks about something.	so let 's start with the concept of opinion that it be not that easy to formally define opinion , but mostly we would define opinion as a subjective statement describe what a person believe or think about something .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538524	57	00:02:08.66	00:02:39.97	Now I highlighted a quite a few words here, and that's because it was thinking a little more about these words and that would help us better understand what is in the opinion and this further helps us to define opinion more formally, which is always needed to computationally solve the problem of opinion mining. So let's first look at the keyword subjective here. Now this is in contrast with objective statement or factual statement.	now I highlight a quite a few word here , and that be because it be think a little more about these word and that would help we well understand what be in the opinion and this far help we to define opinion more formally , which be always need to computationally solve the problem of opinion mining . so let 's first look at the keyword subjective here . now this be in contrast with objective statement or factual statement .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538524	59	00:02:40.76	00:02:44.33	Those statements can be proved right or wrong.	those statement can be prove right or wrong .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538526	64	00:02:45.07	00:02:57.05	And this is a key differentiating factor from opinion, which tends to be not easy to prove wrong or right because it reflects what a person thinks about something.	and this be a key differentiate factor from opinion , which tend to be not easy to prove wrong or right because it reflect what a person think about something .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538527	66	00:02:58.89	00:03:06.72	So in contrast, objective statement can usually be proved wrong or correct.	so in contrast , objective statement can usually be prove wrong or correct .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-12-03 05:43:28.790004	69	00:03:07.55	00:03:14.97	For example, you might say this computer has a screen and a battery.	for example , you might say this computer have a screen and a battery .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538528	71	00:03:16.57	00:03:22.32	Now that's something you can check. It's either having a battery or not.	now that be something you can check . it be either have a battery or not .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538529	74	00:03:23.36	00:03:29.81	But in contrast, if you think about the sentence such as this laptop has the best battery.	but in contrast , if you think about the sentence such as this laptop have the good battery .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.53853	78	00:03:31.66	00:03:43.52	Or this laptop has a nice screen, now these statements are more subjective and it's very hard to prove whether it's wrong or correct.	or this laptop have a nice screen , now these statement be more subjective and it be very hard to prove whether it be wrong or correct .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538531	79	00:03:44.48	00:03:48.01	So opinion is subjective statement.	so opinion be subjective statement .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538534	88	00:03:50.13	00:04:09.96	And next, let's look at the keyword person here and that indicates this opinion Holder 'cause when we talk about opinion, it's about the opinion held by someone and then we notice that there is something here. So that's the target of the opinion. The opinions is expressed on this something.	and next , let 's look at the keyword person here and that indicate this opinion Holder 'cause when we talk about opinion , it be about the opinion hold by someone and then we notice that there be something here . so that be the target of the opinion . the opinion be express on this something .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-12-03 05:45:51.023844	100	00:04:11.32	00:04:36.72	And now, of course, believes or thinks implies that the opinion would depend on the culture or background and context in general, because of person might think differently in the different context. People from different background may also think in different ways. So this analysis shows that there are multiple elements that we need to include in order to characterize an opinion.	and now , of course , believe or think imply that the opinion would depend on the culture or background and context in general , because of person might think differently in the different context . People from different background may also think in different way . so this analysis show that there be multiple element that we need to include in order to characterize an opinion .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538543	108	00:04:38.05	00:04:55.93	So what's the basic opinion representation like, well, it should include at least three measurements, right? First it has to specify what's the opinion Holder. So whose opinion is this, second must also specify the target. What's this opinion about?	so what be the basic opinion representation like , well ,   it should include at least three measurement , right ? first it have to specify what be the opinion Holder . so whose opinion be this , second must also specify the target . what be this opinion about ?
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538546	117	00:04:56.99	00:05:14.22	And 3rd, of course we want opinion content and So what exactly is the opinion? If you can identify this, we get a basic understanding of an opinion and can already be useful. Sometimes if you want to understand further, we want to enrich the opinion representation.	and 3rd , of course we want opinion content and so what exactly be the opinion ? if you can identify this , we get a basic understanding of an opinion and can already be useful . sometimes if you want to understand far , we want to enrich the opinion representation .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538551	129	00:05:15.11	00:05:42.87	And that means we also want to understand, for example, the context of the opinion and what situation was opinion expressed. For example, in what time was it expressed? We also would like to deeply understand opinion sentiment and this is to understand, what the opinion tells us about the opinion holder's feeling, for example, is this opinion positive or negative?	and that mean we also want to understand , for example , the context of the opinion and what situation be opinion express . for example , in what time be it express ? we also would like to deeply understand opinion sentiment and this be to understand , what the opinion tell we about the opinion holder 's feeling , for example , be this opinion positive or negative ?
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538552	131	00:05:43.54	00:05:48.35	Or perhaps the opinion holder is was happy or sad.	or perhaps the opinion holder be be happy or sad .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538552	134	00:05:49	00:05:56.35	And so such understanding obviously goes beyond just extracting the opinion content and needs some analysis.	and so such understanding obviously go beyond just extract the opinion content and need some analysis .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538556	145	00:06:00.35	00:06:20.58	So let's take a simple example of a product review. In this case, this actually explicit opinion Holder and explicit target, so it's It's obviously what's opinion Holder, and that's just a reviewer, and it's also often very clear what's the opinion target, and that's the product being reviewed. For example iPhone 6.	so let 's take a simple example of a product review . in this case , this actually explicit opinion Holder and explicit target , so it be it be obviously what be opinion Holder , and that be just a reviewer , and it be also often very clear what be the opinion target , and that be the product be review . for example iPhone 6 .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538557	148	00:06:21.8	00:06:26.41	When the review was posted, usually you can extract such information easily.	when the review be post , usually you can extract such information easily .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538558	151	00:06:27.23	00:06:31.63	Now the content of course is the review text that's in general also easy to obtain.	now the content of course be the review text that be in general also easy to obtain .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.53856	155	00:06:32.18	00:06:39.89	So you can see product reviews are fairly easy to analyze in terms of obtaining a basic opinion representation.	so you can see product review be fairly easy to analyze in term of obtain a basic opinion representation .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538561	160	00:06:40.57	00:06:50.39	But of course, if you want to get more information, we might want to know the context. For example, the review was written in 2015.	but of course , if you want to get more information , we might want to know the context . for example , the review be write in 2015 .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538563	164	00:06:51.55	00:07:02.58	Or we want to know that the sentiment of this review is positive, and so this additional understanding of course adds value to mining the opinions.	or we want to know that the sentiment of this review be positive , and so this additional understanding of course add value to mine the opinion .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538564	169	00:07:04.23	00:07:13.94	Now you can see in this case the task is relatively easy, and that's because. The opinion Holder and opinion target that have already been identified.	now you can see in this case the task be relatively easy , and that be because . the opinion Holder and opinion target that have already be identify .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-12-04 07:55:13.773898	173	00:07:14.71	00:07:20.67	Now let's take a look at the sentence in the news. In this case, we have implicit holder and implicit target.	now let 's take a look at the sentence in the news . in this case , we have implicit holder and implicit target .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538567	177	00:07:21.29	00:07:31.87	And the task is in general harder so we can identify opinion holder here and that's governor of Connecticut.	and the task be in general hard so we can identify opinion holder here and that be governor of Connecticut .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538569	182	00:07:32.65	00:07:43.02	We can also identify the target. So one target is Hurricane Sandy. But there is also another target, which is a hurricane of 1938.	we can also identify the target . so one target be Hurricane Sandy . but there be also another target , which be a hurricane of 1938 .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538571	186	00:07:44.49	00:07:52.18	So what's the opinion? Well, this negative sentiment here that's indicated by words like a bad and worst.	so what be the opinion ? well , this negative sentiment here that be indicate by word like a bad and bad .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538571	189	00:07:53.23	00:07:59.43	And we can also then identify the context. New England in this case.	and we can also then identify the context . New England in this case .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538572	192	00:08:00.3	00:08:07.46	Now unlike in the product review, all these elements must be extracted by using natural language processing techniques.	now unlike in the product review , all these element must be extract by use natural language processing technique .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538573	194	00:08:08.23	00:08:13.1	So the task is much harder and we need a deeper natural language processing.	so the task be much hard and we need a deep natural language processing .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538575	202	00:08:14.74	00:08:34.22	And these examples also, suggest that a lot of work can be easily done for product reviews, and that's indeed what has happened. Analyzing sentiment in news is still quite difficult. It's more difficult than the analysis of opinions in product reviews.	and these example also , suggest that a lot of work can be easily do for product review , and that be indeed what have happen . analyze sentiment in news be still quite difficult . it be more difficult than the analysis of opinion in product review .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.53858	213	00:08:36.69	00:08:55.17	Now there are also some other interesting variations. In fact, here we're going to examine the variations of opinions more systematically. First, lets think about the opinion Holder. Now the Holder could be an individual or could be a group of people and sometimes opinion was from a committee or from a whole country of people.	now there be also some other interesting variation . in fact , here we be go to examine the variation of opinion more systematically . first , let think about the opinion Holder . now the Holder could be an individual or could be a group of people and sometimes opinion be from a committee or from a whole country of people .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538583	221	00:08:56.33	00:09:10.82	Opinion Target Council vary a lot. It can be about 1 entity, a particular person, a particular product, that particular policy, etc. But it could be about a group of products. Could be about the product from a company in general.	Opinion Target Council vary a lot . it can be about 1 entity , a particular person , a particular product , that particular policy , etc . but it could be about a group of product . could be about the product from a company in general .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538584	225	00:09:11.76	00:09:18.81	Could also be very specific about one attribute, attribute of Entity for example. It's just about the battery of iPhone.	could also be very specific about one attribute , attribute of Entity for example . it be just about the battery of iPhone .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538588	238	00:09:20.51	00:09:46.8	It could be about someone else opinion and one person might comment on another persons opinion etc. So you can see there is a lot of variation here that will cause the problem to vary a lot. Now opinion content, of course, can also vary a lot on the surface. You can identify one sentence opinion or one phrase opinion, but you can also have longer text to express the opinion like a whole article.	it could be about someone else opinion and one person might comment on another person opinion etc . so you can see there be a lot of variation here that will cause the problem to vary a lot . now opinion content , of course , can also vary a lot on the surface . you can identify one sentence opinion or one phrase opinion , but you can also have long text to express the opinion like a whole article .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538591	246	00:09:48.27	00:10:02.53	And Furthermore, we can identify the variation in the sentiment or emotion dimension. That's about the feeling of the opinion Holder. So we can distinguish positive versus negative or neutral or happy versus sad, etc.	and furthermore , we can identify the variation in the sentiment or emotion dimension . that be about the feeling of the opinion Holder . so we can distinguish positive versus negative or neutral or happy versus sad , etc .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538599	274	00:10:03.05	00:10:58.53	Finally, the opinion context can also vary. We can have simple context, like different time or different locations, but there could be also complex text such as some background topic being discussed. So when opinion expressed in the particular discourse context, it has to be interpreted in different ways than when it's expressed in another context, so the context can be very rich to improve the entire discourse context of opinion. From computational perspective, we're most interested in what opinions can be extracted from text data, so it turns out that we can also differentiate distinguish different kinds of opinions in text data from computation perspective. First, the Observer might make a comment about the opinion target in the observed world. So in this case we have the author's opinion. For example, I don't like this phone at all, and that's opinion of this author.	finally , the opinion context can also vary . we can have simple context , like different time or different location , but there could be also complex text such as some background topic be discuss . so when opinion express in the particular discourse context , it have to be interpret in different way than when it be express in another context , so the context can be very rich to improve the entire discourse context of opinion . from computational perspective , we be most interested in what opinion can be extract from text datum , so it turn out that we can also differentiate distinguish different kind of opinion in text datum from computation perspective . first , the Observer might make a comment about the opinion target in the observed world . so in this case we have the author 's opinion . for example , I do n't like this phone at all , and that be opinion of this author .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538603	283	00:10:59.88	00:11:26.53	In contrast, the text might also report opinions about others so the person could also make observation about another persons opinion and report this opinion. So for example, I believe he loves the painting and that opinion is really about, is really expressed by another person.	in contrast , the text might also report opinion about other so the person could also make observation about another person opinion and report this opinion . so for example , I believe he love the painting and that opinion be really about , be really express by another person .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538603	285	00:11:27.14	00:11:32.34	Here, so it doesn't mean this author loves that painting.	here , so it do n't mean this author love that painting .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538606	293	00:11:33.53	00:11:49.86	So clearly the two kinds of opinions need to be analyzed in different ways and sometimes in product reviews you can see, although mostly the opinions are from this reviewer. Sometimes a reviewer might mention opinions of his friend or her friend, right?	so clearly the two kind of opinion need to be analyze in different way and sometimes in product review you can see , although mostly the opinion be from this reviewer . sometimes a reviewer might mention opinion of his friend or her friend , right ?
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538608	302	00:11:50.5	00:12:12.46	And another complication is that there may be indirect opinions or infered opinions that can be obtained by making inferences on what's expressed in the text that might not necessarily look like opinion. For example, one statement might be this phone ran out of battery in just one hour.	and another complication be that there may be indirect opinion or infered opinion that can be obtain by make inference on what be express in the text that might not necessarily look like opinion . for example , one statement might be this phone run out of battery in just one hour .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538609	306	00:12:13.96	00:12:24.64	Now this is in a way, a factual statement, 'cause you know it's either true or false, right? You can even verify that.	now this be in a way , a factual statement , 'cause you know it be either true or false , right ? you can even verify that .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.53861	311	00:12:25.58	00:12:36.49	But from this statement one can also infer some negative opinions about the quality of the battery of this phone or the feeling of the opinion holder about the battery.	but from this statement one can also infer some negative opinion about the quality of the battery of this phone or the feeling of the opinion holder about the battery .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538611	313	00:12:37.04	00:12:40.66	In the opinion, Holder clearly wish the battery to last longer.	in the opinion , Holder clearly wish the battery to last long .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-12-04 05:50:54.800443	318	00:12:42.39	00:12:52.23	So these are interesting variations that we need to pay attention to when we extract opinions. Also, for this reason about the indirect opinions.	so these be interesting variation that we need to pay attention to when we extract opinion . also , for this reason about the indirect opinion .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-12-04 05:53:57.973718	331	00:12:53.68	00:13:18.53	It's often also very useful to extract it or whatever the person had said about the product, and sometimes factual sentences like this are also very useful. So from practical viewpoint, sometimes we don't necessarily extract the subjective sentences. Instead, would you just get all the sentences that are about opinions that are useful for understanding the person or understanding the product we comenting on.	it be often also very useful to extract it or whatever the person have say about the product , and sometimes factual sentence like this be also very useful . so from practical viewpoint , sometimes we do n't necessarily extract the subjective sentence . instead , would you just get all the sentence that be about opinion that be useful for understand the person or understand the product we comente on .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-12-04 05:55:35.573143	342	00:13:19.44	00:13:43.82	So the task of opinion mining can be defined as taking text data as input to generate a set of opinion representations. In each representation we should identify opinion Holder, target content and context. Ideally we can also infer opinion sentiment from the content and context to better understand the opinion.	so the task of opinion mining can be define as take text datum as input to generate a set of opinion representation . in each representation we should identify opinion Holder , target content and context . ideally we can also infer opinion sentiment from the content and context to well understand the opinion .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538622	356	00:13:44.59	00:14:15.69	Now often some elements of the representation are already know. I just gave a good example, in the case of product reviews where the opinion Holder and opinion target are often explicitly identified, and that's not why this turns out to be one of the simplest opinion mining tasks. Now it's interesting to think about other tasks that might be also simple, because those are the cases where you can easily build applications by using opinion mining techniques.	now often some element of the representation be already know . I just give a good example , in the case of product review where the opinion Holder and opinion target be often explicitly identify , and that be not why this turn out to be one of the simple opinion mining task . now it be interesting to think about other task that might be also simple , because those be the case where you can easily build application by use opinion mining technique .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538625	366	00:14:17.8	00:14:38.48	So now that we have talked about what is opinion mining and we have defined the task, let's also just talk a little bit about the why opinion mining is very important and why it's very useful. So here I identify three major reasons, 3 broad reasons. The first is it can help decision support.	so now that we have talk about what be opinion mining and we have define the task , let 's also just talk a little bit about the why opinion mining be very important and why it be very useful . so here I identify three major reason , 3 broad reason . the first be it can help decision support .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538626	372	00:14:39.16	00:14:51.19	I can help us optimize our decisions. We often look at the other peoples opinions and look at the reader reviews in order to make a decision like buying, buying a product, or using the service.	I can help we optimize our decision . we often look at the other people opinion and look at the reader review in order to make a decision like buy , buy a product , or use the service .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538627	373	00:14:52.07	00:14:53.18	We also	we also
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538627	376	00:14:54.08	00:14:59.47	Would be interested in others opinions. When we decide whom to vote, for example.	would be interested in other opinion . when we decide whom to vote , for example .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-12-04 05:59:19.120316	382	00:15:00.19	00:15:11.17	And policymakers may also want to know peoples opinions when designing a new policy. So that's one general kind of applications. And it's very broad, of course.	and policymaker may also want to know people opinion when design a new policy . so that be one general kind of application . and it be very broad , of course .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538632	394	00:15:12.43	00:15:34.59	The second application is to understand people, and this is also very important. For example, that can help understand peoples preferences. So and this could help us better serve people. For example, we can optimize the product search engine, optimize recommender system if we know what people are interested in, what people think about products.	the second application be to understand people , and this be also very important . for example , that can help understand people preference . so and this could help we well serve people . for example , we can optimize the product search engine , optimize recommender system if we know what people be interested in , what people think about product .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538633	399	00:15:35.67	00:15:47.56	It can also help her with advertising, of course, and we can have targeted advertising if we know what kind of people tend to know to like what kind of product.	it can also help she with advertising , of course , and we can have target advertising if we know what kind of people tend to know to like what kind of product .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538635	405	00:15:48.44	00:16:01.43	Now the third kind of applications can be called voluntary survey. Now this is mostly to support research that used to be done by doing surveys, doing manual surveys. question answering.	now the third kind of application can be call voluntary survey . now this be mostly to support research that use to be do by do survey , do manual survey . question answer .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538635	407	00:16:02.52	00:16:05.99	People need to fill in forms to answer some questions.	People need to fill in form to answer some question .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538636	411	00:16:07.56	00:16:17.96	Now this is directly related to humans as sensors, and we can usually aggregate opinions from a lot of humans to kind of assess the general opinion.	now this be directly relate to human as sensor , and we can usually aggregate opinion from a lot of human to kind of assess the general opinion .
f1951cf2-4293-450b-8578-4d74c72f9862	2020-11-02 23:09:49.538638	419	00:16:18.55	00:16:35.98	Now this is would be very useful for business intelligence, where product manufacturers want to know, where their products have advantages over others. What are the winning features of their product or winning features of competitive products?	now this be would be very useful for business intelligence , where product manufacturer want to know , where their product have advantage over other . what be the win feature of their product or win feature of competitive product ?
f1951cf2-4293-450b-8578-4d74c72f9862	2020-12-04 08:12:48.852379	438	00:16:36.98	00:17:18.07	Market research has to do with understanding consumers opinions and this is clearly very useful, directed for that. Data Driven social science research can benefit from this because they can do text mining to understand the people's opinions. And if we can aggregate a lot of opinions from social media from a lot of public information, then you can actually do some study of some questions. For example, we can study the behavior of people on social media or in on social networks, and these can be regarded as voluntary survey, but done by those people.	market research have to do with understand consumer opinion and this be clearly very useful , direct for that . datum drive social science research can benefit from this because they can do text mining to understand the people 's opinion . and if we can aggregate a lot of opinion from social medium from a lot of public information , then you can actually do some study of some question . for example , we can study the behavior of people on social medium or in on social network , and these can be regard as voluntary survey , but do by those people .
f64adab4-578a-4868-8b2c-03fdd4ddf55d	2020-11-25 05:26:29.391708	3	00:00:00.3	00:00:05.31	This lecture is a continued discussion of generative probabilistic models for text clustering.	this lecture be a continue discussion of generative probabilistic model for text clustering .
f64adab4-578a-4868-8b2c-03fdd4ddf55d	2020-11-30 14:15:39.286082	7	00:00:13.33	00:00:21	In this lecture, we're going to continue talking about the tax capture text clustering, particularly "generative	in this lecture , we be go to continue talk about the tax capture text clustering , particularly " generative
f64adab4-578a-4868-8b2c-03fdd4ddf55d	2020-11-02 23:11:34.309392	10	00:00:23.87	00:00:31.45	So this is a slide that you have seen earlier where we have written down the likelihood function for a document.	so this be a slide that you have see early where we have write down the likelihood function for a document .
f64adab4-578a-4868-8b2c-03fdd4ddf55d	2020-11-02 23:11:34.309393	12	00:00:32.04	00:00:38.4	With two distributions in two component mixture model for document clustering.	with two distribution in two component mixture model for document clustering .
f64adab4-578a-4868-8b2c-03fdd4ddf55d	2020-11-02 23:11:34.309394	15	00:00:39.5	00:00:46.58	Now in this lecture, we're going to generalize this to include the K clusters.	now in this lecture , we be go to generalize this to include the K cluster .
f64adab4-578a-4868-8b2c-03fdd4ddf55d	2020-11-02 23:11:34.309399	28	00:00:47.23	00:01:17.89	Now if you look at the formula and think about the question how to generalize it, you will realize that all we need is to add more terms like what you have seen here. So you can just add more thetas and the probabilities of thetas and the probabilities of generating D from those thetas. So this is precisely what we're going to use. This is general presentation of the mixture model for document clustering.	now if you look at the formula and think about the question how to generalize it , you will realize that all we need be to add more term like what you have see here . so you can just add more theta and the probability of theta and the probability of generate d from those theta . so this be precisely what we be go to use . this be general presentation of the mixture model for document clustering .
f64adab4-578a-4868-8b2c-03fdd4ddf55d	2020-11-25 05:27:59.282577	34	00:01:19.17	00:01:32.49	So as more cases we follow these steps using a generated model. First think about our data, right? So in this case our data is a collection of documents N documents denoted by the sub I.	so as more case we follow these step use a generate model . first think about our datum , right ? so in this case our data be a collection of document n document denote by the sub I.
f64adab4-578a-4868-8b2c-03fdd4ddf55d	2020-11-02 23:11:34.309403	40	00:01:33.62	00:01:44.96	And then we talk about the model. Think about the model. In this case, we design a mixture of K unigram language models. It's a little bit different from the topic model.	and then we talk about the model . think about the model . in this case , we design a mixture of K unigram language model . it be a little bit different from the topic model .
f64adab4-578a-4868-8b2c-03fdd4ddf55d	2020-11-25 05:28:55.098753	47	00:01:45.63	00:02:03.95	But we have similar parameters. We have a set of theta i denote the word distributions corresponding to the K unigram language models. We have P of each theta I as the probability of selecting each of the K distributions to generate the document.	but we have similar parameter . we have a set of theta I denote the word distribution correspond to the K unigram language model . we have p of each theta I as the probability of select each of the K distribution to generate the document .
f64adab4-578a-4868-8b2c-03fdd4ddf55d	2020-11-02 23:11:34.309407	53	00:02:05.77	00:02:24.54	Now note that, although our goal is to find the clusters and we actually have used a more general notion of a probability of each cluster. And this, as you see later, would allow us to assign a document to the.	now note that , although our goal be to find the cluster and we actually have use a more general notion of a probability of each cluster . and this , as you see later , would allow we to assign a document to the .
f64adab4-578a-4868-8b2c-03fdd4ddf55d	2020-11-02 23:11:34.309409	58	00:02:25.11	00:02:35.74	Cluster that has the highest probability of being able to generate the document. So as a result, we can also recover some other interesting.	Cluster that have the high probability of be able to generate the document . so as a result , we can also recover some other interesting .
f64adab4-578a-4868-8b2c-03fdd4ddf55d	2020-11-02 23:11:34.30941	59	00:02:36.55	00:02:37.41	Properties.	property .
f64adab4-578a-4868-8b2c-03fdd4ddf55d	2020-11-02 23:11:34.30941	60	00:02:38.56	00:02:40.75	As you will see later.	as you will see later .
f64adab4-578a-4868-8b2c-03fdd4ddf55d	2020-11-02 23:11:34.309415	73	00:02:42.26	00:03:08.21	So the model basically would make the following assumption about the generation of the document. We first choose a theta I according to probability of theta I and then generate all the words in the document using this distribution. Note that it's important that we use this distributed generator. All the words in the document. This is very different from topic model, so the likelihood function would be like what you are seeing here.	so the model basically would make the following assumption about the generation of the document . we first choose a theta I accord to probability of theta I and then generate all the word in the document use this distribution . note that it be important that we use this distribute generator . all the word in the document . this be very different from topic model , so the likelihood function would be like what you be see here .
f64adab4-578a-4868-8b2c-03fdd4ddf55d	2020-11-02 23:11:34.309415	74	00:03:09.7	00:03:11.51	So the.	so the .
f64adab4-578a-4868-8b2c-03fdd4ddf55d	2020-11-02 23:11:34.309416	77	00:03:13.81	00:03:19.72	You can take a look at the formula here. We have used the different.	you can take a look at the formula here . we have use the different .
f64adab4-578a-4868-8b2c-03fdd4ddf55d	2020-11-02 23:11:34.309417	79	00:03:20.42	00:03:25.89	Notation here in the second line of this.	Notation here in the second line of this .
f64adab4-578a-4868-8b2c-03fdd4ddf55d	2020-11-02 23:11:34.309417	80	00:03:27.88	00:03:29.04	Of this equation.	of this equation .
f64adab4-578a-4868-8b2c-03fdd4ddf55d	2020-11-02 23:11:34.309418	81	00:03:30.31	00:03:33.43	But you can see now the.	but you can see now the .
f64adab4-578a-4868-8b2c-03fdd4ddf55d	2020-11-02 23:11:34.309419	85	00:03:34.58	00:03:44.39	notation has been changed to use unique word in the vocabulary in the product instead of particular position in the document.	notation have be change to use unique word in the vocabulary in the product instead of particular position in the document .
f64adab4-578a-4868-8b2c-03fdd4ddf55d	2020-11-02 23:11:34.309422	92	00:03:45.04	00:04:08.58	So from X sub J to W is a change of notation, and this change allows us to show the estimation formulas more easily and you have seen this change also in the topic model presentation, but it's basically still just a product of the probabilities of all the words.	so from X sub j to W be a change of notation , and this change allow we to show the estimation formula more easily and you have see this change also in the topic model presentation , but it be basically still just a product of the probability of all the word .
f64adab4-578a-4868-8b2c-03fdd4ddf55d	2020-11-25 05:32:08.767417	127	00:04:09.88	00:05:28.64	I and so with the lack of functioning. Now we can talk about how to do parameter estimation. Here we can simply use the maximum likelihood estimator, so that's just a standard way of doing things, so all should be familiar to you now, it's just a different model. So after we have estimate the parameters, how can we then allocate clusters to the documents? Let's take a look at this situation more closely, so we just repeated the parameters here. For this mixture model. Now, if you think about what we can get by estimate such a model, we can actually get more information than what we need for doing clustering, right? So see theta. I, for example, represents the content of class I. This is actually a byproduct. It helps summarize what the cluster is about to look at the top terms in this cluster or in this word distribution. And they will tell us what the cluster is about. An P of theta i can be interpreted as. Indicating the size of cluster because it tells us how likely cluster would be used to generate the document. The more likely a cluster is used to generate the document, we can assume the larger the cluster size is.	I and so with the lack of functioning . now we can talk about how to do parameter estimation . here we can simply use the maximum likelihood estimator , so that be just a standard way of do thing , so all should be familiar to you now , it be just a different model . so after we have estimate the parameter , how can we then allocate cluster to the document ? let 's take a look at this situation more closely , so we just repeat the parameter here . for this mixture model . now , if you think about what we can get by estimate such a model , we can actually get more information than what we need for do clustering , right ? so see theta . I , for example , represent the content of class I. this be actually a byproduct . it help summarize what the cluster be about to look at the top term in this cluster or in this word distribution . and they will tell we what the cluster be about . an p of theta I can be interpret as . indicate the size of cluster because it tell we how likely cluster would be use to generate the document . the more likely a cluster be use to generate the document , we can assume the large the cluster size be .
f64adab4-578a-4868-8b2c-03fdd4ddf55d	2020-11-02 23:11:34.309441	140	00:05:29.67	00:05:59.81	Note that unlike in PLSA and this probability of theta I is not dependent on D. Now. You may recall that the topic choice in each document actually depends on D. That means each document can have a potentially different choice of topics, but here we have a generic choice probability for all the documents. But of course, given a particular document that we still have to infer which topic is more likely.	note that unlike in PLSA and this probability of theta I be not dependent on D. Now . you may recall that the topic choice in each document actually depend on D. that mean each document can have a potentially different choice of topic , but here we have a generic choice probability for all the document . but of course , give a particular document that we still have to infer which topic be more likely .
f64adab4-578a-4868-8b2c-03fdd4ddf55d	2020-11-25 05:32:48.207341	143	00:06:00.32	00:06:04.82	To generate the document so in that sense, we can still have a document dependent probability of clusters.	to generate the document so in that sense , we can still have a document dependent probability of cluster .
f64adab4-578a-4868-8b2c-03fdd4ddf55d	2020-11-02 23:11:34.309444	148	00:06:17.85	00:06:27.759999	So lets look at a key problem Lets to compute the C sub D here and this will take one of the values in the range of one to k to indicate which cluster should be assigned to D.	so lets look at a key problem let to compute the c sub d here and this will take one of the value in the range of one to k to indicate which cluster should be assign to D.
f64adab4-578a-4868-8b2c-03fdd4ddf55d	2020-11-25 05:35:28.816098	154	00:06:28.57	00:06:41.29	Let's first you might think about a way to use likelihood only, and that is to assign D to the cluster corresponding to the topic Theta I. That most likely has been used to generate D.	let 's first you might think about a way to use likelihood only , and that be to assign d to the cluster correspond to the topic Theta I. that most likely have be use to generate D.
f64adab4-578a-4868-8b2c-03fdd4ddf55d	2020-11-02 23:11:34.309448	157	00:06:42.32	00:06:48.77	So that means we're going to choose one of those distributions that gives D highest probability.	so that mean we be go to choose one of those distribution that give D high probability .
f64adab4-578a-4868-8b2c-03fdd4ddf55d	2020-11-25 05:35:41.946054	160	00:06:49.38	00:06:55.35	In other words, we see which distribution has a content that matches our D best.	in other word , we see which distribution have a content that match our d good .
f64adab4-578a-4868-8b2c-03fdd4ddf55d	2020-11-02 23:11:34.30945	161	00:06:56.27	00:06:58.29	Intuitively, that makes sense.	intuitively , that make sense .
f64adab4-578a-4868-8b2c-03fdd4ddf55d	2020-11-02 23:11:34.309453	170	00:06:59.07	00:07:24.48	However, this approach does not consider the size of clusters, which is also available to us. And so a better way is to use the likelihood together with the prior. In this case the prior is P of Theta I. And together, that is, we're going to use the base formula to compute the posterior probability of Theta given D.	however , this approach do not consider the size of cluster , which be also available to we . and so a well way be to use the likelihood together with the prior . in this case the prior be p of Theta I. and together , that is , we be go to use the base formula to compute the posterior probability of Theta give D.
f64adab4-578a-4868-8b2c-03fdd4ddf55d	2020-11-02 23:11:34.309458	182	00:07:25.55	00:07:55.35	And if we choose theta based on this posterior probability and we would have the following formula that you see here. On the bottom of this slide, and in this case, we're going to choose the theta that has a large P of Theta I. That means a large cluster and also a high probability of generating D. So we're going to favor a cluster that's large and also consistent with the document.	and if we choose theta base on this posterior probability and we would have the follow formula that you see here . on the bottom of this slide , and in this case , we be go to choose the theta that have a large p of Theta I. that mean a large cluster and also a high probability of generate D. so we be go to favor a cluster that be large and also consistent with the document .
f64adab4-578a-4868-8b2c-03fdd4ddf55d	2020-11-02 23:11:34.309459	186	00:07:56.04	00:08:05.88	And that intuitively makes sense because the chance of a document being a large cluster is generally higher than in a small cluster.	and that intuitively make sense because the chance of a document be a large cluster be generally high than in a small cluster .